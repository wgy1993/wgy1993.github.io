[{"title":"逻辑判断","date":"2020-12-01T07:05:54.000Z","path":"archives/83368142.html","text":"1. 概要逻辑判断主要包括7类题型： 翻译推理 组合排列 真假推理 逻辑论证：削弱、加强 解释原因 日常推理 2. 翻译推理翻译推理是逻辑论证和日常结论的理论基础。 翻译：将逻辑关联词按照翻译规则翻译为①→②的形式（①、②分别代表一句话）。（提别提示：翻译规则和推理规则相当于数学中的“套公式”，熟记即可。避免通过理解语义来解题。） 前推后条件：“如果”可以替换为：假如、一旦、若。“所有”可以替换为：凡是、任何、每一个。 “且”关系：二者同时成立 “或”关系：由”或“连接的所有对象至少有一个成立 “否一推一”：当“否”关系为真时，否定一项可以推出另一项 “有的”同义关联词：有些、一些、某些、有一些…… 3. 组合排列 4. 真假推理 5. 逻辑论证5.1 论证基础知识 论点关键词：因此、所以、表明、说明、认为、显然、意味着、显示出、由此可知、由此推出、据此可知、我们相信、得出结论等。 论据关键词：由于、因为、鉴于、根据、从……推出、理由是、其原因是等。 例子： 论点：公务员是个好职业； 论据：公务员职业稳定； 论证：职业稳定就是好职业。 5.2 削弱论证 5.3 加强论证","tags":[{"name":"公务员","slug":"公务员","permalink":"https://wgy1993.gitee.io/tags/%E5%85%AC%E5%8A%A1%E5%91%98/"},{"name":"判断推理","slug":"判断推理","permalink":"https://wgy1993.gitee.io/tags/%E5%88%A4%E6%96%AD%E6%8E%A8%E7%90%86/"},{"name":"逻辑判断","slug":"逻辑判断","permalink":"https://wgy1993.gitee.io/tags/%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD/"}]},{"title":"定义判断","date":"2020-11-25T09:00:39.000Z","path":"archives/4a892858.html","text":"1. 解题思维 看清提问方式，为“属于/符合”“不属于/不符合”定义等； 识别有效信息，找准关键词、关键句； 逐一分析选项，对比、同构排除 主体：行为、活动的发出者； 客体：行为、活动作用的对象 表示原因的引导词：因为、由于等； 表示条件的引导词：如果、只有、在……情况下、在……作用下、在……时，以及时间（如事前/事后）、地点或其他表示条件的词汇； 表示结果的引导词：所以、因而、从而、带来、引发、导致、使得等表结论的词汇。 方式引导词：以（按照/通过/采用/利用）……的方式（方法/办法/依据/手段）等。 目的引导词：以（达到/实现）……为目的（目标/结果/效果）、为了……等。 补充说明提示词有三种形式。 即、也就是说、意思是、包括、换言之等； 标点符号：括号、破折号、冒号。 分类说明。","tags":[{"name":"公务员","slug":"公务员","permalink":"https://wgy1993.gitee.io/tags/%E5%85%AC%E5%8A%A1%E5%91%98/"},{"name":"判断推理","slug":"判断推理","permalink":"https://wgy1993.gitee.io/tags/%E5%88%A4%E6%96%AD%E6%8E%A8%E7%90%86/"},{"name":"定义判断","slug":"定义判断","permalink":"https://wgy1993.gitee.io/tags/%E5%AE%9A%E4%B9%89%E5%88%A4%E6%96%AD/"}]},{"title":"类比推理","date":"2020-11-25T05:18:48.000Z","path":"archives/112e0f47.html","text":"1. 常见题型 两词型——A：B 三词型——A：B：C 填空型——A对于（）相当于（）对于B","tags":[{"name":"公务员","slug":"公务员","permalink":"https://wgy1993.gitee.io/tags/%E5%85%AC%E5%8A%A1%E5%91%98/"},{"name":"判断推理","slug":"判断推理","permalink":"https://wgy1993.gitee.io/tags/%E5%88%A4%E6%96%AD%E6%8E%A8%E7%90%86/"},{"name":"类比推理","slug":"类比推理","permalink":"https://wgy1993.gitee.io/tags/%E7%B1%BB%E6%AF%94%E6%8E%A8%E7%90%86/"}]},{"title":"图形推理","date":"2020-11-25T05:07:55.000Z","path":"archives/b3af1734.html","text":"1. 概要图形推理一般分为平面推理和空间推理两大类题型。 平面推理规律包括： 位置规律 样式规律 属性规律 数量规律 特殊规律 空间推理规律包括： 空间重构 立体拼合 截面图和三视图 2. 平面推理给出一组或两组图形，要求通过观察分析找出图形排列的规律。 图形构成特点至上 当看到数字时，要立刻告诉自己：这不是数字！这是图形！什么等差、奇偶统统抛到脑后，只看图形构成。 “图形构成特点至上”这一思维，同样可以应用到字母、汉字、星星、月亮等各种图形上，除少数省份会涉及字母的顺序、图形实际含义等考点之外，绝大多数图形推理都需要重点关注图形的构成特点。 2.1 位置规律 2.1.1 平移平移指图形中的某一个或几个元素沿特定方向进行有规律的移动。 在平移题目中，“从头跑”比较普遍，应优先考虑。如果“从头跑”不构成规律，则考虑：“折返跑“（即平移到头后原路返回），其他”变态“规律，如左移1格，右移3格；再左移1格，右移3格……构成周期性变化规律。 宫格方向判定： 九宫格优先横着看。 如果元素只在最外圈出现，优先考虑最外圈按时钟方向平移； 如果平移元素出现在非最外圈位置，优先考虑直线方向平移。 按行看黑块数量一致，优先考虑左右平移； 按列看黑块数量一致，优先考虑上下平移。 秒杀技巧： 看中间黑块的数量：中间颜色数量相同，优先考虑内外圈分开看；中间颜色数量不同，优先考虑直线走。 看哪里空，某行有纯白——按列（上下）走；某列有纯白——按行（左右）走 元素位置互换 当元素组成相同，整体上没有找到上下左右或时钟方向的平移规律，可以用相邻两幅图做对比，观察是否为元素位置互换 2.1.2 旋转和翻转旋转指图形在平面上按特定规律进行转动。 翻转指图形仅通过平面转动无法变化得到，必须要经过类似于“翻书”的动作才能得到。 快速判断旋转、翻转： 左右翻转：左右变、上下不变； 上下翻转：上下变、左右不变； 180°旋转：左右上下全都变。 秒杀技巧： 左右翻左右变，上下翻上下变，180全都变。 “米”字形推理路线 九宫格中间图形特殊（空白面），优先考虑“米”字形推理路线：”米“字形的上下左右以及对角线两端的图形都是由自身旋转180°得到对方的。 时钟法 当小元素比较多，并且呈环形排布（即可以连成一个圈）的时候，可以考虑用”时针法“。但是要注意，选定某种顺序后，所有图形都要按照相同顺序来画时针。 2.2 样式规律 2.2.1 遍历图形特征：元素重复出现 遍历常见考法： 外框的遍历； 内部图案的遍历。 2.2.2 加减同异图形特征：相同线条重复出现 相加：讲两图形中所有的元素（或线条）拼合成一幅新图形，重复的位置保留1次。 相减：当第一幅图的元素（或线条）完全包含第二幅图时，两图相减的结果，就是第一幅图去掉第二幅图所有元素（或线条）之后的图形。 求同：将两图形所有不同的元素（或线条）去掉，只留下相同的部分，形成一幅新图形。 求异：将两图形中所有相同的元素（或线条）去掉，只留下各自不同的部分，形成一幅新图形。 小技巧：从选项挑差异线条入手 保留图形外框，只对内部线条进行求异。（近两年经常出现） 当样式运算特征明显，但单纯考虑加减同异做不出来，考虑样式规律与位置规律结合（平移、旋转、翻转）。 先运算还是先位置？——谁搞特殊，先转谁： 图1与图2更像，先运算最后转； 图1与图3更像，转图2； 图2与图3更像，转图1. 2.2.3 黑白运算图形特征：图形轮廓和分割区域相同，不同区域“黑白”颜色不同，切黑块数量不成规律 运算规则具体题目找，只要将4个公式全部列出，既可以保证不出错 黑+白不一定等于白+黑，要具体题目具体验证。 黑块数量相同，优先平移； 黑块数量不同，优先黑白运算。 黑白运算可优先找大面积相同色块运算 2.3 属性规律 2.3.1 对称性 轴对称： 特征图：“等腰”图形 根据对称轴数量/方向选不出唯一答案，关注对称轴与图形的线、点、面的位置关系 中心对称： 特征图：平行四边形、S、N、Z变形图、相同图形反着放 轴对称+中心对称 特征图：图形存在相互垂直的对称轴 2.3.2 曲直性出现明显的圆、弧等全曲线图，优先考虑曲直性 2.3.3 开闭性图形特征：完整的图形留了小开口，考虑开闭性；生活化、主线条图形。 2.4 数量规律 2.4.1 面数量 面：白色的封闭区域 曲直性：直线构成的面、曲线构成的面、直线和曲线构成的面。 2.4.2 线数量 线：直线、曲线 曲线：出现明显拐点的曲线不算一条 数横/竖线：出现横竖特征图，但数直线无规律，可数横竖线 汉字部首：汉字里面包含的“一“、“丿”数量等 连通图：在不能重复的情况下，能够一笔画成的图形 奇点：发射奇数条线的点，注：端点也是奇点 如果存在多部分，则每部分的笔画数单独算，再相加 “日”：一个面分成两个面 2.4.3 点数量 线与线的交点，端点不是交点 先数笔画再交点 出现数点特征，但整体数点无规律且存在曲直相交，考虑曲直交点 出现数点特征，但整体数点无规律，切图图形都有框架图形（圆居多），考虑框上、框内分开数 2.4.4 素数量 颜色不同算2种 大小不同比例缩放算1种 出现小元素做题思路： 优先考虑元素种类和个数 选不出唯一答案，考虑分开数每一种元素的个数在运算（加减乘除） 部分数：连在一起就是一部分 生活化粗线条图形常见考法： 部分数 面（粗线条内部留白较多） 属性规律（对称、开闭） 2.4.5 角数量 角的个数 直线明显，优先考虑直角 2.5 特殊规律 2.5.1 图形间关系图形特征：题干出现两个或多个封闭图形连在一起 相离：相离 相交：图形间有公共部分 相压：两图形上下覆盖，被压图形有部分线条被覆盖。 2.5.2 功能元素图形特征：每幅图都出现黑点、白点或箭头等小元素 3. 空间推理3.1 空间重构 3.1.1 相对面判断相对面： 同行或同列相隔一个面； “Z”字形两端，且紧靠着“Z”字形中间那条线 3.1.2 相邻面如何确定公共边： 直接相邻的两个面的公共边 平面图中构成直角的两条边是同一条边； 一列/行4个面，两头的两条边是同一条边 相对位置法：如果有自带方向的面、看相对位置解题 如何确定公共点： 相邻三个面的公共点是唯一的 注：公共点发射出的线条的数量或公共点挨着的图案折叠前后不变 画边法： 结合选项，找一个特殊面的唯一点或唯一边 顺/逆时针方向描边标号（描同一个面）","tags":[{"name":"公务员","slug":"公务员","permalink":"https://wgy1993.gitee.io/tags/%E5%85%AC%E5%8A%A1%E5%91%98/"},{"name":"判断推理","slug":"判断推理","permalink":"https://wgy1993.gitee.io/tags/%E5%88%A4%E6%96%AD%E6%8E%A8%E7%90%86/"},{"name":"图形推理","slug":"图形推理","permalink":"https://wgy1993.gitee.io/tags/%E5%9B%BE%E5%BD%A2%E6%8E%A8%E7%90%86/"}]},{"title":"新《公务员法》考点梳理","date":"2020-11-18T11:37:30.000Z","path":"archives/8e85f58f.html","text":"1. 概要 本节课讲解新的《公务员法》，新法自2019年6月1日起施行，如果有些题目或资料和讲解不同，按照老师讲解为准。 授课思路：对于未修改的地方学习怎么考，哪里是出题点：哪里修改哪里是重要考点；面对比较难背的地方用口诀快速记忆。 2. 考点梳理一2.1 公务员定义本法所称公务员，是指依法履行公职、纳入国家行政编制、由国家财政负担工资福利的工作人员。 公务员是干部队伍的重要组成部分，是社会主义事业的中坚力量，是人民的公仆。 【解析】 本法所称公务员，是指依法履行公职、纳入国家行政编制、由国家财政负担工资福利的工作人员。 依法履行公职：公职就是从事公务活动，体现公务员最本质特征。 行政编制：要进行广义理解，并不仅仅指政府工作人员，包括中共、人大、政府、政协、法检等，只要是机关工作人员都属于行政编制范畴。 常见的“坑”：普通党员、国企职工(企业性质)、工勤人员(食堂的厨师、保洁员等，签订劳动合同)不是公务员。 工资福利由财政负担。 “人民公仆”是新增的，老《公务员法》没有这样的表述，公务员是提供服务的，因此是人民的公仆。 2.1.1 试题 1.（2019-420 联考）下列法律法规中，哪一项不是从2019年1月1日起施行？（ ） A.《中华人民共和国电子商务法》 B.《中华人民共和国土壤污染防治法》 C.新修订的《中华人民共和国公务员法》 D.新修订的《中华人民共和国个人所得税法实施条例》 12【解析】1.新修订的《中华人民共和国公务员法》2019年6月1日开始已正式实行。其余选项是2019年1月1日起实行。 【选 C】 2.（2018-天津选调）公务员法对公务员的范围重新做了界定，根据规定，公务员必须具备三个条件，其中体现公务员最本质特征的是（ ）。 A.依法履行公职 B.纳入国家行政编制 C.由国家财政负担工资福利 D.三项均属于 12【解析】2.公务员最本质特征是依法履行公职。【选 A】 3.（2016-北京）根据《中华人民共和国公务员法》 ，公务员是指（ ） 、纳入国家行政编制、由国家财政负担工资福利的工作人员。 A.在国家行政机关工作 B.在党政机关工作 C.行使公共权力 D.依法履行公职 12【解析】 3.三个条件是依法履行公职、 行政编制、 财政负担工资和福利。 【选D】 4.（2011-内蒙古）下列不属于公务员的是（ ） 。 A.某县党委委员 B.某市工商行政管理局干部 C.某国有企业工人 D.某民主党派机关干部 12345【解析】4.选非题。A 项正确：属于机关工作人员。B 项正确：属于机关工作人员。C 项错误：不属于公务员，是企业性质。D 项正确：属于机关工作人员。【选 C】 2.2 公务员的管理原则公务员制度坚持中国共产党领导，坚持以马克思列宁主义、毛泽东思想、邓小平理论、 “三个代表”重要思想、科学发展观、习近平新时代中国特色社会主义思想为指导，贯彻社会主义初级阶段的基本路线，贯彻新时代中国共产党的组织路线，坚持党管干部原则。 公务员的管理，坚持公开、平等、竞争、择优的原则，依照法定的权限、条件、标准和程序进行。 公务员的管理，坚持监督约束与激励保障并重的原则。 公务员的任用，坚持德才兼备、以德为先，坚持五湖四海、任人唯贤，坚持事业为上、公道正派，突出政治标准，注重工作实绩。 公务员就职时应当依照法律规定公开进行宪法宣誓。 【解析】 “坚持党的领导“是新增内容，为了与十九大相关内容相匹配，十九大中表述为“党政军民学，东西南北中，党领导一切” ，因此要坚持党的领导 增加 “科学发展观、 习近平新时代中国特色社会主义思想为指导”：老《公务员法》时还没有这些思想。 “党管干部” ：如果题目表述为我国公务员坚持政治中立（错误），原因：西方某些国家是可以的，但我国是党管干部，要坚决拥护我党的领导。 新增“以德为先”：德才兼备，更强调德的问题。 新增“政治标准”：为了和十九大保持一致。 如果将工作实绩表述为注重工作实效是错误的。 “宪法宣誓”是新增规定，图中是习近平总书记在进行宪法宣誓。 2.3 公务员的条件公务员应当具备下列条件： 具有中华人民共和国国籍； 年满十八周岁； 拥护中华人民共和国宪法，拥护中国共产党领导和社会主义制度； 具有良好的政治素质和道德品行； 具有正常履行职责的身体条件和心理素质； 具有符合职位要求的文化程度和工作能力； 法律规定的其他条件。 【解析】 公务员应当具备下列条件： 具有中华人民共和国国籍：有中国国籍才是中国公民，外国人不能成为中国公务员。 年满十八周岁：成年人。 拥护中华人民共和国宪法，拥护中国共产党领导和社会主义制度：“拥护中国共产党领导和社会主义制度”是新法新增的内容。各个章节体现了加强党的领导，这是最大的特色。 具有良好的政治素质和道德品行： “政治素质”是新法新增的，与十九大保持一致。 具有正常履行职责的身体条件和心理素质： “心理素质” 是新法新增的，当公务员需要有好的心理素质。 具有符合职位要求的文化程度和工作能力：不同岗位有不同要求，比如某些岗位要求本科，某些岗位要求硕士。 法律规定的其他条件：有些岗位特殊的要求，比如某些岗位要求党员，某些岗位要求有基层工作经验。 记忆口诀： 买正品 （对应政治素质和道德品行），写公文（对应工作能力、文化程度），身心健康中国人（对应身体条件和心理素质，中国人）；十八岁（对应年满十八周岁），党领导，社会主义宪法好（对应党的领导、拥护我国宪法）。 2.4 公务员的义务与权利2.4.1 公务员应当履行下列义务 忠于宪法，模范遵守、自觉维护宪法和法律，自觉接受中国共产党领导； 忠于国家，维护国家的安全、荣誉和利益； 忠于人民，全心全意为人民服务，接受人民监督； 忠于职守，勤勉尽责，服从和执行上级依法作出的决定和命令，按照规定的权限和程序履行职责，努力提高工作质量和效率； 保守国家秘密和工作秘密； 带头践行社会主义核心价值观， 坚守法治， 遵守纪律， 恪守职业道德，模范遵守社会公德、家庭美德； 清正廉洁，公道正派； 法律规定的其他义务。 【解析】 公务员应当履行下列义务： 新增：自觉接受中国共产党领导。 新增“带头践行社会主义核心价值观” ：24 个字，富强、民主、文明、和谐、自由、平等、公正、法治、爱国、敬业、诚信、友善。 记忆口诀：爱祖国（对应忠于国家），爱人民（对应忠于人民，为人民服务），法律三德记在心（对应忠于宪法，社会公德、职业道德、家庭美德等）；党领导（对应接受党的领导），价值观（对应带头践行社会主义核心价值观），尽职保密做清官（对应忠于职守、保守国家工作秘密、清正廉洁）。 2.4.2 公务员享有下列权利 获得履行职责应当具有的工作条件； 非因法定事由、非经法定程序，不被免职、降职、辞退或者处分； 获得工资报酬，享受福利、保险待遇； 参加培训； 对机关工作和领导人员提出批评和建议； 提出申诉和控告； 申请辞职； 法律规定的其他权利。 【解析】 公务员享有下列权利： 获得履行职责应当具有的工作条件：比如完成某项任务需要配备工具， 如警察需要配备警车。 非因法定事由、非经法定程序，不被免职、降职、辞退或者处分：“铁饭碗”，企业如果业绩不好或不能带来收益会被开除。 获得工资报酬，享受福利、保险待遇。 参加培训。 对机关工作和领导人员提出批评和建议。 提出申诉和控告。 申请辞职。 法律规定的其他权利。 口诀：干活配铁锹（对应有相应的工作条件），饭碗端的牢（对应铁饭碗）， 拿钱参培（对应工资、报酬、福利等，参加培训）把毛病找（对应给机关领导提 出批评建议）；遇到不平事，申诉与控告（对应申诉、控告），实在不爽就不干了（对应申请辞职）。 2.4.3 试题 1.（2016-广州）根据我国《公务员法》 ，公务员应当履行的义务包括（）。 ①模范遵守宪法和法律 ②获得履行职责应当具有的工作条件 ③清正廉洁，公道正派 ④参加培训 ⑤遵守纪律，恪守职业道德，模范遵守社会公德 A.①④⑤ B.①③⑤ C.②③④⑤ D.①②③④⑤ 123【解析】1.考查义务。口诀是爱祖国，爱人民，法律三德记在心；党领导，价值观，尽职保密做清官。①属于义务， ②属于权利， ③属于义务， ④属于权利，⑤属于义务。 【选 B】 2.（2012-安徽）下列哪项不属于公务员所享受的权利（）。 A.公务员非经所在地人大许可不受公安机关逮捕 B.非因法定事由、法定程序不被免职、降职、辞退或处分 C.小李因个人原因向所在单位申请辞职 D.小王不满上级给予的处分，提出申诉和控告 12345【解析】2.选非题。A 项错误：无此规定，将人大代表的权利进行混淆，县级以上各级代表在开会期间，非经主席团许可不受逮捕或刑事审判；闭会期间非经本级人常许可，不受逮捕或刑事审判。B 项正确：属于“铁饭碗” ，属于权利。C 项正确：属于权利。D 项正确：属于权利。 【选 A】 2.5 考点总结 3. 考点梳理二3.1 公务员的职务、职级与级别3.1.1 职位分类国家实行公务员职位分类制度。公务员职位类别按照公务员职位的性质、特点和管理需要，划分为综合管理类、专业技术类和行政执法类等类别。 国家实行公务员职务与职级并行制度， 根据公务员职位类别和职责设置公务员领导职务、职级序列。 3.1.2 领导职务领导职务层次分为：国家级正职、国家级副职、省部级正职、省部级副职、厅局级正职、 厅局级副职、 县处级正职、 县处级副职、 乡科级正职、 乡科级副职。 【解析】 国家实行公务员职位分类制度，公务员职位类别按照公务员职位的性质、特点和管理需要，划分为综合管理类、专业技术类和行政执法类等类别。 行政执法类：工作在基层管理市场、社会，比如质监、税务、环保。 专业技术类：某些高端技术，比如法医、高级翻译。 综合管理类。 国家实行公务员职务与职级并行制度：新修改的内容，过去分为领导职务和非领导职务，非领导职务由于缺乏上升空间，造成工作没有动力和积极性，为了激发非领导职务工作人员的积极性，改为职级，具有更加通畅的上升空间。 领导职务层次（没有变化）分为国家级正职、国家级副职、省部级正职、省部级副职、 厅局级正职、 厅局级副职、 县处级正职、 县处级副职、 乡科级正职、乡科级副职。 3.1.3 公务员职级公务员职级在厅局级以下设置。 综合管理类公务员职级序列分为：一级巡视员、二级巡视员、一级调研员、二级调研员、三级调研员、四级调研员、一级主任科员、二级主任科员、三级主任科员、四级主任科员、一级科员、二级科员。 3.1.4 级别公务员的领导职务、职级应当对应相应的级别。 公务员的级别根据所任领导职务、 职级及其德才表现、 工作实绩和资历确定。公务员在同一领导职务、职级上，可以按照国家规定晋升级别。 【解析】 公务员职级：从非领导职务改为职级，厅局级以下设置。分为：一级巡视员、二级巡视员、一级调研员、二级调研员、三级调研员、四级调研员、一级主任科员、二级主任科员、三级主任科员、四级主任科员、一级科员、二级科员。过去非领导职务只有八个职级，新公务员法变为12个层级。 无论时领导职务或职级都有对应的级别：国务院总理是1级即正国级，公务员级别分为27级。 3.2 公务员的职务、职级任免公务员领导职务实行选任制、委任制和聘任制。公务员职级实行委任制和聘任制。 领导成员职务按照国家规定实行任期制。 公务员因工作需要在机关外兼职，应当经有关机关批准，并不得领取兼职报酬。 【解析】 公务员领导职务实行选任制（选举）、委任制（组织任命）和聘任制（签合同，新增）。公务员职级（非领导职务）实行委任制和聘任制（新增）。 聘任制：更加灵活，目前用于专业性较强的岗位、辅助性岗位，需要签合同，合同期限是1-5年，试用期1个月-12个月，常规公务员试用期和这里不同。 领导成员职务按照国家规定实行任期制：一般5年一届，不是终身制。 公务员因工作需要在机关外兼职，应当经有关机关批准，并不得领取兼职报酬。经过批准且不领取兼职报酬的情况下可以兼职。 3.2.1 试题 1.2018年12月29日，十三届全国人大常委会第七次会议表决通过了《中华人民共和国公务员法 （修订草案） 》 ， 习近平主席签发第二十号主席令予以公布。公务员法首次修订， 标志着我国公务员管理法治化、 规范化、 科学化进入新阶段。根据新修订公务员法，将非领导职务改造为（）。 A.职系 B.专业技术职称 C.专业技术职务 D.职级 12【解析】1.D 项正确：根据新《公务员法》将非领导职务改造为职级,为了调动非领导职务公务员的工作积极性，有更加高效、畅通的上升空间。 【选 D】 2.（2017-湖北选调生）下列关于我国公务员制度的说法，正确的是（）。 A.职能常任制 B.强调政治中立 C.宗旨是为国家服务 D.实行分类管理 12345【解析】2.A 项错误：领导职务是任期制。B 项错误：党管干部。C 项错误：宗旨是为人民服务。D 项正确：我国公务员制度实行分类管理，分成综合管理、专业技术、行政执法三类。 【选 D】 3.3 公务员的录用录用担任一级主任科员以下及其他相当职级层次的公务员，采取公开考试、严格考察、平等竞争、择优录取的办法。 国家对行政机关中初次从事行政处罚决定审核、行政复议、行政裁决、法律顾问的公务员实行统一法律职业资格考试制度， 由国务院司法行政部门商有关部门组织实施。 【解析】 如考公务员不能考省长和市长，而是人大选举产生。录用担任一级主任科员以下及其他相当职级层次的公务员，采取公开考试、严格考察、平等竞争、择优录取的办法。 新增法考制度：过去是司法考试，后改为法律职业资格考试制度。行政机关中初次从事行政处罚决定审核、行政复议、行政裁决、法律顾问的公务员实行统一法律职业资格考试制度。 下列人员不得录用为公务员： 因犯罪受过刑事处罚的； 被开除中国共产党党籍的； 被开除公职的； 被依法列为失信联合惩戒对象的； 有法律规定不得录用为公务员的其他情形的。 新录用的公务员试用期为一年。试用期满合格的，予以任职；不合格的，取消录用。 【解析】 下列人员不得录用为公务员：重点。 因犯罪受过刑事处罚的：包括主刑、附加刑。受过行政处罚如被行政拘留等可以考公务员，受过刑事处罚不可以考公务员。 被开除中国共产党党籍的（新增）：开除党籍表明受过严重处分。 被开除公职的：如果被辞退则可以再考公务员，开除是处分，而辞退不是处分，定性不同。 被依法列为失信联合惩戒对象的（新增）：我国建立了比较完善的信用体系，追求的目标是一处失信、处处受限，信用是很重要的。如信用记录有污点的人可能没办法乘坐飞机、高铁，不能考取公务员。信用有污点有很多情形，如逃税骗税、 恶意欠薪、 进行传销、 进行非法集资、 不履行法院判决、 逃避兵役等。 有法律规定不得录用为公务员的其他情形的。 记忆口诀：判过刑（对应受过刑事处罚）、被双开（开除党籍、公职）、没信用（被列为失信联合惩戒对象） 新录用的公务员试用期为1年。普通企业签订签劳动合同，试用期最长6个月。 3.3.1 试题 1.（2012-浙江）根据我国公务员制度的相关规定，录用担任（）及其他相当职务层次的非领导职务公务员，采取公开考试、严格考察、平等竞争、择优录取的办法。 A.科员以上 B.主任科员以上 C.科员以下 D.主任科员以下 12【解析】1.D 项正确:之前的表述是主任科员以下,新法变为一级主任科员以下。【选 D】 2.（2010-黑龙江）依照我国《公务员法》 ，下述情况，可录用为公务员的人员是（）。 A.曾受过行政处分的 B.外籍人士 C.曾被开除公职的 D.曾因犯罪受过刑事处罚的 12345【解析】2.A 项正确：受过行政处分可以考公务员。B 项错误：不是中国国籍不能考公务员。C 项错误：开出公职不能考公务员D 项错误：判过刑不能考公务员。 【选 A】 3.4 公务员的考核公务员的考核应当按照管理权限，全面考核公务员的德、能、勤、绩、廉，重点考核政治素质和工作实绩。考核指标根据不同职位类别、不同层级机关分别设置。 公务员的考核分为平时考核、专项考核和定期考核等方式。定期考核以平时考核、专项考核为基础。 定期考核的结果分为优秀、称职、基本称职和不称职四个等次。 【解析】 全面考核公务员的德、能、勤、绩、廉：五个字必须一字不差、多、少、换都是错误的，考试时常将“廉”换成“才”是错误的。 重点考核政治素质（新增）和工作实绩：重点考核政治素质和工作实绩。 公务员的考核分为平时考核（日常考勤、工作检查等）、专项考核（新增，针对某个具体项目）和定期考核（一年进行一次全面考核）等方式。 定期考核的结果分为优秀、称职、基本称职和不称职四个等次。如果改为合格是错误的，事业单位考核是合格，完整为优秀、合格、基本合格和不合格。 定期考核等次后果： 优秀、称职：可以获得年终奖金。 基本称职：无奖金，领导要将进行谈话并要求限期改进。 不称职（常考）：降低一个职务/职级的层次任职，如果连续两年不称职要被辞退。 3.5 考点总结 4. 考点梳理三4.1 公务员的奖励对工作表现突出，有显著成绩和贡献，或者有其他突出事迹的公务员或者公务员集体，给予奖励。奖励坚持定期奖励与及时奖励相结合，精神奖励与物质奖励相结合、以精神奖励为主的原则。 奖励分为：嘉奖、记三等功、记二等功、记一等功、授予称号。 【解析】 奖励坚持定期奖励与及时奖励相结合，精神奖励与物质奖励相结合、以精神奖励为主的原则。精神奖励如颁发证书、奖章等，提升职业荣誉感。 奖励：嘉奖、记三等功、记二等功、记一等功、授予称号（老法为“授予荣誉称号”）。级别递增，嘉奖最低，授予称号最高。 公务员或者公务员集体有下列情形之一的，撤销奖励： 弄虚作假，骗取奖励的； 申报奖励时隐瞒严重错误或者严重违反规定程序的； 有严重违纪违法等行为，影响称号声誉的； 有法律、法规规定应当撤销奖励的其他情形的。 【解析】 公务员或者公务员集体有下列情形之一的，撤销奖励： 弄虚作假，骗取奖励的。 申报奖励时隐瞒严重错误或者严重违反规定程序的。 有严重违纪违法等行为，影响称号声誉的：比如结果进行贪污。 有法律、法规规定应当撤销奖励的其他情形的。 记忆口诀：瞒（隐瞒严重错误）着家（谐音假，弄虚作假）人生育（影响称号声誉）。 4.2 公务员的监督与惩戒机关应当对公务员的思想政治、履行职责、作风表现、遵纪守法等情况进行监督，开展勤政廉政教育，建立日常管理监督制度。对公务员监督发现问题的，应当区分不同情况，予以谈话提醒、批评教育、责令检查、诫勉、组织调整、处分。对公务员涉嫌职务违法和职务犯罪的，应当依法移送监察机关处理。 公务员执行公务时，认为上级的决定或者命令有错误的，可以向上级提出改正或者撤销该决定或者命令的意见；上级不改变该决定或者命令，或者要求立即执行的，公务员应当执行该决定或者命令，执行的后果由上级负责，公务员不承担责任；但是，公务员执行明显违法的决定或者命令的，应当依法承担相应的责任。 【解析】 对公务员监督发现问题的， 应当区分不同情况， 予以谈话提醒、 批评教育、责令检查、诫勉、组织调整、处分。 公务员执行公务时，认为上级的决定或者命令有错误的，可以向上级提出改正或者撤销该决定或者命令的意见；上级不改变该决定或者命令，或者要求立即执行的，公务员应当执行该决定或者命令，执行的后果由上级负责，否则公务员无法开展工作，如果上级安排的工作甲乙都认为有问题而不执行，由于每个人意见不同导致无法完成工作。但是，公务员执行明显违法的决定或者命令的，应当依法承担相应的责任。如局长让小王晚上杀人，杀人这一行为明显违法，小王杀人后由局长承担责任是错误的，小王要承担相应的责任。 处分分为：警告、记过、记大过、降级、撤职、开除。 公务员在受处分期间不得晋升职务、职级和级别，其中受记过、记大过、降级、撤职处分的，不得晋升工资档次。 受处分的期间为：警告，六个月；记过，十二个月；记大过，十八个月；降级、撤职，二十四个月。 【解析】 处分：警告、记过、记大过、降级、撤职、开除。 注意没有“降职” ，处分是“降级、撤职”，如考核不称职要降职任用，但不是处分。 开除：被追究刑事责任是必须开除，如醉酒驾车被抓获并定危险驾驶罪，要被开除。 公务员在受处分期间不得晋升职务、职级和级别，其中受记过、记大过、降级、撤职处分的，不得晋升工资档次。注意：警告处分可以涨工资，其他处分不得涨工资。 受处分的期间为：警告，6 个月；记过，12个月；记大过，18个月；降级、撤职，24个月。 公务员受开除以外的处分，在受处分期间有悔改表现，并且没有再发生违纪违法行为的，处分期满后自动解除。 解除处分后， 晋升工资档次、 级别和职务、 职级不再受原处分的影响。 但是，解除降级、撤职处分的，不视为恢复原级别、原职务、原职级。 【解析】 公务员受开除以外的处分，在受处分期间有悔改表现，并且没有再发生违纪违法行为的，处分期满后自动解除：之前需要处分决定机关解除并书面通知，为了防止关系不好“穿小鞋” ，如果处分期满不通知会引起不必要的麻烦。 解除处分后，晋升工资档次、级别和职务、职级不再受原处分的影响。犯错后也有上升空间。 解除降级、撤职处分的，不视为恢复原级别、原职务、原职级。比如甲原来是 10 级，被降到 15 级，降级期限是24个月，在 24 个月之后解除，此时仍然是 15 级，但是之后表现好可以再次上升，晋升不受原处分影响。 4.2.1 试题 1.（2019-河北选调） 《公务员法》规定，公务员定期考核的结果分为优秀、称职、基本称职和不称职四个等次。在年度考核中被确定为不称职的，按照规定程序（ ） 。 A.降低一个职务或者职级层次任职 B.降级使用 C.降低一级工资或者降低一级职务 D.予以辞退 12【解析】1.不称职要降低职务或职级层次任职 【选 A】 2.（2019-河北选调）2018年12月修订的《中华人民共和国公务员法》规定，公务员的考核应当接照管理权限，全面考核公务员的德、能、勤、绩、廉，重点考核（）。考核分为平时考核、专项考核和定期考核等方式。定期考核以平时考核、专项考核为基础。 A.政治素质和工作能力 B.工作能力和工作态度 C.工作态度和工作实绩 D.政治素质和工作实绩 12【解析】2.重点考核政治素质和工作实绩，政治素质是新增内容。 【选 D】 3.（2010-浙江 A 类） 以下不属于 《公务员法》 中对公务员处分的种类是（）。 A.警告 B.记大过 C.开除 D.降职 12【解析】3.D 项正确：降职是一种任用方式，不是处分，降级和撤职是处分。【选 D】 4.3 公务员的培训机关根据公务员工作职责的要求和提高公务员素质的需要， 对公务员进行分类分级培训。 国家建立专门的公务员培训机构。 机关根据需要也可以委托其他培训机构承担公务员培训任务。 培训包括：初任培训、任职培训、专门业务培训、在职培训。 【解析】 专门的公务员培训机构：国家行政学院、地方行政学院。机关根据需要也可以委托其他培训机构（一般委托高校、科研单位等）承担公务员培训任务。 培训包括：初任培训、任职培训、专门业务培训、在职培训。 初任培训：对新录用的公务员进行初任培训。 任职培训：对晋升领导职务的公务员进行培训。 专门业务培训：对从事专项工作的公务员培训，如特殊的专业知识。 在职培训：针对全体公务员，提升政治素质、工作能力等。 4.4 公务员的交流国家实行公务员交流制度。 公务员可以在公务员和参照本法管理的工作人员队伍内部交流， 也可以与国有企业和不参照本法管理的事业单位中从事公务的人员交流。 交流的方式包括调任、转任。 【解析】 公务员可以在公务员和参照本法管理的工作人员（参公管理）队伍内部交流，也可以与国有企业和不参照本法管理的事业单位中从事公务的人员交流。 交流的方式包括调任、转任。调任是从机关外调入机关内，而转任是内部转任，可以跨地区、跨部门，新法删除挂职锻炼，意味着如果按照新法交流方式只有两种。 国有企业、 高等院校和科研院所以及其他不参照本法管理的事业单位中从事公务的人员,可以调入机关担任领导职务或者四级调研员以上及其他相当层次的职级。 根据工作需要， 机关可以采取挂职方式选派公务员承担重大工程、 重大项目、重点任务或者其他专项工作。公务员在挂职期间，不改变与原机关的人事关系。 【解析】 挂职锻炼在新法中变为挂职，机关可以采取挂职方式选派公务员承担重大工程、重大项目、重点任务或者其他专项工作。挂职目的发生变化，从过去的培养干部转换为承担专项任务和专项工作。公务员在挂职期间，不改变与原机关的人事关系，人事档案还在原机关。 国有企业、 高等院校和科研院所以及其他不参照本法管理的事业单位中从事公务的人员,可以调入机关担任领导职务或者四级调研员以上及其他相当层次的职级。常见的是国企老总调入机关成为领导。 4.5 公务员的回避公务员之间有夫妻关系、直系血亲关系、三代以内旁系血亲关系以及近姻亲关系的， 不得在同一机关双方直接隶属于同一领导人员的职位或者有直接上下级领导关系的职位工作，也不得在其中一方担任领导职务的机关从事组织、人事、纪检、监察、审计和财务工作。 公务员不得在其配偶、子女及其配偶经营的企业、营利性组织的行业监管或者主管部门担任领导成员。 【解析】 夫妻关系、直系血亲关系、三代以内旁系血亲关系以及近姻亲关系。 不得在同一机关双方直接隶属于同一领导人员的职位，如甲和乙是夫妻，在同一个科室上班，有一个共同的科长是不可以的。 有直接上下级领导关系的职位工作，如丈夫是科长，妻子是丈夫手下的科员，有直接上下级领导关系无法开展工作。 不得在其中一方担任领导职务的机关从事组织、人事、纪检、监察、审计和财务工作，如丈夫是局长，妻子是同一单位的会计，可能钱会被贪污。 新增：公务员不得在其配偶、子女及其配偶经营的企业、营利性组织的行业监管或者主管部门担任领导成员，防止企业将该行业垄断，如张三的儿子开办石油企业，可能石油行业被垄断。 公务员担任乡级机关、县级机关、设区的市级机关及其有关部门主要领导职务的，应当按照有关规定实行地域回避。 公务员执行公务时，有下列情形之一的，应当回避： 涉及本人利害关系的； 涉及与本人有亲属关系人员的利害关系的； 其他可能影响公正执行公务的。 【解析】 地域回避：乡级机关、县级机关、设区的市级机关及其有关部门主要领导职务，主要领导不能原籍任职，防止成为“土皇帝” ，如果甲在本县当县长，可能会安排七大姑、八大姨到机关工作。 新法将设区的市加入，乡级、县级、设区的市一级不能原籍任职。 公务回避：可能导致徇私枉法。 涉及本人利害关系的。 涉及与本人有亲属关系人员的利害关系的。 其他可能影响公正执行公务的。 4.5.1 试题 1.（2018-广州单区）以下情形，不违反《中华人民共和国公务员法》规定的任职回避情形的是（）。 A.甲乙为夫妻，在同一机关工作，甲担任领导职务，乙在组织部门任普通科员 B.甲乙为表兄妹，在同一机关工作，甲担任领导职务，乙是甲的下级由甲直接领导 C.甲乙为兄妹，在同一机关工作，甲在组织部门担任部长，乙在行政部门担任普通科员 D.甲乙为父子，在同一机关工作，两人都在人事部门任普通科员 12345【解析】1.考查任职回避A 项错误：不得在其中一方担任领导职务的机关从事组织、人事、纪检、监察、审计和财务工作，甲是领导，而其妻子在组织部门是不可以的，需要任职回避。B 项错误：有直接的上下级关系需要任职回避。C 项正确：两个部门没有关系，不需要任职回避。D 项错误：父子在同一部门有共同领导，需要任职回避。【选 C】 2.（2018-北京） 根据 《中华人民共和国公务员法》 ， 下列说法中正确的是（） A.新录用的公务员，试用期为六个月 B.公务员解除降级、撤职处分的，不视为恢复原级别、原职务 C.公务员在挂职锻炼期间，人事关系转到挂职的单位 D.曾被开除公职的人员，可以被再次录用为公务员 1234【解析】2.A 项错误：公务员试用期为 1 年。C 项错误：挂职&#x2F;挂职锻炼时，人事关系不转移。D 项错误：被开除的不能再次录用。【选 B】 4.5 考点总结 5. 考点梳理四5.1 公务员的工资、福利与保险公务员实行国家统一规定的工资制度。 国家建立公务员工资的正常增长机制。 公务员工资包括基本工资、津贴、补贴和奖金。 公务员按照国家规定享受福利待遇。 国家根据经济社会发展水平提高公务员的福利待遇。公务员执行国家规定的工时制度，按照国家规定享受休假。公务员在法定工作日之外加班的，应当给予相应的补休，不能补休的按照国家规定给予补助。 公务员依法参加社会保险，按照国家规定享受保险待遇。公务员因公牺牲或者病故的，其亲属享受国家规定的抚恤和优待。 【解析】 建立公务员工资的正常增长机制，根据国家的经济状况和财力上涨工资。 常考：工资分为基本工资、津贴、补贴和奖金。 参加社会保险如五险一金、职业年金，福利待遇，因公牺牲或病故后家属有抚恤和优待等。 5.2 公务员的辞职与辞退5.2.1 辞职公务员辞去公职，应当向任免机关提出书面申请。任免机关应当自接到申请之日起三十日内予以审批，其中对领导成员辞去公职的申请，应当自接到申请之日起九十日内予以审批。 公务员有下列情形之一的，不得辞去公职： 未满国家规定的最低服务年限的； 在涉及国家秘密等特殊职位任职或者离开上述职位不满国家规定的脱密期限的； 重要公务尚未处理完毕，且须由本人继续处理的； 正在接受审计、纪律审查、监察调查，或者涉嫌犯罪，司法程序尚未终结的； 法律、行政法规规定的其他不得辞去公职的情形。 【解析】 公务员辞去公职，应当向任免机关提出书面申请。任免机关应当自接到申请之日起三十日内予以审批，其中对领导成员辞去公职的申请，应当自接到申请之日起九十日内予以审批。 不能辞去公职（重点） 。 未满国家规定的最低服务年限的。如甲签订最低服务期5年的合同，如果辞职要承担高额违约金。 在涉及国家秘密等特殊职位任职或者离开上述职位不满国家规定的脱密期限的。 重要公务尚未处理完毕，且须由本人继续处理的。 正在接受审计、纪律审查、监察调查，或者涉嫌犯罪，司法程序尚未终结的。 法律、行政法规规定的其他不得辞去公职的情形。 口诀：签了卖身契（对应最低服务年限），掌握大秘密（对应涉密岗位不满国家规定的脱密期限），大活待处理（对应重要公务尚未处理完毕），有人调查你（对应接受审计、纪律审查、监察调查，或者涉嫌犯罪，司法程序尚未终结）。 5.2.2 引咎辞职与责令辞职领导成员因工作严重失误、失职造成重大损失或者恶劣社会影响的，或者对重大事故负有领导责任的，应当引咎辞去领导职务。 领导成员因其他原因不再适合担任现任领导职务的， 或者应当引咎辞职本人不提出辞职的，应当责令其辞去领导职务。 【解析】 引咎辞职与责令辞职：失职造成重大损失或者恶劣社会影响的，或者对重大事故负有领导责任的，应当引咎辞去领导职务；如果应当引咎辞职但本人并不提出要责令辞去领导职务，一个主动、一个被动 辞职包括四类，分别是因公辞职、自愿辞职、引咎辞职和责令辞职。因公辞职是工作需要， 工作调动要走一个辞旧履新的程序；自愿辞职如想要下海精神；引咎辞职和责令辞职是针对领导，领导主动提出是引咎辞职，被动就是责令辞职 5.2.3 辞退公务员有下列情形之一的，予以辞退： 在年度考核中，连续两年被确定为不称职的； 不胜任现职工作，又不接受其他安排的； 因所在机关调整、撤销、合并或者缩减编制员额需要调整工作，本人拒绝合理安排的； 不履行公务员义务，不遵守法律和公务员纪律，经教育仍无转变，不适合继续在机关工作，又不宜给予开除处分的； 旷工或者因公外出、请假期满无正当理由逾期不归连续超过十五天，或者一年内累计超过三十天的。 【解析】 辞职和辞退的区别：辞职是自己提出，而辞退是单位主动提出。 辞退的情形： 在年度考核中，连续两年被确定为不称职，如第一年不称职、第二年称职、第三年不称职不能辞退。 不胜任现职工作，又不接受其他安排的。 因所在机关调整、撤销、合并或者缩减编制员额需要调整工作，本人拒绝合理安排的。 不履行公务员义务，不遵守法律和公务员纪律，经教育仍无转变，不适合继续在机关工作，又不宜给予开除处分的。比如《人民的名义》中的孙连成不作为，但不贪污、不受贿，就是不履行公务员义务，给予辞退。 旷工或者因公外出、请假期满无正当理由逾期不归连续超过十五天，或者一年内累计超过三十天的。旷工是没有请假。连续是15天，累计是30天。 记忆口诀：连续两年打领导（连续两年不称职），啥活我都干不了（不胜任现职工作，又不接受其他安排，调整了也不服从）；目无法纪不干活（不遵守法律、纪律，不履行公务员义务），十五三十往外跑（超过十五天，或者一年内累计超过三十天）。 5.2.4 不得辞退对有下列情形之一的公务员，不得辞退： 因公致残，被确认丧失或者部分丧失工作能力的； 患病或者负伤，在规定的医疗期内的； 女性公务员在孕期、产假、哺乳期内的； 法律、行政法规规定的其他不得辞退的情形。 【解析】 不能辞退：为了保护某些特定公务员。 因公致残，被确认丧失或者部分丧失工作能力的：如甲为了国家而残疾不得被辞退，非因公致残可以辞退。 患病或者负伤，在规定的医疗期内的：仅限于医疗期内，如果医疗期满可以辞退。 女性公务员在孕期、产假、哺乳期内的，为了保护女星。 法律、行政法规规定的其他不得辞退的情形。 口诀：因公致残，看病生娃。 5.3 公务员的退休公务员达到国家规定的退休年龄或者完全丧失工作能力的，应当退休。 公务员符合下列条件之一的，本人自愿提出申请，经任免机关批准，可以提前退休： 工作年限满三十年的； 距国家规定的退休年龄不足五年，且工作年限满二十年的； 符合国家规定的可以提前退休的其他情形的。 【解析】 退休年龄，男60周岁，女55周岁，以后延迟：男65周岁，女60周岁。或完全丧失工作能力，如因病或因意外事故。 提前退休： 工作年限满三十年的。 距国家规定的退休年龄不足五年，且工作年限满二十年的。如某女姓55岁退休，31岁考上公务员，干了20年后为51岁，距离55岁差4年，此时可以申请提前退休。 符合国家规定的可以提前退休的其他情形的。 5.4 公务员的申诉与控告公务员对涉及本人的下列人事处理不服的， 可以自知道该人事处理之日起三十日内向原处理机关申请复核；对复核结果不服的，可以自接到复核决定之日起十五日内， 按照规定向同级公务员主管部门或者作出该人事处理的机关的上一级机关提出申诉；也可以不经复核，自知道该人事处理之日起三十日内直接提出申诉： 处分； 辞退或者取消录用； 降职； 定期考核定为不称职； 免职； 申请辞职、提前退休未予批准； 不按照规定确定或者扣减工资、福利、保险待遇； 法律、法规规定可以申诉的其他情形。 【解析】 对人事问题不服可以找原处理机关申请复核， 同级公务员主管部门或者作出该人事处理的机关的上一级机关提出申诉，不可以到法院提起诉讼，因为属于内部的行为。 对省级以下机关作出的申诉处理决定不服的， 可以向作出处理决定的上一级机关提出再申诉。 受理公务员申诉的机关应当组成公务员申诉公正委员会， 负责受理和审理公务员的申诉案件。 复核、申诉期间不停止人事处理的执行。 公务员不因申请复核、提出申诉而被加重处理。 公务员认为机关及其领导人员侵犯其合法权益的， 可以依法向上级机关或者监察机关提出控告。受理控告的机关应当按照规定及时处理。 【解析】 对省级以下机关作出的申诉处理决定不服，可以再申诉 总结： 复核：找原机关，谁做决定找谁。 申诉：找同级的主管部门或者决定机关的上一级机关。 再申诉，谁做出的申诉决定找作出申诉决定的上一级。 受理公务员申诉的机关应当组成公务员申诉公正委员会， 负责受理和审理公务员的申诉案件。 （新增，为了更好的解决争议） 复核、申诉期间不停止人事处理的执行。复核、申诉成功则停止人事处理执行。 公务员不因申请复核、提出申诉而被加重处理。如原本要降一级，不服后提出复核，改为降两级是不可以的，为了打消申请复核和申诉的顾虑。 控告：公务员认为机关及其领导人员侵犯其合法权益的，可以依法向上级机关或者监察机关提出控告。受理控告的机关应当按照规定及时处理。侵犯合法权益既包括侵犯公务的合法权益，也包括侵犯个人的合法权益。 公务，公务员执行公务有执行保障权，如警察抓持枪匪徒，别人都配备防弹衣，但是甲和领导关系不好，领导不给甲防弹衣，此时侵犯合法权益，甲可以提出控告。 个人：如政治权利，领导不让某人参加投票，此时可以提出控告。 5.5 任职限制公务员辞去公职或者退休的，原系领导成员、县处级以上领导职务的公务员在离职三年内，其他公务员在离职两年内，不得到与原工作业务直接相关的企业或者其他营利性组织任职，不得从事与原工作业务直接相关的营利性活动。 【解析】 领导成员、县处级以上领导职务的公务员在离职三年内，其他公务员在离职两年内，不得到与原工作业务直接相关的企业或者其他营利性组织任职，不得从事与原工作业务直接相关的营利性活动。防止以权谋私，可能会利用过去的权力谋私利，注意区别时间限制。 如某法官在离任以后2年内不能以律师身份担任诉讼代理人（营利），属于竞业限制。 5.6 试题 1.（2019-北京）公务员王某准备辞去公职，下列说法中错误的是（） A.王某辞去公职后，不再具有公务员身份 B.王某辞去公职的申请，应交由任免机关审批 C.王某辞去公职后，可以自由选择企业或社会组织任职 D.王某离职前，应当办理公务交接手续 12【解析】 1.选非题。C 项错误： 领导3年限制，或普通人2年的限制。【选 C】 2.（2018-山东选调） 某公安分局发生了以下事实， 公务员甲2015年和2017年两年被确定为不称职，公务员乙2017年10月怀孕、不能胜任现职，公务员丙2017年累计旷工 35 天，公务员丁在执行公务时受伤致残、无法胜任现职。2018年1月公安局可以对（）作辞退处理。 A.甲 B.乙 C.丙 D.丁 12345【解析】2.A 项错误：不符合连续两年的要求。B 项错误：怀孕期间不能辞退。C 项正确：连续15日累计30日可以辞退。D 项错误：因公致残不可以辞退。【选 C】 3.（2018 下半年-全国事业单位联考-职测）下列说法不符合我国相关法律法规的是（）。 A.某财政局处长甲正在接受纪律审查，所以不得辞去公职 B.某税务局科长乙因连续两年年度考核不称职而被辞退 C.某国安局科员丙在涉密岗位工作一年后辞职 D.某分管工业的副县长丁对重大生产事故负有领导责任而引咎辞职 12345【解析】3.选非题。A 项正确：表述正确。B 项正确：连续两年不称职可以被辞退。C 项错误：掌握秘密不能辞职。D 项正确：负有责任可以提出引咎辞职。【选 C】 4.（2016-国考地市级）下列处理问题的做法不符合我国相关法律法规的是（） A.某公安局处长对本人被辞退的决定不服，当天即向法院提起行政诉讼 B.某工商局科员拒不赡养父母，情节严重，单位给予其开除处分 C.某省商务厅厅长退休后第二年到商务厅下属商贸公司任职， 该省公务员主管部门责令其限期改正 D.某税务局科长执行公务时，认为局长的决定有错误，向局长提出改正意见 12345【解析】4.选非题。A 项错误：内部人事问题不能起诉。B 项正确：拒不赡养父母情节严重构成遗弃罪，可以被开除。C 项正确：厅长违背了3年内不能到相关企业进行营利性活动的规定，可以责令其改正。D 项正确：有问题可以提出意见，如果局长说要求执行则必须执行。 【选 A】 5.（2014-412 联考）关于我国公务员制度，下列叙述正确的是（） A.正在接受纪律审查的公务员不得辞去公职 B.我国公务员培训基地是党校 C.公务员对降职不服，可以向人民法院提出申诉 D.公务员考核的基本内容包括德、能、勤、绩、才 12345【解析】5.A 项正确:有人调查你，不能辞去公职。B 项错误：培训基地为行政学院， 党校是对党员干部进行培训。C 项错误： 不能到法院申诉。D 项错误：应是德、能、勤、绩、廉。 【选 A】 6.（2013-浙江省考A类）下列关于我国公务员制度的表述， 不正确的是（） A.根据《公务员法》 ，我国公务员职位类别划分为综合管理类、专业技术类和行政执法类等类别 B.已被开除的公职人员，不得录用为公务员 C.工作年满三十年的公务员，可以申请提前退休 D.对公务员考核的内容包括德、能、勤、绩、廉五个方面，重点考核思想道德与工作能力 1234【解析】6.选非题。B 项正确：被开除不得录用。C 项正确：符合申请提前退休的条件。D 项错误：根据新法重点考核政治素质、工作实绩。【选 D】 5.7 考点总结","tags":[{"name":"公务员","slug":"公务员","permalink":"https://wgy1993.gitee.io/tags/%E5%85%AC%E5%8A%A1%E5%91%98/"},{"name":"常识","slug":"常识","permalink":"https://wgy1993.gitee.io/tags/%E5%B8%B8%E8%AF%86/"},{"name":"新《公务员法》考点梳理","slug":"新《公务员法》考点梳理","permalink":"https://wgy1993.gitee.io/tags/%E6%96%B0%E3%80%8A%E5%85%AC%E5%8A%A1%E5%91%98%E6%B3%95%E3%80%8B%E8%80%83%E7%82%B9%E6%A2%B3%E7%90%86/"}]},{"title":"航天史和古代天文历法","date":"2020-11-18T05:44:51.000Z","path":"archives/50286772.html","text":"1. 概要这节课学习航天史和古代天文历法，共有两部分内容。考试的时候两部分内容都有涉及，但古代天文历法考查更多。 2. 航空航天这部分从国内、国际两个维度来讲解，同时归纳一些重点，大家只需要掌握重点即可。航天史是一部很长的历史，有很多的事件，只要掌握考试考的高频内容即可。 2.1 国际航空航天发展史 第一个进入宇宙的人：加加林； 第一个太空行走的人：列昂洛夫； 第一个登上月球的人：阿姆斯特朗； 第一个发射升空的人造卫星的国家：前苏联； 全球四大导航系统： GPS 系统； 北斗系统； GLONASS 系统； 伽利略卫星导航系统。 【解析】 第一个进入宇宙的人：加加林（前苏联） 。进入宇宙是很多人的梦想，中国古代有很多人想上天，由于当时的条件限制没有达成。1961 年 4 月 12 日，加加林乘坐东方一号宇宙飞船从拜科鲁尔发射场起飞，最终到达太空，实现了人类进入太空的愿望，东方一号飞船也成为第一个载人进入外层空间的航天器。后来加加林在 1968 年因为飞机失事遇难。 第一个太空行走的人：列昂洛夫（前苏联） 。1965 年，列昂洛夫和他的伙伴乘坐上升 2 号宇宙飞船进入太空，飞行过程中，列昂洛夫在舱外停留了 20 多分钟时间，本来预计停留时间还要长，但因为他的宇航服压力过大，使得他很难返回上升 2 号舱口，最后还是回去了。 第一个登上月球的人：阿姆斯特朗。考试会考人名和对应的事迹，注意即可。 阿姆斯特朗在 1969 年乘坐阿波罗 11 号宇宙飞船进入太空，最终踏上月球的土地。他的名言：这是个人的一小步，但是人类迈出的一大步。 第一个发射升空的人造卫星的国家：前苏联。前苏联曾经为了争夺世界霸权，和美国展开了各个维度的竞争，包括军备竞赛、空间竞争。1957 年，苏联研发出斯普特尼克一号人造卫星， 直径 58 厘米， 重 83.6 千克， 有四个鞭状天线，有科学仪器， 升空后发射了三个星期左右的信号， 后来在轨道中度过了 3 个多月，再后来就不见了。中国发射的第一颗人造卫星东方红一号在太空中失联后，直到现在还在太空中飞行，2020 年我国捕获了东方红一号卫星。 全球四大导航系统：考试会问哪些属于导航系统，哪些不属于。如墨子号卫星、风云气象卫星，不是定位卫星。 GPS 系统：是美国全球卫星定位系统的简称，始于 1958 年美国军方开始的一个定位项目， 1964 年投入使用。 最开始的目的是为了给陆海军提供实时、全天候的全球导航服务，到现在在民用方面也有重要用途。如车载的导航卫星、导航系统、手机等都会用到 GPS。 北斗系统：中国。1994 年开始建，分成北斗一号、北斗二号、北斗三号系统， 北斗系统整个由 55 颗卫星组成， 北斗三号有 30 颗， 还有 5 颗实验卫星。2020 年 7 月 31 日，习近平总书记向全世界宣布，北斗全球卫星导航系统正式建成运营，开始向 120 多个国家和地区输出导航服务。 GLONASS 系统：是俄罗斯继承前苏联的研究成果，俄罗斯自 1993 年开始独自在原先苏联技术的基础之上，开始建立俄罗斯的导航系统。2007 年开始运营，2009 年拓展到全球服务范围。 伽利略卫星导航系统： 中国曾经是伽利略卫星导航系统的研发成员国，付了 2 亿多欧元后，请求分享原子钟相关的技术，没有得到同意后退出。2019年 7 月份，伽利略导航系统因为技术故障导致服务中断，8 月份才恢复。 2.2 中国航空航天发展史 【解析】 考试会考重要航空航天器的名称和标志性内容之间的对应关系。如 1970年 4 月 24 日，东方红一号，我国第一颗自主建造的人造卫星飞上天空，4 月 24日也是我国的航天日。 2003 年 10 月发射神舟 5 号，是我国首次发射的载人航天飞行器，杨利伟成为我国第一个进入太空的人。 2016 年 10 月发射了神舟 11 号，自从 2016 年以后，我国再没有发射过载人飞船，神舟 11 号是截至当时持续时间最长的一次载人飞行，连同返回共有 33天。 天宫 2 号于 2016 年 9 月发射，是我国自主研发的第二个空间实验室。2016-2017 年的考题中曾经表述为“空间站” ，两者是有区别的，空间实验室是前期比较基础的、 比较小的， 空间站对接口多、 舱位多， 能容纳人活动的时间长。我国将在未来成为唯一一个独立拥有空间站的国家。 考试中如果出现空间站也不能选错，因为以前的公务员考试题目中曾经出现过，而且选项是正确的。 嫦娥 4 号 2018 年发射升空，上面携带着玉兔 2 号月球车，着陆在月球表面之后，把玉兔 2 号放出来，玉兔 2 号借助太阳能发电进行行走。嫦娥 4 号成功实现了人类首次月背软着陆，月球永远一面向着地球，另一面是不知道的，月背软着陆能够对月背进行详细的了解。 最早的长征 5 号于 2016 年发射， 是我国最大推力的新一代运载火箭。 2019年我国发射了长征五号遥三，2020 年发射了长征五号遥四火箭。 长征 7 号的作用是运送货运飞船，我国的货运飞船是天舟系列。注意：神州系列是送人的，天舟系列是送货的。 长征 11 号是海上发射的运载火箭，我国 2019 年、2020 年都在海上进行了发射，首次海上发射是 2019 年。海上发射成功后我们可以把发射台拖到赤道附近，可以用更小的推力送向更高的轨道。 天问 1 号是我国天问系列行星探测任务中的第一颗探测器， 主要任务是探测火星。太阳系有八大行星：水星、金星、地球、火星、木星、土星、天王星、海王星。 神州系列是我国的载人飞船，天宫系列是空间实验室，嫦娥系列是登月探测器， 长征系列是运载火箭， 东风系列是我国的导弹， 北斗卫星是用于导航的，风云系列卫星用于气象观测。我国重要的航天发射场：海南文昌、四川西昌、甘肃酒泉、山西太原。 2.3 总结 【解析】 国际航空航天发展史：加加林进入太空，列昂洛夫太空行走，阿姆斯特朗上月球， 斯普特尼克一号人造卫星， 四大导航系统： 美国 GPS， 俄罗斯格洛纳斯，欧盟伽利略，中国北斗。 中国航空航天发展史： 东方红一号——第一颗人造卫星上天。神舟 5 号——首个载人飞船。神舟 11 号——当时持续时间最长。天宫 2 号——第一个真正意义上的空间实验室。嫦娥 4 号——携带玉兔 2 号月球车，在月背软着陆。长征五号——推力最大。长征七号——搭载货运飞船。长征 11 号——海上发射。天问 1 号——我国首个火星探测器。 2.4 试题 【例 1】 （2017 江西-省考）2016 年 11 月 3 日 20 时 43 分，从中国文昌航天发射场点火升空， 完成首次发射任务并取得圆满成功的我国最大推力新一代运载火箭是（ ） 。 A．长征七号 B．长征六号 C．长征五号 D．长征八号 1【解析】1.【选 C】 【例 2】 （2016 下半年联考-事业单位） 关于航天科技， 下列表述错误的是 （ ） 。 A．1969 年，苏联宇航员加加林在月球上留下人类第一个脚印 B．中国嫦娥三号探月器上搭载的月球车名为“玉兔” C．中国第一位进入太空的宇航员是杨利伟 D．目前，中国已有女宇航员进入太空 123【解析】2.选非题。A 项错误：应该是美国宇航员阿姆斯特朗。D 项正确：刘洋、王亚平。 【选 A】 【例 3】 （2017 国考）下列关于航天器的说法正确的是（ ） 。 A． “风云”系列气象卫星通过光纤实现与地面的数据传输 B． “玉兔”号月球车在月球上行走的动力驱动是电动车 C． “长征一号”属于二级运载火箭 D． “北斗二号”属于通信广播卫星 1234【解析】3.A 项错误：光纤是实体，不能在卫星后面拖一条长长的线。C 项错误：属于三级运载火箭。D 项错误：属于导航卫星。 【选 B】 【例 4】 （2011 国考）下列关于人类航天史的说法，正确的是（ ） 。 A．成功将世界上第一颗人造地球卫星送入太空的是美国 B．前苏联宇航员加加林是世界上第一个进行太空行走的人 C．首次实现登月的载人飞船是“阿波罗 13 号” D．载人飞船首次在地球轨道上实现交会和对接是在 20 世纪 60 年代 1234【解析】4.A 项错误：是苏联。B 项错误：是列昂洛夫。C 项错误：是阿波罗 11 号。 【选 D】 【例 5】 （2015 政法干警）马航 MH370 航班失踪后，人们开始重新考虑在客机上强制安装全球导航系统的可行性和必要性。 下列哪一项不能提供这样的全球导航服务（ ） 。 A．全球定位系统 B．北斗卫星导航定位系统 C．欧盟的“伽利略”计划 D．阿波罗——联盟测试系统 1【解析】5.选非题。 【选 D】 【例 6】 （2016 省考-多省联考）洲际导弹通常指射程大于 8000 公里的远程弹道式导弹。目前，中国研制的洲际弹道导弹主要是什么系列的（ ） 。 A． “东风”系列 B． “长征”系列 C． “红旗”系列 D． “天宫”系列 123【解析】6.B 项错误：长征系列是运载火箭。D 项错误：天宫系列是空间实验室。 【选 A】 【例 7】 （2014 省考-山东） 我国已有或正在建设的航天发射场不包括 （ ） 。 A．文昌 B．西安 C．酒泉 D．太原 1【解析】7.选非题。 【选 B】 【例 8】 （2013 国考）下列卫星系列不属于我国对地观测卫星的是（ ） 。 A． “海洋” B． “风云” C． “天绘” D． “北斗” 12【解析】 8.选非题。 D 项错误： 北斗卫星是导航卫星， 不是对地观测。 【选 D】 【例 9】 （2019 多省联考）2019 年 1 月 3 日上午 10 时 26 分， （ ）探测器成功在月球背面着陆，此次任务实现了人类探测器首次月背软着陆、首次月背与地球的中继通信。 A．玉兔二号 B．嫦娥四号 C．天宫二号 D．鹊桥号 12【解析】 9.A 项错误： 玉兔二号是月球车， 嫦娥四号着陆后才会放出月球车。【选 B】 3. 古代天文历法成就3.1 古代天文历法成就 夏商时期 《夏小正》 。 殷历。 春秋战国时期 《春秋》记载，公元前 613 年， “有星孛入于北斗” 。 《甘石星经》 。 秦汉时期 汉武帝时，制定出中国第一部比较完整的历书——《太初历》 。 西汉时期太黑子的记录。 隋唐时期 僧一行——《大衍历》 。 宋元时期 郭守敬——《授时历》 。 【解析】 这部分比较简单，知道有哪些成就，了解并且把考试重点圈出来即可。 夏商时期： 《夏小正》 ： 作者是谁无从考察， 通常认为这本书介于战国和两汉期间，反映的是夏朝立法，本身可能有残缺和其他错误。真正成书、完全被人记载可能已经到了后期。虽然这一作品无从可考，但目前所存的版本，一定程度上反映了先秦时期中原农业生产发展的水平，也保存了我国比较古老、珍贵的历法知识。 殷历：殷商时期所用的历法，是中国古代历法中的一种，据说把每年分为春、秋，大月 30 天，小月 20 天，闰月置于年末，是我国古代重要的历法。 3.春秋战国时期： 《春秋》 记载， 公元前 613 年， “有星孛入于北斗” 。 描写的是哈雷彗星，是世界上最早关于哈雷彗星的记载， 英国物理学家哈雷首先测定了其轨道数据并成功预言了回归时间，最终命名为哈雷彗星，中国虽然先发现，但并没有以中国发现者的名字命名。 《甘石星经》 ：是世界上现存最早的天文学著作之一，是中国古代天文学专著和记录， “甘”指甘德， “石”指石申，甘德有人说是楚国人，有人说是鲁国人，石申是魏国人，两人各自写了一部天文著作，合起来成为《甘石星经》 ，他们观测了金、 木、 水、 火、 土五颗行星的运行， 还总结了五颗行星运行的规律，是非常重要的天文历法成就。 秦汉时期： 汉武帝时，制定出中国第一部比较完整的历书——《太初历》 。司马迁42 岁时倡导并参与制定了《太初历》 。 《太初历》是中国第一部有完整文字记载的历法，它的朔望月和回归年数据不是很精确，但已经非常不错了。 西汉时期太阳黑子的记录。我国历史上有非常丰富的对太阳黑子的记录，现存世界上最早的明确记录太阳黑子的时间是公元前 28 年，是汉朝人观测到的。在《汉书·五行志》里写到， “三月乙未，日出黄，有黑气大如钱，居日中央” ，就是太阳黑子。哈雷彗星和太阳黑子都是中国最早记录的。 隋唐时期：僧一行——《大衍历》 。僧一行是唐朝僧人，是我国唐朝著名的天文学家，在佛法研究方面也具有非常重要的贡献。 《大衍历》系统周密、比较准确的反映了太阳运行的规律，也表明了中国古代立法体系的成熟。僧一行用科学实地测量的方法，测量了子午线的长度，子午线是经线。 宋元时期：郭守敬——《授时历》 。元朝颁布此历法的时候，郭守敬不是主编， 但考试的时候经常把郭守敬和 《授时历》 放在一起考， 大家对应记忆即可。《授时历》的精度与现行的相当，又比现行历法早了 300 多年。 《授时历》是元世祖忽必烈赐书得来的，它测得一年有 365.2425 天，现在的观测测得一年有365.2422 天。 3.1.1 试题 【例 10】 （2015 北京市考）下列关于哈雷彗星的说法，错误的是（ ） 。 A．哈雷彗星的运行周期最早是英国人爱德蒙•哈雷测量出来的 B．公元前 613 年，我国在世界上第一次确切记录了哈雷彗星的回归 C．哈雷彗星的平均公转周期为 100 年 D．哈雷彗星是人类首颗有记录的周期彗星 12【解析】10.选非题。C 项错误：应该是 76 周年。 【选 C】 【例 11】 （2019 多省联考）下列关于我国古代天文学成就的说法错误的是（ ） 。 A． 《太初历》精度与公历相当 B．唐代僧一行制作了金属黄道游仪 C． 《天文》记录了上百颗恒星的赤道坐标位置 D． 《汉书·五行志》记录了太阳黑子活动 123【解析】11.选非题。A 项错误：与公历相当的是元朝郭守敬主编的《授时历》 ， 《太初历》基本上来说还可以，但还差很多。B、C、D 项不建议记忆，不是常考的点，只会出现一次，只是凑数的。黄道游仪是用来观测日夜星辰位置变化的一种仪器，是机械制造家梁令瓒用木料制造的模型，后来在僧一行主持下铸成铜器。 《天文》的作者是战国时期的天文学家石申。一般来说不知道、不熟悉的选项不会是题目的正确答案，一般常识题解题的核心和关键是大家所熟知的、高频的知识。 【选 A】 3.2 常考古代历法概念 日、月、年 日：反映太阳出没； 月：反映月相变化； 年：反映冷暖交替的周期。 中国历法 中国历法是阴阳合历。 【解析】 日、月、年 日：反映太阳出没。即太阳升起、落下、再升起。日出日落变化为一日。 月：反映月相变化。从开始看不见到朔望变化，从新月到满月。 年：反映冷暖交替的周期。如春夏秋冬四季更迭。 中国历法： 中国历法是阴阳合历。 如果一个历法只有月的变化叫做阴历，只以年来记录叫做阳历，年就是反映地球绕太阳公转的变化状况。中国历法的农历既有年，也有月，所以是阴阳合历。 3.2.1 试题 【例 12】 （2017 多省联考）历法是推算年月日、使其与相关天象对应并协调时间的方法。现行历法主要有三种：阳历即太阳历，主要依据为回归年；阴历或称太阴历，主要依据为朔望月；阴阳历的平均历年为回归年，历月为朔望月。那么，我国农历属于（ ） A．阴历 B．阳历 C．阴阳历 D．以上都不是 1【解析】12.【选 C】 【例 13】 （2016 年江西法检）二十四节气是以（ ）为依据确定的。 A．太阴历 B．太阳历 C．阴阳合历 D．潮汐运动规律 12【解析】13.B 项正确：二十四节气是把地球绕太阳公转过程分成 24 份，24节气完成一圈，春夏秋冬就更替了一圈，是以年为基准的。 【选 B】 3.3 二十四节气 中国古代指导农事的补充历法，属于阳历系统。将地球绕着太阳公转的轨道分成 24 份，因公转的速度不均匀，因此节气便有，14 天，16 天之分。 早在春秋战国时代，汉族劳动人民就有了“日南至” 、 “日北至”的概念，到了秦汉年间，二十四节气已经完全确立。 《淮南子》一书记载了和现代完全一样的二十四节气的名称，公元 104年， 《太初历》正式把二十四节气定于历法，明确了二十四节气在天文历法中的位置。 【解析】 二十四节气：了解即可，不需要精准记忆。 中国古代指导农事的补充历法，属于阳历系统。将地球绕着太阳公转的轨道分成 24 份，因公转的速度不均匀，因此节气便有，14 天，16 天之分。 早在春秋战国时代，汉族劳动人民就有了“日南至” 、 “日北至”的概念，到了秦汉年间，二十四节气已经完全确立。 《淮南子》一书记载了和现代完全一样的二十四节气的名称，公元 104年， 《太初历》正式把二十四节气定于历法，明确了二十四节气在天文历法中的位置。 二十四节气的顺序是： 正月：立春、雨水；二月：惊蛰、春分；三月：清明、谷雨； 四月：立夏、小满；五月：芒种、夏至；六月：小暑、大暑； 七月：立秋、处暑；八月：白露、秋分；九月：寒露、霜降； 十月：立冬、小雪；十一月：大雪、冬至；十二月：小寒、大寒 【解析】 首先要把 24 节气的名称记清楚，口诀：春雨惊春清谷天，夏满芒夏暑相连，秋处露秋寒霜降，冬雪雪冬小大寒。考试一是考顺序，二是考具体某个节气的特点。 不需要把全部都掌握， 只需要掌握核心的知识， 考试基本上就能够做对。 惊蛰：大约在每年的 3 月 5 日到 3 月 6 日，意味着春雷开始，蛰伏在地下的昆虫苏醒过来，春雷惊百虫。 春分：太阳直射赤道，南北半球昼夜等长。 清明：大概在 4 月 4 日至 4 月 6 日。清明节在 4 月 5 日。清明是一个非常重要的节气，清明节这一天通常会祭祖。清明节气意思是清明过后，万物生长、发芽， 出现了春和景明之象， 和万物生长发育有一定的关系。 清明时节气候清爽、温暖。 小满：大概在 5 月 20 日至 5 月 22 日，意思是作物开始成熟、开始灌浆。二十四节气传到南方以后，小满时节是南方的梅雨季节，所以又有“小满大满江河满”的说法，但小满最初的说法就是指作物籽粒开始灌浆饱满但尚未成熟。 芒种：作物成熟，准备收割。大概是 6 月 5 日到 6 月 6 日。 “芒”是指有芒一类的作物，中国古代的小麦都是有芒的。 “种”有两种说法，一种说法是种子，另外的说法是种东西。 夏至：太阳直射北回归线。 处暑：不是夏季的节气，是秋季的节气。 “处”的意思是结束、终结，意味着结束炎热，即不热了。 3.3.1 试题 【例 14】 （2014 黑龙江） “二十四节气”中的“夏满芒夏暑相连”包含了多少个节气（ ） 。 A．6 B．5 C．4 D．7 12【解析】14.A 项正确：立夏、小满、芒种、夏至、小暑、大暑。 【选 A】 【例 15】 （2015 国考-地市级）下列说法符合生活实际的是（ ） 。 A．小满时节，我国东部由低温导致呼吸疾病明显增多 B．芒种时节，我国南方居民发现春困的感觉有所加剧 C．处暑时节，我国北方医院里中暑病人相对有所减少 D．雨水过后我国西部蚊蝇所传播的疾病开始明显增多 12345【解析】15.A 项错误：小满是夏季的节气，温度应该高了。B 项错误：芒种是夏季的节气，不会春困。C 项正确：处暑以后，热就终结了。D 项错误：雨水大约是每年的 2 月 18 日至 2 月 19 日，是低温时期，蚊蝇不多。 【选 C】 【例 16】 （2013 四川省考） 下列对 “惊蛰” 这一节气的描述最准确的是 （ ） 。 A．标志着冬天的结束，春天开始 B．降雨量充沛，利于谷物生长 C．天气晴朗，气温较高，春天已接近尾声 D．温度逐渐升高，渐渐有了春雷，冬眠动物复苏 1234【解析】16.A 项错误：应该是立春。B 项错误：应该是谷雨。C 项错误：没有具体的时间，大约在清明谷雨期间。 【选 D】 【例 17】 （2018 多省联考）下列四个节气所表示的含义错误的是（ ） A．处暑：炎热夏季即将到来 B．惊蛰：天气回暖，春雷始鸣 C．冬至：冬季最寒冷的日子开始 D．小满：夏熟作物籽粒开始灌浆饱满但未成熟 12【解析】17.选非题。A 项错误：处暑意味着夏季已经结束了，是秋天的节气。 【选 A】 3.4 干支纪年法十天干：甲、乙、丙、丁、戊、己、庚、辛、壬、癸。 十二地支：子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥 将天干地支组成六十个数的周期用以纪念，从甲子开始，以癸亥结束，这种方式就是干支纪年法。 【解析】 天干地支纪年法在考试中经常出现，而且在我国古代也是非常重要的。现在很多老黄历上还会标天干地支， 干支记年考试非常简单， 大家要先掌握十天干、十二地支。 将天干地支组成六十个数的周期用以纪念，从甲子开始，以癸亥结束，这种方式就是干支纪年法。当甲子重新出现就是 60 年。 考试题目一般会给出初始年份的干支记年，如某年是甲子年，然后问 5年或者 6 年以后是什么年，假设是 5 年，天干往后数 5 个数，是己，地支数 5个数，是巳，即五年后是己巳年，六年后是庚午年。癸申年下一个是甲酉年。 3.4.1 试题 【例 18】 （2011 多省联考-下半年） 中国古代以天干地支纪年， 天干是： 甲、乙、丙、丁、戊、己、庚、辛、壬、癸。地支是：子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥。甲午战争发生在 1894 年，1900 年八国联军侵华以干支纪年是（ ） 。 A．乙亥年 B．庚子年 C．辛丑年 D．壬寅年 12【解析】18.B 项正确：1900 年和 1894 年相差 6 年，往后数 6 个数，得到庚子年。 【选 B】 【例 19】 （2011 上海市考）我国农历中以天干、地支的搭配来纪年，其中十天干为甲、乙、丙、丁、戊、己、庚、辛、壬、癸；十二地支为子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥。搭配的方式是：在天干中和地支中依次各取一字搭配来纪年， 例如 1920 年是庚申年， 下一年的天干为辛， 地支为酉， 故 1921年，也就是中国共产党成立的这年，是辛酉年。那么，中国共产党成立后的下一个辛酉年是公元多少年（ ） 。 A．1981 B．1991 C．2000 D．2001 12【解析】19.A 项正确：周期是 60 年。 【选 A】 【例 20】 （2015 河北省考） 我国农历采用天干地支纪年法， 天干是： 甲、 乙、丙、丁、戊、己、庚、辛、壬、癸。地支是：子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥。以天干地支纪年，2015 年是乙未年，2020 年是（ ） 。 A．丙申年 B．甲子年 C．庚子年 D．壬申年 12【解析】20.C 项正确：往后数 5 个数，得到庚子年。 【选 C】 3.5 总结 【注意】 古代天文历法成就：夏商时期的《夏小正》 、殷历。春秋战国时期：哈雷彗星、 《甘石星经》 。秦汉时期： 《太初历》 、太阳黑子。隋唐时期：僧一行《大衍历》 、子午线。宋元时期：郭守敬《授时历》 。 常考古代历法概念：日：反映太阳出没；月：反映月相变化；年：反映冷暖交替的周期。 二十四节气：春雨惊春清谷天，夏满芒夏暑相连，秋处露秋寒霜降，冬雪雪冬小大寒。 干支纪年法：甲、乙、丙、丁、戊、己、庚、辛、王、癸；子、丑、寅、卯、辰、巳、午、未、申、酉、戌、亥。","tags":[{"name":"公务员","slug":"公务员","permalink":"https://wgy1993.gitee.io/tags/%E5%85%AC%E5%8A%A1%E5%91%98/"},{"name":"常识","slug":"常识","permalink":"https://wgy1993.gitee.io/tags/%E5%B8%B8%E8%AF%86/"},{"name":"航天史和古代天文历法","slug":"航天史和古代天文历法","permalink":"https://wgy1993.gitee.io/tags/%E8%88%AA%E5%A4%A9%E5%8F%B2%E5%92%8C%E5%8F%A4%E4%BB%A3%E5%A4%A9%E6%96%87%E5%8E%86%E6%B3%95/"}]},{"title":"我国民族节日及文化习俗","date":"2020-11-18T05:02:03.000Z","path":"archives/8ac59bb5.html","text":"1. 概要 本节课讲解我国民族节日及文化习俗，内容相对比较简单，离日常生活很近，大家都比较熟悉，因此学习可以轻松一点，尽量课上都记住。 我国有 56 个民族，节日和文化习俗丰富多样，课程时间有限，不会全部讲到， 针对考试挑一些重点来讲。 从课程安排上来说， 会多花一点时间讲解汉族，因为考试考查较多，也会涉及到常考的少数民族的相关内容。 56 个民族中人口最多的就是汉族，大多数同学应该都很了解汉族的传统节日，重大的传统节日会放假，但现在传统节日的味道越来越淡，很多人忽略了节日背后的意义及其文化习俗，希望大家通过这节课能够重新认识这些传统节日。 应对考试，重点掌握各个节日的时间、别称、习俗和相关诗句。 2. 汉族传统节日及习俗2.1 春节 时间及古称 农历正月初一，古称元日、元旦等 起源 主要习俗 祭灶、扫尘、贴春联、除夕守岁、拜年、给压岁钱等 文化意蕴 团聚、对幸福生活的向往 常考相关诗词句 爆竹声中一岁除，春风送暖入屠苏，千门万户瞳瞳日，总把新桃换旧符。——王安石《元日》 桃符呵笔写，椒酒过花斜。——陆游《己酉元旦》 【解析】 春节： 时间：农历正月初一。按照习俗，春节会从上一年农历腊月初八（腊八粥）或腊月二十三开始，一直到农历正月十五结束，但做题时要知道春节时间是农历正月初一，这是最能够代表春节的一天。 古称元日、元旦： 元日：很多诗句中都提到过，看到这个词要知道说的是春节。 元旦： 古时称农历正月初一为元旦， 辛亥革命之后称公历 1 月 1 号为元旦，农历正月初一称为春节， 做题时要加以区分， 古诗词题目看到元旦要知道是春节。 起源（了解即可）：春节和年的概念最初都是来源于农业，以前的历法规定农历十月为一年中的第一个月，直到西汉汉武帝时颁布了《太初历》，将农历正月定为一年的开始。 主要习俗：联系生活记忆。 祭灶：每年腊月二十三送灶神的仪式，也称为“送灶”。灶神就是灶王爷，传说灶神每年腊月二十三晚上要上天汇报工作，这一天百姓会供上红烛、糖果、瓜果等送灶王爷上天，希望灶王爷上天多说点自家的好话，回来的时候可以给自家多多增福，这就是祭灶。 扫尘：“尘”与“陈”谐音，新春扫尘就是除尘布新，小时候过春节父母会拉着小孩大扫除，很多同学应该都有这样的体验。 贴春联：小时候会跟着长辈帮忙贴春联。春联起源于“桃符”，古人辞旧迎新之际会在桃木板上写上驱鬼辟邪的神仙的名字，希望能够祈福灭祸，后来人们会在上面写上一些吉利话，桃符由驱鬼的桃木牌变成了表达思想的对联。做题时看到“桃符”要知道是古时候的春联，与春节有关。 除夕守岁： 除夕之夜全家团聚在一起点蜡烛、 吃年夜饭， 需要守过十二点，甚至会通宵，守岁意在驱除邪病、瘟疫，祈祷新一年的吉祥、如意。 拜年：从大年初一开始走亲访友、互相拜年，传统的拜年不像现在发个微信、抢个红包，而是有规矩的，一拜诸神仙、祖先，二拜长辈亲友，仪式感更强。 给压岁钱：小孩子最喜欢的环节。传统上给压岁钱不只是为了给钱，而是有寓意的，“岁”与“祟”谐音，长辈给晚辈压岁钱，是为了驱除未来一年可能遭遇的灾祸。 除以上习俗以外，春节还有吃饺子、贴年画、放鞭炮、逛庙会等习俗。 文化意蕴：过春节最大的意义在于团圆，每年的春运可以看出，在外的游子踏上回家的归途，内心非常开心，期盼团圆，与家人团聚也是中国人最朴素的愿望，祭灶、贴春联、守岁等都表达了对幸福、美好生活的向往。 相关诗词句：节日部分最大的考点，也是近几年命题人最喜欢的出题角度，需要重视。 爆竹声中一岁除，春风送暖入屠苏，千门万户瞳瞳日，总把新桃换旧符。——北宋王安石《元日》： 描写春节最出名的诗作，考试如果出现需要知道与春节相关，“元日”即春节，如果不知道诗作的名字，通过诗句中的关键词，如“爆竹”（放鞭炮）、“桃符”（春联）等判断即可。 不要求每一句都背下来，通过诗句中的关键词判断与哪个节日相关即可。 屠苏：饮屠苏酒是古时过年的一种习俗，喝用屠苏草浸泡的酒可以驱邪避瘟疫求得长寿。 桃符呵笔写，椒酒过花斜。——南宋陆游《己酉元旦》：“元旦”可知与春节有关，古时元旦即为春节，关键词“桃符”（春联）。 2.2 元宵节 时间及古称 农历正月十五，古称上元节、元夕等 起源 主要习俗 吃元宵、赏花灯、猜灯谜、迎紫姑等 文化意蕴 如意太平、团团圆圆 常考相关诗词句 去年元夜时，花市灯如昼。月到柳梢头，人约黄昏后。——欧阳修《生查子·元夕》 众里寻他千百度，蓦然回首，那人却在，灯火阑珊处。——辛弃疾《青玉案·元夕》 身闲不睹中兴盛，羞逐乡人赛紫姑。——李商隐《观灯乐行》 火树银花合，星桥铁锁开。暗尘随马去，明月逐人来。游伎皆秾李，行歌尽落梅。金吾不禁夜，玉漏莫相催。——苏味道《正月十五夜》 【解析】 元宵节： 时间：农历正月十五。 古称上元节、元夕：重点记忆。 元宵节的起源有三种说法： 一是来源于佛事活动，东汉明帝刘庄时期提倡佛教，听说佛教有正月十五僧人观佛舍利、 点灯敬佛的做法， 就命令这一天夜晚在皇宫和寺庙里要点灯敬佛，这种礼佛仪式慢慢变成了民间盛大的节日。 二是起源于道教活动， 道教把正月十五称为上元节、 七月十五称为中元节、十月十五称为下元节，合称“三元”，道教说上元节天官赐福、中元节地官赦罪、下元节水官解厄，上元节慢慢形成元宵节，中元节和下元节主要是为了祭祀，后来过得比较少。 三是来源于汉代民众在乡野持火把驱赶虫兽，希望减轻虫害、祈祷获得好收成，这样的习俗演变成为今天的元宵节。 主要习俗： 吃元宵：宋代已经开始流行，当时叫浮元子。 赏花灯：每年元宵之夜街上、公园里会摆出各种各样的花灯，一家人同游赏花灯，非常好看。 猜灯谜：由谜语发展而来，元宵节人们会把谜语绑在花灯上做成灯谜，才出来会有奖品，如“一去就变坏”为“坯”字，下方的“一”去掉就为“坏”字。 迎紫姑：传说紫姑是一个善良的姑娘，家庭条件不好，辛勤劳作，需要做很多粗活，正月十五这一天紫姑去世，老百姓非常同情她、怀念她，有些地方就出现了正月十五迎紫姑的习俗，每到这天夜晚，人们会用稻草扎成真人大小的紫姑像，到紫姑常干活的厕所、厨房旁边去迎接她。看到紫姑要能联想到元宵节。 文化意蕴：自古以来人们把吃元宵看做吉祥、如意、太平的象征，一家人赏花灯也表现出中国人对阖家团圆的期盼。 相关诗词句： 去年元夜时， 花市灯如昼。 月到柳梢头， 人约黄昏后。 ——北宋欧阳修 《生查子·元夕》：列出的是词的上阙，写的是去年元宵节发生的关于爱情的故事，这一天游花灯、赏歌舞，非常适合约会。关键词“元夕”“元夜”“花灯”，需要知道与元宵节相关。 众里寻他千百度， 蓦然回首， 那人却在， 灯火阑珊处。 ——南宋辛弃疾 《青玉案·元夕》：“元夕”即元宵节，元宵节灯市这天寻寻觅觅，虽然周围美女如云，但就是没有看对眼的那一个，一个转身眼前一亮，看到了心仪的那个她。考试中知道对应的是元宵节即可。 身闲不睹中兴盛，羞逐乡人赛紫姑。——唐李商隐《观灯乐行》：关键词“紫姑”。 火树银花合，星桥铁锁开。暗尘随马去，明月逐人来。游伎皆秾李，行歌尽落梅。金吾不禁夜，玉漏莫相催。——唐苏味道《正月十五夜》： 近两年出现频次较高， 苏轼是苏味道的十一世孙， 可见苏轼能成为大文豪，祖上的基因很强大。 “火树银花合”比喻灿烂的上元节的灯光和烟火；“星桥铁锁开”表明京城开禁，唐朝时期都城有宵禁，正月十五会取消宵禁，平民百姓可以自由通行，《长安十二时辰》讲的就是元宵节取消宵禁，敌人可能趁机制造混乱，张小敬、李必携手拯救长安的故事；“金吾不禁夜”即正月十五这天取消宵禁。通过这些关键词判断是元宵节即可。 2.3 寒食节 时间及别称 清明节前二日，亦称“禁烟节”、“冷节” 起源 纪念介子推 主要习俗 禁烟、冷食等 文化意蕴 拜扫祭祖 常考相关诗词句 马上逢寒食，途中属暮春。——宋之问《途中寒食》 雨中禁火空斋冷，江上流莺独坐听。把酒看花想诸弟，杜陵寒食草青青。——韦应物《寒食寄京师诸弟》 【解析】 寒食节：相对来说比较陌生，古时有这个节日，慢慢地与清明节合二为一。 时间：清明节的前二日。 别称：禁烟节、冷节。 起源：纪念春秋时期介子推。春秋时期晋国发生内乱，晋文公重耳遭到晋献公的追杀，颠沛流离十九年，介子推与晋文公患难与共、吃尽苦头，最艰难的时候介子推曾割下自己大腿上的肉给重耳充饥，重耳大为感动，承诺如果有一天自己做了君王一定会好好报答他，但重耳成为晋文公后忘记了介子推，介子推心灰意冷与母亲归隐绵山，晋文公听信谗言为了迫其出山相见下令放火烧山，意外烧死了介子推和他的母亲，晋文公十分懊悔，于是下令这一天家家户户禁烟火，吃三天的冷食，以纪念介子推，因此寒食节又称为“禁烟节”、“冷节”。 主要习俗：禁烟（不能生火）、冷食（不需要生火加热就可以吃的食物）等。 文化意蕴：寒食节慢慢与清明节合并，演变为拜扫祭祖为主的节日。 相关诗词句： 马上逢寒食，途中属暮春。——宋之问《途中寒食》：关键词“寒食”。 雨中禁火空斋冷，江上流莺独坐听。把酒看花想诸弟，杜陵寒食草青青。——韦应物《寒食寄京师诸弟》：关键词“禁火”“寒食”。 补充关键词：新火。寒食节禁烟禁火，寒食节过后可以重新烧火，被称为“新火”，需要注意与寒食节相关，说的是寒食节之后的情景。 2.4 清明节 时间 春分后十五日 起源 清明节得名源于中国农历二十四节气中的清明节气 主要习俗 扫墓、戴柳、植树、踏青、荡秋千、放风筝等 文化意蕴 对先人的缅怀、人与自然的和谐相处 常考相关诗词句 清明时节雨纷纷，路上行人欲断魂。——杜牧《清明》 好风胧月清明夜，碧砌红轩刺史家。——白居易《清明夜》 【解析】 清明节： 时间：春分后十五日，大约在农历三月份、农历四月初，清明节是法定节假日，大约是公历四月四日至六日 起源：清明节得名源于中国农历二十四节气中的清明节。如果题目中问到传统节日中哪一个是以节气命名的，要知道是清明节。 习俗： 扫墓：上坟，祭祀先人。 戴柳：“柳”与“留”谐音，古时柳多指留恋，清明正值春节，戴柳表现出对青春年华的珍惜和留恋。 植树、踏青、荡秋千、放风筝：适合春天的活动。 文化意蕴：扫墓体现了对先人的缅怀，植树、踏青、荡秋千、放风筝等体现了人与自然和谐相处的美好愿景。 相关诗词句： 清明时节雨纷纷，路上行人欲断魂。——唐杜牧《清明》：知道与清明节相关即可。 好风胧月清明夜，碧砌红轩刺史家。——白居易《清明夜》：清明节的晚上，白居易独自在走廊中欣赏眼下的风景，非常惬意。 2.5 端午节 时间及别称 农历五月初五，又称五月节、浴兰节、重五等 传说 主要习俗 吃粽子，赛龙舟，挂菖蒲、艾草，佩香囊，饮雄黄酒等 文化意蕴 爱国情怀、适应节令、驱邪避害 常考相关诗词句 粽包分两髻，艾束著危冠。——陆游《乙卯重五诗》 轻汗微微透碧纨，明朝端午浴芳兰。——苏轼《浣溪沙·端午》 莫唱江南古调，怨抑难招，楚江沉魄。——吴文英的《澡兰香·淮安重午》 【解析】 端午节： 时间：农历五月初五。 别称：五月节（时间在五月）、浴兰节（夏天皮肤病多发，古人以兰草汤沐浴）、重五（月、日都是五）等，见到这几个别称知道是端午节即可。 传说/起源：有很多个，主要是三种。 一是为了纪念春秋末期吴国大夫伍子胥，伍子胥是吴王夫差身边的大臣，对夫差说勾践是心腹大患、必须杀掉勾践，夫差不听，而且听信谗言认为伍子胥密谋反吴，于是派人给伍子胥一把剑，让伍子胥自杀，又将伍子胥的尸体在五月初五投了江，因而相传五月初五是纪念伍子胥的。 二是为了纪念东汉孝女曹娥， 曹娥的父亲溺水而亡， 好多天也没找到尸体，当时曹娥年仅十四岁，昼夜号哭，十七天后五月初五夜里投了江，抱出了她父亲的尸首，就此传为神话。 三是为了纪念屈原，屈原是战国时期的楚国人，遭到排挤被流放，楚国亡国之后，屈原投汨罗江以身殉国，这是三个传说中流传最广的，一般认为端午节是为了纪念屈原，考试中以端午节纪念屈原为准即可。韩国曾提出端午节是他们的节日。 主要习俗： 吃粽子：传说屈原投汨罗江死后为蛟龙所困，蛟龙最怕五色丝线和艾叶，人们怕屈原饿着，于是用艾叶包裹糯米饭，用五色丝线捆绑，投入水中以驱赶蛟龙，这就是端午节吃粽子的来历。 赛龙舟：屈原投汨罗江后，当地百姓闻讯马上划船来救，一直划到了洞庭湖，这就是端午节赛龙舟的来历。 挂菖蒲、挂艾草：端午是入夏后的第一个节日，气温上升、疾病多发，人们往往会在家门口挂菖蒲、艾草以驱病、辟邪、防蚊。 佩香囊、饮雄黄酒：都是为了驱病防病。雄黄酒有杀菌、解五毒的功效，饮了雄黄酒，病魔都远走，《新白娘子传奇》中白素贞饮下雄黄酒就现了原形，变成了大蟒蛇。 文化意蕴：我国的传统节日多是求吉纳祥，端午则是关于屈原的传说，体现了国人的国家意识和民族意识，屈原是楚国亡国后投江，体现了爱国情怀；挂菖蒲、挂艾草、佩香囊、饮雄黄酒体现了适应节令、驱邪避害的追求。 相关诗词句： 粽包分两髻，艾束著危冠。——陆游《乙卯重五诗》：关键词“粽包” “艾束”。 轻汗微微透碧纨，明朝端午浴芳兰。——苏轼《浣溪沙·端午》：关键词“端午”“浴兰”。 莫唱江南古调， 怨抑难招， 楚江沉魄。 ——吴文英的 《澡兰香· 淮安重午》 ：关键词“楚江沉魄”，屈原是楚国人，楚江沉魄指的是屈原。 2.6 七夕节 时间及别称 农历七月初七，又称乞巧节等 传说 主要习俗 乞巧、拜魁星 文化意蕴 崇尚美好与幸福 常考相关诗词句 柔情似水，佳期如梦，忍顾鹊桥归路。两情若是久长时，又岂在朝朝暮暮。——秦观《鹊桥仙·纤云弄巧》 天阶夜色凉如水，坐看牵牛织女星。——杜牧《秋夕》 【解析】 七夕节： 时间：农历七月初七。 别称：乞巧节。 传说：与牛郎织女的故事相关。牛郎织女非常相爱，天帝认为两人门不当、户不对，神仙不能爱上凡人，于是将两人拆散，后来被二人的真情感动，准许二人每年七月初七相会一次，相传这天喜鹊要飞上天庭，在银河搭起鹊桥让牛郎织女相会。 主要习俗： 乞巧：七夕晚上女子会在院子里摆上水果，向织女星祈祷，请求帮助她们提高刺绣、缝纫等技巧，还会穿针引线做一些小物品来比赛，因为古代女子手巧才能嫁得好，图中就是乞巧的场景。 拜魁星：七月七日是魁星的生日，魁星是主宰文章的神，想求取功名的读书人非常崇拜魁星， 七夕这一天会祭拜祈求保佑自己能够高中， 从而迎娶白富美、走上人生巅峰。 文化意蕴：崇尚美好与幸福，祈祷找到能够陪伴自己一生、相爱一生的那个人。 相关诗词句： 柔情似水， 佳期如梦， 忍顾鹊桥归路。 两情若是久长时， 又岂在朝朝暮暮。——北宋秦观《鹊桥仙·纤云弄巧》。 天阶夜色凉如水，坐看牵牛织女星。——杜牧《秋夕》：描写一个宫女在七夕之夜借羡慕牛郎织女表达自己孤独寂寞的心情。 2.7 中秋节 时间及别称 农历八月十五，又称团圆节、八月节、追月节等 起源 “中秋”一词，最早见于《周礼》 传说 主要习俗 拜月、吃月饼 文化意蕴 对团圆的渴望 常考相关诗词句 但愿人长久，千里共婵娟。——苏轼《水调歌头·明月几时有》 此生此夜不长好，明月明年何处看。——苏轼《阳关曲·中秋作》 十轮霜影转庭梧，此夕羁人独向隅。未必素娥无怅恨，玉蟾清冷桂花孤。——晏殊《中秋月》 【解析】 中秋节： 时间：农历八月十五。 别称：团圆节、八月节、追月节（八月十五的月亮比其他月份更圆更明亮）等。 起源（了解即可）：“中秋”一词，最早见于先秦时期儒家经典《周礼》；我国古代有“夕月”的习俗，即祭拜月神。 传说：与后羿嫦娥相关。后羿射掉九个太阳后，王母娘娘为了奖励他就赐给他一颗长生不老的仙丹， 后羿舍不得吃， 于是交给了自己的媳妇嫦娥保管，后羿有个门徒叫逄蒙，特别想要得到仙丹，趁后羿不在家去逼迫嫦娥交出仙丹，嫦娥无奈之下吃下仙丹飞上天去，当天是八月十五，嫦娥舍不得后羿，于是选择住在了离地球最近的月亮上，后羿回到家发现媳妇不见了很伤心，于是每年八月十五摆下宴席希望与嫦娥相聚。 主要习俗： 拜月：院子里摆上香案，放上食物、红烛，全家跪拜，祈求一家人平平安安、团团圆圆。 吃月饼：“月饼”一词最早收录于南宋吴自牧的《梦梁录》，那时月饼只是一种饼形的食品，后来逐渐将中秋赏月与品尝月饼结合在一起，意为希望家人团员。 文化意蕴：对团圆的渴望。 相关诗词句： 但愿人长久，千里共婵娟。——苏轼《水调歌头·明月几时有》：最常考，“婵娟”即月亮，中秋节这一天月亮比较圆。 此生此夜不长好，明月明年何处看。——苏轼《阳关曲·中秋作》：苏轼与他的弟弟苏辙久别重逢共赏中秋月的情景，两人相聚之后又要分离，不知道明年此时都在什么地方。 十轮霜影转庭梧，此夕羁人独向隅。未必素娥无怅恨，玉蟾清冷桂花孤。——北宋晏殊《中秋月》：“素娥”指嫦娥，意为嫦娥未必不感到遗憾，毕竟陪伴她的只有清冷的月宫和孤寂的桂树。 2.8 重阳节 时间及别称 农历九月初九，又称重九节、登高节等 起源 主要习俗 登高、赏菊、饮菊花酒、佩茱萸等 文化意蕴 步步升高、高寿的意愿；对亲朋好友的思念 常考相关诗词句 独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。——王维《九月九日忆山东兄弟》 九日黄花酒，登高会昔闻。——岑参《奉陪封大夫九日登高》 佳节又重阳，玉枕纱厨，半夜凉初透。东篱把酒黄昏后，有暗香盈袖。——李清照《醉花阴·薄雾浓云愁永昼》 【解析】 重阳节： 时间：农历九月初九。 别称：重九节、登高节等。《易经》将六定为阴数，将九定为阳数，金庸武侠小说中张无忌练的就是九阳神功，九九相重为重九、重阳，古人认为这一天很吉利，很早就开始过重阳节。 主要习俗： 登高：重阳节正值秋天，这个季节登高望远可以心旷神怡、健身祛病。 赏菊：菊花象征长寿，菊花大会一般在这天举行，许多人赴会赏菊希望自己能够长寿。 饮菊花酒：古人过节喜欢喝酒，这个时节菊花开得好，菊花酒也被看做驱灾祈福的吉祥酒。 佩茱萸：如图，茱萸可以入药、制药酒，人们喜欢在重阳节佩戴茱萸辟邪求吉。 文化意蕴：登高、赏菊表现出人们步步升高、高寿的意愿；佩戴茱萸表达对亲朋好友的思念，如王维的名句“遥知兄弟登高处，遍插茱萸少一人”。 相关诗词句： 独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。——王维《九月九日忆山东兄弟》：“登高”“茱萸”可以看出是重阳节，如果考试只给出前两句，要知道佳节指的是重阳节。 九日黄花酒， 登高会昔闻。 ——岑参 《奉陪封大夫九日登高》 ： 关键词“九月”“黄花酒”“登高”，黄花酒即为菊花酒。 佳节又重阳，玉枕纱厨，半夜凉初透。东篱把酒黄昏后，有暗香盈袖。——宋李清照《醉花阴·薄雾浓云愁永昼》：描述重阳节把酒赏菊的情景，表达对丈夫赵明诚的思念，关键词“重阳”。陶渊明写过“采菊东篱下，悠然见南山”，“东篱”代指种菊花的地方，“把酒”喝的是菊花酒，是重阳节的习俗。 2.9 总结 【注意】 汉族传统节日及习俗：考查时间、别称、习俗和相关诗词句。 春节： 农历正月初一，古称元日、元旦（需要注意，古今称呼有差别）等。 主要习俗：祭灶、扫尘、贴春联（古称桃符）、除夕守岁、拜年、给压岁钱等。 相关诗词句：典型关键词“桃符”“爆竹”，不要全部背诵，通过关键词判断即可。 元宵节： 农历正月十五，古称上元节、元夕等。 主要习俗：吃元宵、赏花灯、猜灯谜、迎紫姑（注意）等。 相关诗词句：典型关键词“元夕”“花灯”“紫姑”“火树银花”“不禁夜”。“火树银花”形容元宵节赏花灯、放烟花的情景，元宵节取消宵禁，因而称为“不禁夜”。 寒食节： 清明节前二日，纪念介子推。 主要习俗：禁烟、冷食等。 相关诗词句：典型关键词“寒食”“冷食”“禁烟”。 清明节： 春分后十五日。唯一一个得名于二十四节气的传统节日。 主要习俗：扫墓、戴柳、植树、踏青、荡秋千、放风筝等。 相关诗词句：典型关键词“清明”。 端午节： 农历五月初五，又称五月节、浴兰节、重五等。 主要习俗：吃粽子、赛龙舟、挂菖蒲、挂艾草、佩香囊、饮雄黄酒等。 相关诗词句：典型关键词“粽包”“艾束”“浴兰”。 七夕节： 农历七月初七，又称乞巧节等，这一天牛郎织女鹊桥相会。 主要习俗：乞巧、拜魁星。 相关诗词句：典型关键词“鹊桥”“牛郎星”“织女星”。 中秋节： 农历八月十五，又称团圆节、八月节、追月节（中秋月圆）等，与后羿、嫦娥有关。。 主要习俗：拜月、吃月饼等。 相关诗词句：典型关键词“婵娟”“素娥”。 重阳节： 农历九月初九，又称重九节、登高节等。 主要习俗：登高、赏菊、饮菊花酒、佩茱萸等。 相关诗词句：典型关键词“茱萸”“黄花酒”“登高”。 2.10 试题 【2010-下联考】1.下列哪项中的民俗均与端午节有关（ ）。 A.剪窗花、踏青、燃放灯火、放风筝 B.饮菊花酒、赏月、插茱萸、猜灯谜 C.赏菊花、放孔明灯、插柳、贴春联 D.饮雄黄酒、吃粽子、赛龙舟、插菖蒲 1234【解析】1.通过排除法做题。A 项错误：踏青、放风筝是清明节的习俗。B项错误：饮菊花酒、插茱萸是重阳节的习俗，赏月是中秋节的习俗，猜灯谜是元宵节的习俗。C 项错误：赏菊花是重阳节的习俗，插柳是清明节的习俗，贴春联是春节的习俗，放孔明灯一般是元宵节。【选 D】 【2013-山东】2.关于中国传统节日寒食节，下列说法不正确的是（ ）。 A.是农历中的二十四节气之一 B.这一天人们按照习俗吃冷食 C.是为了纪念春秋时的介子推 D.“且将新火试新茶”说的是节日之后的情景 123【解析】2.选非题。A 项错误：得名于二十四节气的传统节日是清明节，古时寒食节和清明节是两个节日。D 项正确：“且将新火试新茶”说的是寒食节之后的情景。【选 A】 【2014-国考】 3.下列诗句按其所描写节日的先后顺序， 排列正确的是 （ ） 。 ①桃符呵笔写，椒酒过花斜 ②九日黄花酒，登高会昔闻 ③粽包分两髻，艾束著危冠 ④马上逢寒食，途中属暮春 A.④②③① B.①④③② C.②④③① D.③①②④ 12345【解析】3.B 项正确：①句“桃符”“椒酒”指春节，②句“九日黄花酒”指重阳节，③句“粽包”“艾束”指端午节，④句“寒食”指寒食节，按时间顺序为春节、寒食节、端午节、重阳节。【选 B】 【2015-黑龙江】4.下列哪组词语全都与端午节有关？（ ）。 A.屈原、雄黄酒 B.汨罗江、银河 C.桃木符、楚国 D.介子推、龙舟 12345【解析】 4.A 项正确： 屈原的传说与端午节相关， 饮雄黄酒是端午节的习俗，知道知识点可以直接选，其他选项平时做题要做好积累。B 项错误：汨罗江与端午节有关，屈原投的是汨罗江，银河与端午节无关。C 项错误：桃符与春节相关。D 项错误：介子推与寒食节相关，龙舟与端午节相关。【选 A】 【2016-江苏】5.下列诗句与传统节日对应不正确的是（ ）。 A.总把新桃换旧符——清明节 B.遍插茱萸少一人——重阳节 C.蓦然回首，那人却在灯火阑珊处——元宵节 D.两情若是久长时，又岂在朝朝暮暮——七夕节 12【解析】5.选非题。A 项错误：“桃符”与春节相关。【选 A】 【2017-联考】6.下列关于古诗名句的说法，错误的是（ ）。 A.“独在异乡为异客，每逢佳节倍思亲”中的“佳节”指的是中秋 B.“但使龙城飞将在，不教胡马度阴山”中的“飞将”指的是李广 C.“窗含西岭千秋雪，门泊东吴万里船”中的“西岭”指的是岷山 D.“忽如一夜春风来，千树万树梨花开”中的“梨花”指的是雪花 12345【解析】6.选非题。常识题考查很杂，其实仔细看看，正确答案并不难，不要被题目蒙蔽。A 项错误：“独在异乡为异客，每逢佳节倍思亲”出自王维《九月九日忆山东兄弟》，佳节指的是重阳节。B 项正确：“但使龙城飞将在，不教胡马度阴山”出自王昌龄《出塞》，飞将一般指西汉大将李广。C 项正确：“窗含西岭千秋雪，门泊东吴万里船”出自杜甫《绝句》，西岭指的是岷山山脉中的西岭雪山，在程度大邑县境内。D 项正确：“忽如一夜春风来，千树万树梨花开”出自唐代岑参《白雪歌送武判官归京》，梨花指雪花，因为他是在雪天送他的好朋友武判官的。【选 A】 【2017-山东】7.关于诗句所描述的节日，下列对应正确的是（ ）。 ①东篱把酒黄昏后，有暗香盈袖 ②雨中禁火空斋冷，江上流莺独坐听 ③莫唱江南古调，怨抑难招，楚江沉魄 A.元宵 清明 中秋 B.重阳 寒食 端午 C.中秋 寒食 端午 D.元宵 清明 重阳 1234【解析】7.B 项正确：①句出自李清照《醉花阴》，“东篱”“把酒”可以想到菊花酒，对应重阳节；②句“禁火”对应寒食节；③句“楚江沉魄”指屈原，对应端午节。【选 B】 【2018-浙江】8.下列诗句中，没有涉及节日的是（ ）。 A.遥知兄弟登高处，遍插茱萸少一人 B.千门万户曈曈日，总把新桃换旧符 C.绿蚁新醅酒，红泥小火炉 D.金吾不禁夜，玉漏莫相催 12345【解析】8.选非题。A 项正确：重阳节。B 项正确：春节。D 项正确：元宵节。C 项错误：“绿蚁新醅酒，红泥小火炉”出自唐代白居易《问刘十九》，描写诗人冬天邀请朋友来喝酒的情形， 新酿的米酒还没有过滤， 表面浮着一层酒渣，颜色微绿，细如小蚁，因此用“绿蚁”形容新酿的酒，燃烧的炭火把炉子烧得通红，坐在炉子旁边喝着小酒感觉很舒服，与节日无关。【选 C】 【2019-广东】9.俗话说：正月十五闹元宵。“闹”元宵的习俗包括张灯、观灯、舞龙、舞狮等。以下诗句与元宵节有关的是（ ）。 A.火树银花合 B.千里共婵娟 C.遍插茱萸少一人 D.爆竹声中一岁除 12345【解析】9.A 项正确：“火树银花”描述元宵节放烟花、掌灯火的场景，出自唐代苏味道《正月十五夜》。B 项错误：中秋节。C 项错误：重阳节。D 项错误：关键词“爆竹”，春节。【选 A】 【2019-河北】10.下列诗词与所描写节令不相符的是（ ）。 A.东风夜放花千树，更吹落，星如雨。宝马雕车香满路，凤萧声动，玉壶光转，一夜鱼龙舞。——元宵节 B.十轮霜影转庭梧，此夕羁人独向隅。未必素娥无怅恨，玉蟾清冷桂花孤。——七夕节 C.冷食方多病，开襟一忻然。终令思故郡，烟火满晴川。杏粥犹堪食，榆羹已稍煎。唯恨乖亲燕，坐度此芳年。——寒食节 D.无云世界秋三五，共看蟾盘上海涯。直到天头天尽处，不曾私照一人家。——中秋节 12345【解析】10.选非题。题干很长，但比较简单。B 项错误：素娥指嫦娥，与中秋节相关。A 项正确：南宋辛弃疾《青玉案·元夕》。C 项正确：关键词“冷食”。D 项正确：唐代曹松《中秋对月》，可做积累，如果考试只给出前半句要知道与中秋节相关。【选 B】 3. 少数民族特色节日、代表性饮食及文化针对考试中常考的少数民族文化节日讲解，考查相对较简单，能够将节日、代表饮食文化与民族对应即可。 3.1 壮族【特色节日】三月三歌节 【代表性饮食】五色糯米饭 【解析】 壮族：我国人口最多的少数民族，集中在广西。 特色节日：三月三歌节。 时间是农历三月初三， 广西壮族自治区政府将每年的三月初三定为法定节假日，全区可以放两天假。 传说是为了纪念歌仙刘三妹 （壮族民间传说中的名字） ， 后来改名刘三姐，刘三姐与秀才对歌相爱，双双化为石像，人们为了纪念他们，在每年的农历三月初三唱山歌表达怀念，一直到今天，壮族人山歌都唱得非常好。 当天除了对歌，还有很多小节目，姑娘们会绣绣球抛给中意的小伙，小伙如果没有接住就要受罚做游戏、唱山歌；一些地方的青年男女会将煮熟的鸭蛋、鸡蛋、鹅蛋等染成红色，用绳子串成串，与异性碰撞，如果两人愿意交朋友，就会离开人群聊天，如果姑娘不愿意，就会用手护住红蛋，不让小伙碰撞，表示拒绝。 农历三月初三是很多民族的节日，黎族也有三月三，与壮族的三月三歌节差别很大。传说远古时期海南发大水，有一对表兄妹漂到了五指山下，洪水退去后其他人都没了，只剩下他们两个，二人为了传宗接代结为夫妻、生儿育女，到了今天的三月三，黎族夫妇要领着孩子聚集到一起欢度节日，知道黎族也有三月三，与壮族的三月三歌节习俗不一样即可。 代表性饮食：五色糯米饭。如图，因黑、红、白、紫、黄五种颜色得名，也称为“乌饭”，象征吉祥如意、五谷丰登。 3.2 傣族【特色节日】泼水节 【代表性文化】孔雀舞 【解析】 傣族： 泰国、 老挝的主体民族， 在我国属于少数民族， 主要分布在云南一带。 特色节日：泼水节，多在公历四月中旬举行。 一般认为这个节日起源于印度，为了纪念佛祖释迦牟尼的诞生，佛祖诞生时“龙喷香雨浴佛身”，因而泼水节也称为“浴佛节”。 一般举行 3-4 天，节日开始时女子要打扫竹楼、街道、家具等，男子要采花到佛寺献佛，听佛爷讲经，中午把一尊佛像摆到院子里，用清水浴佛，此后互相泼水表示祝福， 左图即为泼水节的场景， 被泼的水越多就代表大家的祝福越多。 代表性文化：孔雀舞，是傣族最为喜闻乐见的舞蹈。空缺在傣族人心中地位很高，是吉祥、幸福、美丽、善良的象征，每逢佳节傣族人就要云聚一堂看孔雀舞的表演，右图即为舞蹈家杨丽萍，她的孔雀舞惟妙惟肖。 3.3 藏族【特色节日】雪顿节 【代表性饮食】青稞酒、酥油茶 【代表性文化】格萨尔王、唐卡 【解析】 藏族：青藏高原的原住民，主要分布在西藏、青海、四川西部、甘肃甘南等地区。 特色节日：雪顿节。 时间在藏历七月初一，藏语中“雪”指酸奶，“顿”指宴会，雪顿节意为酸奶宴，后来演变为演藏戏为主，因而又称为“藏戏节”。 起源于藏传佛教格鲁派的规定，每年夏天有几十天禁止喇嘛外出活动，以免他们踏死小虫、小蚂蚁，禁令解除之后才可以下山，这时候牧民会进献酸奶招待他们， 因而形成了这个节日。 现在这天藏族人民要身着盛装看藏戏， 互串帐篷，主人向客人敬酒，客人要三口喝干。 代表性饮食：青稞酒、酥油茶。 代表性文化： 格萨尔王：藏族传说中的英雄人物，人们根据他的传说故事创作了《格萨尔王传》。很久以前天灾人祸在西藏横行，观世音菩萨为了普度众生，派天神之子格萨尔下凡， 格萨尔降临人间之后为民除害、 造福百姓， 十二岁就获得了王位，从此格萨尔东征西讨、南征北战、除魔除妖，战胜了几十个部落和小国家，后来功德圆满回归天界。 唐卡：即卷轴画。 藏族大多信奉藏传佛教， 唐卡对于藏传佛教的僧尼来说是修行必不可少的用具，礼拜唐卡可以获得功德。 唐卡的题材很多，主要以佛教为主，历史、民俗、天文等都可以画，但制作过程非常讲究， 画师绘制唐卡前要沐浴焚香， 针对不同的内容要念不同的经文，画完之后再由高僧活佛开封加持，一幅简单的唐卡需要十几天到几十天，复杂的唐卡甚至需要几年，因为唐卡的制作必须要用纯天然的颜料，色彩可以保持很长时间，甚至数百年不变色。 长期供奉唐卡可以保佑风调雨顺、四季平安或者被超度到极乐世界，因此很多人非常喜欢唐卡。 3.4 回族【特色节日】开斋节、古尔邦节 【代表性饮食】馓子 【解析】 回族：主要聚集在宁夏回族自治区，新疆、青海、甘肃等一些地方也有许多聚集区。回族人较多的地方会建造清真寺，回族信奉伊斯兰教（真主安拉），信徒自称穆斯林，看到这些内容可以联系到回族。 特色节日： 开斋节：伊斯兰教历九月为斋月，斋月期间要在日出之前吃饭，白天不能吃喝，要克制私欲、断绝邪念，以示对真主的信服，只有小孩和老弱的病人可以除外，斋月满后的开斋节一清早，教民要沐浴、去清真寺做礼拜、相互祝贺。封斋的目的是让人们体验饥饿和干渴的痛苦， 珍惜眼下的幸福生活， 有很大的意义，现在粮食危机的同时浪费粮食非常严重。 古尔邦节：牺牲献身，又称“牺牲节”“宰牲节”，时间在伊斯兰教历的12 月 10 号，传说与古代先知易卜拉欣相关，易卜拉欣梦见真主安拉，真主安拉启示他杀掉自己的亲生儿子向真主表示虔诚，正当易卜拉欣准备执行这个命令时，安拉派使者牵来一头羊，说“我已经领略到你的忠心了，你就用宰羊来代替献子”，此后当地部族就保留了宰牲献祭的习俗。这一天教民要打扫庭院盛装打扮，听阿訇（回族穆斯林主持清真寺的教务人员）讲经，宰杀牛羊（不满两岁的小羊、不满三岁的小牛、小骆驼除外），肉要分成三份，一份自己吃，一份送亲友待客，一份施舍给贫穷的孤寡老人，不能出售。 代表性实物：馓子，是一种油炸食品，每逢民族节日，回族人家都要炸馓子招待客人。 3.5 蒙古族【特色节日】那达慕大会 【代表性饮食】马奶酒、手抓羊肉 【代表性文化】蒙古长调、嘎达梅林 【解析】 蒙古族：传统的游牧民族，在我国主要分布于内蒙古地区。 特色节日：那达慕大会，每年农历七八月间举办，“那达慕”是蒙古语，有娱乐游戏的意思，现在那达慕大会除了有摔跤、射箭、骑马等传统内容之外，还增加了马球、武术、摩托车等项目，会有一些奖品，比如摔跤冠军会奖一头羊或一匹马。 代表性饮食： 马奶酒：每年七八月份牛肥马壮，蒙古族妇女就将马奶发酵成酒，如果去蒙古旅游，当地人会把马奶酒倒入碗中唱敬酒歌，酒必须要喝，否则会被认为瞧不起主人，到少数民族地区旅游要尊重当地少数民族的习俗。 手抓羊肉：蒙古族非常喜爱的传统食物，用手抓着吃，因而得名，没有膻味，原汁原味，分量很足。 代表性文化： 蒙古长调： 很难唱， 特点是字少腔长高亢悠扬， 歌声一起， 就像草原骏马、蓝天白云就在眼前，非常惬意。 嘎达梅林：蒙古族的英雄。清朝末期，政府逐渐开始在蒙古开垦土地，牧场缩小，牧民被迫背井离乡，张学良继续开垦蒙古土地，嘎达梅林等人就发起了起义，嘎达梅林不幸战死，虽然他们的起义失败了，但张学良开垦草原的计划被延迟了，嘎达梅林保护了蒙族牧民的利益。开垦草原会破坏环境，科尔沁草原目前出现了将近五千多万亩的沙地，小时候课本里“风吹草低见牛羊”的场景现在已经很难见到了，要好好保护草原。 3.6 维吾尔族【代表性饮食】馕 【代表性文化】阿凡提的故事 【解析】 维吾尔族：主要聚集在新疆天山以南的地区，如喀什、和田，主要信奉伊斯兰教，也有开斋节、古尔邦节，习俗与回族类似。 代表性饮食：馕。岳云鹏说披萨就是打卤馕，如左图，以面粉为主要原料，大多是圆形，非常能抗饿。 代表性文化：阿凡提的故事。阿凡提是一个勤劳勇敢、幽默聪明、富有正义感的人，经常帮助老百姓，右图为上海美术电影制片厂动画片《阿凡提的故事》中的阿凡提形象，阿凡提骑着小毛驴，捉弄巴依老爷。 3.7 满族【特色节日】背灯祭 【代表性饮食】萨其马、满汉全席 【解析】 满族：世居我国东北，最开始称为女真，有很多部落，努尔哈赤起兵统一女真各部落后改国号为金，史称后金，皇太极时期改女真为满洲，后来将东北的女真和其他民族混在一起，形成满族。 特色节日：背灯祭。每年秋冬举行专祭万历妈妈的节日，传说明朝辽东总兵李成梁想要加害努尔哈赤，他的小妾心善放走了努尔哈赤，李成梁发现之后将小妾赤身裸体打死，满族人为了报答小妾的救主之恩，设立了一个神位，小妾死于万历年间，于是称为万历妈妈，因被赤身裸体打死，怕她害羞，因而这一天各家会在祭台上供奉猪肉，用黑布遮住门窗熄灯灭火，在黑暗中念诵祝词。 代表性饮食： 萨其马：满语音译，意为胡麻、砂糖制成的小吃，汉语中比较难翻译，因而音译，与徐福记沙琪玛差不多。 满汉全席：没有固定的席面，满足建立大清之后希望满汉一家，于是就有了满汉全席，让汉族和满族坐在一起吃饭，大家和和睦睦，汉族和满族喜欢的菜都放在一起，传统的满汉全席有一道菜叫双烤，就是把满族人喜欢的烤小猪和汉族人喜欢的烤鸭拼在一起。康熙帝 66 岁寿宴时就特地制作了满汉全席，设宴三天款待大臣，共有三百多道菜，山珍海味应有尽有。 3.8 朝鲜族【特色节日】老人节 【代表性饮食】泡菜、打糕 【代表性文化】《阿里郎》《桔梗谣》民谣 【解析】 朝鲜族：主要分布在朝鲜半岛，在我国主要分布于东北地区。 特色节日：老人节。 因地区不同时间有所差异，黑龙江省一些地方是农历六月二十四，吉林省延边朝鲜族自治州是八月十五。 这一天全村会向 60 岁以上的老人祝寿，给他们戴红花，并进行摔跤、歌舞等表演活动，有老人的家庭要酿制米酒、做打糕为老人祝寿，是敬老尊老的节日。 代表性饮食： 泡菜：朝鲜族饮食中非常重要的组成部分，很多菜都可以用来做泡菜，如萝卜、白菜、桔梗，热量低，纤维素、维生素多。 打糕：糯米煮熟后反复捶打而成，左图为制作打糕，是个体力活，制作不易， 因而朝鲜族历来是将打糕当作上等美味，每逢佳节或接待贵宾时都要制作打糕，外形像驴打滚。 代表性文化：朝鲜族能歌善舞，代表性的民谣有《阿里郎》 《桔梗谣》 。 桔梗是朝鲜族特别喜欢吃的一种野菜，东北同学可能吃过泡桔梗， 《桔梗谣》 表现的就是朝鲜族姑娘采摘桔梗的欢乐场面， 朝鲜语中桔梗音为 “do la ji” 。 《阿里郎》：阿里郎，阿里郎，阿里郎哟，我的郎君翻山越岭，路途遥远，你真无情啊把我扔下， 出了门不到十里路你会想家； 阿里郎， 阿里郎， 阿里郎哟，我的郎君翻山越岭，路途遥远，晴天的黑夜里满天星辰，我们的离别情话千遍难尽。记住是朝鲜族民谣即可。 3.9 高山族【特色节日】背篓会、观月祭 【解析】 高山族：简单了解即可。主要居住在台湾省，因地区、语言的差异，内部分为阿美人、泰雅人、排湾人、卑南人等。 背篓会：农历八月十五晚上，青年男女会到槟榔林里，小伙上树摘槟榔， 扔到姑娘的背篓里， 姑娘只接自己喜欢的人丢来的， 其他人扔来的一概不接，接到槟榔后悔掏出准备好的荷包送给小伙，两人单独到槟榔林里共度良宵。 观月祭：阿美人的传统祭祀节日，祈盼年年丰收、岁岁平安，一般在九月选一个明月之夜举行，男子在林中宰杀水牛，女子盛装与男子一边吃一边唱歌跳舞，阿美人很会唱歌，台湾很多歌手，如张震岳（作品《爱的初体验》《思念是一种病》）、萧敬腾等都是阿美人。 3.10 彝族【特色节日】火把节 【解析】 彝族：主要生活在四川、云南等地，特色节日是火把节。 四川西南有凉山彝族自治州，火把节是凉山彝族节日中规模最大、内容最丰富、参与人最多、场面最壮观的盛大节日，历时三天三夜。 传说很久之前的一个夏天，天神派斯惹阿比到人间收租，当时人间青黄不接，没有粮食交租还债，斯惹阿比的行为激怒了人间的英雄阿体拉巴，斯惹阿比被杀死，天神为了报复放出天虫吃光地上的庄稼，阿体拉巴带领人们燃起火把烧光了天虫取得了胜利，人们为了庆祝这个胜利，就在农历六月二十四这天点燃火把、载歌载舞，世代相传就形成了火把节。 节日期间，人们要举着火把在庄稼地里游走，因为要烧死害虫，祈求庄稼丰收。 3.11 总结 【注意】 能将节日、习俗、饮食、文化等与民族对应即可，如： 壮族三月三歌节，特色饮食是五色糯米饭； 傣族有泼水节、孔雀舞； 藏族雪顿节又称酸奶宴，喜欢吃青稞酒、酥油茶，有格萨尔王和唐卡； 回族特别重要的宗教节日是古尔邦节和开斋节，喜欢吃馓子； 蒙古族有那达慕大会，喜欢吃马奶酒、手抓羊肉，蒙古长调非常悠扬，民族英雄嘎达梅林； 维吾尔族的特色饮食是馕，特色文化是阿凡提的故事； 满族有背灯祭，特色饮食有萨其马、满汉全席； 朝鲜族有老人节（敬老尊老的节日），喜欢吃泡菜、打糕，特色民谣《阿里郎》《桔梗谣》； 高山族有背篓会、观月祭； 彝族有火把节。 3.12 试题 【2010-联考】1.下列依次与蒙古族、回族、藏族、维吾尔族、壮族有关的是（ ）。 A.酥油茶、馓子、青稞酒、馕、萨其马 B.《嘎达梅林》、《穆斯林的葬礼》、《格萨尔王传》、《阿凡提的故事》、《刘三姐》 C.那达慕大会、开斋节、雪顿节、古尔邦节、泼水节 D.马头琴、冬不拉、铜钦、葫芦丝、芦笙 12345【解析】1.A 项错误：酥油茶是藏族的，馓子是回族的，青稞酒是藏族的，馕是维吾尔族的，萨其马是满族的。B 项正确：《嘎达梅林》是蒙古族的英雄，《穆斯林的葬礼》与回族相关，是一篇长篇小说，主人公是回族的手工匠人，《格萨尔王传》对应藏族，《阿凡提的故事》对应维吾尔族，《刘三姐》对应壮族。C 项错误：蒙古族那达慕大会，回族开斋节，藏族雪顿节，维吾尔族古尔邦节（回族也过这个节日），傣族泼水节。D 项错误：音乐戏曲有专门的课程，会讲到民族乐器。马头琴是蒙古族的，冬不拉是哈萨克族的，铜钦是藏族的，葫芦丝流行于云南，主要是傣族、彝族使用，芦笙主要是苗族、瑶族用到的乐器。【选 B】 【2011-联考】2.下列选项正确的是（ ）。 A.黎族同胞说：“我们每年举行盛大的赛马会” B.朝鲜族同胞说：“我们种的莲雾又获得丰收” C.傣族同胞说：“泼水节是我们传统的盛大节日” D.高山族同胞说：“我们用青稞酒招待远方来客” 12345【解析】2.C 项正确：傣族泼水节。A 项错误：黎族三月三，赛马最少要在草原上，黎族主要生活在海南。B 项错误：莲雾主要生长在台湾和广东，是一种热带水果，朝鲜族分布在我国东北。D 项错误：青稞酒与藏族相关，高山族主要居住在台湾。【选 C】 【2015-河北】3.下列依次与蒙古族、回族、傣族关系最密切的是（ ）。 A.酥油茶、馓子、青稞酒 B.马头琴、冬不拉、葫芦丝 C.那达慕大会、开斋节、泼水节 D.努尔哈赤、阿凡提、刘三姐 12345【解析】3.A 项错误：酥油茶、青稞酒都是藏族的特色饮食。B 项错误：冬不拉是哈萨克族的乐器。C 项正确：蒙古族那达慕大会，回族开斋节，傣族泼水节。D 项错误：努尔哈赤是清王朝的奠基者，满族；阿凡提维吾尔族；刘三姐壮族。【选 C】 【2016-江苏】4.下列关于我国少数民族的名称、特色饮食与代表性文化对应错误的是（ ）。 A.傣族—馕—孔雀舞 B.蒙古族—手抓羊肉—蒙古长调 C.朝鲜族—泡菜—《阿里郎》民谣 D.藏族—酥油茶—《格萨尔王》长诗 12【解析】4.选非题。A 项错误：馕是维吾尔族的特色食物。【选 A】 【2018-国考】 5.某高校学生会干事小王负责策划一个民族文化展示周活动，下列哪个设计方案不合适？（ ） A.请维吾尔族学生表演手鼓舞 B.请蒙古族学生制作奶茶 C.请朝鲜族学生展示唐卡 D.请彝族学生展示花腰刺绣 12345【解析】5.选非题。C 项错误：唐卡是藏族的特色文化。A 项正确：手鼓舞是维吾尔族的民间舞蹈。B 项正确：马奶酒、奶茶是蒙古族的特色饮食。D 项正确： 花腰是云南彝族的一个分支， 女子服饰颜色艳丽、 精美大方， 被称为花腰彝，刺绣功力很好。【选 C】 【2018-河南职测】6.下列关于我国少数民族特征的说法正确的是（ ）。 A.黎族的主要节日为泼水节 B.高山族主要信仰喇嘛教 C.傣族喜欢摔跤、善骑射 D.壮族的山歌令人陶醉 12345【解析】6.A 项错误：傣族泼水节。B 项错误：高山族信仰高山族宗教，喇嘛教一般指藏传佛教，信仰喇嘛教的主要是藏族。C 项错误：喜欢摔跤、善骑射的是蒙古族。D 项正确：壮族人山歌唱得很好。【选 D】","tags":[{"name":"公务员","slug":"公务员","permalink":"https://wgy1993.gitee.io/tags/%E5%85%AC%E5%8A%A1%E5%91%98/"},{"name":"常识","slug":"常识","permalink":"https://wgy1993.gitee.io/tags/%E5%B8%B8%E8%AF%86/"},{"name":"我国民族节日及文化习俗","slug":"我国民族节日及文化习俗","permalink":"https://wgy1993.gitee.io/tags/%E6%88%91%E5%9B%BD%E6%B0%91%E6%97%8F%E8%8A%82%E6%97%A5%E5%8F%8A%E6%96%87%E5%8C%96%E4%B9%A0%E4%BF%97/"}]},{"title":"生活常识","date":"2020-11-17T13:27:01.000Z","path":"archives/1319ee66.html","text":"1. 概要主要讲解生活常识有关的知识点，生活常识不是一门严谨的学科，主要来源于生活中的物理、化学、生物中的现象或常识。 根据考试的情况，分为食品与健康（食品添加剂、维生素等相关知识）、环境保护、应急自救三方面。 2. 食品与健康2.1 食品制作发酵工艺： 发酵指人们借助微生物在有氧或无氧条件下的生命活动来制备微生物菌体本身，或者直接代谢产物或次级代谢产物的过程。 发酵是人类较早接触的一种生物化学反应，如今在食品工业、生物和化学工业中均有广泛应用。 【解析】 食品制作：中国作为舌尖上的国家，有很多美食的制作方式，包括炸、烹饪等制作方式，发酵工艺（也叫微生物发酵工艺）是和考试密切相关的，也是自2018 年至今考题几率很高的食品制作方式。 发酵工艺： 是指人们借助微生物在有氧或无氧条件下的生命活动来制备微生物菌体本身，或者产生代谢产物，如酵母菌产生二氧化碳等。 发酵是人类较早接触的一种生物化学反应，如今在食品工业、生物和化学工业中均有广泛应用。 考点：常见的发酵中用到的菌种是酵母菌、乳酸菌；生活种哪些食品用到了发酵工艺。 常见的发酵中用到的菌种是酵母菌、乳酸菌。 【解析】 常见的发酵中用到的菌种是酵母菌、乳酸菌。 酵母菌是一种单细胞的真菌，是一种肉眼看不见的微小的单细胞微生物， 但是可以发酵， 将糖发酵成酒精和二氧化碳， 所有用酵母菌发酵过的食品 （如馒头），蒸熟以后会有蓬松感，有很多微小的孔洞，就是因为二氧化碳气体等充斥形成的。酵母菌在地球上分布十分广泛，是一种典型的异养兼厌氧型微生物，在有氧、无氧条件下都可以存活，是一种天然的发酵剂。 乳酸菌：可以利用乳酸菌将碳水化合物发酵产生大量乳酸的细菌的统称。 区别：乳酸菌是细菌，酵母菌是真菌；细菌是原核生物，没有成型的细胞核，真菌是一种真核生物，有成型的细胞核。 考试中乳酸菌出现的频率较高，在自然界分布十分广泛，具有多样性，包含 18 个属，共计 200 多种，除极少数以外，大部分是人体必不可少的，且具非常重要的功能。重要的乳酸菌具有重要的生理功能，广泛存在于人的肠道之中，能够促进人类机体的生长，维持肠道菌群的平衡，保证大家正常的消化和吸收，还可以提高免疫力，抑制有害菌群的生长，帮助人体更加健康的成长。常见的乳酸菌发酵食品有酸奶、泡菜。 常见的发酵产品有白酒、醋、馒头等。 常见的发酵产品有白酒、醋、馒头、酱油、味精（谷氨酸钠）等。 2.1.1 试题 【例 1】（2013 多省联考）发酵是人类较早接触的一种生物化学反应，如今在食品工业、 生物和化学工业中均有广泛应用。 以下没有用到发酵工艺的是 （ ） 。 A．酱油 B．醋 C．蔗糖 D．味精 1【解析】1.选非题。C 项错误：糖是蔗糖通过提炼加工所形成的，没有用到发酵工艺。【选 C】 【例 2】 （2018 年多省联考）面包制作过程中使用酵母主要是利用其哪一种特性（ ）。 A．酵母在生长过程中可合成蛋白 B．酵母在生长过程中可分解淀粉 C．酵母在生长过程中可产生乙醇 D．酵母在生长过程中可产生二氧化碳 1【解析】2.D 项正确：面包是软大蓬松的，主要是利用了酵母在生长过程中可产生二氧化碳。【选 D】 2.2 食品添加剂2.2.1 着色剂改善食品外观，如胡萝卜素、胭脂红、柠檬黄。 2.2.2 调味剂增添食品味道，如食盐、醋、味精。 2.2.3 防腐剂防止食品腐烂、变质，如山梨酸钾、硝酸盐、茶多酚、二氧化硫等。 2.2.4 营养强化剂增强食品的营养价值，如食盐中加碘、酱油中加铁。 2.2.5 凝固剂盐卤。 【解析】 着色剂：可以改善食品外观，如胡萝卜素、胭脂红、柠檬黄。 胡萝卜素：一种重要营养元素，维生素 A 的主要原物质，如β胡萝卜素，主要来源于蔬菜和水果，可以将食物染成胡萝卜的颜色。 胭脂红：食用色素，常用于果汁、碳酸饮料、糖果、糕点、冰淇淋、酸奶等， 一般不用于肉类， 主要是防止不法分子通过胭脂红色素将不良的原料肉，如将变质肉通过胭脂红掩盖其原有的颜色，从而显的新鲜，欺骗消费者。 柠檬黄：呈黄色粉末，溶于水，主要在涂料、油墨行业使用，在食品行业使用较少。 调味剂——增添食品味道，如食盐（咸）、醋（酸）、味精 味精：化学名字叫谷氨酸钠，是一种鲜味调味剂，溶于水，水溶液，据说稀释 1000 倍以后依旧可以尝到它的鲜味，在以前的时候，味精主要是以面筋、大豆为原料来提炼的，这样的方式劳动强度大，对设备要求高，要求设备耐酸碱腐蚀， 现在主要是通过谷物发酵的方式提纯， 所以现在的味精也是发酵产品。 注意：辣不是一种味觉，而是化学物质灼伤口腔产生的痛觉。 防腐剂：顾名思义，可以防止食品腐烂、变质，比如山梨酸钾、硝酸盐、茶多酚、二氧化硫等。 山梨酸钾：一种不饱和的脂肪酸，而且是一种具有广谱杀菌性的食品防腐剂，应用较为广泛，如图中的饮料瓶中的配料中就含有山梨酸钾。山梨酸和山梨酸钾获得了联合国粮农组织和世界卫生组织安全公认的安全高效的防腐剂，因此应用广泛，而且在食品防腐剂中毒性是最低的，在食品、饲料加工等方面用途非常广泛。 硝酸盐：使用量有严格限制，在实际生活中，食品加工要使用硝酸盐的量是有严格控制的，因为硝酸盐在人体内可以被还原为亚硝酸盐，而亚硝酸盐与人体血液作用之后， 可以形成高铁血红蛋白， 从而使血液失去携带氧气的功能，使人中毒，轻则头晕、呕吐，重则神志不清，甚至有可能会危机生命。 茶多酚：重要的防腐剂，本来是决定茶叶的颜色、味道，大家尝到的茶的味道，实际上是茶多酚的味道，占整个茶叶干重的 20%-30%，其中含量较大的是儿茶素。茶多酚对自然界的近百种细菌均具有抗菌活性，抗菌具有广谱性，也能作为防腐剂，主要用于乳酸饮料、农畜产品加工中。 二氧化硫：是一种酸性气体，同时也是我国非常重要的防腐剂，在工业中应用非常广泛，只不多用量有严格限制。 拓展：甲醛的水溶液叫福尔马林，考试中经常出现“福尔马林的水溶液可以用于防腐，比如制作泡椒鸡爪”（错误）。原因：福尔马林有毒，食用以后会导致人中毒。 营养强化剂：在食品中添加可以增强食品的营养价值，如食盐中加碘、酱油中加铁。 凝固剂：液体或者是流状物体等凝结成固体，最常见的就是盐卤，通常是把海水或者是盐湖里的水制盐之后残留在池子内部的一些母液， 蒸发冷却之后析出的氯化镁的结晶， 形成的卤块， 是中国数千年来用来制作豆腐的传统凝固剂，它可以使蛋白质溶液凝结为凝胶，用盐卤做的豆腐，其硬度、弹性、韧性都比较强，就是大家俗称的老豆腐。 2.2.6 试题 【例 3】（2018 吉林）食品添加剂被誉为现代食品工业的灵魂。下列食品工业中所应用的食品添加剂与其所属类型，对应错误的是（ ）。 A.制作罐头时使用的山梨酸钾——防腐剂 B.制作香肠时使用的谷氨酸钠——增味剂 C.制作老豆腐时使用的盐卤——凝固剂 D.制作面包时使用的碳酸氢钠——酸度调节剂 1【解析】3.选非题。D 项错误：没有该说法。【选 D】 【例 4】（2012 联考）下列说法错误的是（ ）。 A.高浓度糖液破坏微生物的细胞结构，可延长食品保存期 B.山梨酸钾是一种食品添加剂，可用于增加食品的风味 C.茶多酚能够抑制自由基的活性，可以作为食品防腐剂 D.β-胡萝卜素可以作为食用性色素，可以用于食品调色 1【解析】4.选非题。B 项错误：山梨酸钾是防腐剂，不是增味剂。A 项正确：说的是罐头。【选 B】 【例 5】（2019 多省联考）下列关于盐的说法错误的是（ ）。 A.盐又称“百味之王”，是咸味的载体，具有去腥增鲜之用 B.人体如果摄入过多的盐份，容易产生高血压、水肿等问题 C.按来源及开采方式分类，盐可分为：井盐、海盐、湖盐等 D.日常生活中的低钠盐加入了一定比例的氰化钾，其咸味较淡 1【解析】5.选非题。D 项错误：氰化钾是剧毒物质。【选 D】 2.3 食品营养2.3.1 维生素维生素是参与生物生长发育和新陈代谢所必需的一类小分子有机化合物，在天然食物中含量极少。 维生素 A 维生素 B1 维生素 C 维生素 D 维生素 E 维生素 K 【解析】 食品营养：主要包括维生素和微量元素，人体能够补充正常的维生素和微量元素，能够保证人体正常运转，也是人体不可或缺/重要的一部分。 维生素：是参与生物生长发育和新陈代谢所必需的一类小分子有机化合物，这类物质在体内既不构成身体组织的原料（如蛋白质、肌肉、毛发），也不是能量来源，人体的主要能量来源是糖类、脂肪。维生素是一类调节物质，调节人体内物质分泌以及人体机能，在物质代谢中起到了非常重要的作用，这些物质由于在体内不能全部合成，虽然需要量少，但是也需要体外摄入。考试中一般会考查缺少哪一类维生素会导致哪一类病症或者是出现哪种情况。 维生素 A：缺少会得夜盲症（晚上看不清东西），具有促进生长、维持结膜、角膜等正常功能的作用，参与紫红质的合成，增强视网膜的感光力，维生素 A 对于视线具有非常重要的作用，胡萝卜素是维生素 A 的原物质。因为能够预防夜盲症，因此又称为“抗干眼病因子”，主要存在于鱼肝油、动物肝脏、绿色蔬菜之中。 维生素 B1：维生素 B 是一个非常大的家族，维生素 B1 又称“抗脚气病因子”，缺少维生素 B1 易患脚气病，脚气病不等于脚气，脚气是脚部真菌感染所形成的，脚气病是因为缺少维生素 B1 引起的全身疾病。维生素 B1 是水溶性维生素，主要存在于谷物、动物肝脏、大豆、肉类等食品中。维生素 B9 又称叶酸，很多孕妇在怀孕期间都需要补充的，是一种水溶性维生素，叶酸在蛋白质合成以及细胞分裂和生长过程中具有非常重要的作用，且能够预防红细胞贫血。 维生素 C：又被称为抗坏血酸，体内缺少维生素 C 会得坏血病，像牙龈出血一样的病症。维生素 C 是一种水溶性维生素，经常流失，所以需要经常去补，广泛存在于蔬菜、水果、西红柿、菜花、柿子、苦瓜、橘子、柚子、苹果、猕猴桃等。补充：虽然大多数动物都可以靠肝脏合成维生素 C，但是人类是不可以自身合成维生素 C，因此必须要通过食物摄入。 维生素 D：是一种脂溶性维生素，缺乏维生素 D 易患佝偻病，可以预防佝偻病，主要来源是脂肪高的海鱼、动物肝脏、蛋黄、奶油等。小孩子为了长身体，就会吃很多的鱼肝油，因为鱼肝油中有丰富的维生素 D，可以促进钙质的吸收，进而使小朋友体格健壮。 维生素 E：脂溶性维生素，主要和生育功能、美容功能有关，在蔬菜、坚果中含量较多，能够延缓衰老，有效减少皱纹的产生，保持青春貌美，同时可以促进激素分泌，提高生育能力，防止流产等。 维生素 K：又叫凝血维生素，血液流的时候流血不止，有可能是缺乏维生素 K，可以防止新生婴儿出血的疾病，可以预防内出血，可以促进血液正常凝固等一系列的作用，因此在临床医学中，维生素 K 比较重要。 分类：根据溶解性，分为水溶性（溶于水）维生素和脂溶性（溶于有机溶剂）维生素。 水溶性维生素：常见的是维生素 B 族和维生素 C。 脂溶性维生素：维生素 A、维生素 D、维生素 E 和维生素 K 等。 2.3.2 微量元素在人体内含量极少，不到体重的万分之一，但是对维持生命活动，促进健康生长和发育具有重要作用。 碘——缺乏会引起甲状腺肿大 铁——缺铁会引起贫血 锌——缺锌会引起食欲不振、生长迟缓、发育不良 氟——缺乏会得龋(/qǔ/)齿 【解析】 微量元素：在人体内含量极少，不到体重的万分之一，万分之一以上的被称之为常量元素，如氧、钙等，微量元素对于维持生命活动，促进健康生长和发育具有重要作用。常见的微量元素有： 碘：缺乏会引起甲状腺肿大（甲状腺肿大和甲亢是两回事）。 铁：缺铁会引起贫血，主要通过食补的方式进行，铁锅炒菜虽然有一定的作用，但是并不能有效的预防。 锌：被称之为智力元素，缺锌会引起食欲不振、生长迟缓、发育不良，现在很多广告都在说补锌的重要性。 氟：缺乏会得龋齿，即虫牙、蛀牙，一般情况下会通过改善水质的方式来解决。 2.3.3 试题 【例 6】（2019 多省联考）下列关于维生素说法错误的是（ ）。 A.人体无法完全依靠自身合成维生素，食物是人类获取维生素的主要来源 B.维生素 E、K 的重要作用分别是抗氧化、延缓衰老和维持视力、免疫力 C.维生素可分为水溶性维生素和脂溶性维生素，维生素 D 属于后者 D.新鲜的西红柿、猕猴桃、辣椒等果蔬含有丰富的维生素 C 1【解析】6.选非题。B 项错误：维持视力（预防夜盲症）是维生素 A 的主要作用。【选 B】 【例 7】（2012 多省联考）紫外线促使人体合成（ ）以预防佝偻病。 A.维生素 A B.维生素 B C.维生素 C D.维生素 D 1【解析】1.D 项正确：维生素 D 可以预防佝偻病。【选 D】 2.4 总结 【注意】 食品制作：发酵工艺，常见菌种有酵母菌（制作馒头）、乳酸菌（制作酸奶、泡菜）。馒头、醋、酱油、味精都是通过发酵得到的。 食品添加剂： 着色剂：胡萝卜素、胭脂红、柠檬黄，胭脂红和柠檬黄用量较少。 调味剂：食盐、醋、味精，辣不是味道。 防腐剂：山梨酸钾、硝酸盐、茶多酚、二氧化硫。重点注意山梨酸钾和二氧化硫。 营养强化剂：食盐中加碘、酱油中加铁（了解即可）。 凝固剂：盐卤，制作豆腐。 维生素： 维生素 A：夜盲症。 维生素 B1：脚气病。 维生素 C：坏血病。 维生素 D：佝偻病。 维生素 E：美容养颜、抗衰老，和生育功能有关。 维生素 K：主要作用凝血。 微量元素： 碘缺乏会引起甲状腺肿大。 缺铁会引起贫血。 缺锌会引起食欲不振、生长迟缓、发育不良。 缺氟会得龋齿。 3. 环境保护3.1 大气环境保护3.1.1 酸雨pH 小于 5.6 的降水，主要是人为排放硫氧化物和氮氧化物 3.1.2 臭氧层受损20 世纪 80 年代在南极测到臭氧空洞，人类活动排出的氟卤代烷和含溴的卤代烷烃(/tīng/)是主要原因。 3.1.3 温室效应二氧化碳等温室气体排入大气，使得大气平均温度上升，进而导致全球气候变暖。 3.1.4 PM2.5细颗粒物又称细粒、细颗粒、PM2.5。细颗粒物指环境空气中空气动力学当量直径小于等于 2.5 微米的颗粒物。它能较长时间悬浮于空气中，其在空气中含量浓度越高，就代表空气污染越严重。 3.1.5 水污染随着工农业生产的加剧和人口的膨胀，水资源在日益减少的同时， 因为工农业生产排放的污水量增加进而导致水污染。常见的有重金属污染和水体富营养化 【解析】 酸雨：pH 小于 5.6 的降水，主要是人为排放硫氧化物和氮氧化物，我国主要是二氧化碳、二氧化硫，我国以前燃煤较多，煤中含有硫化物，因此硫酸雨是我国的主要酸雨类型，当酸雨落到地球表面，会对农作物造成损害，如下图中的西瓜表面坑坑洼洼，就是由于酸雨滴到了西瓜表皮，还会对建筑物、人体皮肤造成腐蚀，这就要求我们减少含硫化学燃料的使用，增加清洁能源的使用。二氧化硫即使一种酸性气体，也是一种防腐剂，大气中的二氧化碳遇水会反应成为碳酸，但是问题不大。 臭氧层受损：20 世纪 80 年代在南极测到臭氧空洞，南极没有重工业，但是出现了臭氧空洞， 和大气环流有关， 只需要注意是在南极测到的臭氧空洞即可。人类活动排出的氟卤代烷和含溴的卤代烷烃是主要原因，最常见的就是氟利昂，如冰箱、空调中的氟利昂会加速臭氧层的受损，但是现在南极上空臭氧空洞正在逐渐减少。意义：正是因为臭氧层的存在，人体才免于来自于太阳光的强紫外线的照射，才能保持人体的正常发育，否则人类患皮肤癌的疾病会大大增加。 温室效应：是指二氧化碳等温室气体排入大气，使得大气平均温度上升，进而导致全球气候变暖，两极冰川融化，海平面上升，很多地方会变成沼泽，对人类的影响比较大。二氧化碳并不是唯一的温室气体，还有甲烷、水蒸气。下图中的北极熊，就是因为北极地区的冰川融化，找不到食物，饿的瘦骨嶙峋。 PM2.5：又称细颗粒物，又称细粒、细颗粒、PM2.5。细颗粒物指环境空气中空气动力学当量直径小于等于 2.5 微米的颗粒物，能较长时间悬浮于空气中，其在空气中含量浓度越高，就代表空气污染越严重，也就是大家生活中提到的雾霾。注意：雾和霾是两回事，小水滴、小冰晶形成的是雾，固体悬浮的颗粒叫霾。PM10 是直径小于等于 10 微米的颗粒物。 水污染：随着工农业生产的加剧和人口的膨胀，水资源在日益减少的同时，因为工农业生产排放的污水量增加进而导致水污染。 常见的有重金属污染，曾经在日本就发生过水俣(/yǔ/)病事件，就是因为工业生产的水银大量流入水体中， 进而造成人体残疾， 一般情况下， 如果水被污染，千万不要去吃污染水体的鱼类，尤其是鱼头，鱼头中的中重金属含量更高，要吃生活在水体状态良好中的鱼。 水体富营养化是指含磷等营养元素的生活污水、工业污水大量排入江河湖泊，使水中的营养元素增加，进而导致藻类大量繁殖，使水体缺氧，鱼类大量死亡。在淡水中典型的就是水华，看到水特别绿，手伸到水中绿色可以沾到手上（如下图中间），在海洋中，典型的就是赤潮，赤指的是藻类为红色，但并不唯一。 水葫芦不是水体富营养化，属于物种入侵（生物入侵），如小龙虾，先从中国以外的地方引进，最初是为了观赏和饲养，但是由于暴雨等情况，会导致从人工水体跑到自然水体中，由于缺少天敌，会大量繁殖。 3.1.6 试题 【例 8】（2016 国考）某城市空气质量较差，检测结果显示，在主要污染物中，PM10 颗粒浓度严重超标，PM2.5 颗粒浓度及有害气体浓度尚在正常范围。如果你是城市决策者，采取以下哪些措施能在影响最小的情况下，最有效地改善空气质量？ ①整改郊区水泥厂 ②整改郊区造纸厂 ③市区车辆限号行驶 ④改善郊区植被环境 A.①② B.①④ C.③④ D.②③ 1【解析】8.②造纸厂主要是水体污染，在清洗纸浆的时候会用到水，可能会对水体造成一定的污染，不会有烟尘产生。排除 A、D 选项。①水泥厂造成的污染颗粒更大一些，汽车尾气由于燃烧较为充分，产生的颗粒相对较小，题干中指出 PM2.5 在正常范围之内，所以应该整改水泥厂，①当选，排除 C 选项。 【选 B】 【例 9】（2012 江西）2011 年，一场关于 PM2.5 的讨论，牵动着国人神经。PM2.5是指（ ）。 A.极细颗粒物 B.细颗粒物 C.微颗粒物 D.超微颗粒物 1【解析】9.考查概念。B 项正确：PM2.5 是指细颗粒物。【选 B】 【例 10】（2019 广东）赤潮是指在特定的环境条件下，海水中某些浮游生物爆发性增殖，引起水体变色的一种生态现象。下列关于赤潮的说法中，正确的是（ ）。 A．海水富营养化是赤潮爆发的基础 B．赤潮爆发与人类活动关系不大 C．多数赤潮对环境没有危害 D．所有的赤潮都是红色的 1【解析】10.A 项正确：海水富营养化是赤潮爆发的基础。B 项错误：赤潮爆发是由于人类破坏了生态平衡，向水中排放了大量的营养物质造成的。C 项错误：赤潮爆发以后，水中含氧量会降低，鱼类会大量死亡。D 项错误：赤潮主要是红色的，但不都是红色的。【选 A】 3.2 垃圾分类处理根据 2019 年住房和城乡建设部发布的《生活垃圾分类标志》标准，共分为4 个大类，11 个小类。 3.2.1 垃圾的分类3.2.1.1 可回收物可回收物表示适宜回收利用的生活垃圾，包括纸类、塑料、金属、玻璃、织物等。 3.2.1.2 有害垃圾有害垃圾表示《国家危险废物名录》中的家庭源危险废物，包括灯管、家用化学品和电池等。 3.2.1.3 厨余垃圾厨余垃圾表示易腐烂的、含有机质的生活垃圾，包括家庭厨余垃圾、餐厨垃圾和其他厨余垃圾等。 3.2.1.4 其他垃圾其他垃圾表示除可回收物、有害垃圾、厨余垃圾外的生活垃圾。 【解析】 垃圾的分类： 可回收物表示适宜回收利用的生活垃圾，包括纸类（如图书等）、塑料、金属、玻璃、织物等，标志如下图。 有害垃圾：对人体和环境都是有害的，表示《国家危险废物名录》中的家庭源危险废物， 包括灯管、 家用化学品、 过期药品和电池等， 如水银温度计，标志如下图。 厨余垃圾：表示容易腐烂的、含有机质的生活垃圾，包括家庭厨余垃圾（剩饭）、餐厨垃圾（菜叶、果核、果皮）和其他厨余垃圾等，标志如下图。 其他垃圾：表示除可回收物、有害垃圾、厨余垃圾外的生活垃圾。如碎陶瓷、厕纸等，标志如下图。 重点记忆垃圾分类的标志，如其他垃圾的标志下方是开口的。 3.2.2 垃圾处理方式 垃圾填埋 堆肥 焚烧 【解析】 垃圾处理方式：包括垃圾填埋、堆肥、焚烧，现在会使用微生物降解，因为焚烧可能会产生二噁英等有害物质，而堆肥、填埋对于塑料等物质很难分解。 3.2.3 试题 【例 11】（2020 浙江选调）生活垃圾一般可分为四大类：可回收垃圾、厨余垃圾（湿垃圾）、有害垃圾和其他垃圾（干垃圾）。下列说法正确的是（ ） A.报纸和陶瓷属于可回收垃圾 B.蛋壳和塑料餐盒属于厨余垃圾 C.人体毛发和厕纸属于其他垃圾 D.废旧蓄电池和玻璃碎片属于有害垃圾 1【解析】11.A 项错误：陶瓷属于其他垃圾。B 项错误：塑料餐盒不属于厨余垃圾。D 项错误：玻璃属于可回收垃圾。【选 C】 【例 12】（2018 年多省联考）日常中，垃圾一般分为可回收垃圾、有毒垃圾、厨余垃圾和其它垃圾（不可回收垃圾）。据此，下列说法错误的是（ ）。 A.可回收垃圾、其它垃圾的标志分别为 B.充电电池、废旧灯管灯泡属于有毒垃圾 C.受污染的纸张、过期药品等都应归为其它垃圾 D.厨余垃圾包括饭菜、过期食品、菜梗菜叶等废弃的生热食物残渣 1【解析】12.选非题。C 项错误：过期药品属于有害垃圾，纸张要看受污染程度而定。【选 C】 【例 13】（2017 天津）根据我国生活垃圾分类制度的规定，下列物品均为有害垃圾的是（ ）。 A.废血压计和废胶片 B.餐厨垃圾和废电池 C.废塑料和废温度计 D.废玻璃和废油漆 1【解析】13.B 项错误：餐厨垃圾不是有害垃圾。C 项错误：废塑料不是有害垃圾。D 项错误：废玻璃不是有害垃圾。【选 A】 3.3 总结 【注意】 大气环境保护： 酸雨： 我国主要以硫酸雨为主。 二氧化硫既是酸性气体 （酸雨的元凶） ，也是防腐剂。 臭氧层受损：会使紫外线到达地球表面，使人患皮肤癌的几率增加。 温室效应：是温室气体二氧化碳导致的。 PM2.5：是一种细颗粒物，注意雾和霾的区别和联系。 水污染：水的重金属污染、水体富营养化，重点关注淡水中的水华和海水中的赤潮，水葫芦属于物种入侵。 垃圾分类处理：一般会给出一类具体垃圾，选择属于哪一种垃圾分类。 可回收物。 有害垃圾。 厨余垃圾。 其他垃圾：如陶瓷、厕纸、人体毛发等。 4. 应急自救4.1 雷雨天气 蹲下，降低自己的高度，同时将双脚并拢。 不要在大树底下避雨。 不要在水体边(江、河、湖、海、塘、渠等)、洼地及山顶、楼顶上停留。 不要拿着金属物品及接打手机。 不要触摸或者靠近防雷接地线，自来水管、家用电器的接地线。 【解析】 雷雨天气：最大的危害来自于打雷，部分地区遇到强暴雨会有山洪灾害。 蹲下，降低自己的高度，同时将双脚并拢，如下图穿红衣服的人踩在行李上，可以隔绝自己和地面的联系。 不要在大树底下避雨，因为树容易引雷。 不要在水体边(江、河、湖、海、塘、渠等)、洼地及山顶、楼顶上停留，因为水可以导电。 不要拿着金属物品及接打手机。 不要触摸或者靠近防雷接地线，自来水管（钢铁材质）、家用电器的接地线。 在野外，如果遇到雷雨天气，可以选择躲在汽车里。 4.1.1 试题 【例 14】（2013 山东）遇到雷电天气时，下列说法不正确的是（ ）。 A.要及时从汽车中逃离 B.在建筑物中应远离电话、电视等 C.要远离单独的高大树木 D.在野外可以蜷缩在地面较低的区域 1【解析】14.选非题。A 项错误：汽车是密闭的环境，可以有效防雷击。 【选A】 4.2 火灾自救4.2.1 自救注意事项 不要贪恋财物。 受到火势威胁时，要当机立断披上浸湿的衣物、被褥等向安全出口方向冲出去。 穿过浓烟逃生时，要尽量使身体贴近地面，并用湿毛巾捂住口鼻。 遇火灾不可乘坐电梯，要向安全出口方向逃生。 室外着火，门已发烫，千万不要开门，以防大火蹿入室内，要用浸湿的被褥、衣物等堵塞门窗缝，并泼水降温。 若所在逃生线路被大火封锁，要立即退回室内，用打手电筒、挥舞衣物、呼叫等方式向窗外发送求救信号，等待救援。 千万不要盲目跳楼，可利用疏散楼梯、阳台、落水管等逃生自救；也可用绳子把床单、被套撕成条状连成绳索，紧拴在窗框、暖气管、铁栏杆等固定物上，用毛巾、布条等保护手心，顺绳滑下，或下到未着火的楼层脱离险境。 【解析】 火灾自救注意事项： 不要贪恋财物。 受到火势威胁时，要当机立断披上浸湿的衣物、被褥等向安全出口方向冲出去。 重点：穿过浓烟逃生时，要尽量使身体贴近地面，并用湿毛巾捂住口鼻。发生火灾之后，很多人都是被浓烟呛死的，烟比较轻，会自上而下的扩散，因此匍匐前进或者是趴着/贴着地面前进可以保证视线相对清晰，看清楚路线，且底部有足够的空气，不至于被呛死。 遇火灾不可乘坐电梯，要向安全出口方向逃生，因为发生火灾可能会造成电路短路，容易困在电梯里。 室外着火，门已发烫，千万不要开门，以防大火蹿入室内，因为大火之后会有气体，内外会产生气压差，要用浸湿的被褥、衣物等堵塞门窗缝，并泼水降温。 若所在逃生线路被大火封锁，要立即退回室内，用打手电筒、挥舞衣物、呼叫等方式向窗外发送求救信号，等待救援。 千万不要盲目跳楼，可利用疏散楼梯、阳台、落水管等逃生自救；也可用绳子把床单、被套撕成条状连成绳索（确保结实），紧拴在窗框、暖气管、铁栏杆等固定物上，用毛巾、布条等保护手心，顺绳滑下，或下到未着火的楼层脱离险境。 注意：因为燃气泄漏而导致的火灾，也要遵守以上原则，不要吸烟、不要开灯，开灯的瞬间会有火花。 4.2.1.1 试题 【例 15】（2016 江苏 A）发生火灾时应匍匐前进以逃生，其原因的下列说法，错误的是（ ）。 A.低处有残留的新鲜空气 B.有毒有害气体集中于高处 C.浓烟自下而上扩散 D.低处视野比较清晰 1【解析】15.选非题。C 项错误：浓烟是自上而下扩散。【选 C】 4.2.2 灭火器 干粉灭火器 泡沫灭火器 二氧化碳灭火器 清水灭火器 【解析】 灭火器： 干粉灭火器：内部装有磷酸铵盐等，呈粉末状，具有易流动、干燥性，由无机盐和干燥剂组成，可以有效扑灭刚刚发生的火灾，考试出现的几率一般。 泡沫灭火器（如左下图）：灭火的原理是喷出大量的二氧化碳泡沫，粘附在可燃物上，使可燃物于空气隔绝，达到灭火的目的。特点：外壳由铁皮制成，里面装有两部分，一部分是碳酸氢钠、发泡剂的混合物，另外一部分是硫酸铝的水溶液，使用时要将泡沫灭火器上下颠倒，碳酸氢钠和硫酸铝的水溶液会发生反应，产生大量的二氧化碳泡沫。考试会常考泡沫灭火器使用时要上下颠倒，发生反应产生二氧化碳泡沫之后在使用。 二氧化碳灭火器：内部装的是液态二氧化碳，在加压时将二氧化碳压缩在小钢瓶中（如右下图），在灭火时喷出，有降温和隔绝空气的作用，考试也会考，主要用于扑救一些贵重的设备，一期、仪表、档案、书籍等火灾中需要挽救的物品。 清水灭火器。 4.2.2.1 试题 【例 16】（2015 山东）关于灭火器的原理和使用方法，下列说法不正确的是（ ）。 A.泡沫灭火器使用时会产生大量的二氧化碳，将燃烧物与空气隔绝 B.使用干粉灭火器时需要将灭火器倒立，使两种物质混合产生干粉 C.在使用二氧化碳灭火器时，要将手放在钢瓶的木柄上以防冻伤 D.电路失火在没切断电源之前不能使用泡沫灭火器灭火 1【解析】16.选非题。B 项错误：干粉灭火器使用时不需要上下颠倒，使用时需要上下颠倒的是泡沫灭火器。 C 项正确： 二氧化碳从液态变为气态的过程中，会吸收大量的热，周围温度就会降低。D 项正确：泡沫灭火器中有硫酸铝的水溶液，如果没切断电源使用，会造成短路。【选 B】 【例 17】（2011 联考）下列说法错误的是（ ）。 A.家用冰箱不宜存放汽油、乙醇等易燃性液体 B.使用干粉灭火器灭火时，应对准火焰的底部喷射 C.火灾现场中首先对人体造成危害的一般是烟雾和毒气 D.发现室内燃气泄漏，应打开门窗，关闭气源和大功率家用电器 1【解析】17.选非题。D 项错误：在关闭大功率家用电器时，产生火花的概率会更高。【选 D】 【例 18】（2014 四川）下列说法正确的是（ ）。 A.油锅起火立即用水扑灭，并迅速熄灭炉火 B.电路保险丝（片）熔断，用铜线代替以保护电器 C.身上着火，就地打滚，或用厚重衣物覆盖压灭火苗 D.停电的夜晚发现煤气泄漏后，点燃蜡烛查找泄漏原因 1【解析】18.A 项错误：油锅起火应该使用锅盖盖住。B 项错误铜线的熔点更低，起不到保护的作用。D 项错误：煤气泄露不应该点蜡烛。【选 C】 4.3 身体伤害应急处理4.3.1 运动扭伤运动型扭伤，如轻度足踝扭伤，应先冷敷患处，24 小时后改用热敷，用绷带缠住足踝，把脚垫高，即可减轻症状。 【解析】 运动型扭伤，如轻度足踝扭伤，应先冷敷患处，24 小时后改用热敷，运动型扭伤，先采用热敷的话容易导致血管破裂。 4.3.1.1 试题 【例 19】（2010 福建）运动中扭伤后可采取的正确措施是（ ）。 A.立即用冰袋冷敷，以防止毛细血管破裂后充血 B.立即用热水袋热敷，以促进血液循环，加快痊愈 C.立即用手揉捏受伤处，以散解淤血，防止肿胀 D.不做任何处理，等待其自然恢复 1【解析】19.B 项错误：应该先冷敷，24 小时以后在热敷。C 项错误：揉捏受伤处， 也会使毛细血管破裂。 D 项错误： 应该先冷敷， 24 小时以后再热敷。 【选A】 【例 20】（2014 河北）小张打篮球不慎扭伤脚踝，脚踝肿胀但并未划破。如果小张脚踝扭伤在 24 小时以内，以下哪种处理方法最合理（ ）。 A.立即冷敷 B.立即热敷 C.立即用 75%酒精消毒 D.立即用碘酒擦洗 1【解析】20.A 项正确：题干说的是 24 小时内，应该立即冷敷。【选 A】 4.3.2 毒蛇咬伤 不要惊慌奔走，更不要奔跑，要保持镇静，以免加速毒物的吸收和扩散。 立刻对伤口进行局部处理， 立即在伤口近心端 2～5 厘米处用绳带结扎，每 15 分钟左右放松 1 分钟，防止肢体缺血坏死。 尽快到医院急诊室进行处理，伤口切开、冲洗、吸毒和排毒。 特效解毒抗毒血清注射应用越早越好，最好选用多价抗毒血清。 对症及支持治疗，防止继发感染等。 【解析】 毒蛇咬伤应急处理： 不要惊慌奔走，更不要奔跑，要保持镇静，不要喝酒，以免加速毒物的吸收和扩散。 立刻对伤口进行局部处理，立即在伤口近心端（靠近心脏一端）2～5厘米处用绳带结扎，每 15 分钟左右放松 1 分钟，防止肢体缺血坏死。 尽快到医院急诊室进行处理，伤口切开、冲洗、吸毒和排毒。 特效解毒抗毒血清注射应用越早越好，最好选用多价抗毒血清。 对症及支持治疗，防止继发感染等。 4.3.2.1 试题 【例 21】（2009 河南）被毒蛇咬以后，不可采取的措施是（ ）。 A.立即喝酒解毒 B.用清水冲洗伤口或灼烧伤口，除去伤口毒液 C.放低上肢，用布条在咬伤处近心端 5 公分处扎紧 D.将伤口割开，放出有毒的血液 1【解析】21.选非题。A 项错误：喝酒会加速血液循环，加快毒物吸收和扩散。B 项正确：灼烧可以使蛋白质变性，可以达到消毒的作用。【选 A】 【例 22】（2016 吉林）当遇到下列情况时，你的正确选择是（ ）。 A.被农药污染的食品一定要充分煮熟再食用 B.被毒蛇咬伤手臂后，应首先扎住伤口处的近心端 C.进行人工呼吸过程中，吹气者应始终捏紧被救者的鼻孔 D.处方药和非处方药都必须在医生的指导下购买和使用 1【解析】22.A 项错误：被农药污染的食品不要食用。C 项错误：始终捏紧被救者的鼻孔会导致被救者呼吸不畅。D 项错误：非处方药可以自行购买和使用。【选 B】 4.4 总结 【注意】 雷雨天气： 蹲下,同时将双脚并拢。 不要在大树底下避雨。 不要在水体边、洼地及山顶、楼顶上停留。 不要拿着金属物品及接打手机。 汽车内是安全的。 火灾自救： 不要贪恋财物。 穿过浓烟逃生时，要尽量使身体贴近地面遇火灾。 不可乘坐电梯。 灭火器： 干粉灭火器。 泡沫灭火器：上下颠倒使用。 二氧化碳灭火器：扑救贵重物品，如档案、仪器、仪表等。 身体伤害应急处理： 运动扭伤：先冷敷，24 小时以后再热敷。 毒蛇咬伤： 不要奔跑，要保持镇静，不要喝酒。 近心端 2-5 厘米处用绳带结扎。 最好选用多价抗毒血清。","tags":[{"name":"公务员","slug":"公务员","permalink":"https://wgy1993.gitee.io/tags/%E5%85%AC%E5%8A%A1%E5%91%98/"},{"name":"常识","slug":"常识","permalink":"https://wgy1993.gitee.io/tags/%E5%B8%B8%E8%AF%86/"},{"name":"生活常识","slug":"生活常识","permalink":"https://wgy1993.gitee.io/tags/%E7%94%9F%E6%B4%BB%E5%B8%B8%E8%AF%86/"}]},{"title":"SpringCloud(四)","date":"2020-11-16T11:30:47.000Z","path":"archives/ebe8f13b.html","text":"1. Spring Cloud Stream在实际的企业开发中，消息中间件是至关重要的组件之一。消息中间件主要解决应用解耦，异步消息，流量削锋等问题，实现高性能，高可用，可伸缩和最终一致性架构。不同的中间件其实现方式，内部结构是不一样的。如常见的RabbitMQ和Kafka，由于这两个消息中间件的架构上的不同，像RabbitMQ有exchange，kafka有Topic，partitions分区，这些中间件的差异性导致我们实际项目开发给我们造成了一定的困扰，我们如果用了两个消息队列的其中一种，后面的业务需求，我想往另外一种消息队列进行迁移，这时候无疑就是一个灾难性的，一大堆东西都要重新推倒重新做，因为它跟我们的系统耦合了，这时候 springcloud Stream 给我们提供了一种解耦合的方式。 1.1 概述Spring Cloud Stream由一个中间件中立的核组成。应用通过Spring Cloud Stream插入的input(相当于消费者consumer，它是从队列中接收消息的)和output(相当于生产者producer，它是从队列中发送消息的。)通道与外界交流。通道通过指定中间件的Binder实现与外部代理连接。业务开发者不再关注具体消息中间件，只需关注Binder对应用程序提供的抽象概念来使用消息中间件实现业务即可。 说明：最底层是消息服务，中间层是绑定层，绑定层和底层的消息服务进行绑定，顶层是消息生产者和消息消费者，顶层可以向绑定层生产消息和和获取消息消费 1.2 核心概念1.2.1 绑定器Binder 绑定器是Spring Cloud Stream中一个非常重要的概念。在没有绑定器这个概念的情况下，我们的Spring Boot应用要直接与消息中间件进行信息交互的时候，由于各消息中间件构建的初衷不同，它们的实现细节上会有较大的差异性，这使得我们实现的消息交互逻辑就会非常笨重，因为对具体的中间件实现细节有太重的依赖，当中间件有较大的变动升级、或是更换中间件的时候，我们就需要付出非常大的代价来实施。 通过定义绑定器作为中间层，实现了应用程序与消息中间件(Middleware)细节之间的隔离。通过向应用程序暴露统一的Channel通过，使得应用程序不需要再考虑各种不同的消息中间件的实现。当需要升级消息中间件，或者是更换其他消息中间件产品时，我们需要做的就是更换对应的Binder绑定器而不需要修改任何应用逻辑 。甚至可以任意的改变中间件的类型而不需要修改一行代码。 Spring Cloud Stream支持各种binder实现，下表包含GitHub项目的链接。 RabbitMQ Apache Kafka Amazon Kinesis Google PubSub (partner maintained) Solace PubSub+ (partner maintained) Azure Event Hubs (partner maintained) 通过配置把应用和spring cloud stream 的 binder 绑定在一起，之后我们只需要修改 binder 的配置来达到动态修改topic、exchange、type等一系列信息而不需要修改一行代码。 1.2.2 发布/订阅模型在Spring Cloud Stream中的消息通信方式遵循了发布-订阅模式，当一条消息被投递到消息中间件之后，它会通过共享的 Topic 主题进行广播，消息消费者在订阅的主题中收到它并触发自身的业务逻辑处理。这里所提到的 Topic 主题是Spring Cloud Stream中的一个抽象概念，用来代表发布共享消息给消费者的地方。在不同的消息中间件中， Topic 可能对应着不同的概念，比如：在RabbitMQ中的它对应了Exchange、而在Kakfa中则对应了Kafka中的Topic。 1.3 入门案例1.3.1 准备工作案例中通过rabbitMQ作为消息中间件，完成SpringCloud Stream的案例。需要自行安装 参考：https://wgy1993.gitee.io/archives/b543ced0.html 1.3.2 消息生产者1.3.2.1 创建工程引入依赖1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.3.2.2 定义bingding发送消息时需要定义一个接口，不同的是接口方法的返回对象是 MessageChannel，下面是 Spring Cloud Stream 内置的接口： 12345678public interface Source &#123; String OUTPUT = \"output\"; @Output(Source.OUTPUT) MessageChannel output();&#125; 这就接口声明了一个 binding 命名为 “output”。这个binding 声明了一个消息输出流，也就是消息的生产者。 1.3.2.3 配置application.yml123456789101112131415161718server: port: 7001 #服务端口spring: application: name: stream_producer #指定服务名 rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test cloud: stream: bindings: output: destination: wgy-default #指定消息发送的目的地,在rabbitmq中,发送到一个wgy-default的exchange中 binders: #配置绑定器 defaultRabbit: type: rabbit 1.3.2.4 测试发送消息12345678910111213141516171819202122232425262728293031/** * 启动类 * 入门案例: * 1.引入依赖 * 2.配置application.yml文件 * 3.发送消息的话,定义一个通道接口,通过接口中内置的messagechannel * SpringCloudStream中内置接口 Source * 4.@EnableBinding : 绑定对应通道 * 5.发送消息的话,通过MessageChannel发送消息 * 如果需要MessageChannel --&gt; 通过绑定的内置接口获取 * * @author wgy */@SpringBootApplication@EnableBinding(Source.class)public class ProducerApplicationDemo implements CommandLineRunner &#123; @Autowired private MessageChannel output; public static void main(String[] args) &#123; SpringApplication.run(ProducerApplicationDemo.class); &#125; @Override public void run(String... args) throws Exception &#123; //发送MQ消息 //messagesBuilder:工具类：创建消息 output.send(MessageBuilder.withPayload(\"hello world\").build()); &#125;&#125; 1.3.3 消息消费者1.3.3.1 创建工程引入依赖1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.3.3.2 定义bingding同发送消息一致，在Spring Cloud Stream中接受消息，需要定义一个接口，如下是内置的一个接口。 12345678public interface Sink &#123; String INPUT = \"input\"; @Input(Sink.INPUT) SubscribableChannel input();&#125; 注释 @Input 对应的方法，需要返回 SubscribableChannel ，并且参入一个参数值。 这就接口声明了一个 binding 命名为 “input” 。 1.3.3.3 配置application.yml123456789101112131415161718server: port: 7002 #服务端口spring: application: name: rabbitmq-consumer #指定服务名 rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test cloud: stream: bindings: input: #内置的获取消息的通道 , 从wgy-default中获取消息 destination: wgy-default binders: defaultRabbit: type: rabbit 1.3.3.4 测试123456789101112131415161718192021222324252627282930/** * 启动类 * 入门案例: * 1.引入依赖 * 2.配置application.yml * 3.需要配置一个通道的接口 * 内置获取消息的通道接口 sink * 4.绑定通道 * 5.配置一个监听方法 : 当程序从中间件获取数据之后,执行的业务逻辑方法 * 需要在监听方法上配置@StreamListener * * @author wgy */@SpringBootApplication@EnableBinding(Sink.class)public class ConsumerApplicationDemo &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerApplicationDemo.class); &#125; /** * 监听binding中的消息 * @param message */ @StreamListener(Sink.INPUT) public void input(String message) &#123; System.out.println(\"获取到消息：\" + message); &#125;&#125; 1.3.4 定义工具类1.3.4.1 消息生产者123456789101112131415161718192021/** * 负责向中间件发送数据 * * @author wgy */@Component@EnableBinding(Source.class)public class MessageSender &#123; @Autowired private MessageChannel output; /** * 发送消息 * * @param obj */ public void send(Object obj) &#123; output.send(MessageBuilder.withPayload(obj).build()); &#125;&#125; 1.3.4.2 消息消费者12345678910111213141516171819/** * 负责向中间件获取数据 * * @author wgy */@Component@EnableBinding(Sink.class)public class MessageListener &#123; /** * 监听binding中的消息 * * @param message */ @StreamListener(Sink.INPUT) public void input(String message) &#123; System.out.println(\"获取到消息：\" + message); &#125;&#125; 1.3.4.3 测试1234567891011121314151617/** * 测试类 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@SpringBootTestpublic class ProducerTest &#123; @Autowired private MessageSender messageSender; @Test public void testSend() &#123; messageSender.send(\"hello 工具类\"); &#125;&#125; 1.4 自定义消息通道Spring Cloud Stream 内置了两种接口，分别定义了 binding 为 “input” 的输入流，和 “output” 的输出流，而在我们实际使用中，往往是需要定义各种输入输出流。使用方法也很简单。 1234567891011121314151617181920212223/** * 自定义的消息通道 * * @author wgy */public interface MyProcessor &#123; /** * 消息生产者的配置 */ String MYOUTPUT = \"myoutput\"; /** * 消息消费者的配置 */ String MYINPUT = \"myinput\"; @Output(\"myoutput\") MessageChannel myoutput(); @Input(\"myinput\") SubscribableChannel myinput();&#125; 一个接口中，可以定义无数个输入输出流，可以根据实际业务情况划分。上述的接口，定义了一个订单输入，和订单输出两个 binding。 使用时，需要在 @EnableBinding 注解中，添加自定义的接口。 使用 @StreamListener 做监听的时候，需要指定 MyProcessor.MYINPUT 1.4.1 消息生产者12345678910111213141516171819202122/** * 负责向中间件发送数据 * * @author wgy */@Component@EnableBinding(MyProcessor.class)public class MessageSender &#123; @Autowired @Qualifier(value = \"myoutput\") private MessageChannel myoutput; /** * 发送消息 * * @param obj */ public void send(Object obj) &#123; myoutput.send(MessageBuilder.withPayload(obj).build()); &#125;&#125; 1234567891011121314151617181920server: port: 7001 #服务端口spring: application: name: stream_producer #指定服务名 rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test cloud: stream: bindings: output: destination: wgy-default #指定消息发送的目的地,在rabbitmq中,发送到一个wgy-default的exchange中 myoutput: destination: wgy-custom-output binders: #配置绑定器 defaultRabbit: type: rabbit 1.4.2 消息消费者12345678910111213141516171819/** * 负责向中间件获取数据 * * @author wgy */@Component@EnableBinding(MyProcessor.class)public class MessageListener &#123; /** * 监听binding中的消息 * * @param message */ @StreamListener(MyProcessor.MYINPUT) public void input(String message) &#123; System.out.println(\"获取到消息：\" + message); &#125;&#125; 1234567891011121314151617181920server: port: 7002 #服务端口spring: application: name: rabbitmq-consumer #指定服务名 rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test cloud: stream: bindings: input: #内置的获取消息的通道 , 从wgy-default中获取消息 destination: wgy-default myinput: destination: wgy-custom-output binders: defaultRabbit: type: rabbit 1.5 消息分组通常在生产环境，我们的每个服务都不会以单节点的方式运行在生产环境，当同一个服务启动多个实例的时候，这些实例都会绑定到同一个消息通道的目标主题（Topic）上。默认情况下，当生产者发出一条消息到绑定通道上，这条消息会产生多个副本被每个消费者实例接收和处理，但是有些业务场景之下，我们希望生产者产生的消息只被其中一个实例消费，这个时候我们需要为这些消费者设置消费组来实现这样的功能。 实现的方式非常简单，我们只需要在服务消费者端设置spring.cloud.stream.bindings.input.group 属性即可，比如我们可以这样实现： 123456789101112131415161718192021server: port: 7002 #服务端口spring: application: name: rabbitmq-consumer #指定服务名 rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test cloud: stream: bindings: input: #内置的获取消息的通道 , 从wgy-default中获取消息 destination: wgy-default myinput: destination: wgy-custom-output group: group1 #设置消息的组名称（同名组中的多个消费者，只会有一个去消费消息） binders: defaultRabbit: type: rabbit 在同一个group中的多个消费者只有一个可以获取到消息并消费 1.6 消息分区有一些场景需要满足, 同一个特征的数据被同一个实例消费, 比如同一个id的传感器监测数据必须被同一个实例统计计算分析, 否则可能无法获取全部的数据。又比如部分异步任务，首次请求启动task，二次请求取消task，此场景就必须保证两次请求至同一实例. 1.6.1 消息消费者012345678910111213141516171819202122232425server: port: 7002 #服务端口spring: application: name: rabbitmq-consumer #指定服务名 rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test cloud: stream: instanceCount: 2 #消费者总数 instanceIndex: 0 #当前消费者的索引 bindings: input: #内置的获取消息的通道 , 从wgy-default中获取消息 destination: wgy-default myinput: destination: wgy-custom-output group: group1 #设置消息的组名称（同名组中的多个消费者，只会有一个去消费消息） consumer: partitioned: true #开启分区支持 binders: defaultRabbit: type: rabbit 从上面的配置中，我们可以看到增加了这三个参数： spring.cloud.stream.bindings.input.consumer.partitioned ：通过该参数开启消费者分区功能； spring.cloud.stream.instanceCount：该参数指定了当前消费者的总实例数量； spring.cloud.stream.instanceIndex ：该参数设置当前实例的索引号，从0开始，最大值为spring.cloud.stream.instanceCount 参数 - 1。我们试验的时候需要启动多个实例，可以通过运行参数来为不同实例设置不同的索引值。 1.6.2 消息消费者112345678910111213141516171819202122232425server: port: 7003 #服务端口spring: application: name: rabbitmq-consumer #指定服务名 rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test cloud: stream: instanceCount: 2 #消费者总数 instanceIndex: 1 #当前消费者的索引 bindings: input: #内置的获取消息的通道 , 从wgy-default中获取消息 destination: wgy-default myinput: destination: wgy-custom-output group: group1 #设置消息的组名称（同名组中的多个消费者，只会有一个去消费消息） consumer: partitioned: true #开启分区支持 binders: defaultRabbit: type: rabbit 1.6.3 消息生产者1234567891011121314151617181920212223server: port: 7001 #服务端口spring: application: name: stream_producer #指定服务名 rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test cloud: stream: bindings: output: destination: wgy-default #指定消息发送的目的地,在rabbitmq中,发送到一个wgy-default的exchange中 myoutput: destination: wgy-custom-output producer: partition-key-expression: payload #分区关键字 对象中的id,对象 partition-count: 2 #分区大小 binders: #配置绑定器 defaultRabbit: type: rabbit 从上面的配置中，我们可以看到增加了这两个参数： spring.cloud.stream.bindings.output.producer.partitionKeyExpression ：通过该参数指定了分区键的表达式规则，我们可以根据实际的输出消息规则来配置SpEL来生成合适的分区键； spring.cloud.stream.bindings.output.producer.partitionCount ：该参数指定了消息分区的数量。 到这里消息分区配置就完成了，我们可以再次启动这两个应用，同时消费者启动多个，但需要注意的是要为消费者指定不同的实例索引号，这样当同一个消息被发给消费组时，我们可以发现只有一个消费实例在接收和处理这些相同的消息。 2. SpringCloud Config2.1 什么是配置中心2.1.1 配置中心概述对于传统的单体应用而言，常使用配置文件来管理所有配置，比如SpringBoot的application.yml文件，但是在微服务架构中全部手动修改的话很麻烦而且不易维护。微服务的配置管理一般有以下需求： 集中配置管理，一个微服务架构中可能有成百上千个微服务，所以集中配置管理是很重要的。 不同环境不同配置，比如数据源配置在不同环境（开发，生产，测试）中是不同的。 运行期间可动态调整。例如，可根据各个微服务的负载情况，动态调整数据源连接池大小等 配置修改后可自动更新。如配置内容发生变化，微服务可以自动更新配置 综上所述对于微服务架构而言，一套统一的，通用的管理配置机制是不可缺少的总要组成部分。常见的做法就是通过配置服务器进行管理。 2.1.2 常见配置中心Spring Cloud Config为分布式系统中的外部配置提供服务器和客户端支持。 Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 Disconf 专注于各种「分布式系统配置管理」的「通用组件」和「通用平台」, 提供统一的「配置管理服务」包括 百度、滴滴出行、银联、网易、拉勾网、苏宁易购、顺丰科技 等知名互联网公司正在使用!「disconf」在「2015 年度新增开源软件排名 TOP 100(OSC开源中国提供)」中排名第16强。 2.2 Spring Cloud Config简介Spring Cloud Config项目是一个解决分布式系统的配置管理方案。它包含了Client和Server两个部分，server提供配置文件的存储、以接口的形式将配置文件的内容提供出去，client通过接口获取数据、并依据此数据初始化自己的应用。 Spring Cloud Config为分布式系统中的外部配置提供服务器和客户端支持。使用Config Server，您可以为所有环境中的应用程序管理其外部属性。它非常适合spring应用，也可以使用在其他语言的应用上。随着应用程序通过从开发到测试和生产的部署流程，您可以管理这些环境之间的配置，并确定应用程序具有迁移时需要运行的一切。服务器存储后端的默认实现使用git，因此它轻松支持标签版本的配置环境，以及可以访问用于管理内容的各种工具。 Spring Cloud Config服务端特性： HTTP，为外部配置提供基于资源的API（键值对，或者等价的YAML内容） 属性值的加密和解密（对称加密和非对称加密） 通过使用@EnableConfigServer在Spring boot应用中非常简单的嵌入。 Config客户端的特性（特指Spring应用） 绑定Config服务端，并使用远程的属性源初始化Spring环境。 属性值的加密和解密（对称加密和非对称加密） 2.3 Spring Cloud Config入门2.3.1 准备工作Config Server是一个可横向扩展、集中式的配置服务器，它用于集中管理应用程序各个环境下的配置，默认使用Git存储配置文件内容，也可以使用SVN存储，或者是本地文件存储。这里使用git作为学习的环境 使用GitHub时，国内的用户经常遇到的问题是访问速度太慢，有时候还会出现无法连接的情况。如果我们希望体验Git飞一般的速度，可以使用国内的Git托管服务——码云（gitee.com）。和GitHub相比，码云也提供免费的Git仓库。此外，还集成了代码质量检测、项目演示等功能。对于团队协作开发，码云还提供了项目管理、代码托管、文档管理的服务。 2.3.1.1 注册码云 2.3.1.2 创建项目config-repo2.3.1.3 上传配置文件将product_service工程的application.yml改名为product-dev.yml后上传 文件命名规则： {application}-{profile}.yml {application}-{profile}.properties application为应用名称 profile指的开发环境（用于区分开发环境，测试环境、生产环境等） 2.3.2 搭建服务端程序2.3.2.1 引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt; 2.3.2.2 配置启动类12345678910111213/** * 启动类 * * @author wgy */@SpringBootApplication@EnableConfigServer //开启配置中心服务端功能public class ConfigServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigServerApplication.class, args); &#125;&#125; 2.3.2.3 配置application.yml12345678910server: port: 10000 #服务端口spring: application: name: config-server #指定服务名 cloud: config: server: git: uri: https://gitee.com/wgy1993/config-repo.git 2.3.2.4 测试启动此微服务，可以在浏览器上，通过server端访问到git服务器上的文件 2.3.3 修改客户端程序2.3.3.1 引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt; 2.3.3.2 删除application.ymlspringboot的应用配置文件，需要通过Config-server获取，这里不再需要。 2.3.3.3 添加bootstrap.yml使用加载级别更高的 bootstrap.yml 文件进行配置。启动应用时会检查此配置文件，在此文件中指定配置中心的服务地址。会自动的拉取所有应用配置并启用 1234567spring: cloud: config: name: product #应用名称,需要对应git中配置文件名称的前半部分 profile: dev #开发环境 label: master #git中的分支 uri: http://localhost:10000 #config-server的请求地址 2.3.4 手动刷新我们已经在客户端取到了配置中心的值，但当我们修改GitHub上面的值时，服务端（Config Server）能实时获取最新的值，但客户端（Config Client）读的是缓存，无法实时获取最新值。SpringCloud已经为我们解决了这个问题，那就是客户端使用post去触发refresh，获取最新数据，需要依赖spring-boot-starter-actuator 2.3.4.1 引入依赖12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 2.3.4.2 对应的controller类加上@RefreshScope1234567891011121314151617181920212223242526/** * 商品Controller * * @author wgy */@RestController@RequestMapping(\"/product\")@RefreshScope //开启动态刷新public class ProductController &#123; @Autowired private ProductService productService; @Value(\"$&#123;name&#125;\") private String name; @RequestMapping(value = \"/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; return productService.findById(id); &#125; @RequestMapping(value = \"/test\") public String test() &#123; return name; &#125;&#125; 2.3.4.3 配置文件中开发端点12345678910111213spring: cloud: config: name: product #应用名称,需要对应git中配置文件名称的前半部分 profile: dev #开发环境 label: master #git中的分支 uri: http://localhost:10000 #config-server的请求地址#开启动态刷新的请求路径端点management: endpoints: web: exposure: include: refresh 2.3.4.4 测试在postman中访问 http://localhost:9002/actuator/refresh,使用post提交,查看数据已经发生了变化 2.4 配置中心的高可用在之前的代码中，客户端都是直接调用配置中心的server端来获取配置文件信息。这样就存在了一个问题，客户端和服务端的耦合性太高，如果server端要做集群，客户端只能通过原始的方式来路由，server端改变IP地址的时候，客户端也需要修改配置，不符合springcloud服务治理的理念。springcloud提供了这样的解决方案，我们只需要将server端当做一个服务注册到eureka中，client端去eureka中去获取配置中心server端的服务既可。 2.4.1 配置服务端改造2.4.1.1 添加依赖12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 2.4.1.2 配置文件1234567891011121314151617181920server: port: 10000 #服务端口spring: application: name: config-server #指定服务名 cloud: config: server: git: uri: https://gitee.com/wgy1993/config-repo.git#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ #多个eurekaserver之间用,隔开 instance: prefer-ip-address: true #使用ip地址注册 instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; #向注册中心中注册服务id #spring.cloud.client.ip-address:获取ip地址 #lease-renewal-interval-in-seconds: 5 #发送心跳续约间隔 #lease-expiration-duration-in-seconds: 10 #eureka client发送心跳给server端后，续约到期时间（默认90秒） 这样server端的改造就完成了。先启动eureka注册中心，在启动server端，在浏览器中访问：http://localhost:9000/ 就会看到server端已经注册了到注册中心了。 2.4.2 客户端改造12345678910111213141516171819spring: cloud: config: name: product #应用名称,需要对应git中配置文件名称的前半部分 profile: dev #开发环境 label: master #git中的分支 #通过注册中心获取config-server配置 discovery: enabled: true #开启服务发现 service-id: config-servereureka: client: service-url: defaultZone: http://localhost:9000/eureka/ #多个eurekaserver之间用,隔开 instance: prefer-ip-address: true #使用ip地址注册 instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; #向注册中心中注册服务id #spring.cloud.client.ip-address:获取ip地址 #lease-renewal-interval-in-seconds: 5 #发送心跳续约间隔 #lease-expiration-duration-in-seconds: 10 #eureka client发送心跳给server端后，续约到期时间（默认90秒） 2.5 消息总线bus在微服务架构中，通常会使用轻量级的消息代理来构建一个共用的消息主题来连接各个微服务实例，它广播的消息会被所有在注册中心的微服务实例监听和消费，也称消息总线。SpringCloud中也有对应的解决方案，SpringCloud Bus 将分布式的节点用轻量的消息代理连接起来，可以很容易搭建消息总线，配合SpringCloud config 实现微服务应用配置信息的动态更新。 根据此图我们可以看出利用Spring Cloud Bus做配置更新的步骤: 提交代码触发post请求给bus/refresh server端接收到请求并发送给Spring Cloud Bus Spring Cloud bus接到消息并通知给其它客户端 其它客户端接收到通知，请求Server端获取最新配置 全部客户端均获取到最新的配置 2.6 消息总线整合配置中心2.6.1 引入依赖1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--消息总线的依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-bus&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 2.6.2 服务端配置12345678910111213141516171819202122232425262728293031server: port: 10000 #服务端口spring: application: name: config-server #指定服务名 cloud: config: server: git: uri: https://gitee.com/wgy1993/config-repo.git rabbitmq: addresses: 192.168.142.128 username: wgy password: 123456 virtual-host: /test#开启动态刷新的请求路径端点management: endpoints: web: exposure: include: bus-refresh#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ #多个eurekaserver之间用,隔开 instance: prefer-ip-address: true #使用ip地址注册 instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; #向注册中心中注册服务id #spring.cloud.client.ip-address:获取ip地址 #lease-renewal-interval-in-seconds: 5 #发送心跳续约间隔 #lease-expiration-duration-in-seconds: 10 #eureka client发送心跳给server端后，续约到期时间（默认90秒） 2.6.3 微服务客户端配置1234567891011121314151617181920spring: cloud: config: name: product #应用名称,需要对应git中配置文件名称的前半部分 profile: dev #开发环境 label: master #git中的分支 #通过注册中心获取config-server配置 discovery: enabled: true #开启服务发现 service-id: config-server#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ #多个eurekaserver之间用,隔开 instance: prefer-ip-address: true #使用ip地址注册 instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; #向注册中心中注册服务id #spring.cloud.client.ip-address:获取ip地址 #lease-renewal-interval-in-seconds: 5 #发送心跳续约间隔 #lease-expiration-duration-in-seconds: 10 #eureka client发送心跳给server端后，续约到期时间（默认90秒） 需要在码云对应的配置文件中添加rabbitmq的配置信息 重新启动对应的eureka-server ， config-server ， product-service。配置信息刷新后，只需要向配置中心发送对应的请求，即可刷新每个客户端的配置 http://localhost:10000/actuator/bus-refresh 3. 开源配置中心Apollo Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。服务端基于Spring Boot和Spring Cloud开发，打包后可以直接运行，不需要额外安装Tomcat等应用容器。 3.1 Apollo概述Apollo（阿波罗）是携程框架部门研发的开源配置管理中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性。 正是基于配置的特殊性，所以Apollo从设计之初就立志于成为一个有治理能力的配置发布平台，目前提供了以下的特性： 统一管理不同环境、不同集群的配置 Apollo提供了一个统一界面集中式管理不同环境（environment）、不同集群（cluster）、不同命名空间（namespace）的配置。 同一份代码部署在不同的集群，可以有不同的配置，比如zookeeper的地址等 通过命名空间（namespace）可以很方便地支持多个不同应用共享同一份配置，同时还允许应用对共享的配置进行覆盖 配置修改实时生效（热发布） 用户在Apollo修改完配置并发布后，客户端能实时（1秒）接收到最新的配置，并通知到应用程序 版本发布管理 所有的配置发布都有版本概念，从而可以方便地支持配置的回滚 灰度发布 支持配置的灰度发布，比如点了发布后，只对部分应用实例生效，等观察一段时间没问题后再推给所有应用实例 权限管理、发布审核、操作审计 应用和配置的管理都有完善的权限管理机制，对配置的管理还分为了编辑和发布两个环节，从而减少人为的错误。 所有的操作都有审计日志，可以方便地追踪问题 客户端配置信息监控 可以在界面上方便地看到配置在被哪些实例使用 提供Java和.Net原生客户端 提供了Java和.Net的原生客户端，方便应用集成 支持Spring Placeholder, Annotation和Spring Boot的ConfigurationProperties，方便应用使用（需要Spring 3.1.1+） 同时提供了Http接口，非Java和.Net应用也可以方便地使用 提供开放平台API Apollo自身提供了比较完善的统一配置管理界面，支持多环境、多数据中心配置管理、权限、流程治理等特性。不过Apollo出于通用性考虑，不会对配置的修改做过多限制，只要符合基本的格式就能保存，不会针对不同的配置值进行针对性的校验，如数据库用户名、密码，Redis服务地址等 对于这类应用配置，Apollo支持应用方通过开放平台API在Apollo进行配置的修改和发布，并且具备完善的授权和权限控制 部署简单 配置中心作为基础服务，可用性要求非常高，这就要求Apollo对外部依赖尽可能地少 目前唯一的外部依赖是MySQL，所以部署非常简单，只要安装好Java和MySQL就可以让Apollo跑起来 Apollo还提供了打包脚本，一键就可以生成所有需要的安装包，并且支持自定义运行时参数 3.2 Apollo的实现方式 上图简要描述了Apollo客户端的实现原理： 客户端和服务端保持了一个长连接，从而能第一时间获得配置更新的推送。 客户端还会定时从Apollo配置中心服务端拉取应用的最新配置。 这是一个fallback机制，为了防止推送机制失效导致配置不更新 客户端定时拉取会上报本地版本，所以一般情况下，对于定时拉取的操作，服务端都会返回304 - Not Modified 定时频率默认为每5分钟拉取一次，客户端也可以通过在运行时指定System Property: apollo.refreshInterval 来覆盖，单位为分钟。 客户端从Apollo配置中心服务端获取到应用的最新配置后，会保存在内存中 客户端会把从服务端获取到的配置在本地文件系统缓存一份 在遇到服务不可用，或网络不通的时候，依然能从本地恢复配置 应用程序从Apollo客户端获取最新的配置、订阅配置更新通知 3.3 搭建Apollo服务端3.3.1 环境要求3.3.1.1 Java Apollo服务端：1.8+ Apollo客户端：1.7+ 由于需要同时运行服务端和客户端，所以建议安装Java 1.8+。 3.3.1.2 MySQL 版本要求：5.6.5+ Apollo的表结构对 timestamp 使用了多个default声明，所以需要5.6.5以上版本 3.3.2 环境搭建3.3.2.1 下载Apollo通过官网提供的下载连接下载安装包 3.3.2.2 配置数据库Apollo服务端共需要两个数据库： ApolloPortalDB 和 ApolloConfigDB ，我们把数据库、表的创建和样例数据都分别准备了sql文件，只需要导入数据库即可。 注意：如果你本地已经创建过Apollo数据库，请注意备份数据。我们准备的sql文件会清空Apollo相关的表。 3.3.2.3 配置数据库连接Apollo服务端需要知道如何连接到你前面创建的数据库，所以需要编辑demo.sh，修改ApolloPortalDB和ApolloConfigDB相关的数据库连接串信息。 123456789# apollo config db infoapollo_config_db_url=jdbc:mysql://localhost:3306/ApolloConfigDB?characterEncoding=utf8apollo_config_db_username=rootapollo_config_db_password=root# apollo portal db infoapollo_portal_db_url=jdbc:mysql://localhost:3306/ApolloPortalDB?characterEncoding=utf8apollo_portal_db_username=rootapollo_portal_db_password=root 3.3.2.4 启动Apollo配置中心启动脚本会在本地启动3个服务，分别使用8070, 8080, 8090端口，请确保这3个端口当前没有被使用。 1./demo.sh start 当看到如下输出后，就说明启动成功了！ 123456789101112==== starting service ====Service logging file is ./service/apollo-service.logStarted [18357]Waiting for config service startup.......Config service started. You may visit http://localhost:8080 for service status now!Waiting for admin service startup.Admin service started==== starting portal ====Portal logging file is ./portal/apollo-portal.logStarted [18618]Waiting for portal startup....Portal started. You can visit http://localhost:8070 now! 3.3.2.5 测试通过浏览器打开 http://ip:8070 即可访问Apollo配置中心的前端页面 输入默认用户名密码apollo/admin即可登录到应用中 3.4 客户端集成3.4.1 引入依赖Apollo的客户端jar包已经上传到中央仓库，应用在实际使用时只需要按照如下方式引入即可。 123456&lt;!--apollo客户端依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.ctrip.framework.apollo&lt;/groupId&gt; &lt;artifactId&gt;apollo-client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt;&lt;/dependency&gt; 3.4.2 配置文件123456789101112131415161718192021server: port: 9001 #端口spring: application: name: service-product #服务名称 datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/shop?useUnicode=true&amp;characterEncoding=utf8 username: root password: root jpa: database: MySQL show-sql: true open-in-view: truename: zhangsanapollo: bootstrap: #开启apollo enabled: true meta: http://192.168.142.128:8080app: id: product-service #指定apollo配置中心中的appid","tags":[{"name":"分布式架构方案","slug":"分布式架构方案","permalink":"https://wgy1993.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://wgy1993.gitee.io/tags/SpringCloud/"}]},{"title":"SpringCloud(三)","date":"2020-11-11T12:59:21.000Z","path":"archives/11ab1e34.html","text":"1. 微服务网关概述在学习完前面的知识后，微服务架构已经初具雏形。但还有一些问题：不同的微服务一般会有不同的网络地址，客户端在访问这些微服务时必须记住几十甚至几百个地址，这对于客户端方来说太复杂也难以维护。如下图： 如果让客户端直接与各个微服务通讯，可能会有很多问题： 客户端会请求多个不同的服务，需要维护不同的请求地址，增加开发难度 在某些场景下存在跨域请求的问题 加大身份认证的难度，每个微服务需要独立认证 因此，我们需要一个微服务网关，介于客户端与服务器之间的中间层，所有的外部请求都会先经过微服务网关。客户端只需要与网关交互，只知道一个网关地址即可，这样简化了开发还有以下优点： 易于监控 易于认证 减少了客户端与各个微服务之间的交互次数 1.1 服务网关的概念1.1.1 什么是微服务网关API网关是一个服务器，是系统对外的唯一入口。API网关封装了系统内部架构，为每个客户端提供一个定制的API。API网关方式的核心要点是，所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有的非业务功能。通常，网关也是提供REST/HTTP的访问API。服务端通过API-GW注册和管理服务。 1.1.2 作用和应用场景网关具有的职责，如身份验证、监控、负载均衡、缓存、请求分片与管理、静态响应处理。当然，最主要的职责还是与“外界联系”。 1.2 常见的API网关实现方式 Kong 基于Nginx+Lua开发，性能高，稳定，有多个可用的插件(限流、鉴权等等)可以开箱即用。问题：只支持Http协议；二次开发，自由扩展困难；提供管理API，缺乏更易用的管控、配置方式。 Zuul Netflix开源，功能丰富，使用JAVA开发，易于二次开发；需要运行在web容器中，如Tomcat。问题：缺乏管控，无法动态配置；依赖组件较多；处理Http请求依赖的是Web容器，性能不如Nginx； Traefik Go语言开发；轻量易用；提供大多数的功能：服务路由，负载均衡等等；提供WebUI问题：二进制文件部署，二次开发难度大；UI更多的是监控，缺乏配置、管理能力； Spring Cloud Gateway SpringCloud提供的网关服务 Nginx+lua实现 使用Nginx的反向代理和负载均衡可实现对api服务器的负载均衡及高可用问题：自注册的问题和网关本身的扩展性 1.3 基于Ngi inx的网关实现1.3.1 Nginx介绍 1.3.2 正向/反向代理1.3.2.1 正向代理 正向代理，”它代理的是客户端，代客户端发出请求”，是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。 1.3.2.2 反向代理 多个客户端给服务器发送的请求，Nginx服务器接收到之后，按照一定的规则分发给了后端的业务处理服务器进行处理了。此时~请求的来源也就是客户端是明确的，但是请求具体由哪台服务器处理的并不明确了，Nginx扮演的就是一个反向代理角色。客户端是无感知代理的存在的，反向代理对外都是透明的，访问者并不知道自己访问的是一个代理。因为客户端不需要任何配置就可以访问。反向代理，”它代理的是服务端，代服务端接收请求”，主要用于服务器集群分布式部署的情况下，反向代理隐藏了服务器的信息 如果只是单纯的需要一个最基础的具备转发功能的网关，那么使用Ngnix是一个不错的选择。 1.3.3 准备工作启动 shop_service_order 微服务，单独请求地址： http://127.0.0.1:9002/ 启动 shop_service_product 微服务,单独请求地址：http://127.0.0.1:9001/ 安装ngnix。参考：https://wgy1993.gitee.io/archives/65b69107.html 1.3.4 配置Nginx的请求转发123456789#路由到订单服务location &#x2F;api-order &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:9002&#x2F;;&#125;#路由到商品服务location &#x2F;api-product &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1:9001&#x2F;;&#125; 2. 微服务网关Zuul2.1 Zuul简介ZUUL是Netflix开源的微服务网关，它可以和Eureka、Ribbon、Hystrix等组件配合使用，Zuul组件的核心是一系列的过滤器，这些过滤器可以完成以下功能： 动态路由：动态将请求路由到不同后端集群 压力测试：逐渐增加指向集群的流量，以了解性能 负载分配：为每一种负载类型分配对应容量，并弃用超出限定值的请求 静态响应处理：边缘位置进行响应，避免转发到内部集群 身份认证和安全: 识别每一个资源的验证要求，并拒绝那些不符的请求。Spring Cloud对Zuul进行了整合和增强。 Spring Cloud对Zuul进行了整合和增强 2.2 搭建Zuul网关服务器2.2.1 创建工程导入依赖在IDEA中创建ZUUL网关工程 shop_zuul_server ，并添加响应依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt;&lt;/dependency&gt; 2.2.2 编写启动类1234567891011121314/** * 启动类 * * @author wgy */@SpringBootApplication//开启zuul网关功能@EnableZuulProxypublic class ZuulServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ZuulServerApplication.class, args); &#125;&#125; 2.2.3 编写配置创建配置文件 application.yml ，并添加相应配置 12345server: port: 8080 #端口spring: application: name: api-zuul-server #服务名称 2.3 Zuul 中的路由转发最直观的理解：“路由”是指根据请求URL，将请求分配到对应的处理程序。在微服务体系中，Zuul负责接收所有的请求。根据不同的URL匹配规则，将不同的请求转发到不同的微服务处理。 12345678##路由配置zuul: routes: #以商品微服务 product-service: #路由id,随便写 path: /product-service/** #映射路径 #localhost:8080/product-service/sxxssds url: http://127.0.0.1:9001 #映射路径对应的实际微服务url地址 sensitiveHeaders: #默认zuul会屏蔽cookie，cookie不会传到下游服务，这里设置为空则取消默认的黑名单，如果设置了具体的头信息则不会传到下游服务 这里将所有请求前缀为/product-service/的请求，转发到 http://127.0.0.1:9001 配置好Zuul路由之后启动服务，在浏览器中输入 http://localhost:8080/product-service/product/1 ，即可访问到商品微服务。 2.3.1 面向服务的路由微服务一般是由几十、上百个服务组成，对于一个URL请求，最终会确认一个服务实例进行处理。如果对每个服务实例手动指定一个唯一访问地址，然后根据URL去手动实现请求匹配，这样做显然就不合理。 Zuul支持与Eureka整合开发，根据ServiceID自动的从注册中心中获取服务地址并转发请求，这样做的好处不仅可以通过单个端点来访问应用的所有服务，而且在添加或移除服务实例的时候不用修改Zuul的路由配置。 2.3.1.1 添加Eureka客户端依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 2.3.1.2 开启Eureka客户端发现功能12345678910111213141516/** * 启动类 * * @author wgy */@SpringBootApplication//开启zuul网关功能@EnableZuulProxy//eureka的服务发现@EnableDiscoveryClientpublic class ZuulServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ZuulServerApplication.class, args); &#125;&#125; 2.3.1.3 添加Eureka配置，获取服务信息12345678#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ registry-fetch-interval-seconds: 5 # 获取服务列表的周期：5s instance: prefer-ip-address: true #使用ip地址注册 2.3.1.4 修改映射配置，通过服务名称获取因为已经有了Eureka客户端，我们可以从Eureka获取服务的地址信息，因此映射时无需指定IP地址，而是通过服务名称来访问，而且Zuul已经集成了Ribbon的负载均衡功能。 1234567##路由配置zuul: routes: #以商品微服务 product-service: #路由id,随便写 path: /product-service/** #映射路径 serviceId: service-product #配置转发的微服务的服务名称 2.3.2 简化的路由配置在刚才的配置中，我们的规则是这样的： zuul.routes.&lt;route&gt;.path=/xxx/** ： 来指定映射路径。 &lt;route&gt; 是自定义的路由名 zuul.routes.&lt;route&gt;.serviceId=/product -service ：来指定服务名。 而大多数情况下，我们的 &lt;route&gt; 路由名称往往和服务名会写成一样的。因此Zuul就提供了一种简化的配置语法： zuul.routes.&lt;serviceId&gt;=&lt;path&gt; 上面的配置可以简化为一条： 12345##路由配置zuul: routes: #简化路由配置：如果路由id 和 对应的微服务的serviceId一致的话 zuul.routes.&lt;serviceId&gt;=&lt;path&gt; service-product: /product-service/** 2.3.3 默认的路由规则在使用Zuul的过程中，上面讲述的规则已经大大的简化了配置项。但是当服务较多时，配置也是比较繁琐的。因此Zuul就指定了默认的路由规则： 默认情况下，一切服务的映射路径就是服务名本身。 例如服务名为： service-product ，则默认的映射路径就是： /service-product/** 2.3.4 Zuul加入后的架构 2.4 Zuul 中的过滤器通过之前的学习，我们得知 Zuul它包含了两个核心功能：对请求的路由和过滤。其中路由功能负责将外部请求转发到具体的微服务实例上，是实现外部访问统一入口的基础；而过滤器功能则负责对请求的处理过程进行干预，是实现请求校验、服务聚合等功能的基础。其实，路由功能在真正运行时，它的路由映射和请求转发同样也由几个不同的过滤器完成的。所以，过滤器可以说是Zuul实现API网关功能最为核心的部件，每一个进入Zuul的HTTP请求都会经过一系列的过滤器处理链得到请求响应并返回给客户端。 2.4.1 ZuulFilter简介Zuul 中的过滤器跟我们之前使用的 javax.servlet.Filter 不一样，javax.servlet.Filter 只有一种类型，可以通过配置 urlPatterns 来拦截对应的请求。而 Zuul 中的过滤器总共有 4 种类型，且每种类型都有对应的使用场景。 PRE：这种过滤器在请求被路由之前调用。我们可利用这种过滤器实现身份验证、在集群中选择请求的微服务、记录调试信息等。 ROUTING：这种过滤器将请求路由到微服务。这种过滤器用于构建发送给微服务的请求，并使用Apache HttpClient或Netfilx Ribbon请求微服务。 POST：这种过滤器在路由到微服务以后执行。这种过滤器可用来为响应添加标准的HTTP Header、收集统计信息和指标、将响应从微服务发送给客户端等。 ERROR：在其他阶段发生错误时执行该过滤器。 Zuul提供了自定义过滤器的功能实现起来也十分简单，只需要编写一个类去实现zuul提供的接口 1234567891011public abstract ZuulFilter implements IZuulFilter&#123; abstract public String filterType(); abstract public int filterOrder(); boolean shouldFilter();// 来自IZuulFilter Object run() throws ZuulException;// IZuulFilter&#125; ZuulFilter 是过滤器的顶级父类。在这里我们看一下其中定义的4个最重要的方法 shouldFilter ：返回一个 Boolean 值，判断该过滤器是否需要执行。返回true执行，返回false不执行。 run ：过滤器的具体业务逻辑。 filterType ：返回字符串，代表过滤器的类型。包含以下4种： pre ：请求在被路由之前执行 routing ：在路由请求时调用 post ：在routing和errror过滤器之后调用 error ：处理请求时发生错误调用 filterOrder ：通过返回的int值来定义过滤器的执行顺序，数字越小优先级越高。 2.4.2 生命周期 正常流程： 请求到达首先会经过 pre类型过滤器，而后到达routing类型，进行路由，请求就到达真正的服务提供者，执行请求，返回结果后，会到达post过滤器。而后返回响应。 异常流程： 整个过程中， pre或者routing过滤器出现异常，都会直接进入error过滤器，再error处理完毕后，会将请求交给POST过滤器，最后返回给用户。 如果是 error过滤器自己出现异常，最终也会进入POST过滤器，而后返回。 如果是 POST过滤器出现异常，会跳转到error过滤器，但是与pre和routing不同的时，请求不会再到达POST过滤器了。 不同过滤器的场景： 请求鉴权：一般放在 pre类型，如果发现没有访问权限，直接就拦截了 异常处理：一般会在 error类型和post类型过滤器中结合来处理。 服务调用时长统计： pre和post结合使用。 所有内置过滤器列表： 2.4.3 自定义过滤器接下来我们来自定义一个过滤器，模拟一个登录的校验。基本逻辑：如果请求中有access-token参数，则认为请求有效，放行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * 自定义的zuul过滤器 * 继承抽象父类 * * @author wgy */@Componentpublic class LoginFilter extends ZuulFilter &#123; /** * 定义过滤器类型 * pre * routing * post * error * * @return */ @Override public String filterType() &#123; return \"pre\"; &#125; /** * 指定过滤器的执行顺序 * 返回值越小,执行顺序越高 * * @return */ @Override public int filterOrder() &#123; return 1; &#125; /** * 当前过滤器是否生效 * true : 使用此过滤器 * false : 不使用此过滤器 * * @return */ @Override public boolean shouldFilter() &#123; return true; &#125; /** * 指定过滤器中的业务逻辑 * 身份认证: * 1.所有的请求需要携带一个参数 : access-token * 2.获取request请求 * 3.通过request获取参数access-token * 4.判断token是否为空 * 4.1 token==null : 身份验证失败 * 4.2 token!=null : 执行后续操作 * 在zuul网关中,通过RequestContext的上下文对象,可以获取对象request对象 * * @return * @throws ZuulException */ @Override public Object run() throws ZuulException &#123; //System.out.println(\"执行了过滤器\"); //1.获取zuul提供的上下文对象RequestContext RequestContext ctx = RequestContext.getCurrentContext(); //2.从RequestContext中获取request HttpServletRequest request = ctx.getRequest(); //3.获取请求参数access-token String token = request.getParameter(\"access-token\"); //4.判断 if (token == null) &#123; //4.1 如果token==null ,拦截请求,返回认证失败 ctx.setSendZuulResponse(false); //返回401状态码 ctx.setResponseStatusCode(HttpStatus.UNAUTHORIZED.value()); &#125; //4.2 如果token!=null ,继续后续操作 return null; &#125;&#125; RequestContext ：用于在过滤器之间传递消息。它的数据保存在每个请求的ThreadLocal中。它用于存储请求路由到哪里、错误、HttpServletRequest、HttpServletResponse都存储在RequestContext中。RequestContext扩展了ConcurrentHashMap，所以，任何数据都可以存储在上下文中 2.5 服务网关Zuul的核心源码解析 在 Zuul中， 整个请求的过程是这样的，首先将请求给zuulservlet处理，zuulservlet中有一个zuulRunner对象，该对象中初始化了RequestContext：作为存储整个请求的一些数据，并被所有的zuulfilter共享。zuulRunner中还有 FilterProcessor，FilterProcessor作为执行所有的zuulfilter的管理器。FilterProcessor从filterloader 中获取zuulfilter，而zuulfilter是被filterFileManager所加载，并支持groovy热加载，采用了轮询的方式热加载。有了这些filter之后，zuulservelet首先执行的Pre类型的过滤器，再执行 route类型的过滤器，最后执行的是post 类型的过滤器，如果在执行这些过滤器有错误的时候则会执行error类型的过滤器。执行完这些过滤器，最终将请求的结果返回给客户端。 2.6 Zuul网关存在的问题在实际使用中我们会发现直接使用Zuul会存在诸多问题，包括： 性能问题 Zuul1x 版本本质上就是一个同步Servlet，采用多线程阻塞模型进行请求转发。简单讲，每来一个请求，Servlet容器要为该请求分配一个线程专门负责处理这个请求，直到响应返回客户端这个线程才会被释放返回容器线程池。如果后台服务调用比较耗时，那么这个线程就会被阻塞，阻塞期间线程资源被占用，不能干其它事情。我们知道Servlet容器线程池的大小是有限制的，当前端请求量大，而后台慢服务比较多时，很容易耗尽容器线程池内的线程，造成容器无法接受新的请求。 不支持任何长连接，如 websocket 2.7 Zuul网关的替换方案 Zuul2.x版本 SpringCloud Gateway 3. 微服务网关GateWayZuul 1.x 是一个基于阻塞 IO 的 API Gateway 以及 Servlet；直到 2018 年 5 月，Zuul 2.x（基于Netty，也是非阻塞的，支持长连接）才发布，但 Spring Cloud 暂时还没有整合计划。Spring Cloud Gateway 比 Zuul 1.x 系列的性能和功能整体要好。 3.1 Gateway简介3.1.1 简介Spring Cloud Gateway 是 Spring 官方基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，旨在为微服务架构提供一种简单而有效的统一的 API 路由管理方式，统一访问接口。Spring Cloud Gateway 作为 Spring Cloud 生态系中的网关，目标是替代 Netflix ZUUL，其不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/埋点，和限流等。它是基于Nttey的响应式开发模式。 组件 RPS(request per second) Spring Cloud Gateway Requests/sec: 32213.38 Zuul1X Requests/sec: 20800.13 上表为Spring Cloud Gateway与Zuul的性能对比，从结果可知，Spring Cloud Gateway的RPS是Zuul的1.6倍 3.1.2 核心概念 路由（route） 路由是网关最基础的部分，路由信息由一个ID、一个目的URL、一组断言工厂和一组Filter组成。如果断言为真，则说明请求URL和配置的路由匹配。 断言（predicates） Java8中的断言函数，Spring Cloud Gateway中的断言函数输入类型是Spring5.0框架中的ServerWebExchange。Spring Cloud Gateway中的断言函数允许开发者去定义匹配来自Http Request中的任何信息，比如请求头和参数等。 过滤器（filter） 一个标准的Spring webFilter，Spring Cloud Gateway中的Filter分为两种类型，分别是Gateway Filter和Global Filter。过滤器Filter可以对请求和响应进行处理。 3.2 入门案例3.2.1 入门案例3.2.1.1 创建工程导入依赖在项目中添加新的模块 shop_gateway_server ，并导入依赖 123456789&lt;!-- SpringCloudGateway的内部是通过netty+webflux实现 webflux实现和SpringMVC存在冲突 引入的限流组件是hystrix。redis底层不再使用jedis，而是lettuce。 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt; 3.2.1.2 配置启动类123456789101112/** * 启动类 * * @author wgy */@SpringBootApplicationpublic class GatewayServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(GatewayServerApplication.class, args); &#125;&#125; 3.2.1.3 编写配置文件创建 application.yml 配置文件 1234567891011121314server: port: 8080 #端口spring: application: name: api-gateway-server #服务名称 #配置SpringCloudGateway的路由 cloud: gateway: routes: #配置路由 : 路由id,路由到微服务的uri,断言(判断条件) - id: product-service #保持唯一 uri: http://127.0.0.1:9001 #目标微服务请求地址 predicates: - Path=/product/** #路由条件 path : 路由匹配条件 上面这段配置的意思是，配置了一个 id 为 product-service 的路由规则，当访问网关请求地址以 product 开头时，会自动转发到地址： http://127.0.0.1:9001/ 。配置完成启动项目即可在浏览器访问进行测试，当我们访问地址 http://localhost:8080/product/1 时会展示页面展示如下： 3.2.2 路由规则Spring Cloud Gateway 的功能很强大，前面我们只是使用了 predicates 进行了简单的条件匹配，其实Spring Cloud Gataway 帮我们内置了很多 Predicates 功能。在 Spring Cloud Gateway 中 Spring 利用Predicate 的特性实现了各种路由匹配规则，有通过 Header、请求参数等不同的条件来进行作为条件匹配到对应的路由。 示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#请求时间满足在配置时间之后spring: cloud: gateway: routes: - id: after_route uri: https://xxxx.com predicates: - After=xxxxx #请求时间满足在配置时间之前spring: cloud: gateway: routes: - id: before_route uri: https://xxxx.com predicates: - Before=xxxxx #请求时间满足在配置时间之间spring: cloud: gateway: routes: - id: between_route uri: https://xxxx.com predicates: - Between=xxxx,xxxx #请求指定Cookie正则匹配指定值,此predicate匹配给定名称(chocolate)和正则表达式(ch.p)spring: cloud: gateway: routes: - id: cookie_route uri: https://xxxx.com predicates: - Cookie=chocolate,ch.p#请求指定Header正则匹配指定值,header名称匹配X-Request-Id,且正则表达式匹配\\d+ spring: cloud: gateway: routes: - id: header_route uri: https://xxxx.com predicates: - Header=X-Request-Id,\\d+ #请求Host匹配指定值,匹配下面Host主机列表,**代表可变参数spring: cloud: gateway: routes: - id: host_route uri: https://xxxx.com predicates: - Host=**.somehost.org,**.anotherhost.org #请求Method匹配配置的method,匹配的是请求的HTTP方法spring: cloud: gateway: routes: - id: method_route uri: https://xxxx.com predicates: - Method=GET #请求路径正则匹配指定值,&#123;segment&#125;为可变参数spring: cloud: gateway: routes: - id: path_route uri: https://xxxx.com predicates: - Path=/foo/&#123;segment&#125;,/bar/&#123;segment&#125; #请求查询参数正则匹配指定值,将请求的参数param(baz)进行匹配，也可以进行regexp正则表达式匹配 (参数包含foo,并且foo的值匹配ba.)spring: cloud: gateway: routes: - id: query_route uri: https://xxxx.com predicates: - Query=baz 或 Query=foo,ba. #请求远程地址正则匹配指定值,将匹配192.168.1.1~192.168.1.254之间的ip地址，其中24为子网掩码位数即255.255.255.0spring: cloud: gateway: routes: - id: remoteaddr_route uri: https://xxxx.com predicates: - RemoteAddr=192.168.1.1/24 3.2.3 动态路由和zuul网关类似，在SpringCloud GateWay中也支持动态路由：即自动的从注册中心中获取服务列表并访问。 3.2.3.1 添加注册中心依赖在工程的pom文件中添加注册中心的客户端依赖（这里以Eureka为例） 12345&lt;!-- eureka --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 3.2.3.2 配置动态路由修改 application.yml 配置文件，添加eureka注册中心的相关配置，并修改访问映射的URL为服务名称 12345678910111213141516171819202122server: port: 8080 #端口spring: application: name: api-gateway-server #服务名称 #配置SpringCloudGateway的路由 cloud: gateway: routes: #配置路由 : 路由id,路由到微服务的uri,断言(判断条件) - id: product-service #保持唯一 uri: lb://service-product #lb:// 根据微服务名称从注册中心中拉取服务请求路径 predicates: - Path=/product/** #路由条件 path : 路由匹配条件#eureka注册中心eureka: client: service-url: defaultZone: http://localhost:9000/eureka/ registry-fetch-interval-seconds: 5 # 获取服务列表的周期：5s instance: prefer-ip-address: true #使用ip地址注册 3.2.4 重写转发路径在SpringCloud Gateway中，路由转发是直接将匹配的路由path直接拼接到映射路径（URI）之后，那么在微服务开发中往往没有那么便利。这里就可以通过RewritePath机制来进行路径重写。 3.2.4.1 案例改造修改 application.yml ，将匹配路径改为 /product -service/** 123456789101112spring: application: name: api-gateway-server #服务名称 #配置SpringCloudGateway的路由 cloud: gateway: routes: #配置路由 : 路由id,路由到微服务的uri,断言(判断条件) - id: product-service #保持唯一 uri: lb://service-product #lb:// 根据微服务名称从注册中心中拉取服务请求路径 predicates: - Path=/product-service/** #路由条件 path : 路由匹配条件 重新启动网关，我们在浏览器访问 http://127.0.0.1:8080/product-service/product/1，会抛出404。这是由于路由转发规则默认转发到商品微服务（ http://127.0.0.1:9001/product-service/product/1 ）路径上，而商品微服务又没有 product -service 对应的映射配置。 3.2.4.2 添加RewritePath重写转发路径修改 application.yml ，添加重写规则。 1234567891011121314spring: application: name: api-gateway-server #服务名称 #配置SpringCloudGateway的路由 cloud: gateway: routes: #配置路由 : 路由id,路由到微服务的uri,断言(判断条件) - id: product-service #保持唯一 uri: lb://service-product #lb:// 根据微服务名称从注册中心中拉取服务请求路径 predicates: - Path=/product-service/** #将当前请求转发到 http://127.0.0.1:9001/product/1 filters: #配置路由过滤器 http://localhost:8080/product-service/product/1 --&gt; http://127.0.0.1:9001/product/1 - RewritePath=/product-service/(?&lt;segment&gt;.*), /$\\&#123;segment&#125; # 路径重写的过滤器 ,在yml中 $ 写为 $\\ 通过 RewritePath配置重写转发的url，将/product-service/(?.*)，重写为{segment}，然后转发到商品微服务。比如在网页上请求 http://localhost:8080/product-service/product，此时会将请求转发到http://127.0.0.1:9001/product/1（ 值得注意的是在 yml文档中 $ 要写成 $\\ ） 3.2.4.3 微服务名路由转发12345678spring: cloud: gateway: #配置自动的根据微服务名称进行路由转发 http://localhost:8080/service-product/product/1 discovery: locator: enabled: true #开启根据服务名称自动转发 lower-case-service-id: true #微服务名称已小写形式呈现 3.3 过滤器Spring Cloud Gateway除了具备请求路由功能之外，也支持对请求的过滤。通过Zuul网关类似，也是通过过滤器的形式来实现的。那么接下来我们一起来研究一下Gateway中的过滤器 3.3.1 过滤器基础3.3.1.1 过滤器的生命周期Spring Cloud Gateway 的 Filter 的生命周期不像 Zuul 的那么丰富，它只有两个：“pre” 和 “post”。 PRE ： 这种过滤器在请求被路由之前调用。我们可利用这种过滤器实现身份验证、在集群中选择请求的微服务、记录调试信息等。 POST ：这种过滤器在路由到微服务以后执行。这种过滤器可用来为响应添加标准的 HTTP Header、收集统计信息和指标、将响应从微服务发送给客户端等。 3.3.1.2 过滤器类型Spring Cloud Gateway 的 Filter 从作用范围可分为另外两种GatewayFilter 与 GlobalFilter。 GatewayFilter ：应用到单个路由或者一个分组的路由上。 GlobalFilter ：应用到所有的路由上。 3.3.2 局部过滤器局部过滤器（GatewayFilter），是针对单个路由的过滤器。可以对访问的URL过滤，进行切面处理。在Spring Cloud Gateway中通过GatewayFilter的形式内置了很多不同类型的局部过滤器。这里简单将Spring Cloud Gateway内置的所有过滤器工厂整理成了一张表格，虽然不是很详细，但能作为速览使用。如下： 过滤器工厂 作用 参数 AddRequestHeader 为原始请求添加Header Header的名称及值 AddRequestParameter 为原始请求添加请求参数 参数名称及值 AddResponseHeader 为原始响应添加Header Header的名称及值 DedupeResponseHeader 剔除响应头中重复的值 需要去重的Header名称及去重策略 Hystrix 为路由引入Hystrix的断路器保护 HystrixCommand 的名称 FallbackHeaders 为fallbackUri的请求头中添加具体的异常信息 Header的名称 PrefixPath 为原始请求路径添加前缀 前缀路径 PreserveHostHeader 为请求添加一个preserveHostHeader=true的属性，路由过滤器会检查该属性以决定是否要发送原始的Host 无 RequestRateLimiter 用于对请求限流，限流算法为令牌桶 keyResolver、rateLimiter、statusCode、denyEmptyKey、emptyKeyStatus RedirectTo 将原始请求重定向到指定的URL http状态码及重定向的url RemoveHopByHopHeadersFilter 为原始请求删除IETF组织规定的一系列Header 默认就会启用，可以通过配置指定仅删除哪些Header RemoveRequestHeader 为原始请求删除某个Header Header名称 RemoveResponseHeader 为原始响应删除某个Header Header名称 RewritePath 重写原始的请求路径 原始路径正则表达式以及重写后路径的正则表达式 RewriteResponseHeader 重写原始响应中的某个Header Header名称，值的正则表达式，重写后的值 SaveSession 在转发请求之前，强制执行WebSession::save 操作 无 secureHeaders 为原始响应添加一系列起安全作用的响应头 无，支持修改这些安全响应头的值 SetPath 修改原始的请求路径 修改后的路径 SetResponseHeader 修改原始响应中某个Header的值 Header名称，修改后的值 SetStatus 修改原始响应的状态码 HTTP 状态码，可以是数字，也可以是字符串 StripPrefix 用于截断原始请求的路径 使用数字表示要截断的路径的数量 Retry 针对不同的响应进行重试 retries、statuses、methods、series RequestSize 设置允许接收最大请求包的大小。如果请求包大小超过设置的值，则返回 413 Payload Too Large 请求包大小，单位为字节，默认值为5M ModifyRequestBody 在转发请求之前修改原始请求体内容 修改后的请求体内容 ModifyResponseBody 修改原始响应体的内容 修改后的响应体内容 每个过滤器工厂都对应一个实现类，并且这些类的名称必须以 GatewayFilterFactory 结尾，这是Spring Cloud Gateway的一个约定，例如 AddRequestHeader 对应的实现类为AddRequestHeaderGatewayFilterFactory 。对于这些过滤器的使用方式可以参考官方文档 3.3.3 全局过滤器全局过滤器（GlobalFilter）作用于所有路由，Spring Cloud Gateway 定义了Global Filter接口，用户可以自定义实现自己的Global Filter。通过全局过滤器可以实现对权限的统一校验，安全性验证等功能，并且全局过滤器也是程序员使用比较多的过滤器。 Spring Cloud Gateway内部也是通过一系列的内置全局过滤器对整个路由转发进行处理如下： 3.4 统一鉴权内置的过滤器已经可以完成大部分的功能，但是对于企业开发的一些业务功能处理，还是需要我们自己编写过滤器来实现的，那么我们一起通过代码的形式自定义一个过滤器，去完成统一的权限校验。 3.4.1 鉴权逻辑开发中的鉴权逻辑： 当客户端第一次请求服务时，服务端对用户进行信息认证（登录） 认证通过，将用户信息进行加密形成 token，返回给客户端，作为登录凭证 以后每次请求，客户端都携带认证的 token 服务端对 token进行解密，判断是否有效。 如上图，对于验证用户是否已经登录鉴权的过程可以在网关层统一检验。检验的标准就是请求中是否携带token凭证以及token的正确性。 3.4.2 代码实现下面的我们自定义一个GlobalFilter，去校验所有请求的请求参数中是否包含“token”，如何不包含请求参数“token”则不转发路由，否则执行正常的逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 自定义一个全局过滤器 * 实现 GlobalFilter , Ordered接口 * @author wgy */@Componentpublic class LoginFilter implements GlobalFilter, Ordered &#123; /** * 执行过滤器中的业务逻辑 * 对请求参数中的access-token进行判断 * 如果存在此参数:代表已经认证成功 * 如果不存在此参数 : 认证失败 * ServerWebExchange : 相当于请求和响应的上下文(zuul中的RequestContext) * * @param exchange * @param chain * @return */ @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; System.out.println(\"执行了自定义的全局过滤器\"); //1.获取请求参数access-token String token = exchange.getRequest().getQueryParams().getFirst(\"access-token\"); //2.判断是否存在 if(token == null) &#123; //3.如果不存在 : 认证失败 System.out.println(\"没有登录\"); exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED); return exchange.getResponse().setComplete(); //请求结束 &#125; //4.如果存在,继续执行 return chain.filter(exchange); //继续向下执行 &#125; /** * 指定过滤器的执行顺序 , 返回值越小,执行优先级越高 * * @return */ @Override public int getOrder() &#123; return 0; &#125;&#125; 自定义全局过滤器需要实现 GlobalFilter和Ordered接口。 在 filter方法中完成过滤器的逻辑判断处理 在 getOrder方法指定此过滤器的优先级，返回值越大级别越低 ServerWebExchange 就相当于当前请求和响应的上下文，存放着重要的请求-响应属性、请求实例和响应实例等等。一个请求中的request，response都可以通过 ServerWebExchange 获取 调用 chain.filter 继续向下游执行 3.5 网关限流3.5.1 常见的限流算法3.5.1.1 计数器计数器限流算法是最简单的一种限流实现方式。其本质是通过维护一个单位时间内的计数器，每次请求计数器加1，当单位时间内计数器累加到大于设定的阈值，则之后的请求都被拒绝，直到单位时间已经过去，再将计数器重置为零 3.5.1.2 漏桶算法漏桶算法可以很好地限制容量池的大小，从而防止流量暴增。漏桶可以看作是一个带有常量服务时间的单服务器队列，如果漏桶（包缓存）溢出，那么数据包会被丢弃。 在网络中，漏桶算法可以控制端口的流量输出速率，平滑网络上的突发流量，实现流量整形，从而为网络提供一个稳定的流量。 为了更好的控制流量，漏桶算法需要通过两个变量进行控制：一个是桶的大小，支持流量突发增多时可以存多少的水（burst），另一个是水桶漏洞的大小（rate）。 3.5.1.3 令牌桶算法令牌桶算法是对漏桶算法的一种改进，桶算法能够限制请求调用的速率，而令牌桶算法能够在限制调用的平均速率的同时还允许一定程度的突发调用。在令牌桶算法中，存在一个桶，用来存放固定数量的令牌。算法中存在一种机制，以一定的速率往桶中放令牌。每次请求调用需要先获取令牌，只有拿到令牌，才有机会继续执行，否则选择选择等待可用的令牌、或者直接拒绝。放令牌这个动作是持续不断的进行，如果桶中令牌数达到上限，就丢弃令牌，所以就存在这种情况，桶中一直有大量的可用令牌，这时进来的请求就可以直接拿到令牌执行，比如设置qps为100，那么限流器初始化完成一秒后，桶中就已经有100个令牌了，这时服务还没完全启动好，等启动完成对外提供服务时，该限流器可以抵挡瞬时的100个请求。所以，只有桶中没有令牌时，请求才会进行等待，最后相当于以一定的速率执行。 3.5.2 基于Filter的限流SpringCloudGateway官方就提供了基于令牌桶的限流支持。基于其内置的过滤器工厂RequestRateLimiterGatewayFilterFactory 实现。在过滤器工厂中是通过Redis和lua脚本结合的方式进行流量控制。 3.5.2.1 环境搭建导入 redis的依赖 首先在工程的pom文件中引入gateway的起步依赖和redis的reactive依赖，代码如下： 12345678910111213141516&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--监控依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--redis的依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis-reactive&lt;/artifactId&gt;&lt;/dependency&gt; 准备 redis 参考：https://wgy1993.gitee.io/archives/bae4ff13.html 3.5.2.2 修改application.yml配置文件在application.yml配置文件中加入限流的配置，代码如下： 1234567891011121314151617181920212223242526272829303132server: port: 8080 #端口spring: application: name: api-gateway-server #服务名称 redis: host: 192.168.142.128 pool: 6379 database: 0 #配置SpringCloudGateway的路由 cloud: gateway: routes: #配置路由 : 路由id,路由到微服务的uri,断言(判断条件) - id: order-service uri: lb://service-order predicates: - Path=/order-service/** filters: - name: RequestRateLimiter args: # 使用SpEL从容器中获取对象 key-resolver: '#&#123;@pathKeyResolver&#125;' # 令牌桶每秒填充平均速率 redis-rate-limiter.replenishRate: 1 # 令牌桶的上限 redis-rate-limiter.burstCapacity: 3 - RewritePath=/order-service/(?&lt;segment&gt;.*), /$\\&#123;segment&#125; # RequestRateLimiter : 使用限流过滤器,是springcloud gateway提供的 # 参数 replenishRate : 向令牌桶中填充的速率 # burstCapacity :令牌桶的容量 在 application.yml 中添加了redis的信息，并配置了RequestRateLimiter的限流过滤器： burstCapacity ，令牌桶总容量。 replenishRate ，令牌桶每秒填充平均速率。 key-resolver ，用于限流的键的解析器的 Bean 对象的名字。它使用 SpEL 表达式根据#{@beanName}从 Spring 容器中获取 Bean 对象。 3.5.2.3 配置KeyResolver为了达到不同的限流效果和规则，可以通过实现 KeyResolver 接口，定义不同请求类型的限流键。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 配置KeyResolver * 定义不同请求类型的限流键 * * @author wgy */@Configurationpublic class KeyResolverConfiguration &#123; /** * 编写基于请求路径的限流规则 * //abc * * @return */ @Bean public KeyResolver pathKeyResolver() &#123; //自定义的KeyResolver return new KeyResolver() &#123; /** * ServerWebExchange : * 上下文参数 * * @param exchange * @return */ @Override public Mono&lt;String&gt; resolve(ServerWebExchange exchange) &#123; return Mono.just(exchange.getRequest().getPath().toString()); &#125; &#125;; &#125; /** * 基于请求参数的限流 * 请求 abc ? userId=1 * * @return */ //@Bean public KeyResolver userKeyResolver() &#123; return exchange -&gt; Mono.just( exchange.getRequest().getQueryParams().getFirst(\"userId\") //exchange.getRequest().getHeaders().getFirst(\"X-Forwarded-For\") 基于请求ip的限流 ); &#125;&#125; 使用 Jmetter模拟5组线程访问，会发现如下结果，当达到令牌桶的总容量3时，其他的请求会返回429错误。 通过reids的MONITOR可以监听redis的执行过程。这时候Redis中会有对应的数据： 3.5.3 基于Sentinel的限流Sentinel 支持对 Spring Cloud Gateway、Zuul 等主流的 API Gateway 进行限流。 从 1.6.0 版本开始，Sentinel 提供了 Spring Cloud Gateway 的适配模块，可以提供两种资源维度的限流： route 维度：即在 Spring 配置文件中配置的路由条目，资源名为对应的 routeId 自定义 API 维度：用户可以利用 Sentinel 提供的 API 来自定义一些 API 分组 Sentinel 1.6.0 引入了 Sentinel API Gateway Adapter Common 模块，此模块中包含网关限流的规则和自定义 API 的实体和管理逻辑： GatewayFlowRule ：网关限流规则，针对 API Gateway 的场景定制的限流规则，可以针对不同route 或自定义的 API 分组进行限流，支持针对请求中的参数、Header、来源 IP 等进行定制化的限流。 ApiDefinition ：用户自定义的 API 定义分组，可以看做是一些 URL 匹配的组合。比如我们可以定义一个 API 叫 my_api ，请求 path 模式为 /foo/** 和 /baz/** 的都归到 my_api 这个 API分组下面。限流的时候可以针对这个自定义的 API 分组维度进行限流。 3.5.3.1 环境搭建导入Sentinel 的响应依赖 123456&lt;!--sentinel限流--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-spring-cloud-gateway-adapter&lt;/artifactId&gt; &lt;version&gt;1.6.3&lt;/version&gt;&lt;/dependency&gt; 3.5.3.2 编写配置类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * sentinel限流的配置 * * @author wgy */@Configurationpublic class GatewayConfiguration &#123; private final List&lt;ViewResolver&gt; viewResolvers; private final ServerCodecConfigurer serverCodecConfigurer; public GatewayConfiguration(ObjectProvider&lt;List&lt;ViewResolver&gt;&gt; viewResolversProvider, ServerCodecConfigurer serverCodecConfigurer) &#123; this.viewResolvers = viewResolversProvider.getIfAvailable(Collections::emptyList); this.serverCodecConfigurer = serverCodecConfigurer; &#125; /** * 配置限流的异常处理器:SentinelGatewayBlockExceptionHandler */ @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public SentinelGatewayBlockExceptionHandler sentinelGatewayBlockExceptionHandler() &#123; return new SentinelGatewayBlockExceptionHandler(viewResolvers, serverCodecConfigurer); &#125; /** * 配置限流过滤器 */ @Bean @Order(Ordered.HIGHEST_PRECEDENCE) public GlobalFilter sentinelGatewayFilter() &#123; return new SentinelGatewayFilter(); &#125; /** * 配置初始化的限流参数 * 用于指定资源的限流规则 * 1.资源名称 (路由id) * 2.配置统计时间 * 3.配置限流阈值 */ @PostConstruct public void initGatewayRules() &#123; Set&lt;GatewayFlowRule&gt; rules = new HashSet&lt;&gt;(); rules.add(new GatewayFlowRule(\"order-service\") //资源名称 .setCount(1) // 限流阈值 .setIntervalSec(1) // 统计时间窗口，单位是秒，默认是 1 秒 ); GatewayRuleManager.loadRules(rules); &#125;&#125; 基于 Sentinel 的Gateway限流是通过其提供的Filter来完成的，使用时只需注入对应的SentinelGatewayFilter 实例以及 SentinelGatewayBlockExceptionHandler 实例即可。 @PostConstruct 定义初始化的加载方法，用于指定资源的限流规则。这里资源的名称为 product-service ，统计时间是1秒内，限流阈值是1。表示每秒只能访问一个请求。 3.5.3.3 网关配置1234567891011121314151617181920server: port: 8080 #端口spring: application: name: api-gateway-server #服务名称 redis: host: 192.168.142.128 pool: 6379 database: 0 #配置SpringCloudGateway的路由 cloud: gateway: routes: #配置路由 : 路由id,路由到微服务的uri,断言(判断条件) - id: order-service uri: lb://service-order predicates: - Path=/order-service/** filters: - RewritePath=/order-service/(?&lt;segment&gt;.*), /$\\&#123;segment&#125; 在一秒钟内多次访问 http://localhost:8080/order-service/order/buy/1就可以看到限流启作用了。 3.5.3.4 自定义异常提示当触发限流后页面显示的是Blocked by Sentinel: FlowException。为了展示更加友好的限流提示，Sentinel支持自定义异常处理。 您可以在 GatewayCallbackManager 注册回调进行定制： setBlockHandler ：注册函数用于实现自定义的逻辑处理被限流的请求，对应接口为BlockRequestHandler 。默认实现为 DefaultBlockRequestHandler ，当被限流时会返回类似于下面的错误信息： Blocked by Sentinel: FlowException 。 123456789101112131415161718/** * 自定义限流处理器 */@PostConstructpublic void initBlockHandlers() &#123; BlockRequestHandler blockRequestHandler = new BlockRequestHandler() &#123; @Override public Mono&lt;ServerResponse&gt; handleRequest(ServerWebExchange serverWebExchange, Throwable throwable) &#123; Map map = new HashMap&lt;&gt;(); map.put(\"code\", 001); map.put(\"message\", \"对不起,接口限流了\"); return ServerResponse.status(HttpStatus.OK). contentType(MediaType.APPLICATION_JSON_UTF8). body(BodyInserters.fromObject(map)); &#125; &#125;; GatewayCallbackManager.setBlockHandler(blockRequestHandler);&#125; 3.5.3.5 参数限流上面的配置是针对整个路由来限流的，如果我们只想对某个路由的参数做限流，那么可以使用参数限流方式： 12345678910111213141516171819202122/** * 配置初始化的限流参数 * 用于指定资源的限流规则 * 1.资源名称 (路由id) * 2.配置统计时间 * 3.配置限流阈值 */@PostConstructpublic void initGatewayRules() &#123; Set&lt;GatewayFlowRule&gt; rules = new HashSet&lt;&gt;(); //参数限流 rules.add(new GatewayFlowRule(\"product-service\") .setCount(1) .setIntervalSec(1) .setParamItem(new GatewayParamFlowItem() .setParseStrategy(SentinelGatewayConstants.PARAM_PARSE_STRATEGY_URL_PARAM).setFieldName(\"id\") ) ); GatewayRuleManager.loadRules(rules);&#125; 通过指定 PARAM_PARSE_STRATEGY_URL_PARAM表示从url中获取参数，setFieldName指定参数名称 3.5.3.6 自定义API分组12345678910111213141516171819202122232425262728293031323334353637383940/** * 配置初始化的限流参数 * 用于指定资源的限流规则 * 1.资源名称 (路由id) * 2.配置统计时间 * 3.配置限流阈值 */@PostConstructpublic void initGatewayRules() &#123; Set&lt;GatewayFlowRule&gt; rules = new HashSet&lt;&gt;(); rules.add(new GatewayFlowRule(\"product_api\") .setCount(1) .setIntervalSec(1) ); GatewayRuleManager.loadRules(rules);&#125;/** * 自定义API限流分组 * 1.定义分组 * 2.对小组配置限流规则 */@PostConstructprivate void initCustomizedApis() &#123; Set&lt;ApiDefinition&gt; definitions = new HashSet&lt;&gt;(); ApiDefinition api1 = new ApiDefinition(\"product_api\") .setPredicateItems(new HashSet&lt;ApiPredicateItem&gt;() &#123;&#123; add(new ApiPathPredicateItem().setPattern(\"/product-service/product/**\"). //以/product-service/product/开头的所有url setMatchStrategy(SentinelGatewayConstants.URL_MATCH_STRATEGY_PREFIX)); &#125;&#125;); ApiDefinition api2 = new ApiDefinition(\"order_api\") .setPredicateItems(new HashSet&lt;ApiPredicateItem&gt;() &#123;&#123; add(new ApiPathPredicateItem().setPattern(\"/order-service/order\")); //完全匹配/order-service/order 的url &#125;&#125;); definitions.add(api1); definitions.add(api2); GatewayApiDefinitionManager.loadApiDefinitions(definitions);&#125; 3.6 网关高可用高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。我们都知道，单点是系统高可用的大敌，单点往往是系统高可用最大的风险和敌人，应该尽量在系统设计的过程中避免单点。方法论上，高可用保证的原则是“集群化”，或者叫“冗余”：只有一个单点，挂了服务会受影响；如果有冗余备份，挂了还有其他backup能够顶上。 我们实际使用 Spring Cloud Gateway 的方式如上图，不同的客户端使用不同的负载将请求分发到后端的 Gateway，Gateway 再通过HTTP调用后端服务，最后对外输出。因此为了保证 Gateway 的高可用性，前端可以同时启动多个 Gateway 实例进行负载，在 Gateway 的前端使用 Nginx 或者 F5 进行负载转发以达到高可用性。 3.6.1 准备多个GateWay工程修改 shop_gateway_server 的application.yml。添加如下配置 12345678910111213141516171819202122232425262728293031spring: application: name: api-gateway-server #服务名称 #配置SpringCloudGateway的路由 cloud: gateway: routes: #配置路由 : 路由id,路由到微服务的uri,断言(判断条件) - id: order-service uri: lb://service-order predicates: - Path=/order-service/** filters: - RewritePath=/order-service/(?&lt;segment&gt;.*), /$\\&#123;segment&#125;#eureka注册中心eureka: client: service-url: defaultZone: http://localhost:9000/eureka/ instance: prefer-ip-address: true #使用ip地址注册---spring: profiles: gateway01server: port: 8080 #服务端口---spring: profiles: gateway02server: port: 8081 #服务端口 通过不同的 profiles配置启动两个网关服务，请求端口分别为8080和8081。浏览器验证发现效果是一致的。 3.6.2 配置ngnix找到ngnix添加负载均衡配置 12345678910#集群配置upstream gateway &#123; server 127.0.0.1:8081; server 127.0.0.1:8080;&#125;#请求转向gateway定义的服务器列表location &#x2F; &#123; proxy_pass http:&#x2F;&#x2F;gateway;&#125; 在浏览器上通过访问http://localhost/order-service/order/buy/1请求的效果和之前是一样的。这次关闭一台网关服务器，还是可以支持部分请求的访问。 3.7 执行流程分析 Spring Cloud Gateway 核心处理流程如上图所示，Gateway的客户端向 Spring Cloud Gateway 发送请求，请求首先被 HttpWebHandlerAdapter 进行提取组装成网关上下文，然后网关的上下文会传递到 DispatcherHandler 。 DispatcherHandler 是所有请求的分发处理器， DispatcherHandler 主要负责分发请求对应的处理器。比如请求分发到对应的 RoutePredicateHandlerMapping （路由断言处理映射器）。路由断言处理映射器主要作用用于路由查找，以及找到路由后返回对应的FilterWebHandler 。 FilterWebHandler主要负责组装Filter链并调用Filter执行一系列的Filter处理，然后再把请求转到后端对应的代理服务处理，处理完毕之后将Response返回到Gateway客户端。 4. 微服务的链路追踪概述4.1 微服务架构下的问题在大型系统的微服务化构建中，一个系统会被拆分成许多模块。这些模块负责不同的功能，组合成系统，最终可以提供丰富的功能。在这种架构中，一次请求往往需要涉及到多个服务。互联网应用构建在不同的软件模块集上，这些软件模块，有可能是由不同的团队开发、可能使用不同的编程语言来实现、有可能布在了几千台服务器，横跨多个不同的数据中心，也就意味着这种架构形式也会存在一些问题： 如何快速发现问题？ 如何判断故障影响范围？ 如何梳理服务依赖以及依赖的合理性？ 如何分析链路性能问题以及实时容量规划？ 分布式链路追踪（Distributed Tracing），就是将一次分布式请求还原成调用链路，进行日志记录，性能监控并将 一次分布式请求的调用情况集中展示。比如各个服务节点上的耗时、请求具体到达哪台机器上、每个服务节点的请求状态等等。 目前业界比较流行的链路追踪系统如：Twitter的Zipkin，阿里的鹰眼，美团的Mtrace，大众点评的cat等，大部分都是基于google发表的 Dapper。Dapper阐述了分布式系统，特别是微服务架构中链路追踪的概念、数据表示、埋点、传递、收集、存储与展示等技术细节。 4.2 Sleuth概述4.2.1 简介Spring Cloud Sleuth 主要功能就是在分布式系统中提供追踪解决方案，并且兼容支持了 zipkin，你只需要在pom文件中引入相应的依赖即可。 4.2.2 相关概念Spring Cloud Sleuth 为Spring Cloud提供了分布式根据的解决方案。它大量借用了Google Dapper的设计。先来了解一下Sleuth中的术语和相关概念。 Spring Cloud Sleuth采用的是Google的开源项目Dapper的专业术语。 Span ：基本工作单元，例如，在一个新建的span中发送一个RPC等同于发送一个回应请求给RPC，span通过一个64位ID唯一标识，trace以另一个64位ID表示，span还有其他数据信息，比如摘要、时间戳事件、关键值注释(tags)、span的ID、以及进度ID(通常是IP地址)span在不断的启动和停止，同时记录了时间信息，当你创建了一个span，你必须在未来的某个时刻停止它。 Trace ：一系列spans组成的一个树状结构，例如，如果你正在跑一个分布式大数据工程，你可能需要创建一个trace。 Annotation ：用来及时记录一个事件的存在，一些核心annotations用来定义一个请求的开始和结束 cs - Client Sent - 客户端发起一个请求，这个annotion描述了这个span的开始 sr - Server Received - 服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳便可得到网络延迟 ss - Server Sent - 注解表明请求处理的完成(当请求返回客户端)，如果ss减去sr时间戳便可得到服务端需要的处理请求时间 cr - Client Received - 表明span的结束，客户端成功接收到服务端的回复，如果cr减去cs时间戳便可得到客户端从服务端获取回复的所有所需时间 4.3 链路追踪Sleuth入门4.3.1 配置依赖修改微服务工程引入Sleuth依赖 12345&lt;!--sleuth链路追踪--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;&lt;/dependency&gt; 4.3.2 修改配置文件修改application.yml添加日志级别 123456#日志logging: level: root: info org.springframework.web.servlet.DispatcherServlet: DEBUG org.springframework.cloud.sleuth: DEBUG 每个微服务都需要添加如上的配置。启动微服务，调用之后，我们可以在控制台观察到 sleuth的日志输出。 其中 ff8ff8b803a3b558 是TraceId，后面跟着的是SpanId，依次调用有一个全局的TraceId，将调用链路串起来。仔细分析每个微服务的日志，不难看出请求的具体过程。 查看日志文件并不是一个很好的方法，当微服务越来越多日志文件也会越来越多，通过Zipkin可以将日志聚合，并进行可视化展示和全文检索。 4.4 Zipkin的概述Zipkin 是 Twitter 的一个开源项目，它基于 Google Dapper 实现，它致力于收集服务的定时数据，以解决微服务架构中的延迟问题，包括数据的收集、存储、查找和展现。 我们可以使用它来收集各个服务器上请求链路的跟踪数据，并通过它提供的 REST API 接口来辅助我们查询跟踪数据以实现对分布式系统的监控程序，从而及时地发现系统中出现的延迟升高问题并找出系统性能瓶颈的根源。除了面向开发的 API 接口之外，它也提供了方便的 UI 组件来帮助我们直观的搜索跟踪信息和分析请求链路明细，比如：可以查询某段时间内各用户请求的处理时间等。 Zipkin 提供了可插拔数据存储方式：In-Memory、MySql、Cassandra 以及 Elasticsearch。 上图展示了 Zipkin 的基础架构，它主要由 4 个核心组件构成： Collector ：收集器组件，它主要用于处理从外部系统发送过来的跟踪信息，将这些信息转换为Zipkin 内部处理的 Span 格式，以支持后续的存储、分析、展示等功能。 Storage ：存储组件，它主要对处理收集器接收到的跟踪信息，默认会将这些信息存储在内存中，我们也可以修改此存储策略，通过使用其他存储组件将跟踪信息存储到数据库中。 RESTful API ：API 组件，它主要用来提供外部访问接口。比如给客户端展示跟踪信息，或是外接系统访问以实现监控等。 Web UI ：UI 组件，基于 API 组件实现的上层应用。通过 UI 组件用户可以方便而有直观地查询和分析跟踪信息。 Zipkin 分为两端，一个是 Zipkin 服务端，一个是 Zipkin 客户端，客户端也就是微服务的应用。客户端会配置服务端的 URL 地址，一旦发生服务间的调用的时候，会被配置在微服务里面的 Sleuth 的监听器监听，并生成相应的 Trace 和 Span 信息发送给服务端。发送的方式主要有两种，一种是 HTTP 报文的方式，还有一种是消息总线的方式如 RabbitMQ。 不论哪种方式，我们都需要： 一个 Eureka 服务注册中心，这里我们就用之前的 eureka 项目来当注册中心。 一个 Zipkin 服务端。 多个微服务，这些微服务中配置 Zipkin 客户端。 4.5 Zipkin Server的部署和配置4.5.1 Zipkin Server下载从spring boot 2.0开始，官方就不再支持使用自建Zipkin Server的方式进行服务链路追踪，而是直接提供了编译好的 jar 包来给我们使用。可以从官方网站下载先下载Zipkin的web UI，我们这里下载的是zipkin -server-2.12.9-exec.jar 4.5.2 启动在命令行输入 java -jar zipkin-server-2.12.9-exec.jar 启动 Zipkin Server 默认 Zipkin Server的请求端口为 9411 Zipkin Server 的启动参数可以通过官方提供的 yml配置文件查找 在浏览器输入 http://127.0.0.1:9411即可进入到Zipkin Server的管理后台 4.6 客户端Zipkin+Sleuth整合通过查看日志分析微服务的调用链路并不是一个很直观的方案，结合zipkin可以很直观地显示微服务之间的调用关系。 4.6.1 客户端添加依赖客户端指的是需要被追踪的微服务 12345&lt;!--zipkin依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; 4.6.2 修改客户端配置文件123456789spring: #配置zipkin采集数据 zipkin: base-url: http://127.0.0.1:9411/ #server的请求地址 sender: type: web #数据的传输方式 , 以http的形式向server端发送数据 sleuth: sampler: probability: 1 #采样比 指定了zipkin server的地址，下面制定需采样的百分比，默认为0.1，即10%，这里配置1，是记录全部的sleuth信息，是为了收集到更多的数据（仅供测试用）。在分布式系统中，过于频繁的采样会影响系统性能，所以这里配置需要采用一个合适的值。 4.6.3 测试以此启动每个微服务，启动Zipkin Service。通过浏览器发送一次微服务请求。打开 Zipkin Service控制台，我们可以根据条件追踪每次请求调用过程 单击该trace可以看到请求的细节 4.7 基于消息中间件收集数据在默认情况下，Zipkin客户端和Server之间是使用HTTP请求的方式进行通信（即同步的请求方式），在网络波动，Server端异常等情况下可能存在信息收集不及时的问题。Zipkin支持与rabbitMQ整合完成异步消息传输。 加了MQ之后，通信过程如下图所示： 4.7.1 RabbitMQ的安装与启动参考：https://wgy1993.gitee.io/archives/b543ced0.html 4.7.2 服务端启动1java -jar zipkin-server-2.12.9-exec.jar --RABBIT_ADDRESSES=192.168.142.128:5672 --RABBIT_USER=wgy --RABBIT_PASSWORD=123456 --RABBIT_VIRTUAL_HOST=/test RABBIT_ADDRESSES ： 指定RabbitMQ地址 RABBIT_USER： 用户名（默认guest） RABBIT_PASSWORD ： 密码（默认guest） RABBIT_VIRTUAL_HOST：RabbitMQ服务器（默认/） 启动Zipkin Server之后，我们打开RabbitMQ的控制台可以看到多了一个Queue 其中 zipkin 就是为我们自动创建的Queue队列 4.7.3 客户端配置4.7.3.1 配置依赖12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.amqp&lt;/groupId&gt; &lt;artifactId&gt;spring-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 导入 spring-rabbit 依赖，是Spring提供的对rabbit的封装，客户端会根据配置自动的生产消息并发送到目标队列中 4.7.3.2 配置消息中间件rabbit mq地址等信息123456789101112131415161718192021spring: #修改zipkin使用rabbitmq采集数据 zipkin: sender: type: rabbit #向rabbitmq中发送消息 sleuth: sampler: probability: 1 #采样比 rabbitmq: host: 192.168.142.128 port: 5672 username: wgy password: 123456 virtual-host: /test listener: # 这里配置了重试策略 direct: retry: enabled: true simple: retry: enabled: true 4.7.3.3 测试关闭Zipkin Server，并随意请求连接。打开rabbitmq管理后台可以看到，消息已经推送到rabbitmq。当Zipkin Server启动时，会自动的从rabbitmq获取消息并消费，展示追踪数据 可以看到如下效果： 请求的耗时时间不会出现突然耗时特长的情况 当 ZipkinServer不可用时（比如关闭、网络不通等），追踪信息不会丢失，因为这些信息会保存在Rabbitmq服务器上，直到Zipkin服务器可用时，再从Rabbitmq中取出这段时间的信息 4.8 存储跟踪数据Zipkin Server默认时间追踪数据信息保存到内存，这种方式不适合生产环境。因为一旦Service关闭重启或者服务崩溃，就会导致历史数据消失。Zipkin支持将追踪数据持久化到mysql数据库或者存储到elasticsearch中。这里已mysql为例。 4.8.1 准备数据库可以从官网找到Zipkin Server持久mysql的数据库脚本。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&#x2F;*SQLyog Ultimate v11.33 (64 bit)MySQL - 5.5.58 : Database - zipkin**********************************************************************&#x2F;&#x2F;*!40101 SET NAMES utf8 *&#x2F;;&#x2F;*!40101 SET SQL_MODE&#x3D;&#39;&#39;*&#x2F;;&#x2F;*!40014 SET @OLD_UNIQUE_CHECKS&#x3D;@@UNIQUE_CHECKS, UNIQUE_CHECKS&#x3D;0 *&#x2F;;&#x2F;*!40014 SET @OLD_FOREIGN_KEY_CHECKS&#x3D;@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS&#x3D;0 *&#x2F;;&#x2F;*!40101 SET @OLD_SQL_MODE&#x3D;@@SQL_MODE, SQL_MODE&#x3D;&#39;NO_AUTO_VALUE_ON_ZERO&#39; *&#x2F;;&#x2F;*!40111 SET @OLD_SQL_NOTES&#x3D;@@SQL_NOTES, SQL_NOTES&#x3D;0 *&#x2F;;CREATE DATABASE &#x2F;*!32312 IF NOT EXISTS*&#x2F;&#96;zipkin&#96; &#x2F;*!40100 DEFAULT CHARACTER SET utf8 *&#x2F;;USE &#96;zipkin&#96;;&#x2F;*Table structure for table &#96;zipkin_annotations&#96; *&#x2F;DROP TABLE IF EXISTS &#96;zipkin_annotations&#96;;CREATE TABLE &#96;zipkin_annotations&#96; ( &#96;trace_id_high&#96; bigint(20) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;If non zero, this means the trace uses 128 bit traceIds instead of 64 bit&#39;, &#96;trace_id&#96; bigint(20) NOT NULL COMMENT &#39;coincides with zipkin_spans.trace_id&#39;, &#96;span_id&#96; bigint(20) NOT NULL COMMENT &#39;coincides with zipkin_spans.id&#39;, &#96;a_key&#96; varchar(255) NOT NULL COMMENT &#39;BinaryAnnotation.key or Annotation.value if type &#x3D;&#x3D; -1&#39;, &#96;a_value&#96; blob COMMENT &#39;BinaryAnnotation.value(), which must be smaller than 64KB&#39;, &#96;a_type&#96; int(11) NOT NULL COMMENT &#39;BinaryAnnotation.type() or -1 if Annotation&#39;, &#96;a_timestamp&#96; bigint(20) DEFAULT NULL COMMENT &#39;Used to implement TTL; Annotation.timestamp or zipkin_spans.timestamp&#39;, &#96;endpoint_ipv4&#96; int(11) DEFAULT NULL COMMENT &#39;Null when Binary&#x2F;Annotation.endpoint is null&#39;, &#96;endpoint_ipv6&#96; binary(16) DEFAULT NULL COMMENT &#39;Null when Binary&#x2F;Annotation.endpoint is null, or no IPv6 address&#39;, &#96;endpoint_port&#96; smallint(6) DEFAULT NULL COMMENT &#39;Null when Binary&#x2F;Annotation.endpoint is null&#39;, &#96;endpoint_service_name&#96; varchar(255) DEFAULT NULL COMMENT &#39;Null when Binary&#x2F;Annotation.endpoint is null&#39;, UNIQUE KEY &#96;trace_id_high&#96; (&#96;trace_id_high&#96;,&#96;trace_id&#96;,&#96;span_id&#96;,&#96;a_key&#96;,&#96;a_timestamp&#96;) COMMENT &#39;Ignore insert on duplicate&#39;, KEY &#96;trace_id_high_2&#96; (&#96;trace_id_high&#96;,&#96;trace_id&#96;,&#96;span_id&#96;) COMMENT &#39;for joining with zipkin_spans&#39;, KEY &#96;trace_id_high_3&#96; (&#96;trace_id_high&#96;,&#96;trace_id&#96;) COMMENT &#39;for getTraces&#x2F;ByIds&#39;, KEY &#96;endpoint_service_name&#96; (&#96;endpoint_service_name&#96;) COMMENT &#39;for getTraces and getServiceNames&#39;, KEY &#96;a_type&#96; (&#96;a_type&#96;) COMMENT &#39;for getTraces&#39;, KEY &#96;a_key&#96; (&#96;a_key&#96;) COMMENT &#39;for getTraces&#39;, KEY &#96;trace_id&#96; (&#96;trace_id&#96;,&#96;span_id&#96;,&#96;a_key&#96;) COMMENT &#39;for dependencies job&#39;) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 ROW_FORMAT&#x3D;COMPRESSED;&#x2F;*Data for the table &#96;zipkin_annotations&#96; *&#x2F;&#x2F;*Table structure for table &#96;zipkin_dependencies&#96; *&#x2F;DROP TABLE IF EXISTS &#96;zipkin_dependencies&#96;;CREATE TABLE &#96;zipkin_dependencies&#96; ( &#96;day&#96; date NOT NULL, &#96;parent&#96; varchar(255) NOT NULL, &#96;child&#96; varchar(255) NOT NULL, &#96;call_count&#96; bigint(20) DEFAULT NULL, UNIQUE KEY &#96;day&#96; (&#96;day&#96;,&#96;parent&#96;,&#96;child&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 ROW_FORMAT&#x3D;COMPRESSED;&#x2F;*Data for the table &#96;zipkin_dependencies&#96; *&#x2F;&#x2F;*Table structure for table &#96;zipkin_spans&#96; *&#x2F;DROP TABLE IF EXISTS &#96;zipkin_spans&#96;;CREATE TABLE &#96;zipkin_spans&#96; ( &#96;trace_id_high&#96; bigint(20) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;If non zero, this means the trace uses 128 bit traceIds instead of 64 bit&#39;, &#96;trace_id&#96; bigint(20) NOT NULL, &#96;id&#96; bigint(20) NOT NULL, &#96;name&#96; varchar(255) NOT NULL, &#96;parent_id&#96; bigint(20) DEFAULT NULL, &#96;debug&#96; bit(1) DEFAULT NULL, &#96;start_ts&#96; bigint(20) DEFAULT NULL COMMENT &#39;Span.timestamp(): epoch micros used for endTs query and to implement TTL&#39;, &#96;duration&#96; bigint(20) DEFAULT NULL COMMENT &#39;Span.duration(): micros used for minDuration and maxDuration query&#39;, UNIQUE KEY &#96;trace_id_high&#96; (&#96;trace_id_high&#96;,&#96;trace_id&#96;,&#96;id&#96;) COMMENT &#39;ignore insert on duplicate&#39;, KEY &#96;trace_id_high_2&#96; (&#96;trace_id_high&#96;,&#96;trace_id&#96;,&#96;id&#96;) COMMENT &#39;for joining with zipkin_annotations&#39;, KEY &#96;trace_id_high_3&#96; (&#96;trace_id_high&#96;,&#96;trace_id&#96;) COMMENT &#39;for getTracesByIds&#39;, KEY &#96;name&#96; (&#96;name&#96;) COMMENT &#39;for getTraces and getSpanNames&#39;, KEY &#96;start_ts&#96; (&#96;start_ts&#96;) COMMENT &#39;for getTraces ordering and range&#39;) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 ROW_FORMAT&#x3D;COMPRESSED;&#x2F;*Data for the table &#96;zipkin_spans&#96; *&#x2F;&#x2F;*!40101 SET SQL_MODE&#x3D;@OLD_SQL_MODE *&#x2F;;&#x2F;*!40014 SET FOREIGN_KEY_CHECKS&#x3D;@OLD_FOREIGN_KEY_CHECKS *&#x2F;;&#x2F;*!40014 SET UNIQUE_CHECKS&#x3D;@OLD_UNIQUE_CHECKS *&#x2F;;&#x2F;*!40111 SET SQL_NOTES&#x3D;@OLD_SQL_NOTES *&#x2F;; 4.8.2 配置启动服务端1java -jar zipkin-server-2.12.9-exec.jar --STORAGE_TYPE=mysql --MYSQL_HOST=127.0.0.1 --MYSQL_TCP_PORT=3306 --MYSQL_DB=zipkin --MYSQL_USER=root --MYSQL_PASS=root STORAGE_TYPE : 存储类型 MYSQL_HOST： mysql主机地址 MYSQL_TCP_PORT：mysql端口 MYSQL_DB： mysql数据库名称 MYSQL_USER：mysql用户名 MYSQL_PASS ：mysql密码 配置好服务端之后，可以在浏览器请求几次。回到数据库查看会发现数据已经持久化到mysql中","tags":[{"name":"分布式架构方案","slug":"分布式架构方案","permalink":"https://wgy1993.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://wgy1993.gitee.io/tags/SpringCloud/"}]},{"title":"SpringCloud(二)","date":"2020-11-04T07:46:08.000Z","path":"archives/6f583e1f.html","text":"1. 服务调用Feign入门前面我们使用的RestTemplate实现REST API调用，代码大致如下： 123456@RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET)public Product findById(@PathVariable Long id) &#123; Product product = null; product = restTemplate.getForObject(\"http://service-product/product/1\", Product.class); return product;&#125; 由代码可知，我们是使用拼接字符串的方式构造URL的，该URL只有一个参数。但是，在现实中，URL中往往含有多个参数。这时候我们如果还用这种方式构造URL，那么就会非常痛苦。那应该如何解决？我们带着这样的问题进入到本章的学习。 1.1 Feign简介Feign是Netflix开发的声明式，模板化的HTTP客户端，其灵感来自Retrofit,JAXRS-2.0以及WebSocket. Feign可帮助我们更加便捷，优雅的调用HTTP API。 在SpringCloud中，使用Feign非常简单——创建一个接口，并在接口上添加一些注解，代码就完成了。 Feign支持多种注解，例如Feign自带的注解或者JAX-RS注解等。 SpringCloud对Feign进行了增强，使Feign支持了SpringMVC注解，并整合了Ribbon和Eureka，从而让Feign的使用更加方便。 1.2 基于Feign的服务调用1.2.1 引入依赖在服务消费者 shop_service_order 添加Fegin依赖 12345&lt;!--springcloud整合的openFeign--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 1.2.2 配置调用接口12345678910111213141516171819/** * 声明需要调用的微服务名称 * * @author wgy * @FeignClient * name : 服务提供者的名称 */@FeignClient(name = \"service-product\")public interface ProductFeignClient &#123; /** * 配置需要调用的微服务接口 * * @param id * @return */ @RequestMapping(value = \"/product/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable(\"id\") Long id);&#125; 定义各参数绑定时， @PathVariable、@RequestParam、@RequestHeader等可以指定参数属性，在Feign中绑定参数必须通过value属性来指明具体的参数名，不然会抛出异常 @FeignClient ：注解通过name指定需要调用的微服务的名称，用于创建Ribbon的负载均衡器。会通过动态代理的形式创建ProductFeignClient接口的实现类。 1.2.3 启动类激活Feign1234567891011121314/** * 启动类 * * @author wgy */@SpringBootApplication//激活Feign@EnableFeignClientspublic class OrderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderApplication.class, args); &#125;&#125; 通过@EnableFeignClients注解开启Spring Cloud Feign的支持功能 1.2.4 通过自动的接口调用远程微服务修改 OrderController ，添加ProductFeginClient的自动注入，并在order方法中使用ProductFeginClient 完成微服务调用 1234567891011121314151617181920212223/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; @Autowired private ProductFeignClient productFeignClient; /** * 通过订单系统,调用商品服务根据id查询商品信息 * * @param id * @return */ @RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; Product product = null; product = productFeignClient.findById(id); return product; &#125;&#125; 1.2.5 测试效果 1.3 Feign 和Ribbon的联系Ribbon是一个基于 HTTP 和 TCP客户端的负载均衡的工具。它可以在客户端配置RibbonServerList（服务端列表），使用 HttpClient 或 RestTemplate 模拟http请求，步骤相当繁琐。 Feign是在 Ribbon的基础上进行了一次改进，是一个使用起来更加方便的 HTTP 客户端。采用接口的方式， 只需要创建一个接口，然后在上面添加注解即可 ，将需要调用的其他服务的方法定义成抽象方法即可， 不需要自己构建http请求。然后就像是调用自身工程的方法调用，而感觉不到是调用远程方法，使得编写客户端变得非常容易 1.4 负载均衡Feign 中本身已经集成了Ribbon依赖和自动配置，因此我们不需要额外引入依赖，也不需要再注册RestTemplate 对象。另外，我们可以像上节课中讲的那样去配置Ribbon，可以通过 ribbon.xx 来进行全局配置。也可以通过 服务名.ribbon.xx 来对指定服务配置。 1234#修改ribbon的负载均衡策略 服务名 - ribbon - NFLoadBalancerRuleClassName : 策略service-product: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 2. 服务调用Feign高级2.1 Feign的配置从Spring Cloud Edgware开始，Feign支持使用属性自定义Feign。对于一个指定名称的Feign Client（例如该Feign Client的名称为 feignName ），Feign支持如下配置项： 12345678910111213feign: client: config: feignName: #定义FeginClient的名称 connectTimeout: 5000 #建立链接的超时时长 相当于Request.Options readTimeout: 5000 #读取超时时长 相当于Request.Options loggerLevel: full # 配置Feign的日志级别，相当于代码配置方式中的Logger errorDecoder: com.example.SimpleErrorDecoder # Feign的错误解码器，相当于代码配置方式中的ErrorDecoder retryer: com.example.SimpleRetryer # 配置重试，相当于代码配置方式中的Retryer requestInterceptors: # 配置请求拦截器，相当于代码配置方式中的RequestInterceptor - com.example.FooRequestInterceptor - com.example.BarRequestInterceptor decode404: false # 配置熔断不处理404异常 2.2 请求压缩Spring Cloud Feign 支持对请求和响应进行GZIP压缩，以减少通信过程中的性能损耗。通过下面的参数即可开启请求与响应的压缩功能： 123456feign: compression: request: enabled: true # 开启请求压缩 response: enabled: true # 开启响应压缩 同时，我们也可以对请求的数据类型，以及触发压缩的大小下限进行设置： 123456feign: compression: request: enabled: true # 开启请求压缩 mime-types: text/html,application/xml,application/json # 设置压缩的数据类型 min-request-size: 2048 # 设置触发压缩的大小下限 注：上面的数据类型、压缩大小下限均为默认值。 2.3 日志级别在开发或者运行阶段往往希望看到Feign请求过程的日志记录，默认情况下Feign的日志是没有开启的。要想用属性配置方式来达到日志效果，只需在 application.yml 中添加如下内容即可： 1234567891011#配置feign日志的输出#日志配置 NONE : 不输出日志(高) BASIC: 适用于生产环境追踪问题#HEADERS : 在BASIC的基础上,记录请求和响应头信息 FULL : 记录所有feign: client: config: service-product: #需要调用的服务名称 loggerLevel: FULLlogging: level: com.wgy.order.feign.ProductFeignClient: debug #Feign日志只会对日志级别为debug的做出响应 2.4 源码分析 3. 微服务架构的高并发问题通过注册中心已经实现了微服务的服务注册和服务发现，并且通过Ribbon实现了负载均衡，已经借助Feign可以优雅的进行微服务调用。那么我们编写的微服务的性能怎么样呢，是否存在问题呢？ 3.1 性能工具Jmetter Apache JMeter 是Apache组织开发的基于Java的压力测试工具。用于对软件做压力测试，它最初被设计用于Web应用测试，但后来扩展到其他测试领域。 它可以用于测试静态和动态资源，例如静态文件、Java 小服务程序、CGI 脚本、Java 对象、数据库、FTP 服务器， 等等。JMeter 可以用于对服务器、网络或对象模拟巨大的负载，来自不同压力类别下测试它们的强度和分析整体性能。另外JMeter能够对应用程序做功能/回归测试，通过创建带有断言的脚本来验证你的程序返回了你期望的结果。为了最大限度的灵活性，JMeter允许使用正则表达式创建断言。 3.1.1 安装JmetterJmetter安装十分简单，将 apache -jmeter-2.13.zip 完整压缩包解压，找到安装目录下bin/jmeter.bat 已管理员身份启动即可 3.1.2 配置Jmetter3.1.2.1 创建新的测试计划 3.1.2.2 测试计划下创建发起请求的线程组 可以配置请求的线程数 以及每个请求发送的请求次数 3.1.2.3 创建http请求模板 3.1.2.4 配置测试的接口信息 3.2 系统负载过高存在的问题3.2.1 问题分析在微服务架构中，我们将业务拆分成一个个的服务，服务与服务之间可以相互调用，由于网络原因或者自身的原因，服务并不能保证服务的100%可用，如果单个服务出现问题，调用这个服务就会出现网络延迟，此时若有大量的网络涌入，会形成任务累计，导致服务瘫痪。 在SpringBoot程序中，默认使用内置tomcat作为web服务器。单tomcat支持最大的并发请求是有限的，如果某一接口阻塞，待执行的任务积压越来越多，那么势必会影响其他接口的调用。 3.2.2 线程池的形式实现服务隔离3.2.2.1 配置坐标为了方便实现线以线程池的形式完成资源隔离，需要引入如下依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-metrics-event-stream&lt;/artifactId&gt; &lt;version&gt;1.5.12&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-javanica&lt;/artifactId&gt; &lt;version&gt;1.5.12&lt;/version&gt;&lt;/dependency&gt; 3.2.2.2 配置线程池配置HystrixCommand接口的实现类，再实现类中可以对线程池进行配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * @author wgy */public class OrderCommand extends HystrixCommand&lt;Product&gt; &#123; private RestTemplate restTemplate; private Long id; public OrderCommand(RestTemplate restTemplate, Long id) &#123; super(setter()); this.restTemplate = restTemplate; this.id = id; &#125; private static Setter setter() &#123; // 服务分组 HystrixCommandGroupKey groupKey = HystrixCommandGroupKey.Factory.asKey(\"order_product\"); // 服务标识 HystrixCommandKey commandKey = HystrixCommandKey.Factory.asKey(\"product\"); // 线程池名称 HystrixThreadPoolKey threadPoolKey = HystrixThreadPoolKey.Factory.asKey(\"order_product_pool\"); /** * 线程池配置 * withCoreSize : 线程池大小为10 * withKeepAliveTimeMinutes: 线程存活时间15秒 * withQueueSizeRejectionThreshold :队列等待的阈值为100,超过100执行拒绝策略 */ HystrixThreadPoolProperties.Setter threadPoolProperties = HystrixThreadPoolProperties.Setter().withCoreSize(50) .withKeepAliveTimeMinutes(15).withQueueSizeRejectionThreshold(100); // 命令属性配置Hystrix 开启超时 HystrixCommandProperties.Setter commandProperties = HystrixCommandProperties.Setter() // 采用线程池方式实现服务隔离 .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.THREAD) // 禁止 .withExecutionTimeoutEnabled(false); return Setter.withGroupKey(groupKey).andCommandKey(commandKey).andThreadPoolKey(threadPoolKey) .andThreadPoolPropertiesDefaults(threadPoolProperties).andCommandPropertiesDefaults(commandProperties); &#125; @Override protected Product run() throws Exception &#123; System.out.println(Thread.currentThread().getName()); return restTemplate.getForObject(\"http://127.0.0.1/product/\" + id, Product.class); &#125; /** * 降级方法 */ @Override protected Product getFallback() &#123; Product product = new Product(); product.setProductName(\"不好意思,出错了\"); return product; &#125;&#125; 3.2.2.3 配置调用修改 OrderController ，使用自定义的OrderCommand完成调用 12345678910111213141516171819202122232425/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; //注入restTemplate对象 @Autowired private RestTemplate restTemplate; /** * 使用OrderCommand调用远程服务 */ @RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; return new OrderCommand(restTemplate, id).execute(); &#125; @RequestMapping(value = \"/&#123;id&#125;\", method = RequestMethod.GET) public String findOrder(@PathVariable Long id) &#123; System.out.println(Thread.currentThread().getName()); return \"根据id查询订单\"; &#125;&#125; 4. 服务熔断Hystrix入门4.1 服务容错的核心知识4.1.1 雪崩效应在微服务架构中，一个请求需要调用多个服务是非常常见的。如客户端访问A服务，而A服务需要调用B服务，B服务需要调用C服务，由于网络原因或者自身的原因，如果B服务或者C服务不能及时响应，A服务将处于阻塞状态，直到B服务C服务响应。此时若有大量的请求涌入，容器的线程资源会被消耗完毕，导致服务瘫痪。服务与服务之间的依赖性，故障会传播，造成连锁反应，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的“雪崩”效应。 雪崩是系统中的蝴蝶效应导致其发生的原因多种多样，有不合理的容量设计，或者是高并发下某一个方法响应变慢，亦或是某台机器的资源耗尽。从源头上我们无法完全杜绝雪崩源头的发生，但是雪崩的根本原因来源于服务之间的强依赖，所以我们可以提前评估，做好熔断，隔离，限流。 4.1.2 服务隔离顾名思义，它是指将系统按照一定的原则划分为若干个服务模块，各个模块之间相对独立，无强依赖。当有故障发生时，能将问题和影响隔离在某个模块内部，而不扩散风险，不波及其它模块，不影响整体的系统服务。 4.1.3 熔断降级熔断这一概念来源于电子工程中的断路器（Circuit Breaker）。在互联网系统中，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。这种牺牲局部，保全整体的措施就叫做熔断。 所谓降级，就是当某个服务熔断之后，服务器将不再被调用，此时客户端可以自己准备一个本地的fallback回调，返回一个缺省值。 也可以理解为兜底 4.1.4 服务限流限流可以认为服务降级的一种，限流就是限制系统的输入和输出流量已达到保护系统的目的。一般来说系统的吞吐量是可以被测算的，为了保证系统的稳固运行，一旦达到的需要限制的阈值，就需要限制流量并采取少量措施以完成限制流量的目的。比方：推迟解决，拒绝解决，或者者部分拒绝解决等等。 4.2 Hystrix介绍 Hystrix 是由Netflix开源的一个延迟和容错库，用于隔离访问远程系统、服务或者第三方库，防止级联失败，从而提升系统的可用性与容错性。Hystrix主要通过以下几点实现延迟和容错。 包裹请求：使用 HystrixCommand包裹对依赖的调用逻辑，每个命令在独立线程中执行。这使用了设计模式中的“命令模式”。 跳闸机制：当某服务的错误率超过一定的阈值时， Hystrix可以自动或手动跳闸，停止请求该服务一段时间。 资源隔离： Hystrix为每个依赖都维护了一个小型的线程池（或者信号量）。如果该线程池已满，发往该依赖的请求就被立即拒绝，而不是排队等待，从而加速失败判定。 监控： Hystrix可以近乎实时地监控运行指标和配置的变化，例如成功、失败、超时、以及被拒绝的请求等。 回退机制：当请求失败、超时、被拒绝，或当断路器打开时，执行回退逻辑。回退逻辑由开发人员自行提供，例如返回一个缺省值。 自我修复：断路器打开一段时间后，会自动进入 “半开”状态。 4.3 Rest实现服务熔断4.3.1 引入Hystrix的依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 4.3.2 启动类激活Hystrix12345678910111213141516171819202122232425262728/** * 启动类 * * @author wgy */@SpringBootApplication//激活hystrix@EnableCircuitBreakerpublic class OrderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderApplication.class, args); &#125; /** * 使用spring提供的RestTemplate发送http请求到商品服务 * 1.创建RestTemplate对象交给容器管理 * 2.在使用的时候,调用其方法完成操作 (getXX,postxxx) * * @return restTemplate * @LoadBalanced : 是ribbon提供的负载均衡的注解 */ @LoadBalanced @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 4.3.3 配置熔断触发的降级逻辑123456789101112131415161718192021222324252627282930313233343536373839/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; //注入restTemplate对象 @Autowired private RestTemplate restTemplate; /** * 使用注解配置熔断保护 * fallbackmethod : 配置熔断之后的降级方法 * * @param id * @return */ @HystrixCommand(fallbackMethod = \"orderFallBack\") @RequestMapping(value = \"/product/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; return restTemplate.getForObject(\"http://service-product/product/1\", Product.class); &#125; /** * 降级方法 * 和需要收到保护的方法的返回值一致 * 方法参数一致 * * @param id * @return */ public Product orderFallBack(Long id) &#123; Product product = new Product(); product.setProductName(\"熔断:触发降级方法\"); return product; &#125;&#125; 有代码可知，为 findById 方法编写一个回退方法orderFallBack，该方法与 findById 方法具有相同的参数与返回值类型，该方法返回一个默认的错误信息。 在 findById方法上，使用注解@HystrixCommand的fallbackMethod属性，指定熔断触发的降级方法是orderFallBack。 因为熔断的降级逻辑方法必须跟正常逻辑方法保证： 相同的参数列表和返回值声明。 在 findById 方法上 HystrixCommand(fallbackMethod = “orderFallBack”) 用来声明一个降级逻辑的方法 当 shop -service-product 微服务正常时，浏览器访问 http://localhost:9001/order/product/1 可以正常调用服务提供者获取数据。当将商品微服务停止时继续访问 此时 Hystrix配置已经生效进入熔断降级方法。 4.3.3.1 默认的Fallback我们刚才把fallback写在了某个业务方法上，如果这样的方法很多，那岂不是要写很多。所以我们可以把Fallback配置加在类上，实现默认fallback： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @author wgy */@RestController@RequestMapping(\"/order\")/** * @DefaultProperties : 指定此接口中公共的熔断设置 * 如果过在@DefaultProperties指定了公共的降级方法 * 在@HystrixCommand不需要单独指定了 */@DefaultProperties(defaultFallback = \"defaultFallBack\")public class OrderController &#123; //注入restTemplate对象 @Autowired private RestTemplate restTemplate; /** * 使用注解配置熔断保护 * * @param id * @return */ @HystrixCommand @RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; return restTemplate.getForObject(\"http://service-product/product/1\", Product.class); &#125; /** * 指定统一的降级方法 * 参数 : 没有参数 * * @return */ public Product defaultFallBack() &#123; Product product = new Product(); product.setProductName(\"触发统一的降级方法\"); return product; &#125;&#125; 4.3.3.2 超时设置在之前的案例中，请求在超过1秒后都会返回错误信息，这是因为Hystix的默认超时时长为1，我们可以通过配置修改这个值： 1234567hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 2000 #默认的连接超时时间1秒,若1秒没有返回数据,自动的触发降级逻辑 4.4 Feign 实现服务熔断SpringCloud Fegin默认已为Feign整合了hystrix，所以添加Feign依赖后就不用在添加hystrix，那么怎么才能让Feign的熔断机制生效呢，只要按以下步骤开发： 4.4.1 在Feign中配置开启Hystrix在Feign中已经内置了hystrix，但是默认是关闭的需要在工程的 application.yml 中开启对hystrix的支持 1234feign: #开启对hystrix的支持 hystrix: enabled: true 4.4.2 配置FeignClient接口的实现类基于Feign实现熔断降级，那么降级方法需要配置到FeignClient接口的实现类中 12345678910111213141516171819/** * 实现自定义的ProductFeginClient接口 * 在接口实现类中编写熔断降级方法 * * @author wgy */@Componentpublic class ProductFeignClientCallBack implements ProductFeignClient &#123; @Override /** * 熔断降级的方法 */ public Product findById(Long id) &#123; Product product = new Product(); product.setProductName(\"feign调用触发熔断降级方法\"); return product; &#125;&#125; 4.4.3 修改FeignClient添加hystrix熔断@FeignClient 注解中以fallback声明降级方法 1234567891011121314151617181920/** * 声明需要调用的微服务名称 * * @author wgy * @FeignClient * name : 服务提供者的名称 * fallback : 配置熔断发生降级方法实现类 */@FeignClient(name = \"service-product\", fallback = ProductFeignClientCallBack.class)public interface ProductFeignClient &#123; /** * 配置需要调用的微服务接口 * * @param id * @return */ @RequestMapping(value = \"/product/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable(\"id\") Long id);&#125; 5. 服务熔断Hystrix高级我们知道，当请求失败，被拒绝，超时的时候，都会进入到降级方法中。但进入降级方法并不意味着断路器已经被打开。那么如何才能了解断路器中的状态呢？ 5.1 Hystrix的监控平台除了实现容错功能，Hystrix还提供了近乎实时的监控，HystrixCommand和HystrixObservableCommand在执行时，会生成执行结果和运行指标。比如每秒的请求数量，成功数量等。这些状态会暴露在Actuator提供的/health端点中。只需为项目添加 spring -boot-actuator 依赖，重启项目，访问 http://localhost:9001/actuator/hystrix.stream ,即可看到实时的监控数据。 123456#默认只有几个,配置暴露所有actuator监控的端点management: endpoints: web: exposure: include: '*' 5.1.1 搭建Hystrix DashBoard监控刚刚讨论了Hystrix的监控，但访问/hystrix.stream接口获取的都是已文字形式展示的信息。很难通过文字直观的展示系统的运行状态，所以Hystrix官方还提供了基于图形化的DashBoard（仪表板）监控平台。Hystrix仪表板可以显示每个断路器（被@HystrixCommand注解的方法）的状态。 5.1.1.1 导入依赖12345678910111213&lt;!--引入hystrix的监控信息--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; 5.1.1.2 添加EnableHystrixDashboard 注解在启动类使用@EnableHystrixDashboard注解激活仪表盘项目 12345678910111213141516171819/** * 启动类 * * @author wgy */@SpringBootApplication//激活Feign@EnableFeignClients//激活hystrix@EnableCircuitBreaker//激活hytrix的web监控平台@EnableHystrixDashboardpublic class OrderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderApplication.class, args); &#125;&#125; 5.1.1.3 访问测试http://localhost:9001/hystrix 输入监控断点展示监控的详细数据：http://localhost:9001/actuator/hystrix.stream hystrix控制台面板说明： 5.1.2 断路器聚合监控Turbine在微服务架构体系中，每个服务都需要配置Hystrix DashBoard监控。如果每次只能查看单个实例的监控数据，就需要不断切换监控地址，这显然很不方便。要想看这个系统的Hystrix Dashboard数据就需要用到Hystrix Turbine。Turbine是一个聚合Hystrix 监控数据的工具，他可以将所有相关微服务的Hystrix 监控数据聚合到一起，方便使用。引入Turbine后，整个监控系统架构如下： 5.1.2.1 搭建TurbineServer创建工程 shop_hystrix_turbine 引入相关坐标 1234567891011121314&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-turbine&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 5.1.2.2 配置多个微服务的hystrix监控在application.yml的配置文件中开启turbine并进行相关配置 123456789101112131415server: port: 8031spring: application: name: hystrix-turbineeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ instance: prefer-ip-address: trueturbine: # 要监控的微服务列表，多个用,分隔 appConfig: service-order clusterNameExpression: \"'default'\" eureka 相关配置 ： 指定注册中心地址 turbine 相关配置：指定需要监控的微服务列表 turbine会自动的从注册中心中获取需要监控的微服务，并聚合所有微服务中的 /hystrix.stream 数据 5.1.2.3 配置启动类12345678910111213141516/** * 启动类 * * @author wgy */@SpringBootApplication//trubin配置@EnableTurbine//激活hytrix的web监控平台@EnableHystrixDashboardpublic class TurbinAppliation &#123; public static void main(String[] args) &#123; SpringApplication.run(TurbinAppliation.class, args); &#125;&#125; 作为一个独立的监控项目，需要配置启动类，开启 HystrixDashboard监控平台，并激活Turbine 5.1.2.4 测试浏览器访问 http://localhost:8031/hystrix 展示HystrixDashboard。并在url位置输入 http://localhost:8031/turbine.stream ，动态根据turbine.stream数据展示多个微服务的监控数据 5.2 熔断器的状态熔断器有三个状态 CLOSED 、 OPEN 、 HALF_OPEN 熔断器默认关闭状态，当触发熔断后状态变更为OPEN ,在等待到指定的时间，Hystrix会放请求检测服务是否开启，这期间熔断器会变为 HALF_OPEN 半开启状态，熔断探测服务可用则继续变更为 CLOSED 关闭熔断器。 Closed ：关闭状态（断路器关闭），所有请求都正常访问。代理类维护了最近调用失败的次数，如果某次调用失败，则使失败次数加1。如果最近失败次数超过了在给定时间内允许失败的阈值，则代理类切换到断开(Open)状态。此时代理开启了一个超时时钟，当该时钟超过了该时间，则切换到半断开（Half-Open）状态。该超时时间的设定是给了系统一次机会来修正导致调用失败的错误。 Open ：打开状态（断路器打开），所有请求都会被降级。Hystix会对请求情况计数，当一定时间内失败请求百分比达到阈值，则触发熔断，断路器会完全关闭。默认失败比例的阈值是50%，请求次数最少不低于20次。 Half Open ：半开状态，open状态不是永久的，打开后会进入休眠时间（默认是5S）。随后断路器会自动进入半开状态。此时会释放1次请求通过，若这个请求是健康的，则会关闭断路器，否则继续保持打开，再次进行5秒休眠计时。 为了能够精确控制请求的成功或失败，我们在 shop_service_order 的调用业务中加入一段逻辑： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; //注入restTemplate对象 @Autowired private RestTemplate restTemplate; /** * 使用注解配置熔断保护 * fallbackmethod : 配置熔断之后的降级方法 * * @param id * @return */ @HystrixCommand(fallbackMethod = \"orderFallBack\") @RequestMapping(value = \"/product/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; if (id != 1) &#123; throw new RuntimeException(\"服务器异常\"); &#125; return restTemplate.getForObject(\"http://service-product/product/1\", Product.class); &#125; /** * 降级方法 * 和需要收到保护的方法的返回值一致 * 方法参数一致 * * @param id * @return */ public Product orderFallBack(Long id) &#123; Product product = new Product(); product.setProductName(\"熔断:触发降级方法\"); return product; &#125;&#125; 这样如果参数是 id为1，一定失败，其它情况都成功。 我们准备两个请求窗口： 一个请求：http://localhost:9001/order/product/2，注定失败 一个请求：http://localhost:9001/order/product/1，肯定成功 熔断器的默认触发阈值是20次请求，不好触发。休眠时间时5秒，时间太短，不易观察，为了测试方便，我们可以通过配置修改熔断策略： 1234567hystrix: command: default: circuitBreaker: requestVolumeThreshold: 5 #触发熔断的最小请求次数，默认20 /10秒 sleepWindowInMilliseconds: 10000 #熔断多少秒后去尝试请求 默认 5 打开状态的时间 errorThresholdPercentage: 50 #触发熔断的失败请求最小占比，默认50% 当我们疯狂访问id为2的请求时（超过10次），就会触发熔断。断路器会打开，一切请求都会被降级处理。 此时你访问id为1的请求，会发现返回的也是失败，而且失败时间很短，只有20毫秒左右： 5.3 熔断器的隔离策略微服务使用Hystrix熔断器实现了服务的自动降级，让微服务具备自我保护的能力，提升了系统的稳定性，也较好的解决雪崩效应。其使用方式目前支持两种策略： 线程池隔离策略： 使用一个线程池来存储当前的请求，线程池对请求作处理，设置任务返回处理超时时间，堆积的请求堆积入线程池队列。这种方式需要为每个依赖的服务申请线程池，有一定的资源消耗，好处是可以应对突发流量（流量洪峰来临时，处理不完可将数据存储到线程池队里慢慢处理） 信号量隔离策略： 使用一个原子计数器（或信号量）来记录当前有多少个线程在运行，请求来先判断计数器的数值，若超过设置的最大线程个数则丢弃改类型的新请求，若不超过则执行计数操作请求来计数器+1，请求返回计数器-1。这种方式是严格的控制线程且立即返回模式，无法应对突发流量（流量洪峰来临时，处理的线程超过数量，其他的请求会直接返回，不继续去请求依赖的服务） 线程池和型号量两种策略功能支持对比如下： 12345678hystrix: command: default: execution: isolation: strategy: ExecutionIsolationStrategy.SEMAPHORE #信号量隔离 #strategy: # ExecutionIsolationStrategy.THREAD 线程池隔离 maxConcurrentRequests: 30 #最大信号量上限 5.4 Hystrix的核心源码Hystrix 底层基于 RxJava，RxJava 是响应式编程开发库，因此Hystrix的整个实现策略简单说即：把一个HystrixCommand封装成一个Observable（待观察者），针对自身要实现的核心功能，对Observable进行各种装饰，并在订阅各步装饰的Observable，以便在指定事件到达时，添加自己的业务。 6. 服务熔断Hystrix的替换方案18年底Netflix官方宣布Hystrix 已经足够稳定，不再积极开发 Hystrix，该项目将处于维护模式。就目前来看Hystrix是比较稳定的，并且Hystrix只是停止开发新的版本，并不是完全停止维护，Bug什么的依然会维护的。因此短期内，Hystrix依然是继续使用的。但从长远来看，Hystrix总会达到它的生命周期，那么Spring Cloud生态中是否有替代产品呢？ 6.1 替换方案介绍6.1.1 Alibaba SentinelSentinel 是阿里巴巴开源的一款断路器实现，目前在Spring Cloud的孵化器项目 Spring Cloud Alibaba中的一员Sentinel本身在阿里内部已经被大规模采用，非常稳定。因此可以作为一个较好的替代品。 6.1.2 Resilience4JResilicence4J 一款非常轻量、简单，并且文档非常清晰、丰富的熔断工具，这也是Hystrix官方推荐的替代产品。不仅如此，Resilicence4j还原生支持Spring Boot 1.x/2.x，而且监控也不像Hystrix一样弄Dashboard/Hystrix等一堆轮子，而是支持和Micrometer（Pivotal开源的监控门面，Spring Boot 2.x中的Actuator就是基于Micrometer的）、prometheus（开源监控系统，来自谷歌的论文）、以及Dropwizard metrics（Spring Boot曾经的模仿对象，类似于Spring Boot）进行整合。 6.2 Sentinel概述6.2.1 Sentinel简介 随着微服务的流行，服务和服务之间的稳定性变得越来越重要。 Sentinel 以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。 Sentinel 具有以下特征: 丰富的应用场景 ：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等。 完备的实时监控 ：Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。 广泛的开源生态 ：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入Sentinel。 完善的 SPI 扩展点：Sentinel 提供简单易用、完善的 SPI 扩展接口。您可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。 Sentinel 的主要特性： 6.2.2 Sentinel 与Hystrix的区别 Sentinel Hystrix resilience4j 隔离策略 信号量隔离（并发线程数限流） 线程池隔离/信号量隔离 信号量隔离 熔断降级策略 基于响应时间、异常比率、异常数 基于异常比率 基于异常比率、响应时间 实时统计实现 滑动窗口（LeapArray） 滑动窗口（基于 RxJava） Ring Bit Buffer 动态规则配置 支持多种数据源 支持多种数据源 有限支持 扩展性 多个扩展点 插件的形式 接口的形式 基于注解的支持 支持 支持 支持 限流 基于 QPS，支持基于调用关系的限流 有限的支持 Rate Limiter 流量整形 支持预热模式、匀速器模式、预热排队模式 不支持 简单的 Rate Limiter模式 系统自适应保护 支持 不支持 不支持 控制台 提供开箱即用的控制台，可配置规则、查看秒级监控、机器发现等 简单的监控查看 不提供控制台，可对接其它监控系统 6.2.3 名词解释Sentinel 可以简单的分为 Sentinel 核心库和 Dashboard。核心库不依赖 Dashboard，但是结合Dashboard 可以取得最好的效果。 使用 Sentinel 来进行熔断保护，主要分为几个步骤: 定义资源 定义规则 检验规则是否生效 资源：可以是任何东西，一个服务，服务里的方法，甚至是一段代码。 规则：Sentinel 支持以下几种规则：流量控制规则、熔断降级规则、系统保护规则、来源访问控制规则和热点参数规则。Sentinel 的所有规则都可以在内存态中动态地查询及修改，修改之后立即生效 先把可能需要保护的资源定义好，之后再配置规则。也可以理解为，只要有了资源，我们就可以在任何时候灵活地定义各种流量控制规则。在编码的时候，只需要考虑这个代码是否需要保护，如果需要保护，就将之定义为一个资源。 6.3 Sentinel中的管理控制台6.3.1 下载启动控制台6.3.1.1 获取 Sentinel 控制台您可以从官方网站中 下载最新版本的控制台 jar 包，下载地址如下：https://github.com/alibaba/Sentinel/releases/download/1.6.3/sentinel-dashboard-1.6.3.jar 6.3.1.2 启动使用如下命令启动控制台： 1java -Dserver.port=8080 -Dcsp.sentinel.dashboard.server=localhost:8080 -Dproject.name=sentinel-dashboard -jar sentinel-dashboard-1.6.3.jar 其中 - Dserver.port=8080 用于指定 Sentinel 控制台端口为 8080 。 从 Sentinel 1.6.0 起，Sentinel 控制台引入基本的登录功能，默认用户名和密码都是 sentinel 。 启动 Sentinel 控制台需要 JDK 版本为 1.8 及以上版本。 6.3.2 客户端能接入控制台控制台启动后，客户端需要按照以下步骤接入到控制台。 6.3.2.1 引入JAR包客户端需要引入 Transport 模块来与 Sentinel 控制台进行通信。可以通过 pom.xml 引入 JAR 包: 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt; &lt;artifactId&gt;sentinel-transport-simple-http&lt;/artifactId&gt;&lt;/dependency&gt; 6.3.2.2 配置启动参数在工程的application.yml中添加Sentinel 控制台配置信息 12345spring: cloud: sentinel: transport: dashboard: localhost:8080 #sentinel控制台的请求地址 6.3.3 查看机器列表以及健康情况默认情况下Sentinel 会在客户端首次调用的时候进行初始化，开始向控制台发送心跳包。也可以配置sentinel.eager=true ,取消Sentinel控制台懒加载。 123456789101112spring: cloud: sentinel: transport: dashboard: localhost:8081 #sentinel控制台的请求地址 datasource: #加载本地规则 ds1: file: file: classpath:flowrule.json data-type: json rule-type: flow eager: true #立即加载 flowrule.json 12345678910[ &#123; \"resource\": \"orderFindById\", \"controlBehavior\": 0, \"count\": 1, \"grade\": 1, \"limitApp\": \"default\", \"strategy\": 0 &#125;] 打开浏览器即可展示Sentinel的管理控制台 6.4 基于Sentinel的服务保护6.4.1 通用资源保护6.4.1.1 引入依赖需要注意SpringCloud-Alibaba与SpringCloud的版本关系 父工程引入 alibaba实现的SpringCloud 123456789101112131415161718&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 子工程中引入 sentinel 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; 6.4.1.2 配置熔断降级方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; //注入restTemplate对象 @Autowired private RestTemplate restTemplate; /** * @param id * @return * * @SentinelResource * blockHandler : 声明熔断时调用的降级方法 * fallback : 抛出异常执行的降级方法 * value : 自定义的资源名称 * 不设置:当前全类名.方法名 */ @SentinelResource(value = \"orderFindById\", blockHandler = \"orderBlockHandler\", fallback = \"orderFallback\") @RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; if (id != 1) &#123; throw new RuntimeException(\"错误\"); &#125; return restTemplate.getForObject(\"http://service-product/product/1\", Product.class); &#125; /** * 定义降级逻辑 * hystrix和sentinel * 熔断执行的降级方法 * 抛出异常执行的降级方法 */ public Product orderBlockHandler(Long id) &#123; Product product = new Product(); product.setProductName(\"触发熔断的降级方法\"); return product; &#125; public Product orderFallback(Long id) &#123; Product product = new Product(); product.setProductName(\"抛出异常执行的降级方法\"); return product; &#125;&#125; 在需要被保护的方法上使用 @SentinelResource注解进行熔断配置。与Hystrix不同的是，Sentinel对抛出异常和熔断降级做了更加细致的区分，通过 blockHandler 指定熔断降级方法，通过 fallback 指定触发异常执行的降级方法。 6.4.2 Rest实现熔断Spring Cloud Alibaba Sentinel 支持对 RestTemplate 的服务调用使用 Sentinel 进行保护，在构造RestTemplate bean的时候需要加上 @SentinelRestTemplate 注解。 123456789101112131415161718192021222324252627282930313233343536373839/** * 启动类 * * @author wgy */@SpringBootApplicationpublic class RestOrderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(RestOrderApplication.class, args); &#125; /** * sentinel支持对restTemplate的服务调用使用sentinel方法.在构造 * RestTemplate对象的时候,只需要加载@SentinelRestTemplate即可 * * 资源名: * httpmethod:schema://host:port/path ：协议、主机、端口和路径 * httpmethod:schema://host:port ：协议、主机和端口 * * @SentinelRestTemplate: * 异常降级 * fallback : 降级方法 * fallbackClass : 降级配置类 * 限流熔断 * blockHandler * blockHandlerClass * * @return restTemplate */ @LoadBalanced @Bean @SentinelRestTemplate(fallbackClass = ExceptionUtils.class, fallback = \"handleFallback\", blockHandler = \"handleBlock\", blockHandlerClass = ExceptionUtils.class ) public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; @SentinelRestTemplate 注解的属性支持限流( blockHandler , blockHandlerClass )和降级( fallback , fallbackClass )的处理。 其中 blockHandler 或 fallback 属性对应的方法必须是对应 blockHandlerClass 或fallbackClass 属性中的静态方法。 该方法的参数跟返回值跟org.springframework.http.client.ClientHttpRequestInterceptor#interceptor 方法一致，其中参数多出了一个 BlockException 参数用于获取 Sentinel 捕获的异常。 比如上述 @SentinelRestTemplate 注解中 ExceptionUtil 的 handleException 属性对应的方法声明如下： 12345678910111213141516171819202122232425262728/** * 熔断降级 * * @author wgy */public class ExceptionUtils &#123; /** * 静态方法 * 返回值: SentinelClientHttpResponse * 参数 : request , byte[] , clientRquestExcetion , blockException */ //限流熔断业务逻辑 public static SentinelClientHttpResponse handleBlock(HttpRequest request, byte[] body, ClientHttpRequestExecution execution, BlockException ex) &#123; Product product = new Product(); product.setProductName(\"block\"); return new SentinelClientHttpResponse(JSON.toJSONString(product)); &#125; //异常降级业务逻辑 public static SentinelClientHttpResponse handleFallback(HttpRequest request, byte[] body, ClientHttpRequestExecution execution, BlockException ex) &#123; Product product = new Product(); product.setProductName(\"Fallback\"); return new SentinelClientHttpResponse(JSON.toJSONString(product)); &#125;&#125; Sentinel RestTemplate 限流的资源规则提供两种粒度： httpmethod:schema://host:port/path ：协议、主机、端口和路径 httpmethod:schema://host:port ：协议、主机和端口 6.4.3 Feign实现熔断Sentinel 适配了 Feign 组件。如果想使用，除了引入 sentinel -starter 的依赖外还需要 2 个步骤： 配置文件打开 sentinel 对 feign 的支持： feign.sentinel.enabled=true 加入 openfeign starter 依赖使 sentinel starter 中的自动化配置类生效： 6.4.3.1 引入依赖12345678910&lt;!--springcloud整合的openFeign--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; 6.4.3.2 开启sentinel 支持在工程的application.yml中添加sentinel 对 feign 的支持 1234feign: #激活sentinel的支持 sentinel: enabled: true 6.4.3.3 配置FeignClient和使用Hystrix的方式基本一致，需要配置FeignClient接口以及通过 fallback 指定熔断降级方法 1234567891011121314151617181920/** * 声明需要调用的微服务名称 * * @author wgy * @FeignClient * name : 服务提供者的名称 * fallback : 配置熔断发生降级方法实现类 */@FeignClient(name = \"service-product\", fallback = ProductFeignClientCallBack.class)public interface ProductFeignClient &#123; /** * 配置需要调用的微服务接口 * * @param id * @return */ @RequestMapping(value = \"/product/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable(\"id\") Long id);&#125; 6.4.3.4 配置熔断方法12345678910111213141516171819/** * 实现自定义的ProductFeginClient接口 * 在接口实现类中编写熔断降级方法 * * @author wgy */@Componentpublic class ProductFeignClientCallBack implements ProductFeignClient &#123; @Override /** * 熔断降级的方法 */ public Product findById(Long id) &#123; Product product = new Product(); product.setProductName(\"feign调用触发熔断降级方法\"); return product; &#125;&#125; Feign 对应的接口中的资源名策略定义：httpmethod:protocol://requesturl。 @FeignClient 注解中的所有属性，Sentinel 都做了兼容。 ProductFeginClient 接口中方法 findById 对应的资源名为 GET: http://shop-service-product/product/{str}。","tags":[{"name":"分布式架构方案","slug":"分布式架构方案","permalink":"https://wgy1993.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://wgy1993.gitee.io/tags/SpringCloud/"}]},{"title":"SpringCloud(一)","date":"2020-10-28T07:07:01.000Z","path":"archives/c069a57d.html","text":"1. 微服务基础知识1.1 系统架构的演变随着互联网的发展，网站应用的规模不断扩大，常规的应用架构已无法应对，分布式服务架构以及微服务架构势在必行，亟需一个治理系统确保架构有条不紊的演进。 1.1.1 单体应用架构Web应用程序发展的早期，大部分web工程(包含前端页面,web层代码,service层代码,dao层代码)是将所有的功能模块,打包到一起并放在一个web容器中运行。 比如搭建一个电商系统：客户下订单，商品展示，用户管理。这种将所有功能都部署在一个web容器中运行的系统就叫做单体架构。 优点：开发简单，适用于小型应用 缺点：不易扩展，维护。代码耦合 1.1.2 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率 优点： 解决高并发问题 针对不同的模块优化 方便水平扩展，容错 缺点： 系统间相互独立 重复开发工作 1.1.3 分布式SOA架构1.1.3.1 什么是SOASOA 全称为 Service-Oriented Architecture，即面向服务的架构。它可以根据需求通过网络对松散耦合的粗粒度应用组件(服务)进行分布式部署、组合和使用。一个服务通常以独立的形式存在于操作系统进程中。 站在功能的角度，把业务逻辑抽象成可复用、可组装的服务，通过服务的编排实现业务的快速再生，目的：把原先固有的业务功能转变为通用的业务服务，实现业务逻辑的快速复用。 通过上面的描述可以发现 SOA 有如下几个特点：分布式、可重用、扩展灵活、松耦合 1.1.3.2 SOA架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求 优点： 抽取公共的功能为服务,提高开发效率 对不同的服务进行集群化部署解决系统压力 基于ESB/DUBBO减少系统耦合 缺点： 抽取服务的粒度较大 服务提供方与调用方接口耦合度较高 1.1.4 微服务架构 优点： 通过服务的原子化拆分，以及微服务的独立打包、部署和升级，小团队的交付周期将缩短，运维成本也将大幅度下降 微服务遵循单一原则。微服务之间采用Restful等轻量协议传输。 缺点： 微服务过多，服务治理成本高，不利于系统维护。 分布式系统开发的技术成本高（容错、分布式事务等）。 1.1.5 SOA与微服务的关系SOA（ Service Oriented Architecture ）“面向服务的架构”:他是一种设计方法，其中包含多个服务， 服务之间通过相互依赖最终提供一系列的功能。一个服务通常以独立的形式存在与操作系统进程中。各个服务之间通过网络调用。 微服务架构:其实和 SOA 架构类似,微服务是在 SOA 上做的升华，微服务架构强调的一个重点是“业务需要彻底的组件化和服务化”，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成。 功能 SOA 微服务 组件大小 大块业务逻辑 单独任务或小块业务逻辑 耦合 通常松耦合 总是松耦合 公司架构 任何类型 小型、专注于功能交叉团队 管理 着重中央管理 着重分散管理 目标 确保应用能够交互操作 执行新功能、快速拓展开发团队 1.2 分布式核心知识1.2.1 分布式中的远程调用在微服务架构中，通常存在多个服务之间的远程调用的需求。远程调用通常包含两个部分：序列化和通信协议。常见的序列化协议包括json、xml、hession、protobuf、thrift、text、bytes等，目前主流的远程调用技术有基于HTTP的RESTful接口以及基于TCP的RPC协议。 1.2.1.1 RESTful接口REST，即Representational State Transfer的缩写，如果一个架构符合REST原则，就称它为RESTful架构。 资源（Resources） 所谓”资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的URI。要获取这个资源，访问它的URI就可以，因此URI就成了每一个资源的地址或独一无二的识别符。REST的名称”表现层状态转化”中，省略了主语。”表现层”其实指的是”资源”（Resources）的”表现层”。 表现层（Representation） “资源”是一种信息实体，它可以有多种外在表现形式。我们把”资源”具体呈现出来的形式，叫做它的”表现层”（Representation）。比如，文本可以用txt格式表现，也可以用HTML格式、XML格式、JSON格式表现，甚至可以采用二进制格式；图片可以用JPG格式表现，也可以用PNG格式表现。URI只代表资源的实体，不代表它的形式。严格地说，有些网址最后的”.html”后缀名是不必要的，因为这个后缀名表示格式，属于”表现层”范畴，而URI应该只代表”资源”的位置。 状态转化（State Transfer） 访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生”状态转化”（State Transfer）。客户端用到的手段，只能是HTTP协议。具体来说，就是HTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。 综合上面的解释，我们总结一下什么是RESTful架构： 每一个URI代表一种资源； 客户端和服务器之间，传递这种资源的某种表现层； 客户端通过四个HTTP动词，对服务器端资源进行操作，实现”表现层状态转化”。 1.2.1.2 RPC协议RPC（Remote Procedure Call ） 一种进程间通信方式。允许像调用本地服务一样调用远程服务。RPC框架的主要目标就是让远程服务调用更简单、透明。RPC框架负责屏蔽底层的传输方式（TCP或者UDP）、序列化方式（XML/JSON/二进制）和通信细节。开发人员在使用的时候只需要了解谁在什么位置提供了什么样的远程服务接口即可，并不需要关心底层通信细节和调用过程。 1.2.1.3 区别与联系 比较项 RESTful RPC 通讯协议 HTTP 一般使用TCP 性能 略低 较高 灵活度 高 低 应用 微服务架构 SOA架构 HTTP相对更规范，更标准，更通用，无论哪种语言都支持http协议。如果你是对外开放API，例如开放平台，外部的编程语言多种多样，你无法拒绝对每种语言的支持，现在开源中间件，基本最先支持的几个协议都包含RESTful。 RPC 框架作为架构微服务化的基础组件，它能大大降低架构微服务化的成本，提高调用方与服务提供方的研发效率，屏蔽跨进程调用函数（服务）的各类复杂细节。让调用方感觉就像调用本地函数一样调用远端函数、让服务提供方感觉就像实现一个本地函数一样来实现服务。 1.2.2 分布式中的CAP原理现如今，对于多数大型互联网应用，分布式系统（distributed system）正变得越来越重要。分布式系统的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理，也是理解分布式系统的起点。 CAP理论由 Eric Brewer 在ACM研讨会上提出，而后CAP被奉为分布式领域的重要理论。分布式系统的CAP理论，首先把分布式系统中的三个特性进行了如下归纳： Consistency（一致性）：数据一致更新，所有数据的变化都是同步的Availability（可用性）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求Partition tolerance（分区容忍性）：某个节点的故障，并不影响整个系统的运行 通过学习CAP理论，我们得知任何分布式系统只可同时满足二点，没法三者兼顾，既然一个分布式系统无法同时满足一致性、可用性、分区容错性三个特点，所以我们就需要抛弃一样： 选择 说明 CA 放弃分区容错性，加强一致性和可用性，其实就是传统的关系型数据库的选择 AP 放弃一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择，例如很多NoSQL系统就是如此 CP 放弃可用性，追求一致性和分区容错性，基本不会选择，网络问题会直接让整个系统不可用 需要明确一点的是，在一个分布式系统当中，分区容忍性和可用性是最基本的需求，所以在分布是系统中，我们的系统最当关注的就是A（可用性）P（容忍性），通过补偿的机制寻求数据的一致性 1.3 常见微服务框架1.3.1 SpringCloud Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、断路器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring Cloud并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。 1.3.2 ServiceComb Apache ServiceComb 是业界第一个Apache微服务顶级项目， 是一个开源微服务解决方案,致力于帮助企业、用户和开发者将企业应用轻松微服务化上云，并实现对微服务应用的高效运维管理。其提供一站式开源微服务解决方案，融合SDK框架级、0侵入ServiceMesh场景并支持多语言。 1.3.3 ZeroC ICE ZeroC IceGrid 是ZeroC公司的杰作，继承了CORBA的血统，是新一代的面向对象的分布式系统中间件。作为一种微服务架构，它基于RPC框架发展而来，具有良好的性能与分布式能力。 2. SpringCloud概述2.1 微服务中的相关概念2.1.1 服务注册与发现服务注册：服务实例将自身服务信息注册到注册中心。这部分服务信息包括服务所在主机IP和提供服务的Port，以及暴露服务自身状态以及访问协议等信息。 服务发现：服务实例请求注册中心获取所依赖服务信息。服务实例通过注册中心，获取到注册到其中的服务实例的信息，通过这些信息去请求它们提供的服务。 2.1.2 负载均衡负载均衡是高可用网络基础架构的关键组件，通常用于将工作负载分布到多个服务器来提高网站、应用、数据库或其他服务的性能和可靠性。 2.1.3 熔断熔断这一概念来源于电子工程中的断路器（Circuit Breaker）。在互联网系统中，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。这种牺牲局部，保全整体的措施就叫做熔断。 2.1.4 链路追踪随着微服务架构的流行，服务按照不同的维度进行拆分，一次请求往往需要涉及到多个服务。互联网应用构建在不同的软件模块集上，这些软件模块，有可能是由不同的团队开发、可能使用不同的编程语言来实现、有可能布在了几千台服务器，横跨多个不同的数据中心。因此，就需要对一次请求涉及的多个服务链路进行日志记录，性能监控即链路追踪 2.1.5 API网关随着微服务的不断增多，不同的微服务一般会有不同的网络地址，而外部客户端可能需要调用多个服务的接口才能完成一个业务需求，如果让客户端直接与各个微服务通信可能出现： 客户端需要调用不同的url地址，增加难度 再一定的场景下，存在跨域请求的问题 每个微服务都需要进行单独的身份认证 针对这些问题，API网关顺势而生。 API网关直面意思是将所有API调用统一接入到API网关层，由网关层统一接入和输出。一个网关的基本功能有：统一接入、安全防护、协议适配、流量管控、长短链接支持、容错能力。有了网关之后，各个API服务提供团队可以专注于自己的的业务逻辑处理，而API网关更专注于安全、流量、路由等问题。 2.2 SpringCloud的介绍 Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、断路器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring Cloud并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。 2.3 SpringCloud的架构2.3.1 SpringCloud中的核心组件Spring Cloud的本质是在 Spring Boot 的基础上，增加了一堆微服务相关的规范，并对应用上下文（Application Context）进行了功能增强。既然 Spring Cloud 是规范，那么就需要去实现，目前Spring Cloud 规范已有 Spring官方，Spring Cloud Netflix，Spring Cloud Alibaba等实现。通过组件化的方式，Spring Cloud将这些实现整合到一起构成全家桶式的微服务技术栈。 2.3.1.1 Spring Cloud Netflix组件 组件名称 作用 Eureka 服务注册中心 Ribbon 客户端负载均衡 Feign 声明式服务调用 Hystrix 客户端容错保护 Zuul API服务网关 2.3.1.2 Spring Cloud Alibaba组件 组件名称 作用 Nacos 服务注册中心 Sentinel 客户端容错保护 2.3.1.3 Spring Cloud原生及其他组件 组件 作用 Consul 服务注册中心 Config 分布式配置中心 Gateway API服务网关 Sleuth/Zipkin 分布式链路追踪 2.3.2 SpringCloud的体系结构 从上图可以看出Spring Cloud各个组件相互配合，合作支持了一套完整的微服务架构。 注册中心负责服务的注册与发现，很好将各服务连接起来 断路器负责监控服务之间的调用情况，连续多次失败进行熔断保护。 API网关负责转发所有对外的请求和服务 配置中心提供了统一的配置信息管理服务,可以实时的通知各个服务获取最新的配置信息 链路追踪技术可以将所有的请求数据记录下来，方便我们进行后续分析 各个组件又提供了功能完善的dashboard监控平台,可以方便的监控各组件的运行状况 3. 案例搭建使用微服务架构的分布式系统,微服务之间通过网络通信。我们通过服务提供者与服务消费者来描述微服务间的调用关系。 服务提供者：服务的被调用方，提供调用接口的一方服务消费者：服务的调用方，依赖于其他服务的一方 我们以电商系统中常见的用户下单为例，用户向订单微服务发起一个购买的请求。在进行保存订单之前需要调用商品微服务查询当前商品库存，单价等信息。在这种场景下，订单微服务就是一个服务消费者，商品微服务就是一个服务提供者 3.1 数据库表商品表 12345678910CREATE TABLE &#96;tb_product&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;product_name&#96; varchar(40) DEFAULT NULL COMMENT &#39;名称&#39;, &#96;status&#96; int(2) DEFAULT NULL COMMENT &#39;状态&#39;, &#96;price&#96; decimal(10,2) DEFAULT NULL COMMENT &#39;单价&#39;, &#96;product_desc&#96; varchar(255) DEFAULT NULL COMMENT &#39;描述&#39;, &#96;caption&#96; varchar(255) DEFAULT NULL COMMENT &#39;标题&#39;, &#96;inventory&#96; int(11) DEFAULT NULL COMMENT &#39;库存&#39;, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;2 DEFAULT CHARSET&#x3D;utf8 3.2 搭建环境3.2.1 创建父工程shop_parent在IDEA中创建父工程 shop_parent 并引入坐标 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.6.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.4&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;http://repo.spring.io/libs-snapshot-local&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;http://repo.spring.io/libs-milestone-local&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;spring-releases&lt;/id&gt; &lt;name&gt;Spring Releases&lt;/name&gt; &lt;url&gt;http://repo.spring.io/libs-release-local&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-snapshots&lt;/id&gt; &lt;name&gt;Spring Snapshots&lt;/name&gt; &lt;url&gt;http://repo.spring.io/libs-snapshot-local&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt; &lt;pluginRepository&gt; &lt;id&gt;spring-milestones&lt;/id&gt; &lt;name&gt;Spring Milestones&lt;/name&gt; &lt;url&gt;http://repo.spring.io/libs-milestone-local&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 3.2.2 创建微服务工程模块 创建订单微服务模块 shop_service_order 创建商品微服务模块 shop_service_product 3.3 搭建商品微服务3.3.1 编写实体类12345678910111213141516171819/** * 商品实体类 * * @author wgy */@Data@Entity@Table(name = \"tb_product\")public class Product &#123; @Id private Long id; private String productName; private Integer status; private BigDecimal price; private String productDesc; private String caption; private Integer inventory;&#125; 这里使用了 lombok简化实体类的开发Lombok能以简单的注解形式来简化java代码，提高开发人员的开发效率 3.3.2 编写dao接口1234567/** * dao接口 * * @author wgy */public interface ProductDao extends JpaRepository&lt;Product, Long&gt;, JpaSpecificationExecutor&lt;Product&gt; &#123;&#125; 3.3.3 编写service层service接口 123456789101112131415161718192021222324252627282930313233343536/** * 业务接口 * * @author wgy */public interface ProductService &#123; /** * 根据id查询 * * @param id * @return */ Product findById(Long id); /** * 保存 * * @param product */ void save(Product product); /** * 更新 * * @param product */ void update(Product product); /** * 删除 * * @param id */ void delete(Long id);&#125; service实现类 12345678910111213141516171819202122232425262728293031/** * 业务实现 * * @author wgy */@Servicepublic class ProductServiceImpl implements ProductService &#123; @Autowired private ProductDao productDao; @Override public Product findById(Long id) &#123; return productDao.findById(id).get(); &#125; @Override public void save(Product product) &#123; productDao.save(product); &#125; @Override public void update(Product product) &#123; productDao.save(product); &#125; @Override public void delete(Long id) &#123; productDao.deleteById(id); &#125;&#125; 3.3.4 编写web层1234567891011121314151617181920212223/** * 商品Controller * * @author wgy */@RestController@RequestMapping(\"/product\")public class ProductController &#123; @Autowired private ProductService productService; @RequestMapping(value = \"/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; return productService.findById(id); &#125; @RequestMapping(value = \"\", method = RequestMethod.POST) public String save(@RequestBody Product product) &#123; productService.save(product); return \"保存成功\"; &#125;&#125; 3.3.5 配置启动类123456789101112/** * 启动类 * * @author wgy */@SpringBootApplicationpublic class ProductApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ProductApplication.class, args); &#125;&#125; 3.3.6 配置yml文件1234567891011121314server: port: 9001 #端口spring: application: name: service-product #服务名称 datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/shop?useUnicode=true&amp;characterEncoding=utf8 username: root password: root jpa: database: MySQL show-sql: true open-in-view: true 3.4 搭建订单微服务3.4.1 编写实体类123456789101112131415/** * 商品实体类 * * @author wgy */@Datapublic class Product &#123; private Long id; private String productName; private Integer status; private BigDecimal price; private String productDesc; private String caption; private Integer inventory;&#125; 3.4.2 编写web层1234567891011121314151617181920/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; /** * 参数:商品id * 通过订单系统,调用商品服务根据id查询商品信息 * 1.需要配置商品对象 * 2.需要调用商品服务 */ @RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; Product product = null; //如何调用商品服务? return product; &#125;&#125; 3.5 服务调用前文已经编写了两个基础的微服务，在用户下单时需要调用商品微服务获取商品数据。那应该怎么做呢？总人皆知商品微服务提供了供人调用的HTTP接口。所以可以再下定单的时候使用http请求的相关工具类完成，如常见的HttpClient，OkHttp，当然也可以使用Spring提供的RestTemplate 3.5.1 RestTemplate介绍Spring框架提供的RestTemplate类可用于在应用中调用rest服务，它简化了与http服务的通信方式，统一了RESTful的标准，封装了http链接， 我们只需要传入url及返回值类型即可。相较于之前常用的HttpClient，RestTemplate是一种更优雅的调用RESTful服务的方式。 在Spring应用程序中访问第三方REST服务与使用Spring RestTemplate类有关。RestTemplate类的设计原则与许多其他Spring 模板类(例如JdbcTemplate、JmsTemplate)相同，为执行复杂任务提供了一种具有默认行为的简化方法。 RestTemplate默认依赖JDK提供http连接的能力（HttpURLConnection），如果有需要的话也可以通过setRequestFactory方法替换为例如 Apache HttpComponents、Netty或OkHttp等其它HTTP library。 考虑到RestTemplate类是为调用REST服务而设计的，因此它的主要方法与REST的基础紧密相连就不足为奇了，后者是HTTP协议的方法:HEAD、GET、POST、PUT、DELETE和OPTIONS。例如，RestTemplate类具有headForHeaders()、getForObject()、postForObject()、put()和delete()等方法。 3.5.2 RestTemplate方法介绍 3.5.3 通过RestTemplate调用微服务1、在 shop_service_order工程中OrderApplication启动类 中配置RestTemplate 123456789101112131415161718192021222324/** * 启动类 * * @author wgy */@SpringBootApplicationpublic class OrderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderApplication.class, args); &#125; /** * 使用spring提供的RestTemplate发送http请求到商品服务 * 1.创建RestTemplate对象交给容器管理 * 2.在使用的时候,调用其方法完成操作 (getXX,postxxx) * * @return restTemplate */ @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 2、编写下订单方法 1234567891011121314151617181920212223242526/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; //注入restTemplate对象 @Autowired private RestTemplate restTemplate; /** * 参数:商品id * 通过订单系统,调用商品服务根据id查询商品信息 * 1.需要配置商品对象 * 2.需要调用商品服务 * 使用java中的urlconnection,httpclient,okhttp */ @RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; Product product = null; //如何调用商品服务? product = restTemplate.getForObject(\"http://127.0.0.1:9001/product/1\", Product.class); return product; &#125;&#125; 3.5.4 硬编码存在的问题至此已经可以通过RestTemplate调用商品微服务的RESTFul API接口。但是我们把提供者的网络地址（ip，端口）等硬编码到了代码中，这种做法存在许多问题： 应用场景有局限 无法动态调整 那么应该怎么解决呢，就需要通过注册中心动态的对服务注册和服务发现 4. 服务注册Eureka基础4.1 微服务的注册中心注册中心可以说是微服务架构中的”通讯录“，它记录了服务和服务地址的映射关系。在分布式架构中，服务会注册到这里，当服务需要调用其它服务时，就这里找到服务的地址，进行调用。 4.1.1 注册中心的主要作用服务注册中心（下称注册中心）是微服务架构非常重要的一个组件，在微服务架构里主要起到了协调者的一个作用。注册中心一般包含如下几个功能： 服务发现： 服务注册/反注册：保存服务提供者和服务调用者的信息 服务订阅/取消订阅：服务调用者订阅服务提供者的信息，最好有实时推送的功能 服务路由（可选）：具有筛选整合服务提供者的能力。 服务配置： 配置订阅：服务提供者和服务调用者订阅微服务相关的配置 配置下发：主动将配置推送给服务提供者和服务调用者 服务健康检测 检测服务提供者的健康情况 4.1.2 常见的注册中心4.1.2.1 Zookeeperzookeeper它是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。简单来说zookeeper=文件系统+监听通知机制。 4.1.2.2 EurekaEureka是在Java语言上，基于Restful Api开发的服务注册与发现组件，Springcloud Netflix中的重要组件 4.1.2.3 ConsulConsul是由HashiCorp基于Go语言开发的支持多数据中心分布式高可用的服务发布和注册服务软件，采用Raft算法保证服务的一致性，且支持健康检查。 4.1.2.4 NacosNacos是一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。简单来说 Nacos 就是注册中心 + 配置中心的组合，提供简单易用的特性集，帮助我们解决微服务开发必会涉及到的服务注册与发现，服务配置，服务管理等问题。Nacos 还是 Spring Cloud Alibaba 组件之一，负责服务注册与发现。 最后我们通过一张表格大致了解Eureka、Consul、Zookeeper的异同点。选择什么类型的服务注册与发现组件可以根据自身项目要求决定。 组件名 语言 CAP 一致性算法 服务健康检查 对外暴露接口 Eureka Java AP 无 可配支持 HTTP Consul Go CP Raft 支持 HTTP/DNS Zookeeper Java CP Paxos 支持 客户端 Nacos Java AP Raft 支持 HTTP 4.2 Eureka的概述4.2.1 Eureka的基础知识Eureka是Netflix开发的服务发现框架，SpringCloud将它集成在自己的子项目spring-cloud-netflix中，实现SpringCloud的服务发现功能。 上图简要描述了Eureka的基本架构，由3个角色组成： Eureka Server 提供服务注册和发现 Service Provider 服务提供方 将自身服务注册到Eureka，从而使服务消费方能够找到 Service Consumer 服务消费方 从Eureka获取注册服务列表，从而能够消费服务 4.2.2 Eureka的交互流程与原理 图是来自Eureka官方的架构图，大致描述了Eureka集群的工作过程。图中包含的组件非常多，可能比较难以理解，我们用通俗易懂的语言解释一下： Application Service 相当于本书中的服务提供者，Application Client相当于服务消费者； Make Remote Call，可以简单理解为调用RESTful API； us-east-1c、us-east-1d等都是zone，它们都属于us-east-1这个region； 由图可知，Eureka包含两个组件：Eureka Server 和 Eureka Client，它们的作用如下： Eureka Client是一个Java客户端，用于简化与Eureka Server的交互； Eureka Server提供服务发现的能力，各个微服务启动时，会通过Eureka Client向Eureka Server进行注册自己的信息（例如网络信息），Eureka Server会存储该服务的信息； 微服务启动后，会周期性地向Eureka Server发送心跳（默认周期为30秒）以续约自己的信息。如果Eureka Server在一定时间内没有接收到某个微服务节点的心跳，Eureka Server将会注销该微服务节点（默认90秒）； 每个Eureka Server同时也是Eureka Client，多个Eureka Server之间通过复制的方式完成服务注册表的同步； Eureka Client会缓存Eureka Server中的信息。即使所有的Eureka Server节点都宕掉，服务消费者依然可以使用缓存中的信息找到服务提供者。 综上，Eureka通过心跳检测、健康检查和客户端缓存等机制，提高了系统的灵活性、可伸缩性和可用性。 4.3 搭建Eureka注册中心4.3.1 搭建Eureka服务中心4.3.1.1 引入maven坐标1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 4.3.1.2 配置application.yml1234567891011121314spring: application: name: eureka-serverserver: port: 9000 #端口#配置eureka servereureka: instance: hostname: localhost client: register-with-eureka: false #是否将自己注册到注册中心 fetch-registry: false #是否从eureka中获取注册信息 service-url: #配置暴露给Eureka Client的请求地址 defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 4.3.1.3 配置启动类1234567891011121314/** * 启动类 * * @author wgy */@SpringBootApplication//激活eureakaserver@EnableEurekaServerpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 4.3.2 服务注册中心管理后台打开浏览器访问 http://localhost:9000即可进入EurekaServer内置的管理控制台,显示效果如下 4.4 服务注册到Eureka注册中心4.4.1 商品服务注册4.4.1.1 引入maven坐标pom文件中添加eureka client的相关坐标 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--引入EurekaClient--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.4.1.2 配置application.yml文件添加Eureka Server的主机地址 123456789101112131415161718192021server: port: 9001 #端口spring: application: name: service-product #服务名称 datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/shop?useUnicode=true&amp;characterEncoding=utf8 username: root password: root jpa: database: MySQL show-sql: true open-in-view: true#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ #多个eurekaserver之间用,隔开 instance: prefer-ip-address: true #使用ip地址注册 4.4.1.3 启动类添加服务注册注解123456789101112131415/** * 启动类 * * @author wgy */@SpringBootApplication//激活eurekaClient//@EnableEurekaClient//@EnableDiscoveryClientpublic class ProductApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ProductApplication.class, args); &#125;&#125; 从Spring Cloud Edgware版本开始， @EnableDiscoveryClient 或 @EnableEurekaClient 可省略。只需加上相关依赖，并进行相应配置，即可将微服务注册到服务发现组件上。 4.4.2 订单服务注册4.4.2.1 引入maven坐标pom文件中添加eureka client的相关坐标 12345678910111213141516&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--引入EurekaClient--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.4.2.2 配置application.yml文件添加Eureka Server的主机地址 123456789101112131415161718192021server: port: 9002 #端口spring: application: name: service-order #服务名称 datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/shop?useUnicode=true&amp;characterEncoding=utf8 username: root password: root jpa: database: MySQL show-sql: true open-in-view: true#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ instance: prefer-ip-address: true #使用ip地址注册 4.5 Eureka中的自我保护微服务第一次注册成功之后，每30秒会发送一次心跳将服务的实例信息注册到注册中心。通知 Eureka Server 该实例仍然存在。如果超过90秒没有发送更新，则服务器将从注册信息中将此服务移除。 Eureka Server在运行期间，会统计心跳失败的比例在15分钟之内是否低于85%，如果出现低于的情况（在单机调试的时候很容易满足，实际在生产环境上通常是由于网络不稳定导致），Eureka Server会将当前的实例注册信息保护起来，同时提示这个警告。保护模式主要用于一组客户端和Eureka Server之间存在网络分区场景下的保护。一旦进入保护模式，Eureka Server将会尝试保护其服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务） 验证完自我保护机制开启后，并不会马上呈现到web上，而是默认需等待 5 分钟（可以通过eureka.server.wait-time-in-ms-when-sync-empty 配置），即 5 分钟后你会看到下面的提示信息： 如果关闭自我保护 通过设置 eureka.enableSelfPreservation=false 来关闭自我保护功能。 1234567891011#配置eureka servereureka: instance: hostname: localhost client: register-with-eureka: false #是否将自己注册到注册中心 fetch-registry: false #是否从eureka中获取注册信息 service-url: #配置暴露给Eureka Client的请求地址 defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ server: enable-self-preservation: false #关闭自我保护 4.6 Eureka中的元数据Eureka的元数据有两种：标准元数据和自定义元数据。 标准元数据：主机名、IP地址、端口号、状态页和健康检查等信息，这些信息都会被发布在服务注册表中，用于服务之间的调用。 自定义元数据：可以使用eureka.instance.metadata-map配置，符合KEY/VALUE的存储格式。这些元数据可以在远程客户端中访问。 在程序中可以使用DiscoveryClient 获取指定微服务的所有元数据信息 12345678910111213141516171819202122232425262728293031323334353637383940/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; //注入restTemplate对象 @Autowired private RestTemplate restTemplate; /** * 注入DiscoveryClient : * springCloud提供的获取原数组的工具类 * 调用方法获取服务的元数据信息 */ @Autowired private DiscoveryClient discoveryClient; /** * 参数:商品id * 通过订单系统,调用商品服务根据id查询商品信息 * 1.需要配置商品对象 * 2.需要调用商品服务 * 使用java中的urlconnection,httpclient,okhttp */ @RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; //调用discoveryClient方法 //以调用服务名称获取所有的元数据 List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(\"service-product\"); //获取唯一的一个元数据 ServiceInstance instance = instances.get(0); //根据元数据中的主机地址和端口号拼接请求微服务的URL Product product = null; //如何调用商品服务? product = restTemplate.getForObject(\"http://\" + instance.getHost() + \":\" + instance.getPort() + \"/product/1\", Product.class); return product; &#125;&#125; 5. 服务注册Eureka高级5.1 Eureka Server 高可用集群在上一个章节，实现了单节点的Eureka Server的服务注册与服务发现功能。Eureka Client会定时连接Eureka Server，获取注册表中的信息并缓存到本地。微服务在消费远程API时总是使用本地缓存中的数据。因此一般来说，即使Eureka Server发生宕机，也不会影响到服务之间的调用。但如果Eureka Server宕机时，某些微服务也出现了不可用的情况，Eureka Server中的缓存若不被刷新，就可能会影响到微服务的调用，甚至影响到整个应用系统的高可用。因此，在生成环境中，通常会部署一个高可用的Eureka Server集群。 Eureka Server可以通过运行多个实例并相互注册的方式实现高可用部署，Eureka Server实例会彼此增量地同步信息，从而确保所有节点数据一致。事实上，节点之间相互注册是Eureka Server的默认行为。 5.1.1 搭建 Eureka Server高可用集群5.1.1.1 修改yml配置文件12345678910111213141516171819202122232425262728#指定应用名称spring: application: name: eureka-server---#执行peer1的配置信息spring: profiles: eureka1server: port: 9000eureka: instance: hostname: localhost client: service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/---#执行peer2的配置信息spring: profiles: eureka2server: port: 8000eureka: instance: hostname: localhost client: service-url: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ 在配置文件中通过连字符（—）将文件分为三个部分，第一部分为应用名称，第二部分和第三部分是根据不同的profiles选项动态添加，可以在IDEA启动时进行激活配置 使用IDEA启动历次EurekaServerApplicaion分别激活eureka1和eureka2配置。访问http://localhost:9000和http://localhost:8000/。会发现注册中心 EUREKA-SERVER 已经有两个节点，并且registered-replicas (相邻集群复制节点)中已经包含对方。 5.1.2 服务注册到Eureka Server集群如果需要将微服务注册到Eureka Server集群只需要修改yml配置文件即可 12345eureka: client: serviceUrl: #多个eurekaserver之间用,隔开 defaultZone: http://localhost:9000/eureka/,http://localhost:8000/eureka/ 5.2 Eureka中的常见问题5.2.1 服务注册慢默认情况下，服务注册到Eureka Server的过程较慢。SpringCloud官方文档中给出了详细的原因 大致含义：服务的注册涉及到心跳，默认心跳间隔为30s。在实例、服务器、客户端都在本地缓存中具有相同的元数据之前，服务不可用于客户端发现（所以可能需要3次心跳）。可以通过配置eureka.instance.leaseRenewalIntervalInSeconds (心跳频率)加快客户端连接到其他服务的过程。在生产中，最好坚持使用默认值，因为在服务器内部有一些计算，他们对续约做出假设。 12345678#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ #多个eurekaserver之间用,隔开 instance: prefer-ip-address: true #使用ip地址注册 lease-renewal-interval-in-seconds: 5 #发送心跳续约间隔 5.2.2 服务节点剔除问题默认情况下，由于Eureka Server剔除失效服务间隔时间为90s且存在自我保护的机制。所以不能有效而迅速的剔除失效节点，这对开发或测试会造成困扰。解决方案如下： Eureka Server： 配置关闭自我保护，设置剔除无效节点的时间间隔 123456789101112#配置eureka servereureka: instance: hostname: localhost client: register-with-eureka: false #是否将自己注册到注册中心 fetch-registry: false #是否从eureka中获取注册信息 service-url: #配置暴露给Eureka Client的请求地址 defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ server: enable-self-preservation: false #关闭自我保护 eviction-interval-timer-in-ms: 4000 #剔除时间间隔,单位:毫秒 Eureka Client： 配置开启健康检查，并设置续约时间 123456789#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ #多个eurekaserver之间用,隔开 instance: prefer-ip-address: true #使用ip地址注册 lease-renewal-interval-in-seconds: 5 #发送心跳续约间隔 lease-expiration-duration-in-seconds: 10 #eureka client发送心跳给server端后，续约到期时间（默认90秒） 5.2.3 监控页面显示ip在Eureka Server的管控台中，显示的服务实例名称默认情况下是微服务定义的名称和端口。为了更好的对所有服务进行定位，微服务注册到Eureka Server的时候可以手动配置示例ID。配置方式如下 12345678910#配置Eurekaeureka: client: service-url: defaultZone: http://localhost:9000/eureka/ #多个eurekaserver之间用,隔开 instance: prefer-ip-address: true #使用ip地址注册 #向注册中心中注册服务id #spring.cloud.client.ip-address:获取ip地址 instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; 6. Eureka替换方案Consul6.1 Eureka闭源的影响6.1.1 Eureka闭源影响 在Euraka的GitHub上，宣布Eureka 2.x闭源。近这意味着如果开发者继续使用作为 2.x 分支上现有工作repo 一部分发布的代码库和工件，则将自负风险。 6.1.2 Eureka的替换方案6.1.2.1 ZookeeperZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 6.1.2.2 Consulconsul是近几年比较流行的服务发现工具，工作中用到，简单了解一下。consul的三个主要应用场景：服务发现、服务隔离、服务配置 6.1.2.3 NacosNacos 是阿里巴巴推出来的一个新开源项目，这是一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。Nacos 致力于帮助您发现、配置和管理微服务。Nacos 提供了一组简单易用的特性集，帮助您快速实现动态服务发现、服务配置、服务元数据及流量管理。Nacos 帮助您更敏捷和容易地构建、交付和管理微服务平台。 Nacos 是构建以“服务”为中心的现代应用架构 (例如微服务范式、云原生范式) 的服务基础设施。 6.2 什么是consul6.2.1 consul 概述 Consul 是 HashiCorp 公司推出的开源工具，用于实现分布式系统的服务发现与配置。与其它分布式服务注册与发现的方案，Consul 的方案更“一站式”，内置了服务注册与发现框 架、分布一致性协议实现、健康检查、Key/Value 存储、多数据中心方案，不再需要依赖其它工具（比如 ZooKeeper 等）。使用起来也较 为简单。Consul 使用 Go 语言编写，因此具有天然可移植性(支持Linux、windows和Mac OS X)；安装包仅包含一个可执行文件，方便部署，与 Docker 等轻量级容器可无缝配合。 Consul 的优势： 使用 Raft 算法来保证一致性, 比复杂的 Paxos 算法更直接. 相比较而言, zookeeper 采用的是Paxos, 而 etcd 使用的则是 Raft。 支持多数据中心，内外网的服务采用不同的端口进行监听。 多数据中心集群可以避免单数据中心的单点故障,而其部署则需要考虑网络延迟, 分片等情况等。 zookeeper 和 etcd 均不提供多数据中心功能的支持。 支持健康检查。 etcd 不提供此功能。 支持 http 和 dns 协议接口。 zookeeper 的集成较为复杂, etcd 只支持 http 协议。 官方提供 web 管理界面, etcd 无此功能。 综合比较, Consul 作为服务注册和配置管理的新星, 比较值得关注和研究。 特性： 服务发现 健康检查 Key/Value 存储 多数据中心 6.2.2 consul与Eureka的区别6.2.2.1 一致性Consul强一致性（CP） 服务注册相比Eureka会稍慢一些。因为Consul的raft协议要求必须过半数的节点都写入成功才认为注册成功 Leader挂掉时，重新选举期间整个consul不可用。保证了强一致性但牺牲了可用性。 Eureka保证高可用和最终一致性（AP） 服务注册相对要快，因为不需要等注册信息replicate到其他节点，也不保证注册信息是否replicate成功 当数据出现不一致时，虽然A, B上的注册信息不完全相同，但每个Eureka节点依然能够正常对外提供服务，这会出现查询服务信息时如果请求A查不到，但请求B就能查到。如此保证了可用性但牺牲了一致性。 6.2.2.2 开发语言和使用eureka就是个servlet程序，跑在servlet容器中 Consul则是go编写而成，安装启动即可 6.2.3 consul的下载与安装Consul 不同于 Eureka 需要单独安装，访问 Consul 官网下载 Consul 的最新版本，我这里是consul1.5x。根据不同的系统类型选择不同的安装包，从下图也可以看出 Consul 支持所有主流系统。 在linux虚拟中下载consul服务 12345678## 从官网下载最新版本的Consul服务wget https://releases.hashicorp.com/consul/1.5.3/consul_1.5.3_linux_amd64.zip##使用unzip命令解压unzip consul_1.5.3_linux_amd64.zip##将解压好的consul可执行命令拷贝到/usr/local/bin目录下cp consul /usr/local/bin##测试一下consul 启动consul服务 12##已开发者模式快速启动，-client指定客户端可以访问的ip地址consul agent -dev -client=0.0.0.0 启动成功之后访问： http://IP:8500 ，可以看到 Consul 的管理界面 6.3 consul的基本使用Consul 支持健康检查,并提供了 HTTP 和 DNS 调用的API接口完成服务注册，服务发现，以及K/V存储这些功能。接下来通过发送HTTP请求的形式来了解一下Consul 6.3.1 服务注册与发现6.3.1.1 注册服务通过postman发送put请求到 http://192.168.142.128:8500/v1/catalog/register地址可以完成服务注册 123456789101112&#123; \"Datacenter\": \"dc1\", \"Node\": \"node01\", \"Address\": \"192.168.142.128\", \"Service\": &#123; \"ID\":\"mysql-01\", \"Service\": \"mysql\", \"tags\": [\"master\",\"v1\"], \"Address\": \"192.168.142.128\", \"Port\": 3306 &#125;&#125; 6.3.1.2 服务查询通过postman发送get请求到http://192.168.142.128:8500/v1/catalog/services查看所有的服务列表 通过postman发送get请求到http://192.168.142.128:8500/v1/catalog/service/服务名 查看具体的服务详情 6.3.1.3 服务删除通过postman发送put请求到http://192.168.142.128:8500/v1/catalog/deregister删除服务 12345&#123; \"Datacenter\": \"dc1\", \"Node\": \"node01\", \"ServiceID\": \"mysql-01\"&#125; 6.3.2 Consul的KV存储可以参照Consul提供的KV存储的 API完成基于Consul的数据存储 含义 请求路径 请求方式 查看key v1/kv/:key GET 保存或更新 v1/kv/:key put 删除 /v1/kv/:key DELETE key值中可以带/, 可以看做是不同的目录结构。 value的值经过了base64_encode,获取到数据后base64_decode才能获取到原始值。数据不能大于512Kb 不同数据中心的kv存储系统是独立的，使用dc=?参数指定。 6.4 基于consul的服务注册6.4.1 案例准备拷贝一份新的工程，起名为 shop_consul_parent ，并导入相关的子模块 6.4.1.1 修改pom文件修改每个微服务的pom文件，添加SpringCloud提供的基于Consul的依赖 12345678910111213141516171819202122&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--springcloud 提供的对基于consul的服务发现--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-consul-discovery&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--actuator的健康检查--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 6.4.2 配置服务注册修改每个微服务的application.yml配置文件，添加consul服务发现的相关配置信息 123456789101112131415161718192021222324spring: ...省略 ###开始配置consul的服务注册 cloud: consul: host: 192.168.142.128 #consul服务器的主机地址 port: 8500 #consul服务器的ip地址 discovery: #是否需要注册 register: true #注册的实例ID (唯一标志,推荐必填) instance-id: $&#123;spring.application.name&#125;-1 #服务的名称 service-name: $&#123;spring.application.name&#125; #服务的请求端口 port: $&#123;server.port&#125; #健康检查路径 healthCheckPath: /actuator/health #健康检查时间间隔 healthCheckInterval: 15s #指定开启ip地址注册 prefer-ip-address: true #当前服务的请求ip ip-address: $&#123;spring.cloud.client.ip-address&#125; 6.5 基于consul的服务发现由于SpringCloud对Consul进行了封装。对于在消费者端获取服务提供者信息和Eureka是一致的。同样使用 DiscoveryClient 完成调用获取微服务实例信息 6.6 consul高可用集群 此图是官网提供的一个事例系统图，图中的Server是consul服务端高可用集群，Client是consul客户端。consul客户端不保存数据，客户端将接收到的请求转发给响应的Server端。Server之间通过局域网或广域网通信实现数据一致性。每个Server或Client都是一个consul agent。Consul集群间使用了GOSSIP协议通信和raft一致性算法。 6.6.1 Consul的核心知识6.6.1.1 Gossip协议传统的监控，如ceilometer，由于每个节点都会向server报告状态，随着节点数量的增加server的压力随之增大。在所有的Agent之间（包括服务器模式和普通模式）运行着Gossip协议。服务器节点和普通Agent都会加入这个Gossip集群，收发Gossip消息。每隔一段时间，每个节点都会随机选择几个节点发送Gossip消息，其他节点会再次随机选择其他几个节点接力发送消息。这样一段时间过后，整个集群都能收到这条消息。示意图如下。 6.6.1.2 RAFT一致性算法 6.6.2 Consul 集群搭建 首先需要有一个正常的Consul集群，有Server，有Leader。这里在服务器Server1、Server2、Server3上分别部署了Consul Server。（这些服务器上最好只部署Consul程序，以尽量维护Consul Server的稳定） 服务器Server4和Server5上通过Consul Client分别注册Service A、B、C，这里每个Service分别部署在了两个服务器上，这样可以避免Service的单点问题。（一般微服务和Client绑定）在服务器Server6中Program D需要访问Service B，这时候Program D首先访问本机Consul Client提供的HTTP API，本机Client会将请求转发到Consul Server，Consul Server查询到Service B当前的信息返回 6.6.2.1 准备环境 服务器ip consul类型 Node（节点名称） 序号 192.168.74.101 server server-1 s1 192.168.74.102 server server-2 s2 192.168.74.103 server server-3 s3 192.168.71.1 client clent-1 s4 Agent 以 client 模式启动的节点。在该模式下，该节点会采集相关信息，通过 RPC 的方式向 server 发送。Client模式节点有无数个，官方建议搭配微服务配置 Agent 以 server 模式启动的节点。一个数据中心中至少包含 1 个 server 节点。不过官方建议使用 3 或 5 个 server 节点组建成集群，以保证高可用且不失效率。server 节点参与 Raft、维护会员信息、注册服务、健康检查等功能。 6.6.2.2 安装consul并启动 在每个consul节点上安装consul服务，下载安装过程和单节点一致。 12345678##从官网下载最新版本的Consul服务wget https://releases.hashicorp.com/consul/1.5.3/consul_1.5.3_linux_amd64.zip##使用unzip命令解压unzip consul_1.5.3_linux_amd64.zip##将解压好的consul可执行命令拷贝到/usr/local/bin目录下cp consul /usr/local/bin##测试一下consul 启动每个consul server节点 12345678##登录s1虚拟机，以server形式运行consul agent -server -bootstrap-expect 3 -data-dir /etc/consul.d -node=server-1 -bind=192.168.74.101 -ui -client 0.0.0.0 &amp;##登录s2 虚拟机，以server形式运行consul agent -server -bootstrap-expect 2 -data-dir /etc/consul.d -node=server-2 -bind=192.168.74.102 -ui -client 0.0.0.0 &amp; ##登录s3 虚拟机，以server形式运行consul agent -server -bootstrap-expect 2 -data-dir /etc/consul.d -node=server-3 -bind=192.168.74.103 -ui -client 0.0.0.0 &amp; -server： 以server身份启动。 -bootstrap-expect：集群要求的最少server数量，当低于这个数量，集群即失效。 -data-dir：data存放的目录，更多信息请参阅consul数据同步机制 -node：节点id，在同一集群不能重复。 -bind：监听的ip地址。 -client：客户端的ip地址(0.0.0.0表示不限制) &amp; ：在后台运行，此为linux脚本语法 至此三个Consul Server模式服务全部启动成功 12##在本地电脑中使用client形式启动consulconsul agent -client=0.0.0.0 -data-dir /etc/consul.d -node=client-1 （3） 每个节点加入集群 在s2，s3，s4 服务其上通过consul join 命令加入 s1中的consul集群中 12##加入consul集群consul join 192.168.74.101 （4） 测试 在任意一台服务器中输入 consul members查看集群中的所有节点信息 12##查看consul集群节点信息consul members 6.6.3 Consul 常见问题6.6.3.1 节点和服务注销当服务或者节点失效，Consul不会对注册的信息进行剔除处理，仅仅标记已状态进行标记（并且不可使用）。如果担心失效节点和失效服务过多影响监控。可以通过调用HTTP API的形式进行处理节点和服务的注销可以使用HTTP API: 注销任意节点和服务：/catalog/deregister 注销当前节点的服务：/agent/service/deregister/:service_id 如果某个节点不继续使用了，也可以在本机使用consul leave命令，或者在其它节点使用consul force-leave 节点Id。 6.6.3.2 健康检查与故障转移在集群环境下，健康检查是由服务注册到的Agent来处理的，那么如果这个Agent挂掉了，那么此节点的健康检查就处于无人管理的状态。 从实际应用看，节点上的服务可能既要被发现，又要发现别的服务，如果节点挂掉了，仅提供被发现的功能实际上服务还是不可用的。当然发现别的服务也可以不使用本机节点，可以通过访问一个Nginx实现的若干Consul节点的负载均衡来实现。 7. 服务调用Ribbon入门经过以上的学习，已经实现了服务的注册和服务发现。当启动某个服务的时候，可以通过HTTP的形式将信息注册到注册中心，并且可以通过SpringCloud提供的工具获取注册中心的服务列表。但是服务之间的调用还存在很多的问题，如何更加方便的调用微服务，多个微服务的提供者如何选择，如何负载均衡等。 7.1 Ribbon概述7.1.1 什么是Ribbon是 Netflixfa 发布的一个负载均衡器，有助于控制 HTTP 和 TCP客户端行为。在 SpringCloud 中，Eureka一般配合Ribbon进行使用，Ribbon提供了客户端负载均衡的功能，Ribbon利用从Eureka中读取到的服务信息，在调用服务节点提供的服务时，会合理的进行负载。 在SpringCloud中可以将注册中心和Ribbon配合使用，Ribbon自动的从注册中心中获取服务提供者的列表信息，并基于内置的负载均衡算法，请求服务 7.1.2 Ribbon的主要作用7.1.2.1 服务调用基于Ribbon实现服务调用， 是通过拉取到的所有服务列表组成（服务名-请求路径的）映射关系。借助RestTemplate 最终进行调用 7.1.2.2 负载均衡当有多个服务提供者时，Ribbon可以根据负载均衡的算法自动的选择需要调用的服务地址 7.2 基于Ribbon实现订单调用商品服务不论是基于Eureka的注册中心还是基于Consul的注册中心，SpringCloud Ribbon统一进行了封装，所以对于服务调用，两者的方式是一样的。 7.2.1 坐标依赖在springcloud提供的服务发现的jar中已经包含了Ribbon的依赖。所以这里不需要导入任何额外的坐标 7.2.2 工程改造7.2.2.1 服务提供者12345678910111213141516171819202122232425262728293031/** * 商品Controller * * @author wgy */@RestController@RequestMapping(\"/product\")public class ProductController &#123; @Autowired private ProductService productService; @Value(\"$&#123;server.port&#125;\") private String port; @Value(\"$&#123;spring.cloud.client.ip-address&#125;\") //spring cloud 自动的获取当前应用的ip地址 private String ip; @RequestMapping(value = \"/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; Product product = productService.findById(id); product.setProductName(\"访问的服务地址:\" + ip + \":\" + port); return product; &#125; @RequestMapping(value = \"\", method = RequestMethod.POST) public String save(@RequestBody Product product) &#123; productService.save(product); return \"保存成功\"; &#125;&#125; 7.2.2.2 服务消费者修改服务消费者 shop_service_order模块中的启动类OrderApplication ，在创建RestTemplate方法上添加 @LoadBalanced 注解 123456789101112131415161718192021222324252627/** * 启动类 * * @author wgy */@SpringBootApplicationpublic class OrderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderApplication.class, args); &#125; /** * 使用spring提供的RestTemplate发送http请求到商品服务 * 1.创建RestTemplate对象交给容器管理 * 2.在使用的时候,调用其方法完成操作 (getXX,postxxx) * Springcloud对consul进行了进一步的处理 * 向其中集成了ribbon的支持 * @LoadBalanced : 是ribbon提供的负载均衡的注解 * @return restTemplate */ @LoadBalanced @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 1234567891011121314151617181920212223/** * @author wgy */@RestController@RequestMapping(\"/order\")public class OrderController &#123; //注入restTemplate对象 @Autowired private RestTemplate restTemplate; /** * 基于ribbon的形式调用远程微服务 * 1.使用@LoadBalanced声明RestTemplate * 2.使用服务名称替换ip地址 */ @RequestMapping(value = \"/buy/&#123;id&#125;\", method = RequestMethod.GET) public Product findById(@PathVariable Long id) &#123; Product product = null; product = restTemplate.getForObject(\"http://service-product/product/1\", Product.class); return product; &#125;&#125; 8. 服务调用Ribbon高级8.1 负载均衡概述8.1.1 什么是负载均衡在搭建网站时，如果单节点的 web服务性能和可靠性都无法达到要求；或者是在使用外网服务时，经常担心被人攻破，一不小心就会有打开外网端口的情况，通常这个时候加入负载均衡就能有效解决服务问题。 负载均衡是一种基础的网络服务，其原理是通过运行在前面的负载均衡服务，按照指定的负载均衡算法，将流量分配到后端服务集群上，从而为系统提供并行扩展的能力。 负载均衡的应用场景包括流量包、转发规则以及后端服务，由于该服务有内外网个例、健康检查等功能，能够有效提供系统的安全性和可用性。 8.1.2 客户端负载均衡与服务端负载均衡8.1.2.1 服务端负载均衡先发送请求到负载均衡服务器或者软件，然后通过负载均衡算法，在多个服务器之间选择一个进行访问；即在服务器端再进行负载均衡算法分配 8.1.2.2 客户端负载均衡客户端会有一个服务器地址列表，在发送请求前通过负载均衡算法选择一个服务器，然后进行访问，这是客户端负载均衡；即在客户端就进行负载均衡算法分配 8.2 基于Ribbon实现负载均衡8.2.1 搭建多服务实例修改 shop_service_product 的 application.yml 配置文件，已profiles的形式配置多个实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455spring: profiles: product1 application: name: shop-service-product #服务名称 datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/shop?useUnicode=true&amp;characterEncoding=utf8 username: root password: root jpa: database: MySQL show-sql: true open-in-view: true ###开始配置consul的服务注册 cloud: consul: host: 192.168.142.128 #consul服务器的主机地址 port: 8500 #consul服务器的ip地址 discovery: #实例ID instance-id: $&#123;spring.application.name&#125;-1 #开启ip地址注册 prefer-ip-address: true #实例的请求ip ip-address: $&#123;spring.cloud.client.ip-address&#125;server: port: 9002---spring: profiles: product2 application: name: shop-service-product datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/shop?useUnicode=true&amp;characterEncoding=utf8 username: root password: root jpa: database: MySQL show-sql: true open-in-view: true ###开始配置consul的服务注册 cloud: consul: host: 192.168.142.128 #consul服务器的主机地址 port: 8500 #consul服务器的ip地址 discovery: #实例ID instance-id: $&#123;spring.application.name&#125;-2 #开启ip地址注册 prefer-ip-address: true #实例的请求ip ip-address: $&#123;spring.cloud.client.ip-address&#125;server: port: 9004 分别启动两次服务器验证效果，并查看两个控制台发现已轮询的方式调用了商品服务 8.2.2 负载均衡策略Ribbon内置了多种负载均衡策略，内部负责复杂均衡的顶级接口为com.netflix.loadbalancer.IRule ，实现方式如下 com.netflix.loadbalancer.RoundRobinRule ：以轮询的方式进行负载均衡。 com.netflix.loadbalancer.RandomRule ：随机策略 com.netflix.loadbalancer.RetryRule ：重试策略。 com.netflix.loadbalancer.WeightedResponseTimeRule ：权重策略。会计算每个服务的权重，越高的被调用的可能性越大。 com.netflix.loadbalancer.BestAvailableRule ：最佳策略。遍历所有的服务实例，过滤掉故障实例，并返回请求数最小的实例返回。 com.netflix.loadbalancer.AvailabilityFilteringRule ：可用过滤策略。过滤掉故障和请求数超过阈值的服务实例，再从剩下的实力中轮询调用。 在服务消费者的application.yml配置文件中修改负载均衡策略 1234#修改ribbon的负载均衡策略 服务名 - ribbon - NFLoadBalancerRuleClassName : 策略service-product: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 策略选择： 如果每个机器配置一样，则建议不修改策略 (推荐) 如果部分机器配置强，则可以改为 WeightedResponseTimeRule","tags":[{"name":"分布式架构方案","slug":"分布式架构方案","permalink":"https://wgy1993.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://wgy1993.gitee.io/tags/SpringCloud/"}]},{"title":"Dubbo","date":"2020-10-25T07:51:50.000Z","path":"archives/f6253398.html","text":"1. 分布式基础理论1.1 什么是分布式系统分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像单个相关系统 分布式系统（distributed system）是建立在网络之上的软件系统。 随着互联网的发展，网站应用的规模不断扩大，常规的垂直应用架构已无法应对，分布式服务架构以及流动计算架构势在必行，亟需一个治理系统确保架构有条不紊的演进 1.2 发展演变 1.2.1 单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 适用于小型网站，小型管理系统，将所有功能都部署到一个功能里，简单易用。 缺点： 性能扩展比较难 协同开发问题 不利于升级维护 1.2.2 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 通过切分业务来实现各个模块独立部署，降低了维护和部署的难度，团队各司其职更易管理，性能扩展也更方便，更有针对性。 缺点： 公用模块无法重复利用，开发性的浪费 1.2.3 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 1.2.4 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)[ Service Oriented Architecture]是关键。 1.3 RPC1.3.1 什么叫RPCRPC【Remote Procedure Call】是指远程过程调用，是一种进程间通信方式，他是一种技术的思想，而不是规范。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。 1.3.2 RPC基本原理 RPC两个核心模块：通讯，序列化。 RPC框架有很多如：dubbo、gRPC、Thrift、HSF(High Speed Service Framework) 2. dubbo核心概念2.1 简介Apache Dubbo (incubating) |ˈdʌbəʊ| 是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 官网：http://dubbo.apache.org/ 2.2 基本概念 服务提供者（Provider）：暴露服务的服务提供方，服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者（Consumer）: 调用远程服务的服务消费方，服务消费者在启动时，向注册中心订阅自己所需的服务，服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 注册中心（Registry）：注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者 监控中心（Monitor）：服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心 调用关系说明： 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 3. dubbo环境搭建3.1 安装zookeeper参考文章：https://wgy1993.gitee.io/archives/e18db595.html 3.2 安装dubbo-admin管理控制台dubbo本身并不是一个服务软件。它其实就是一个jar包能够帮你的java程序连接到zookeeper，并利用zookeeper消费、提供服务。所以你不用在Linux上启动什么dubbo服务。但是为了让用户更好的管理监控众多的dubbo服务，官方提供了一个可视化的监控程序，不过这个监控即使不装也不影响使用。 3.2.1 下载dubbo-adminhttps://github.com/apache/incubator-dubbo-ops 3.2.2 修改dubbo-admin配置修改 src\\main\\resources\\application.properties 指定zookeeper地址 3.2.3 打包dubbo-admin1mvn clean package -Dmaven.test.skip&#x3D;true 3.2.4 运行dubbo-admin1java -jar dubbo-admin-0.0.1-SNAPSHOT.jar 默认使用root/root 登陆 http://localhost:7001/ 4. dubbo-helloworld4.1 提出需求某个电商系统，订单服务需要调用用户服务获取某个用户的所有地址； 我们现在需要创建两个服务模块进行测试 模块 功能 订单服务web模块 创建订单等 用户服务service模块 查询用户地址等 测试预期结果： 订单服务web模块在A服务器，用户服务模块在B服务器，A可以远程调用B的功能。 4.2 工程架构4.2.1 分包建议将服务接口，服务模型，服务异常等均放在 API 包中，因为服务模型及异常也是 API 的一部分，同时，这样做也符合分包原则：重用发布等价原则(REP)，共同重用原则(CRP)。 如果需要，也可以考虑在 API 包中放置一份 spring 的引用配置，这样使用方便，只需在 spring 加载过程中引用此配置即可，配置建议放在模块的包目录下，以免冲突，如：com/alibaba/china/xxx/dubbo-reference.xml。 4.2.2 粒度服务接口尽可能大粒度，每个服务方法应代表一个功能，而不是某功能的一个步骤，否则将面临分布式事务问题，Dubbo 暂未提供分布式事务支持。 服务接口建议以业务场景为单位划分，并对相近业务做抽象，防止接口数量爆炸。 不建议使用过于抽象的通用接口，如：Map query(Map)，这样的接口没有明确语义，会给后期维护带来不便。 4.3 创建模块4.3.1 gmall-interface公共接口层（model，service，exception…） 作用：定义公共接口，也可以导入公共依赖 4.3.1.1 Bean模型12345678910111213141516/** * 用户信息 * * @author wgy */public class UserAddress implements Serializable &#123; private Integer id; private String userAddress; //用户地址 private String userId; //用户id private String consignee; //收货人 private String phoneNum; //电话号码 private String isDefault; //是否为默认地址 Y-是 N-否 //get/set/toString&#125; 4.3.1.2 Service接口12345678910111213141516/** * 用户服务接口 * * @author wgy */public interface UserService &#123; /** * 按照用户id返回所有的收货地址 * * @param userId * @return */ public List&lt;UserAddress&gt; getUserAddressList(String userId);&#125; 12345678910111213141516/** * 订单接口 * * @author wgy */public interface OrderService &#123; /** * 初始化订单 * * @param userId * @return */ public List&lt;UserAddress&gt; initOrder(String userId);&#125; 4.3.2 gmall-user用户模块（对用户接口的实现） 4.3.2.1 pom.xml1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;gmall-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.3.2.2 Service1234567891011121314/** * 用户服务实现类 * * @author wgy */public class UserServiceImpl implements UserService &#123; @Override public List&lt;UserAddress&gt; getUserAddressList(String userId) &#123; UserAddress address1 = new UserAddress(1, \"北京市昌平区宏福科技园综合楼3层\", \"1\", \"李老师\", \"010-56253825\", \"Y\"); UserAddress address2 = new UserAddress(2, \"深圳市宝安区西部硅谷大厦B座3层（深圳分校）\", \"1\", \"王老师\", \"010-56253825\", \"N\"); return Arrays.asList(address1, address2); &#125;&#125; 4.3.3 gmall-order-web订单模块（调用用户模块） 4.3.3.1 pom.xml1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;gmall-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies 4.3.3.2 测试123456789101112131415161718192021/** * * 订单实现类 * * @author wgy */public class OrderServiceImpl implements OrderService &#123; UserService userService; @Override public List&lt;UserAddress&gt; initOrder(String userId) &#123; System.out.println(\"用户id：\" + userId); //1、查询用户的收货地址 List&lt;UserAddress&gt; addressList = userService.getUserAddressList(userId); for (UserAddress userAddress : addressList) &#123; System.out.println(userAddress.getUserAddress()); &#125; return addressList; &#125;&#125; 现在这样是无法进行调用的。我们gmall-order-web引入了gmall-interface，但是interface的实现是gmall-user，我们并没有引入，而且实际他可能还在别的服务器中。 4.4 使用dubbo改造4.4.1 改造gmall-user作为服务提供者4.4.1.1 引入dubbo1234567891011121314151617181920212223&lt;!-- 引入dubbo --&gt;&lt;!-- https://mvnrepository.com/artifact/com.alibaba/dubbo --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 由于我们使用zookeeper作为注册中心，所以需要操作zookeeper dubbo 2.6以前的版本引入zkclient操作zookeeper dubbo 2.6及以后的版本引入curator操作zookeeper 下面两个zk客户端根据dubbo版本2选1即可 &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt; &lt;/dependency&gt;--&gt;&lt;!-- curator --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 4.4.1.2 配置提供者1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://dubbo.apache.org/schema/dubbo\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd\"&gt; &lt;!-- 1、指定当前服务/应用的名字（同样的服务名字相同，不要和别的服务同名） --&gt; &lt;dubbo:application name=\"user-service-provider\"/&gt; &lt;!-- 2、指定注册中心的位置 --&gt; &lt;!-- &lt;dubbo:registry address=\"zookeeper://192.168.142.128:2181\"&gt;&lt;/dubbo:registry&gt; --&gt; &lt;dubbo:registry protocol=\"zookeeper\" address=\"192.168.142.128:2181\"/&gt; &lt;!-- 3、指定通信规则（通信协议？通信端口） --&gt; &lt;dubbo:protocol name=\"dubbo\" port=\"20880\"/&gt; &lt;!-- 4、暴露服务 ref：指向服务的真正的实现对象 --&gt; &lt;dubbo:service interface=\"com.wgy.gmall.service.UserService\" ref=\"userServiceImpl\"/&gt; &lt;!-- 服务的实现 --&gt; &lt;bean id=\"userServiceImpl\" class=\"com.wgy.gmall.service.impl.UserServiceImpl\"/&gt;&lt;/beans&gt; 4.4.1.3 启动服务1234567891011121314/** * 启动类 * * @author wgy */public class MainApplication &#123; public static void main(String[] args) throws IOException &#123; ClassPathXmlApplicationContext ioc = new ClassPathXmlApplicationContext(\"provider.xml\"); ioc.start(); // 按任意键退出 System.in.read(); &#125;&#125; 4.4.2 改造gmall-order-web作为服务消费者4.4.2.1 引入dubbo1234567891011121314151617181920212223&lt;!-- 引入dubbo --&gt;&lt;!-- https://mvnrepository.com/artifact/com.alibaba/dubbo --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 由于我们使用zookeeper作为注册中心，所以需要操作zookeeper dubbo 2.6以前的版本引入zkclient操作zookeeper dubbo 2.6及以后的版本引入curator操作zookeeper 下面两个zk客户端根据dubbo版本2选1即可 &lt;dependency&gt; &lt;groupId&gt;com.101tec&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.10&lt;/version&gt; &lt;/dependency&gt;--&gt;&lt;!-- curator --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 4.4.2.2 配置消费者信息12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://dubbo.apache.org/schema/dubbo\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 注解扫描 --&gt; &lt;context:component-scan base-package=\"com.wgy.gmall.service.impl\"/&gt; &lt;!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 --&gt; &lt;dubbo:application name=\"order-service-consumer\"/&gt; &lt;!-- 使用zookeeper注册中心暴露发现服务地址 --&gt; &lt;dubbo:registry protocol=\"zookeeper\" address=\"192.168.142.128:2181\"/&gt; &lt;!--声明需要调用的远程服务的接口；生成远程服务代理 --&gt; &lt;dubbo:reference id=\"userService\" interface=\"com.wgy.gmall.service.UserService\"/&gt;&lt;/beans&gt; 4.4.2.3 修改消费者服务实现类123456789101112131415161718192021222324252627/** * 1、将服务提供者注册到注册中心（暴露服务） * 1.1 导入dubbo依赖（2.6.2）\\操作zookeeper的客户端(curator) * 1.2 配置服务提供者 * &lt;p&gt; * 2、让服务消费者去注册中心订阅服务提供者的服务地址 * 订单实现类 * * @author wgy */@Servicepublic class OrderServiceImpl implements OrderService &#123; @Autowired UserService userService; @Override public List&lt;UserAddress&gt; initOrder(String userId) &#123; System.out.println(\"用户id：\" + userId); //1、查询用户的收货地址 List&lt;UserAddress&gt; addressList = userService.getUserAddressList(userId); for (UserAddress userAddress : addressList) &#123; System.out.println(userAddress.getUserAddress()); &#125; return addressList; &#125;&#125; 4.4.3 测试调用访问gmall-order-web的initOrder请求，会调用UserService获取用户地址； 调用成功。说明我们order已经可以调用远程的UserService了； 123456789101112131415161718/** * 启动类 * * @author wgy */public class MainApplication &#123; public static void main(String[] args) throws IOException &#123; ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext(\"consumer.xml\"); OrderService orderService = applicationContext.getBean(OrderService.class); orderService.initOrder(\"1\"); System.out.println(\"调用完成....\"); System.in.read(); &#125;&#125; 4.4.4 注解版4.4.4.1 服务提供方1234567891011121314151617181920212223&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://dubbo.apache.org/schema/dubbo\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd\"&gt; &lt;!-- 1、指定当前服务/应用的名字（同样的服务名字相同，不要和别的服务同名） --&gt; &lt;dubbo:application name=\"user-service-provider\"/&gt; &lt;!-- 2、指定注册中心的位置 --&gt; &lt;!-- &lt;dubbo:registry address=\"zookeeper://192.168.142.128:2181\"&gt;&lt;/dubbo:registry&gt; --&gt; &lt;dubbo:registry protocol=\"zookeeper\" address=\"192.168.142.128:2181\"/&gt; &lt;!-- 3、指定通信规则（通信协议？通信端口） --&gt; &lt;dubbo:protocol name=\"dubbo\" port=\"20880\"/&gt; &lt;!-- 4、暴露服务 注解 --&gt; &lt;dubbo:annotation package=\"com.wgy.gmall.service.impl\"/&gt;&lt;/beans&gt; 123456789101112131415/** * 用户服务实现类 * * @author wgy */@Service //使用dubbo提供的service注解，注册暴露服务public class UserServiceImpl implements UserService &#123; @Override public List&lt;UserAddress&gt; getUserAddressList(String userId) &#123; UserAddress address1 = new UserAddress(1, \"北京市昌平区宏福科技园综合楼3层\", \"1\", \"李老师\", \"010-56253825\", \"Y\"); UserAddress address2 = new UserAddress(2, \"深圳市宝安区西部硅谷大厦B座3层（深圳分校）\", \"1\", \"王老师\", \"010-56253825\", \"N\"); return Arrays.asList(address1, address2); &#125;&#125; 4.4.4.2 服务消费方123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://dubbo.apache.org/schema/dubbo\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 --&gt; &lt;dubbo:application name=\"order-service-consumer\"/&gt; &lt;!-- 使用zookeeper注册中心暴露发现服务地址 --&gt; &lt;dubbo:registry protocol=\"zookeeper\" address=\"192.168.142.128:2181\"/&gt; &lt;!-- 注解声明需要调用的远程服务的接口；生成远程服务代理 --&gt; &lt;dubbo:annotation package=\"com.wgy.gmall.service.impl\"/&gt;&lt;/beans&gt; 123456789101112131415161718192021222324252627/** * 1、将服务提供者注册到注册中心（暴露服务） * 1.1 导入dubbo依赖（2.6.2）\\操作zookeeper的客户端(curator) * 1.2 配置服务提供者 * &lt;p&gt; * 2、让服务消费者去注册中心订阅服务提供者的服务地址 * 订单实现类 * * @author wgy */@Servicepublic class OrderServiceImpl implements OrderService &#123; @Reference //使用dubbo提供的reference注解引用远程服务 UserService userService; @Override public List&lt;UserAddress&gt; initOrder(String userId) &#123; System.out.println(\"用户id：\" + userId); //1、查询用户的收货地址 List&lt;UserAddress&gt; addressList = userService.getUserAddressList(userId); for (UserAddress userAddress : addressList) &#123; System.out.println(userAddress.getUserAddress()); &#125; return addressList; &#125;&#125; 5. 监控中心5.1 dubbo-admin图形化的服务管理页面；安装时需要指定注册中心地址，即可从注册中心中获取到所有的提供者/消费者进行配置管理 5.2 dubbo-monitor-simple简单的监控中心 5.2.1 安装5.2.1.1 下载 dubbo-opshttps://github.com/apache/incubator-dubbo-ops 5.2.1.2 修改配置指定注册中心地址进入 dubbo-monitor-simple\\src\\main\\resources\\conf，修改 dubbo.properties文件 5.2.1.3 打包dubbo-monitor-simple1mvn clean package -Dmaven.test.skip&#x3D;true 5.2.1.4 解压 tar.gz 文件，并运行start.bat如果缺少servlet-api，自行导入servlet-api再访问监控中心 5.2.1.5 启动访问80805.2.2 监控中心配置所有服务配置连接监控中心，进行监控统计 123&lt;!-- 监控中心协议，如果为protocol=\"registry\"，表示从注册中心发现监控中心地址，否则直连监控中心。 --&gt;&lt;dubbo:monitor protocol=\"registry\"/&gt;&lt;!-- &lt;dubbo:monitor address=\"127.0.0.1:7070\"&gt;&lt;/dubbo:monitor&gt; --&gt; Simple Monitor 挂掉不会影响到 Consumer 和 Provider 之间的调用，所以用于生产环境不会有风险。 Simple Monitor 采用磁盘存储统计信息，请注意安装机器的磁盘限制，如果要集群，建议用mount共享磁盘。 6. 整合SpringBoot6.1 引入依赖1234567891011121314151617181920212223242526272829303132&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;gmall-interface&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 注意starter版本适配： 6.2 提供者6.2.1 application.properties12345678910111213141516#服务名，不能跟别的dubbo提供端重复dubbo.application.name=boot-user-service-provider#注册中心的地址加端口号dubbo.registry.address=192.168.142.128:2181#指定注册中心协议dubbo.registry.protocol=zookeeper#分布式固定是dubbo,不要改dubbo.protocol.name=dubbodubbo.protocol.port=20880#监控中心dubbo.monitor.protocol=registry#注解方式要扫描的包#dubbo.scan.base-packages=com.wgy.gmall 6.2.2 xml配置文件12345678910111213141516171819202122232425262728&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dubbo=\"http://dubbo.apache.org/schema/dubbo\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd\"&gt; &lt;!-- 1、指定当前服务/应用的名字（同样的服务名字相同，不要和别的服务同名） --&gt; &lt;dubbo:application name=\"user-service-provider\"/&gt; &lt;!-- 2、指定注册中心的位置 --&gt; &lt;!-- &lt;dubbo:registry address=\"zookeeper://192.168.142.128:2181\"&gt;&lt;/dubbo:registry&gt; --&gt; &lt;dubbo:registry protocol=\"zookeeper\" address=\"192.168.142.128:2181\"/&gt; &lt;!-- 3、指定通信规则（通信协议？通信端口） --&gt; &lt;dubbo:protocol name=\"dubbo\" port=\"20880\"/&gt; &lt;!-- 4、暴露服务 ref：指向服务的真正的实现对象 --&gt; &lt;dubbo:service interface=\"com.wgy.gmall.service.UserService\" ref=\"userServiceImpl\"/&gt; &lt;!-- 服务的实现 --&gt; &lt;bean id=\"userServiceImpl\" class=\"com.wgy.gmall.service.impl.UserServiceImpl\"/&gt; &lt;!-- 连接监控中心 --&gt; &lt;dubbo:monitor protocol=\"registry\"/&gt;&lt;/beans&gt; 6.2.3 手动创建配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * 手动配置 * * @author wgy */@Configurationpublic class MyDubboConfig &#123; @Bean public ApplicationConfig applicationConfig() &#123; ApplicationConfig applicationConfig = new ApplicationConfig(); applicationConfig.setName(\"boot-user-service-provider\"); return applicationConfig; &#125; //&lt;dubbo:registry protocol=\"zookeeper\" address=\"192.168.142.128:2181\"&gt;&lt;/dubbo:registry&gt; @Bean public RegistryConfig registryConfig() &#123; RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setProtocol(\"zookeeper\"); registryConfig.setAddress(\"192.168.142.128:2181\"); return registryConfig; &#125; //&lt;dubbo:protocol name=\"dubbo\" port=\"20882\"&gt;&lt;/dubbo:protocol&gt; @Bean public ProtocolConfig protocolConfig() &#123; ProtocolConfig protocolConfig = new ProtocolConfig(); protocolConfig.setName(\"dubbo\"); protocolConfig.setPort(20882); return protocolConfig; &#125; /** * &lt;dubbo:service interface=\"com.wgy.gmall.service.UserService\" * ref=\"userServiceImpl\" timeout=\"1000\" version=\"1.0.0\"&gt; * &lt;dubbo:method name=\"getUserAddressList\" timeout=\"1000\"&gt;&lt;/dubbo:method&gt; * &lt;/dubbo:service&gt; */ @Bean public ServiceConfig&lt;UserService&gt; userServiceConfig(UserService userService) &#123; ServiceConfig&lt;UserService&gt; serviceConfig = new ServiceConfig&lt;&gt;(); serviceConfig.setInterface(UserService.class); serviceConfig.setRef(userService); serviceConfig.setVersion(\"1.0.0\"); //配置每一个method的信息 MethodConfig methodConfig = new MethodConfig(); methodConfig.setName(\"getUserAddressList\"); methodConfig.setTimeout(1000); //将method的设置关联到service配置中 List&lt;MethodConfig&gt; methods = new ArrayList&lt;&gt;(); methods.add(methodConfig); serviceConfig.setMethods(methods); //ProviderConfig //MonitorConfig return serviceConfig; &#125;&#125; 6.2.4 启动类123456789101112131415161718192021222324252627/** * 1、导入依赖； * 1）、导入dubbo-starter * 2）、导入dubbo的其他依赖 * &lt;p&gt; * 启动类 * * @author wgy * &lt;p&gt; * SpringBoot与dubbo整合的三种方式： * 1）、导入dubbo-starter，在application.properties配置属性，使用@Service【暴露服务】使用@Reference【引用服务】 * 2）、保留dubbo xml配置文件; * 导入dubbo-starter，使用@ImportResource导入dubbo的配置文件即可 * 3）、使用注解API的方式： * 将每一个组件手动创建到容器中,让dubbo来扫描其他的组件 *///@EnableDubbo //开启基于注解的dubbo功能//@ImportResource(locations=\"classpath:provider.xml\")@EnableDubbo(scanBasePackages=\"com.wgy.gmall\")@SpringBootApplicationpublic class BootUserServiceProviderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(BootUserServiceProviderApplication.class, args); &#125;&#125; 6.2.5 服务提供方123456789101112131415/** * 用户服务实现类 * * @author wgy */@Service //使用dubbo提供的service注解，注册暴露服务public class UserServiceImpl implements UserService &#123; @Override public List&lt;UserAddress&gt; getUserAddressList(String userId) &#123; UserAddress address1 = new UserAddress(1, \"北京市昌平区宏福科技园综合楼3层\", \"1\", \"李老师\", \"010-56253825\", \"Y\"); UserAddress address2 = new UserAddress(2, \"深圳市宝安区西部硅谷大厦B座3层（深圳分校）\", \"1\", \"王老师\", \"010-56253825\", \"N\"); return Arrays.asList(address1, address2); &#125;&#125; 6.3 消费者6.3.1 application.properties12345678server.port=8082dubbo.application.name=boot-order-service-consumerdubbo.registry.address=zookeeper://192.168.142.128:2181dubbo.monitor.protocol=registry#如果没有在配置中写dubbo.scan.base-package,还需要使用@EnableDubbo注解 6.3.2 启动类1234567891011121314/** * 启动类 * * @author wgy */@EnableDubbo@SpringBootApplicationpublic class BootOrderServiceConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(BootOrderServiceConsumerApplication.class, args); &#125;&#125; 6.3.3 服务消费方6.3.3.1 Controller1234567891011121314151617/** * 订单controller * * @author wgy */@Controllerpublic class OrderController &#123; @Autowired OrderService orderService; @ResponseBody @RequestMapping(\"/initOrder\") public List&lt;UserAddress&gt; initOrder(@RequestParam(\"uid\") String userId) &#123; return orderService.initOrder(userId); &#125;&#125; 6.3.3.2 Service1234567891011121314151617181920212223242526272829/** * 1、将服务提供者注册到注册中心（暴露服务） * 1.1 导入dubbo依赖（2.6.2）\\操作zookeeper的客户端(curator) * 1.2 配置服务提供者 * &lt;p&gt; * 2、让服务消费者去注册中心订阅服务提供者的服务地址 * 订单实现类 * * @author wgy */@Servicepublic class OrderServiceImpl implements OrderService &#123; @Reference //@Reference(url = \"192.168.142.1:20882\")//dubbo直连 UserService userService; @Override public List&lt;UserAddress&gt; initOrder(String userId) &#123; System.out.println(\"用户id：\" + userId); //1、查询用户的收货地址 List&lt;UserAddress&gt; addressList = userService.getUserAddressList(userId); for (UserAddress userAddress : addressList) &#123; System.out.println(userAddress.getUserAddress()); &#125; return addressList; &#125; &#125; 7. dubbo配置7.1 配置原则 JVM 启动 -D 参数优先，这样可以使用户在部署和启动时进行参数重写，比如在启动时需改变协议的端口。 XML 次之，如果在 XML 中有配置，则 dubbo.properties 中的相应配置项无效。 Properties 最后，相当于缺省值，只有 XML 没有配置时，dubbo.properties 的相应配置项才会生效，通常用于共享公共配置，比如应用名。 7.2 启动时检查Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，以便上线时，能及早发现问题，默认 check=&quot;true&quot;。 可以通过 check=&quot;false&quot; 关闭检查，比如，测试时，有些服务不关心，或者出现了循环依赖，必须有一方先启动。 另外，如果你的 Spring 容器是懒加载的，或者通过 API 编程延迟引用服务，请关闭 check，否则服务临时不可用时，会抛出异常，拿到 null 引用，如果 check=&quot;false&quot;，总是会返回引用，当服务恢复时，能自动连上。 7.2.1 spring 配置文件关闭某个服务的启动时检查 (没有提供者时报错)： 1&lt;dubbo:reference interface=\"com.foo.BarService\" check=\"false\" /&gt; 关闭所有服务的启动时检查 (没有提供者时报错)： 1&lt;dubbo:consumer check=\"false\" /&gt; 关闭注册中心启动时检查 (注册订阅失败时报错)： 1&lt;dubbo:registry check=\"false\" /&gt; 7.2.2 通过 dubbo.properties1234dubbo.reference.com.foo.BarService.check=falsedubbo.reference.check=falsedubbo.consumer.check=falsedubbo.registry.check=false 7.3 超时时间由于网络或服务端不可靠，会导致调用出现一种不确定的中间状态（超时）。为了避免超时导致客户端资源（线程）挂起耗尽，必须设置超时时间。 7.3.1 Dubbo消费端12345678&lt;!-- 全局超时配置 --&gt;&lt;!-- timeout=\"0\" 默认是1000ms--&gt;&lt;dubbo:consumer timeout=\"5000\" /&gt;&lt;!-- 指定接口以及特定方法超时配置 --&gt;&lt;dubbo:reference interface=\"com.wgy.gmall.service.UserService\" timeout=\"2000\"&gt; &lt;dubbo:method name=\"sayHello\" timeout=\"3000\" /&gt;&lt;/dubbo:reference&gt; 7.3.2 Dubbo服务端1234567&lt;!-- 全局超时配置 --&gt;&lt;dubbo:provider timeout=\"5000\" /&gt;&lt;!-- 指定接口以及特定方法超时配置 --&gt;&lt;dubbo:provider interface=\"com.wgy.gmall.service.UserService\" timeout=\"2000\"&gt; &lt;dubbo:method name=\"sayHello\" timeout=\"3000\" /&gt;&lt;/dubbo:provider&gt; 7.3.3 配置原则dubbo推荐在Provider上尽量多配置Consumer端属性： 作服务的提供者，比服务使用方更清楚服务性能参数，如调用的超时时间，合理的重试次数，等等 在Provider配置后，Consumer不配置则会使用Provider的配置值，即Provider配置可以作为Consumer的缺省值。否则，Consumer会使用Consumer端的全局设置，这对于Provider不可控的，并且往往是不合理的 配置的覆盖规则： 精确优先 (方法级优先，接口级次之，全局配置再次之) 消费者设置优先(如果级别一样，则消费方优先，提供方次之) 7.4 重试次数失败自动切换，当出现失败，重试其它服务器，但重试会带来更长延迟。可通过 retries=”2” 来设置重试次数(不含第一次)。 12345678910&lt;!-- retries=\"\":重试次数，不包含第一次调用，0代表不重试 --&gt;&lt;!-- 幂等（设置重试次数）【查询、删除、修改】、非幂等（不能设置重试次数）【新增】 --&gt;&lt;!-- 重试次数配置如下--&gt;&lt;dubbo:service retries=\"2\" /&gt;&lt;!-- 或 --&gt;&lt;dubbo:reference retries=\"2\" /&gt;&lt;!-- 或 --&gt;&lt;dubbo:reference&gt; &lt;dubbo:method name=\"findFoo\" retries=\"2\" /&gt;&lt;/dubbo:reference&gt; 7.5 版本号当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。 可以按照以下的步骤进行版本迁移： 在低压力时间段，先升级一半提供者为新版本 再将所有消费者升级为新版本 然后将剩下的一半提供者升级为新版本 1234567891011121314&lt;!-- 老版本服务提供者配置 --&gt;&lt;dubbo:service interface=\"com.foo.BarService\" version=\"1.0.0\" /&gt;&lt;!-- 新版本服务提供者配置 --&gt;&lt;dubbo:service interface=\"com.foo.BarService\" version=\"2.0.0\" /&gt;&lt;!-- 老版本服务消费者配置 --&gt;&lt;dubbo:reference id=\"barService\" interface=\"com.foo.BarService\" version=\"1.0.0\" /&gt;&lt;!-- 新版本服务消费者配置 --&gt;&lt;dubbo:reference id=\"barService\" interface=\"com.foo.BarService\" version=\"2.0.0\" /&gt;&lt;!-- 如果不需要区分版本，可以按照以下的方式配置 --&gt;&lt;dubbo:reference id=\"barService\" interface=\"com.foo.BarService\" version=\"*\" /&gt; 8. 高可用8.1 zookeeper宕机与dubbo直连现象：zookeeper注册中心宕机，还可以消费dubbo暴露的服务。 原因：健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 高可用：通过设计，减少系统不能提供服务的时间 8.2 集群下dubbo负载均衡配置在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。 负载均衡策略 1234567891011121314151617Random LoadBalance随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。RoundRobin LoadBalance轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。LeastActive LoadBalance最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。ConsistentHash LoadBalance一致性 Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。算法参见：http:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Consistent_hashing缺省只对第一个参数 Hash，如果要修改，请配置 &lt;dubbo:parameter key&#x3D;&quot;hash.arguments&quot; value&#x3D;&quot;0,1&quot; &#x2F;&gt;缺省用 160 份虚拟节点，如果要修改，请配置 &lt;dubbo:parameter key&#x3D;&quot;hash.nodes&quot; value&#x3D;&quot;320&quot; &#x2F;&gt; 1234567891011121314151617181920212223/** * 订单实现类 * * @author wgy */@Servicepublic class OrderServiceImpl implements OrderService &#123; @Reference(loadbalance=\"random\",timeout=1000) UserService userService; @Override public List&lt;UserAddress&gt; initOrder(String userId) &#123; System.out.println(\"用户id：\" + userId); //1、查询用户的收货地址 List&lt;UserAddress&gt; addressList = userService.getUserAddressList(userId); for (UserAddress userAddress : addressList) &#123; System.out.println(userAddress.getUserAddress()); &#125; return addressList; &#125; &#125; 8.3 整合hystrix，服务熔断与降级处理8.3.1 服务降级当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作。 可以通过服务降级功能临时屏蔽某个出错的非关键服务，并定义降级后的返回策略。 mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响 8.3.2 集群容错在集群调用失败时，Dubbo 提供了多种容错方案，缺省为 failover 重试。 集群容错模式 1234567891011121314151617181920212223242526272829303132Failover Cluster失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries&#x3D;&quot;2&quot; 来设置重试次数(不含第一次)。重试次数配置如下：&lt;dubbo:service retries&#x3D;&quot;2&quot; &#x2F;&gt;或&lt;dubbo:reference retries&#x3D;&quot;2&quot; &#x2F;&gt;或&lt;dubbo:reference&gt; &lt;dubbo:method name&#x3D;&quot;findFoo&quot; retries&#x3D;&quot;2&quot; &#x2F;&gt;&lt;&#x2F;dubbo:reference&gt;Failfast Cluster快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。Failsafe Cluster失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。Failback Cluster失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。Forking Cluster并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks&#x3D;&quot;2&quot; 来设置最大并行数。Broadcast Cluster广播调用所有提供者，逐个调用，任意一台报错则报错 [2]。通常用于通知所有提供者更新缓存或日志等本地资源信息。集群模式配置按照以下示例在服务提供方和消费方配置集群模式&lt;dubbo:service cluster&#x3D;&quot;failsafe&quot; &#x2F;&gt;或&lt;dubbo:reference cluster&#x3D;&quot;failsafe&quot; &#x2F;&gt; 8.3.3 整合hystrixHystrix 旨在通过控制那些访问远程系统、服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。Hystrix具备拥有回退机制和断路器功能的线程和信号隔离，请求缓存和请求打包，以及监控和配置等功能 8.3.3.1 配置spring-cloud-starter-netflix-hystrixspring boot官方提供了对hystrix的集成，直接在pom.xml里加入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; 然后在Application类上增加@EnableHystrix来启用hystrix starter： 123456789101112131415&#x2F;** * 启动类 * * @author wgy *&#x2F;@EnableDubbo@EnableHystrix@SpringBootApplicationpublic class BootOrderServiceConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(BootOrderServiceConsumerApplication.class, args); &#125;&#125; 8.3.3.2 配置Provider端在Dubbo的Provider上增加@HystrixCommand配置，这样子调用就会经过Hystrix代理。 12345678910111213141516/** * 用户服务实现类 * * @author wgy */@Service //使用dubbo提供的service注解，注册暴露服务public class UserServiceImpl implements UserService &#123; @HystrixCommand @Override public List&lt;UserAddress&gt; getUserAddressList(String userId) &#123; UserAddress address1 = new UserAddress(1, \"北京市昌平区宏福科技园综合楼3层\", \"1\", \"李老师\", \"010-56253825\", \"Y\"); UserAddress address2 = new UserAddress(2, \"深圳市宝安区西部硅谷大厦B座3层（深圳分校）\", \"1\", \"王老师\", \"010-56253825\", \"N\"); return Arrays.asList(address1, address2); &#125;&#125; 8.3.3.3 配置Consumer端对于Consumer端，则可以增加一层method调用，并在method上配置@HystrixCommand。当调用出错时，会走到fallbackMethod = “hello”的调用里。 12345678910111213141516171819202122232425262728/** * 订单实现类 * * @author wgy */@Servicepublic class OrderServiceImpl implements OrderService &#123; @Reference(loadbalance=\"random\",timeout=1000) //dubbo直连 UserService userService; @HystrixCommand(fallbackMethod = \"hello\") @Override public List&lt;UserAddress&gt; initOrder(String userId) &#123; System.out.println(\"用户id：\" + userId); //1、查询用户的收货地址 List&lt;UserAddress&gt; addressList = userService.getUserAddressList(userId); for (UserAddress userAddress : addressList) &#123; System.out.println(userAddress.getUserAddress()); &#125; return addressList; &#125; public List&lt;UserAddress&gt; hello(String userId) &#123; return Arrays.asList(new UserAddress(10, \"测试地址\", \"1\", \"测试\", \"测试\", \"Y\")); &#125;&#125; 9. dubbo原理9.1 RPC原理 1234567891011一次完整的RPC调用流程（同步调用，异步另说）如下： 1）服务消费方（client）调用以本地调用方式调用服务； 2）client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； 3）client stub找到服务地址，并将消息发送到服务端； 4）server stub收到消息后进行解码； 5）server stub根据解码结果调用本地的服务； 6）本地服务执行并将结果返回给server stub； 7）server stub将返回结果打包成消息并发送至消费方； 8）client stub接收到消息，并进行解码； 9）服务消费方得到最终结果。RPC框架的目标就是要2~8这些步骤都封装起来，这些细节对用户来说是透明的，不可见的。 9.2 netty通信原理Netty是一个异步事件驱动的网络应用程序框架， 用于快速开发可维护的高性能协议服务器和客户端。它极大地简化并简化了TCP和UDP套接字服务器等网络编程。 BIO：(Blocking IO) NIO (Non-Blocking IO) Selector 一般称 为选择器 ，也可以翻译为 多路复用器， Connect（连接就绪）、Accept（接受就绪）、Read（读就绪）、Write（写就绪） Netty基本原理： 9.3 dubbo原理9.3.1 dubbo原理 -框架设计 config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 9.3.2 dubbo原理 -启动解析、加载配置信息 9.3.3 dubbo原理 -服务暴露 9.3.4 dubbo原理 -服务引用 9.3.5 dubbo原理 -服务调用","tags":[{"name":"分布式架构方案","slug":"分布式架构方案","permalink":"https://wgy1993.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/"},{"name":"Dubbo","slug":"Dubbo","permalink":"https://wgy1993.gitee.io/tags/Dubbo/"}]},{"title":"RabbitMQ","date":"2020-10-10T04:39:03.000Z","path":"archives/b543ced0.html","text":"1. 消息中间件概述1.1 什么是消息中间件MQ全称为Message Queue，消息队列是应用程序和应用程序之间的通信方法。 为什么使用MQ 在项目中，可将一些无需即时返回且耗时的操作提取出来，进行异步处理，而这种异步处理的方式大大的节省了服务器的请求响应时间，从而提高了系统的吞吐量。 开发中消息队列通常有如下应用场景： 1、任务异步处理 将不需要同步处理的并且耗时长的操作由消息队列通知消息接收方进行异步处理。提高了应用程序的响应时间。 2、应用程序解耦合 MQ相当于一个中介，生产方通过MQ与消费方交互，它将应用程序进行解耦合。 3、削峰填谷 如订单系统，在下单的时候就会往数据库写数据。但是数据库只能支撑每秒1000左右的并发写入，并发量再高就容易宕机。低峰期的时候并发也就100多个，但是在高峰期时候，并发量会突然激增到5000以上，这个时候数据库肯定卡死了。 消息被MQ保存起来了，然后系统就可以按照自己的消费能力来消费，比如每秒1000个数据，这样慢慢写入数据库，这样就不会卡死数据库了。 但是使用了MQ之后，限制消费消息的速度为1000，但是这样一来，高峰期产生的数据势必会被积压在MQ中，高峰就被“削”掉了。但是因为消息积压，在高峰期过后的一段时间内，消费消息的速度还是会维持在1000QPS，直到消费完积压的消息,这就叫做“填谷” 1.2 AMQP 和 JMSMQ是消息通信的模型；实现MQ的大致有两种主流方式：AMQP、JMS。 1.2.1 AMQPAMQP是一种协议，更准确的说是一种binary wire-level protocol（链接协议）。这是其和JMS的本质差别，AMQP不从API层进行限定，而是直接定义网络交换的数据格式。 1.2.2 JMSJMS即Java消息服务（JavaMessage Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 1.2.3 AMQP 与 JMS 区别 JMS是定义了统一的接口，来对消息操作进行统一；AMQP是通过规定协议来统一数据交互的格式 JMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的。 JMS规定了两种消息模式；而AMQP的消息模式更加丰富 1.3 消息队列产品市场上常见的消息队列有如下： ActiveMQ：基于JMS ZeroMQ：基于C语言开发 RabbitMQ：基于AMQP协议，erlang语言开发，稳定性好 RocketMQ：基于JMS，阿里巴巴产品 Kafka：类似MQ的产品；分布式消息系统，高吞吐量 1.4 RabbitMQRabbitMQ是由erlang语言开发，基于AMQP（Advanced Message Queue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。 RabbitMQ官方地址：http://www.rabbitmq.com/ RabbitMQ提供了6种模式：简单模式，work模式，Publish/Subscribe发布与订阅模式，Routing路由模式，Topics主题模式，RPC远程调用模式（远程调用，不太算MQ；暂不作介绍）； 官网对应模式介绍：https://www.rabbitmq.com/getstarted.html 2. 安装及配置RabbitMQ2.1 安装Socat在线安装依赖环境： 123yum install gccyum install socat 2.2 安装Erlang 123456mkdir /rabbitmq &amp;&amp; cd /rabbitmq# 上传 erlang-22.0.7-1.el7.x86_64.rpm 安装包上传# 安装rpm -ivh erlang-22.0.7-1.el7.x86_64.rpm 2.3 安装RabbitMQ123456cd /rabbitmq# 上传 rabbitmq-server-3.7.17-1.el7.noarch.rpm 安装包# 安装rpm -ivh rabbitmq-server-3.7.17-1.el7.noarch.rpm 2.4 开启管理界面及配置123456789101112131415# 开启管理界面rabbitmq-plugins enable rabbitmq_management# 配置远程可使用guest登录mqcd /usr/share/doc/rabbitmq-server-3.7.17cp rabbitmq.config.example /etc/rabbitmq/rabbitmq.config# 修改配置文件vi /etc/rabbitmq/rabbitmq.config// 创建文件vi /etc/rabbitmq/rabbitmq-env.conf#添加内容NODENAME=rabbit@localhost 修改/etc/rabbitmq/rabbitmq.config配置文件： 2.5 启动12345centos6用这个命令：/sbin/service rabbitmq-server restartcentos7用这个命令：systemctl start rabbitmq-server 2.6 配置虚拟主机及用户2.6.1 用户角色RabbitMQ在安装好后，可以访问http://ip地址:15672 ；其自带了guest/guest的用户名和密码；如果需要创建自定义用户；那么也可以登录管理界面后，如下操作： 角色说明： 超级管理员(administrator) 可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。 监控者(monitoring) 可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等) 策略制定者(policymaker) 可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。 普通管理者(management) 仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。 其他 无法登陆管理控制台，通常就是普通的生产者和消费者。 2.6.2 Virtual Hosts配置像mysql拥有数据库的概念并且可以指定用户对库和表等操作的权限。RabbitMQ也有类似的权限管理；在RabbitMQ中可以虚拟消息服务器Virtual Host，每个Virtual Hosts相当于一个相对独立的RabbitMQ服务器，每个VirtualHost之间是相互隔离的。exchange、queue、message不能互通。 相当于mysql的db。Virtual Name一般以/开头。 2.6.2.1 创建Virtual Hosts 2.6.2.2 设置Virtual Hosts权限 3. RabbitMQ入门3.1 创建工程添加依赖1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;rabbitmq-hello&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!--RabbitMQ的依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;5.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 3.2 编写生产者编写消息生产者com.wgy.rabbitmq.simple.Producer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 简单模式 * &lt;p&gt; * 消息生产者 * * @author wgy */public class Producer &#123; static final String QUEUE_NAME = \"simple_queue\"; public static void main(String[] args) throws Exception &#123; //创建连接工厂 ConnectionFactory connectionFactory = new ConnectionFactory(); //主机地址;默认为 localhost connectionFactory.setHost(\"192.168.142.128\"); //连接端口;默认为 5672 connectionFactory.setPort(5672); //虚拟主机名称;默认为 / connectionFactory.setVirtualHost(\"/test\"); //连接用户名；默认为guest connectionFactory.setUsername(\"wgy\"); //连接密码；默认为guest connectionFactory.setPassword(\"123456\"); //创建连接 Connection connection = connectionFactory.newConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(QUEUE_NAME, true, false, false, null); // 要发送的信息 String message = \"你好；小兔子！\"; /* * 参数1：交换机名称，如果没有指定则使用默认Default Exchange * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(\"\", QUEUE_NAME, null, message.getBytes()); System.out.println(\"已发送消息：\" + message); // 关闭资源 channel.close(); connection.close(); &#125;&#125; 在执行上述的消息发送之后；可以登录rabbitMQ的管理控制台，可以发现队列和其消息： 3.3 编写消费者抽取创建connection的工具类com.wgy.rabbitmq.util.ConnectionUtil； 12345678910111213141516171819202122232425/** * connection的工具类 * * @author wgy */public class ConnectionUtil &#123; public static Connection getConnection() throws Exception &#123; //创建连接工厂 ConnectionFactory connectionFactory = new ConnectionFactory(); //主机地址;默认为 localhost connectionFactory.setHost(\"192.168.142.128\"); //连接端口;默认为 5672 connectionFactory.setPort(5672); //虚拟主机名称;默认为 / connectionFactory.setVirtualHost(\"/test\"); //连接用户名；默认为guest connectionFactory.setUsername(\"wgy\"); //连接密码；默认为guest connectionFactory.setPassword(\"123456\"); //创建连接 return connectionFactory.newConnection(); &#125;&#125; 编写消息的消费者com.wgy.rabbitmq.simple.Consumer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 消息消费者 * * @author wgy */public class Consumer &#123; public static void main(String[] args) throws Exception &#123; //获取连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"接收到的消息为：\" + new String(body, \"UTF-8\")); &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, true, consumer); //不关闭资源，应该一直监听消息 //channel.close(); //connection.close(); &#125;&#125; 3.4 小结上述的入门案例中其实使用的是如下的简单模式： 在上图的模型中，有以下概念： P：生产者，也就是要发送消息的程序 C：消费者：消息的接受者，会一直等待消息到来。 queue：消息队列，图中红色部分。类似一个邮箱，可以缓存消息；生产者向其中投递消息，消费者从其中取出消息。 在rabbitMQ中消费者是一定要到某个消息队列中去获取消息的 4. AMQP4.1. 相关概念介绍AMQP 一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。 AMQP是一个二进制协议，拥有一些现代化特点：多信道、协商式，异步，安全，扩平台，中立，高效。 RabbitMQ是AMQP协议的Erlang的实现。 概念 说明 连接Connection 一个网络连接，比如TCP/IP套接字连接。 会话Session 端点之间的命名对话。在一个会话上下文中，保证“恰好传递一次”。 信道Channel 多路复用连接中的一条独立的双向数据流通道。为会话提供物理传输介质。 客户端Client AMQP连接或者会话的发起者。AMQP是非对称的，客户端生产和消费消息，服务器存储和路由这些消息。 服务节点Broker 消息中间件的服务节点；一般情况下可以将一个RabbitMQ Broker看作一台RabbitMQ 服务器。 端点 AMQP对话的任意一方。一个AMQP连接包括两个端点（一个是客户端，一个是服务器）。 消费者Consumer 一个从消息队列里请求消息的客户端程序。 生产者Producer 一个向交换机发布消息的客户端应用程序。 4.2 RabbitMQ运转流程在入门案例中： 生产者发送消息 生产者创建连接（Connection），开启一个信道（Channel），连接到RabbitMQ Broker； 声明队列并设置属性；如是否排它，是否持久化，是否自动删除； 将路由键（空字符串）与队列绑定起来； 发送消息至RabbitMQ Broker； 关闭信道； 关闭连接； 消费者接收消息 消费者创建连接（Connection），开启一个信道（Channel），连接到RabbitMQ Broker 向Broker 请求消费相应队列中的消息，设置相应的回调函数； 等待Broker回应闭关投递响应队列中的消息，消费者接收消息； 确认（ack，自动确认）接收到的消息； RabbitMQ从队列中删除相应已经被确认的消息； 关闭信道； 关闭连接； 4.3 生产者流转过程说明 客户端与代理服务器Broker建立连接。会调用newConnection() 方法,这个方法会进一步封装Protocol Header 0-9-1 的报文头发送给Broker ，以此通知Broker 本次交互采用的是AMQP 0-9-1 协议，紧接着Broker 返回Connection.Start 来建立连接，在连接的过程中涉及Connection.Start/.Start-OK 、Connection.Tune/.Tune-Ok ，Connection.Open/ .Open-Ok 这6 个命令的交互。 客户端调用connection.createChannel方法。此方法开启信道，其包装的channel.open命令发送给Broker,等待channel.basicPublish方法，对应的AMQP命令为Basic.Publish,这个命令包含了content Header 和content Body()。content Header 包含了消息体的属性，例如:投递模式，优先级等，content Body 包含了消息体本身。 客户端发送完消息需要关闭资源时，涉及到Channel.Close和Channl.Close-Ok 与Connetion.Close和Connection.Close-Ok的命令交互。 4.4 消费者流转过程说明 消费者客户端与代理服务器Broker建立连接。会调用newConnection() 方法,这个方法会进一步封装Protocol Header 0-9-1 的报文头发送给Broker ，以此通知Broker 本次交互采用的是AMQP 0-9-1 协议，紧接着Broker 返回Connection.Start 来建立连接，在连接的过程中涉及Connection.Start/.Start-OK 、Connection.Tune/.Tune-Ok ，Connection.Open/ .Open-Ok 这6 个命令的交互。 消费者客户端调用connection.createChannel方法。和生产者客户端一样，协议涉及Channel.Open/Open-Ok命令。 在真正消费之前，消费者客户端需要向Broker 发送Basic.Consume 命令(即调用channel.basicConsume 方法〉将Channel 置为接收模式，之后Broker 回执Basic.Consume - Ok 以告诉消费者客户端准备好消费消息。 Broker 向消费者客户端推送(Push) 消息，即Basic.Deliver 命令，这个命令和Basic.Publish 命令一样会携带Content Header 和Content Body。 消费者接收到消息并正确消费之后，向Broker 发送确认，即Basic.Ack 命令。 客户端发送完消息需要关闭资源时，涉及到Channel.Close和Channl.Close-Ok 与Connetion.Close和Connection.Close-Ok的命令交互。 5. RabbitMQ工作模式5.1 Work queues工作队列模式5.1.1 模式说明 Work Queues与入门程序的简单模式相比，多了一个或一些消费端，多个消费端共同消费同一个队列中的消息。 应用场景：对于任务过重或任务较多情况使用工作队列可以提高任务处理的速度。 5.1.2 代码Work Queues与入门程序的简单模式的代码是几乎一样的；可以完全复制，并复制多一个消费者进行多个消费者同时消费消息的测试。 5.1.2.1 生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Work queues工作队列模式 * &lt;p&gt; * 消息生产者 * * @author wgy */public class Producer &#123; static final String QUEUE_NAME = \"work_queue\"; public static void main(String[] args) throws Exception &#123; //创建连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(QUEUE_NAME, true, false, false, null); for (int i = 1; i &lt;= 30; i++) &#123; // 发送信息 String message = \"你好；小兔子！work模式--\" + i; /* * 参数1：交换机名称，如果没有指定则使用默认Default Exchange * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(\"\", QUEUE_NAME, null, message.getBytes()); System.out.println(\"已发送消息：\" + message); &#125; // 关闭资源 channel.close(); connection.close(); &#125;&#125; 5.1.2.2 消费者1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 消息消费者1 * * @author wgy */public class Consumer1 &#123; public static void main(String[] args) throws Exception &#123; //获取连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"消费者1-接收到的消息为：\" + new String(body, \"utf-8\")); try &#123; Thread.sleep(1000);//休眠一秒钟 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, true, consumer); &#125;&#125; 5.1.2.3 消费者2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 消息消费者2 * * @author wgy */public class Consumer2 &#123; public static void main(String[] args) throws Exception &#123; //获取连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"消费者2-接收到的消息为：\" + new String(body, \"utf-8\")); try &#123; Thread.sleep(1000);//休眠一秒钟 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, true, consumer); &#125;&#125; 5.1.3 测试启动两个消费者，然后再启动生产者发送消息；到IDEA的两个消费者对应的控制台查看是否竞争性的接收到消息。 5.1.4 小结在一个队列中如果有多个消费者，那么消费者之间对于同一个消息的关系是竞争的关系。 5.2 订阅模式类型订阅模式示例图： 前面2个案例中，只有3个角色： P：生产者，也就是要发送消息的程序 C：消费者：消息的接受者，会一直等待消息到来。 queue：消息队列，图中红色部分 而在订阅模型中，多了一个exchange角色，而且过程略有变化： P：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机） C：消费者，消息的接受者，会一直等待消息到来。 Queue：消息队列，接收消息、缓存消息。 Exchange：交换机，图中的X。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有常见以下3种类型： Fanout：广播，将消息交给所有绑定到交换机的队列 Direct：定向，把消息交给符合指定routing key 的队列 Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列 Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！ 5.3 Publish/Subscribe发布与订阅模式5.3.1 模式说明 发布订阅模式： 1、每个消费者监听自己的队列。 2、生产者将消息发给broker，由交换机将消息转发到绑定此交换机的每个队列，每个绑定交换机的队列都将接收到消息 5.3.2 代码5.3.2.1 生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * Publish/Subscribe发布与订阅模式 * &lt;p&gt; * 生产者 * 发布与订阅使用的交换机类型为：fanout * * @author wgy */public class Producer &#123; /** * 交换机名称 */ static final String FANOUT_EXCHANGE = \"fanout_exchange\"; /** * 队列名称 */ static final String FANOUT_QUEUE_1 = \"fanout_queue_1\"; /** * 队列名称 */ static final String FANOUT_QUEUE_2 = \"fanout_queue_2\"; public static void main(String[] args) throws Exception &#123; //创建连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); /* * 声明交换机 * 参数1：交换机名称 * 参数2：交换机类型，fanout、topic、direct、headers */ channel.exchangeDeclare(FANOUT_EXCHANGE, BuiltinExchangeType.FANOUT); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(FANOUT_QUEUE_1, true, false, false, null); channel.queueDeclare(FANOUT_QUEUE_2, true, false, false, null); //队列绑定交换机 channel.queueBind(FANOUT_QUEUE_1, FANOUT_EXCHANGE, \"\"); channel.queueBind(FANOUT_QUEUE_2, FANOUT_EXCHANGE, \"\"); for (int i = 1; i &lt;= 10; i++) &#123; // 发送信息 String message = \"你好；小兔子！发布订阅模式--\" + i; /* * 参数1：交换机名称，如果没有指定则使用默认Default Exchange * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(FANOUT_EXCHANGE, \"\", null, message.getBytes()); System.out.println(\"已发送消息：\" + message); &#125; // 关闭资源 channel.close(); connection.close(); &#125;&#125; 5.3.2.2 消费者112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 消费者1 * * @author wgy */public class Consumer1 &#123; public static void main(String[] args) throws Exception &#123; //创建连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); //声明交换机 channel.exchangeDeclare(Producer.FANOUT_EXCHANGE, BuiltinExchangeType.FANOUT); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.FANOUT_QUEUE_1, true, false, false, null); //队列绑定交换机 channel.queueBind(Producer.FANOUT_QUEUE_1, Producer.FANOUT_EXCHANGE, \"\"); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"消费者1-接收到的消息为：\" + new String(body, \"utf-8\")); &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.FANOUT_QUEUE_1, true, consumer); &#125;&#125; 5.3.2.3 消费者212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 消费者2 * * @author wgy */public class Consumer2 &#123; public static void main(String[] args) throws Exception &#123; //创建连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); //声明交换机 channel.exchangeDeclare(Producer.FANOUT_EXCHANGE, BuiltinExchangeType.FANOUT); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.FANOUT_QUEUE_2, true, false, false, null); //队列绑定交换机 channel.queueBind(Producer.FANOUT_QUEUE_2, Producer.FANOUT_EXCHANGE, \"\"); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"消费者2-接收到的消息为：\" + new String(body, \"utf-8\")); &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.FANOUT_QUEUE_2, true, consumer); &#125;&#125; 5.3.3. 测试启动所有消费者，然后使用生产者发送消息；在每个消费者对应的控制台可以查看到生产者发送的所有消息；到达广播的效果。 在执行完测试代码后，其实到RabbitMQ的管理后台找到Exchanges选项卡，点击 fanout_exchange 的交换机，可以查看到如的绑定： 5.3.4 小结交换机需要与队列进行绑定，绑定之后；一个消息可以被多个消费者都收到。 发布订阅模式与工作队列模式的区别： 工作队列模式不用定义交换机，而发布/订阅模式需要定义交换机。 发布/订阅模式的生产方是面向交换机发送消息，工作队列模式的生产方是面向队列发送消息(底层使用默认交换机)。 发布/订阅模式需要设置队列和交换机的绑定，工作队列模式不需要设置，实际上工作队列模式会将队列绑 定到默认的交换机 。 5.4 Routing路由模式5.4.1 模式说明路由模式特点： 队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key） 消息的发送方在 向 Exchange发送消息时，也必须指定消息的 RoutingKey。 Exchange不再把消息交给每一个绑定的队列，而是根据消息的Routing Key进行判断，只有队列的Routingkey与消息的 Routing key完全一致，才会接收到消息 图解： P：生产者，向Exchange发送消息，发送消息时，会指定一个routing key。 X：Exchange（交换机），接收生产者的消息，然后把消息递交给 与routing key完全匹配的队列 C1：消费者，其所在队列指定了需要routing key 为 error 的消息 C2：消费者，其所在队列指定了需要routing key 为 info、error、warning 的消息 5.4.2 代码在编码上与 Publish/Subscribe发布与订阅模式 的区别是交换机的类型为：Direct，还有队列绑定交换机的时候需要指定routing key。 5.4.2.1 生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * Routing路由模式 * &lt;p&gt; * 生产者 * 路由模式的交换机类型为：direct * * @author wgy */public class Producer &#123; /** * 交换机名称 */ static final String DIRECT_EXCHANGE = \"direct_exchange\"; /** * 队列名称 */ static final String DIRECT_QUEUE_INSERT = \"direct_queue_insert\"; /** * 队列名称 */ static final String DIRECT_QUEUE_UPDATE = \"direct_queue_update\"; public static void main(String[] args) throws Exception &#123; //创建连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); /* * 声明交换机 * 参数1：交换机名称 * 参数2：交换机类型，fanout、topic、direct、headers */ channel.exchangeDeclare(DIRECT_EXCHANGE, BuiltinExchangeType.DIRECT); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(DIRECT_QUEUE_INSERT, true, false, false, null); channel.queueDeclare(DIRECT_QUEUE_UPDATE, true, false, false, null); //队列绑定交换机 channel.queueBind(DIRECT_QUEUE_INSERT, DIRECT_EXCHANGE, \"insert\"); channel.queueBind(DIRECT_QUEUE_UPDATE, DIRECT_EXCHANGE, \"update\"); // 发送信息 String message = \"新增了商品。路由模式；routing key 为 insert \"; /* * 参数1：交换机名称，如果没有指定则使用默认Default Exchange * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(DIRECT_EXCHANGE, \"insert\", null, message.getBytes()); System.out.println(\"已发送消息：\" + message); // 发送信息 message = \"修改了商品。路由模式；routing key 为 update\"; /* * 参数1：交换机名称，如果没有指定则使用默认Default Exchange * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(DIRECT_EXCHANGE, \"update\", null, message.getBytes()); System.out.println(\"已发送消息：\" + message); // 关闭资源 channel.close(); connection.close(); &#125;&#125; 5.4.2.2 消费者112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 消息消费者1 * * @author wgy */public class Consumer1 &#123; public static void main(String[] args) throws Exception &#123; //获取连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); //声明交换机 channel.exchangeDeclare(Producer.DIRECT_EXCHANGE, BuiltinExchangeType.DIRECT); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.DIRECT_QUEUE_INSERT, true, false, false, null); //队列绑定交换机 channel.queueBind(Producer.DIRECT_QUEUE_INSERT, Producer.DIRECT_EXCHANGE, \"insert\"); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"消费者1-接收到的消息为：\" + new String(body, \"utf-8\")); &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.DIRECT_QUEUE_INSERT, true, consumer); &#125;&#125; 5.4.2.3 消费者212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 消息消费者2 * * @author wgy */public class Consumer2 &#123; public static void main(String[] args) throws Exception &#123; //获取连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); //声明交换机 channel.exchangeDeclare(Producer.DIRECT_EXCHANGE, BuiltinExchangeType.DIRECT); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.DIRECT_QUEUE_UPDATE, true, false, false, null); //队列绑定交换机 channel.queueBind(Producer.DIRECT_QUEUE_UPDATE, Producer.DIRECT_EXCHANGE, \"update\"); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"消费者2-接收到的消息为：\" + new String(body, \"utf-8\")); &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.DIRECT_QUEUE_UPDATE, true, consumer); &#125;&#125; 5.4.3 测试启动所有消费者，然后使用生产者发送消息；在消费者对应的控制台可以查看到生产者发送对应routing key对应队列的消息；到达按照需要接收的效果。 在执行完测试代码后，其实到RabbitMQ的管理后台找到Exchanges选项卡，点击 direct_exchange 的交换机，可以查看到如下的绑定： 5.4.4. 小结Routing模式要求队列在绑定交换机时要指定routing key，消息会转发到符合routing key的队列。 5.5 Topics通配符模式5.5.1 模式说明Topic类型与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！ Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert 通配符规则： #：匹配一个或多个词 *：匹配不多不少恰好1个词 举例： item.#：能够匹配item.insert.abc 或者 item.insert item.*：只能匹配item.insert 5.5.2 代码5.5.2.1 生产者使用topic类型的Exchange，发送消息的routing key有3种： item.insert、item.update、item.delete： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Topics通配符模式 * &lt;p&gt; * 生产者 * 通配符Topic的交换机类型为：topic * * @author wgy */public class Producer &#123; /** * 交换机名称 */ static final String TOPIC_EXCHANGE = \"topic_exchange\"; /** * 队列名称 */ static final String TOPIC_QUEUE_1 = \"topic_queue_1\"; /** * 队列名称 */ static final String TOPIC_QUEUE_2 = \"topic_queue_2\"; public static void main(String[] args) throws Exception &#123; //创建连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); /* * 声明交换机 * 参数1：交换机名称 * 参数2：交换机类型，fanout、topic、topic、headers */ channel.exchangeDeclare(TOPIC_EXCHANGE, BuiltinExchangeType.TOPIC); // 发送信息 String message = \"新增了商品。Topic模式；routing key 为 item.insert \"; channel.basicPublish(TOPIC_EXCHANGE, \"item.insert\", null, message.getBytes()); System.out.println(\"已发送消息：\" + message); // 发送信息 message = \"修改了商品。Topic模式；routing key 为 item.update\"; channel.basicPublish(TOPIC_EXCHANGE, \"item.update\", null, message.getBytes()); System.out.println(\"已发送消息：\" + message); // 发送信息 message = \"删除了商品。Topic模式；routing key 为 item.delete\"; channel.basicPublish(TOPIC_EXCHANGE, \"item.delete\", null, message.getBytes()); System.out.println(\"已发送消息：\" + message); // 关闭资源 channel.close(); connection.close(); &#125;&#125; 5.2.2.2 消费者1接收两种类型的消息：更新商品和删除商品 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 消息消费者1 * * @author wgy */public class Consumer1 &#123; public static void main(String[] args) throws Exception &#123; //获取连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); //声明交换机 channel.exchangeDeclare(Producer.TOPIC_EXCHANGE, BuiltinExchangeType.TOPIC); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.TOPIC_QUEUE_1, true, false, false, null); //队列绑定交换机 channel.queueBind(Producer.TOPIC_QUEUE_1, Producer.TOPIC_EXCHANGE, \"item.update\"); channel.queueBind(Producer.TOPIC_QUEUE_1, Producer.TOPIC_EXCHANGE, \"item.delete\"); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"消费者1-接收到的消息为：\" + new String(body, \"utf-8\")); &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.TOPIC_QUEUE_1, true, consumer); &#125;&#125; 5.2.2.3 消费者2接收所有类型的消息：新增商品，更新商品和删除商品。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 消息消费者2 * * @author wgy */public class Consumer2 &#123; public static void main(String[] args) throws Exception &#123; //获取连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); //声明交换机 channel.exchangeDeclare(Producer.TOPIC_EXCHANGE, BuiltinExchangeType.TOPIC); // 声明（创建）队列 /* * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.TOPIC_QUEUE_2, true, false, false, null); //队列绑定交换机 channel.queueBind(Producer.TOPIC_QUEUE_2, Producer.TOPIC_EXCHANGE, \"item.*\"); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel) &#123; @Override /* * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(\"路由key为：\" + envelope.getRoutingKey()); //交换机 System.out.println(\"交换机为：\" + envelope.getExchange()); //消息id System.out.println(\"消息id为：\" + envelope.getDeliveryTag()); //收到的消息 System.out.println(\"消费者2-接收到的消息为：\" + new String(body, \"utf-8\")); &#125; &#125;; //监听消息 /* * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.TOPIC_QUEUE_2, true, consumer); &#125;&#125; 5.5.3 测试启动所有消费者，然后使用生产者发送消息；在消费者对应的控制台可以查看到生产者发送对应routing key对应队列的消息；到达按照需要接收的效果；并且这些routing key可以使用通配符。 在执行完测试代码后，其实到RabbitMQ的管理后台找到Exchanges选项卡，点击 topic_exchange 的交换机，可以查看到如下的绑定： 5.5.4. 小结Topic主题模式可以实现 Publish/Subscribe发布与订阅模式 和 Routing路由模式 的功能；只是Topic在配置routing key 的时候可以使用通配符，显得更加灵活。 5.6 模式总结RabbitMQ工作模式： 1、简单模式 HelloWorld 一个生产者、一个消费者，不需要设置交换机（使用默认的交换机） 2、工作队列模式 Work Queue 一个生产者、多个消费者（竞争关系），不需要设置交换机（使用默认的交换机） 3、发布订阅模式 Publish/subscribe 需要设置类型为fanout的交换机，并且交换机和队列进行绑定，当发送消息到交换机后，交换机会将消息发送到绑定的队列 4、路由模式 Routing 需要设置类型为direct的交换机，交换机和队列进行绑定，并且指定routing key，当发送消息到交换机后，交换机会根据routing key将消息发送到对应的队列 5、通配符模式 Topic 需要设置类型为topic的交换机，交换机和队列进行绑定，并且指定通配符方式的routing key，当发送消息到交换机后，交换机会根据routing key将消息发送到对应的队列 6. Spring Boot整合RabbitMQ6.1 简介在Spring项目中，可以使用Spring-Rabbit去操作RabbitMQ https://github.com/spring-projects/spring-amqp 尤其是在spring boot项目中只需要引入对应的amqp启动器依赖即可，方便的使用RabbitTemplate发送消息，使用注解接收消息。 一般在开发过程中： 生产者工程： application.yml文件配置RabbitMQ相关信息； 在生产者工程中编写配置类，用于创建交换机和队列，并进行绑定 注入RabbitTemplate对象，通过RabbitTemplate对象发送消息到交换机 消费者工程： application.yml文件配置RabbitMQ相关信息 创建消息处理类，用于接收队列中的消息并进行处理 6.2 搭建生产者工程6.2.1 创建工程添加依赖12345678910111213141516171819202122232425262728293031323334&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;springboot-rabbitmq-producer&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!-- 使用springmvc来进行测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--amqp的起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--单元测试类--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 6.2.2 配置RabbitMQ6.2.2.1 配置文件创建application.yml，内容如下： 1234567891011#端口server: port: 8888#Rabbitmq的配置spring: rabbitmq: host: 192.168.142.128 port: 5672 virtual-host: /test username: wgy password: 123456 6.2.2.2 绑定交换机和队列创建RabbitMQ队列与交换机绑定的配置类com.wgy.rabbitmq.config.RabbitMQConfig 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * RabbitMQ配置类 * * @author wgy */@Configurationpublic class RabbitMQConfig &#123; /** * 交换机名称 */ public static final String ITEM_TOPIC_EXCHANGE = \"item_topic_exchange\"; /** * 队列名称 */ public static final String ITEM_QUEUE = \"item_queue\"; /** * 声明交换机 * * @return */ @Bean(\"itemTopicExchange\") public Exchange topicExchange() &#123; return ExchangeBuilder.topicExchange(ITEM_TOPIC_EXCHANGE).durable(true).build(); &#125; /** * 声明队列 * * @return */ @Bean(\"itemQueue\") public Queue itemQueue() &#123; return QueueBuilder.durable(ITEM_QUEUE).build(); &#125; /** * 绑定队列和交换机 * * @param queue * @param exchange * @return */ @Bean public Binding itemQueueExchange(@Qualifier(\"itemQueue\") Queue queue, @Qualifier(\"itemTopicExchange\") Exchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange).with(\"item.#\").noargs(); &#125;&#125; 6.2.3 启动类123456789101112/** * 生产者的启动类 * * @author wgy */@SpringBootApplicationpublic class ProducerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ProducerApplication.class, args); &#125;&#125; 6.2.4 消息发送Controller我们创建一个SpringMVC的Controller方便我们进行测试 1234567891011121314151617181920212223242526272829303132/** * 发送消息的测试类 * * @author wgy */@RestControllerpublic class SendMsgController &#123; /** * 注入RabbitMQ的模板 */ @Autowired private RabbitTemplate rabbitTemplate; /** * 测试 */ @GetMapping(\"/sendmsg\") public String sendMsg(@RequestParam String msg, @RequestParam String key) &#123; /* * 发送消息 * 参数一：交换机名称 * 参数二：路由key * 参数三：发送的消息 */ rabbitTemplate.convertAndSend(RabbitMQConfig.ITEM_TOPIC_EXCHANGE, key, msg); //返回消息 return \"发送消息成功！\"; &#125;&#125; 6.3 搭建消费者工程6.3.1 创建工程添加依赖123456789101112131415161718192021222324252627&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;springboot-rabbitmq-consumer&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 6.3.2 配置RabbitMQ创建application.yml，内容如下： 1234567891011#端口server: port: 9999#Rabbitmq的配置spring: rabbitmq: host: 192.168.142.128 port: 5672 virtual-host: /test username: wgy password: 123456 6.3.3 启动类123456789101112/** * 启动类 * * @author wgy */@SpringBootApplicationpublic class ConsumerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ConsumerApplication.class, args); &#125;&#125; 6.3.4 消息监听处理类编写消息监听器com.wgy.rabbitmq.listener.MyListener 123456789101112131415161718/** * 消费者监听类 * * @author wgy */@Componentpublic class MyListener &#123; /** * 监听某个队列的消息 * * @param message 接收到的消息 */ @RabbitListener(queues = \"item_queue\") public void myListener1(String message) &#123; System.out.println(\"消费者接收到的消息为：\" + message); &#125;&#125; 6.3.5 测试 在生产者工程springboot-rabbitmq-producer中创建测试类，发送消息： 12345678910111213141516171819/** * 测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class ProducerTest &#123; @Autowired private RabbitTemplate rabbitTemplate; @Test public void test() &#123; rabbitTemplate.convertAndSend(RabbitMQConfig.ITEM_TOPIC_EXCHANGE, \"item.insert\", \"商品新增，routing key 为item.insert\"); rabbitTemplate.convertAndSend(RabbitMQConfig.ITEM_TOPIC_EXCHANGE, \"item.update\", \"商品修改，routing key 为item.update\"); rabbitTemplate.convertAndSend(RabbitMQConfig.ITEM_TOPIC_EXCHANGE, \"item.delete\", \"商品删除，routing key 为item.delete\"); &#125;&#125; 先运行上述测试程序（交换机和队列才能先被声明和绑定），然后启动消费者；在消费者工程springboot-rabbitmq-consumer中控制台查看是否接收到对应消息。 7. RabbitMQ 高级7.1 过期时间TTL过期时间TTL表示可以对消息设置预期的时间，在这个时间内都可以被消费者接收获取；过了之后消息将自动被删除。RabbitMQ可以对消息和队列设置TTL。目前有两种方法可以设置。 第一种方法是通过队列属性设置，队列中所有消息都有相同的过期时间。 第二种方法是对消息进行单独设置，每条消息TTL可以不同。 如果上述两种方法同时使用，则消息的过期时间以两者之间TTL较小的那个数值为准。消息在队列的生存时间一旦超过设置的TTL值，就称为dead message被投递到死信队列， 消费者将无法再收到该消息。 7.1.1 设置队列TTL在 springboot-rabbitmq-producer\\src\\main\\resources\\spring\\spring-rabbitmq.xml 文件中添加如下内容： 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:rabbit=\"http://www.springframework.org/schema/rabbit\" xsi:schemaLocation=\" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/rabbit http://www.springframework.org/schema/rabbit/spring-rabbit.xsd\"&gt; &lt;!--定义过期队列及其属性，不存在则自动创建--&gt; &lt;rabbit:queue id=\"my_ttl_queue\" name=\"my_ttl_queue\" auto-declare=\"true\"&gt; &lt;rabbit:queue-arguments&gt; &lt;!--投递到该队列的消息如果没有消费都将在6秒之后被删除--&gt; &lt;entry key=\"x-message-ttl\" value-type=\"long\" value=\"6000\"/&gt; &lt;/rabbit:queue-arguments&gt; &lt;/rabbit:queue&gt;&lt;/beans&gt; 启动类导入配置文件 1234567891011121314/** * 生产者的启动类 * * @author wgy */@SpringBootApplication//导入配置文件@ImportResource(\"classpath:/spring/spring-rabbitmq.xml\")public class ProducerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ProducerApplication.class, args); &#125;&#125; 然后在测试类 springboot-rabbitmq-producer\\src\\test\\java\\com\\wgy\\rabbitmq\\ProducerTest.java 中编写如下方法发送消息到上述定义的队列： 123456789/** * 过期队列消息 * 投递到该队列的消息如果没有消费都将在6秒之后被删除 */@Testpublic void ttlQueueTest() &#123; //路由键与队列同名 rabbitTemplate.convertAndSend(\"my_ttl_queue\", \"发送到过期队列my_ttl_queue，6秒内不消费则不能再被消费。\");&#125; 参数 x-message-ttl 的值 必须是非负 32 位整数 (0 &lt;= n &lt;= 2^32-1) ，以毫秒为单位表示 TTL 的值。这样，值 6000 表示存在于 队列 中的当前 消息 将最多只存活 6 秒钟。 如果不设置TTL,则表示此消息不会过期。如果将TTL设置为0，则表示除非此时可以直接将消息投递到消费者，否则该消息会被立即丢弃。 7.1.2 设置消息TTL消息的过期时间；只需要在发送消息（可以发送到任何队列，不管该队列是否属于某个交换机）的时候设置过期时间即可。在测试类中编写如下方法发送消息并设置过期时间到队列： 1234567891011121314/** * 过期消息 * 该消息投递任何交换机或队列中的时候；如果到了过期时间则将从该队列中删除 */@Testpublic void ttlMessageTest() &#123; MessageProperties messageProperties = new MessageProperties(); //设置消息的过期时间，3秒 messageProperties.setExpiration(\"3000\"); Message message = new Message(\"测试过期消息，3秒钟过期\".getBytes(), messageProperties); //路由键与队列同名 rabbitTemplate.convertAndSend(\"my_ttl_queue\", message);&#125; expiration 字段以微秒为单位表示 TTL 值。且与 x-message-ttl 具有相同的约束条件。因为 expiration 字段必须为字符串类型，broker 将只会接受以字符串形式表达的数字。 当同时指定了 queue 和 message 的 TTL 值，则两者中较小的那个才会起作用。 7.2 死信队列DLX，全称为Dead-Letter-Exchange , 可以称之为死信交换机，也有人称之为死信邮箱。当消息在一个队列中变成死信(dead message)之后，它能被重新发送到另一个交换机中，这个交换机就是DLX ，绑定DLX的队列就称之为死信队列。 消息变成死信，可能是由于以下的原因： 消息被拒绝 消息过期 队列达到最大长度 DLX也是一个正常的交换机，和一般的交换机没有区别，它能在任何的队列上被指定，实际上就是设置某一个队列的属性。当这个队列中存在死信时，Rabbitmq就会自动地将这个消息重新发布到设置的DLX上去，进而被路由到另一个队列，即死信队列。 要想使用死信队列，只需要在定义队列的时候设置队列参数 x-dead-letter-exchange 指定交换机即可。 具体步骤如下面的章节。 7.2.1 定义死信交换机在 springboot-rabbitmq-producer\\src\\main\\resources\\spring\\spring-rabbitmq.xml 文件中添加如下内容： 1234567891011&lt;!--定义定向交换机中的持久化死信队列，不存在则自动创建--&gt;&lt;rabbit:queue id=\"my_dlx_queue\" name=\"my_dlx_queue\" auto-declare=\"true\"/&gt;&lt;!--定义广播类型交换机；并绑定上述两个队列--&gt;&lt;rabbit:direct-exchange id=\"my_dlx_exchange\" name=\"my_dlx_exchange\" auto-declare=\"true\"&gt; &lt;rabbit:bindings&gt; &lt;!--绑定路由键my_ttl_dlx、my_max_dlx，可以将过期的消息转移到my_dlx_queue队列--&gt; &lt;rabbit:binding key=\"my_ttl_dlx\" queue=\"my_dlx_queue\"/&gt; &lt;rabbit:binding key=\"my_max_dlx\" queue=\"my_dlx_queue\"/&gt; &lt;/rabbit:bindings&gt;&lt;/rabbit:direct-exchange&gt; 7.2.2 队列设置死信交换机为了测试消息在过期、队列达到最大长度后都将被投递死信交换机上；所以添加配置如下： 在 springboot-rabbitmq-producer\\src\\main\\resources\\spring\\spring-rabbitmq.xml 文件中添加如下内容： 123456789101112131415161718192021222324252627&lt;!--定义过期队列及其属性，不存在则自动创建--&gt;&lt;rabbit:queue id=\"my_ttl_dlx_queue\" name=\"my_ttl_dlx_queue\" auto-declare=\"true\"&gt; &lt;rabbit:queue-arguments&gt; &lt;!--投递到该队列的消息如果没有消费都将在6秒之后被投递到死信交换机--&gt; &lt;entry key=\"x-message-ttl\" value-type=\"long\" value=\"6000\"/&gt; &lt;!--设置当消息过期后投递到对应的死信交换机--&gt; &lt;entry key=\"x-dead-letter-exchange\" value=\"my_dlx_exchange\"/&gt; &lt;/rabbit:queue-arguments&gt;&lt;/rabbit:queue&gt;&lt;!--定义限制长度的队列及其属性，不存在则自动创建--&gt;&lt;rabbit:queue id=\"my_max_dlx_queue\" name=\"my_max_dlx_queue\" auto-declare=\"true\"&gt; &lt;rabbit:queue-arguments&gt; &lt;!--投递到该队列的消息最多2个消息，如果超过则最早的消息被删除投递到死信交换机--&gt; &lt;entry key=\"x-max-length\" value-type=\"long\" value=\"2\"/&gt; &lt;!--设置当消息过期后投递到对应的死信交换机--&gt; &lt;entry key=\"x-dead-letter-exchange\" value=\"my_dlx_exchange\"/&gt; &lt;/rabbit:queue-arguments&gt;&lt;/rabbit:queue&gt;&lt;!--定义定向交换机 根据不同的路由key投递消息--&gt;&lt;rabbit:direct-exchange id=\"my_normal_exchange\" name=\"my_normal_exchange\" auto-declare=\"true\"&gt; &lt;rabbit:bindings&gt; &lt;rabbit:binding key=\"my_ttl_dlx\" queue=\"my_ttl_dlx_queue\"/&gt; &lt;rabbit:binding key=\"my_max_dlx\" queue=\"my_max_dlx_queue\"/&gt; &lt;/rabbit:bindings&gt;&lt;/rabbit:direct-exchange&gt; 7.2.3 消息过期的死信队列测试7.2.3.1 发送消息代码添加 springboot-rabbitmq-producer\\src\\test\\java\\com\\itheima\\rabbitmq\\ProducerTest.java方法 12345678910/** * 过期消息投递到死信队列 * 投递到一个正常的队列，但是该队列有设置过期时间，到过期时间之后消息会被投递到死信交换机（队列） */@Testpublic void dlxTTLMessageTest() &#123; rabbitTemplate.convertAndSend(\"my_normal_exchange\", \"my_ttl_dlx\", \"测试过期消息；6秒过期后会被投递到死信交换机\");&#125; 7.2.3.2 RabbitMQ管理界面未过期： 过期后： 7.2.3.3 流程具体因为队列消息过期而被投递到死信队列的流程： 7.2.4 消息过长的死信队列测试7.2.4.1 发送消息代码添加 springboot-rabbitmq-producer\\src\\test\\java\\com\\itheima\\rabbitmq\\ProducerTest.java方法 1234567891011121314151617181920/** * 消息长度超过2，会投递到死信队列中 */@Testpublic void dlxMaxMessageTest() &#123; rabbitTemplate.convertAndSend( \"my_normal_exchange\", \"my_max_dlx\", \"发送消息1：消息长度超过2，会被投递到死信队列中！\"); rabbitTemplate.convertAndSend( \"my_normal_exchange\", \"my_max_dlx\", \"发送消息2：消息长度超过2，会被投递到死信队列中！\"); rabbitTemplate.convertAndSend( \"my_normal_exchange\", \"my_max_dlx\", \"发送消息3：消息长度超过2，会被投递到死信队列中！\");&#125; 7.2.4.2 RabbitMQ管理界面上面发送的3条消息中的第1条消息会被投递到死信队列中（如果启动了消费者，那么队列消息很快会被取走消费掉）； 7.2.4.3 消费者接收死信队列消息与过期消息投递到死信队列的代码和配置是共用的，并不需要重新编写。 7.2.4.4 流程消息超过队列最大消息长度而被投递到死信队列的流程在前面的图中已包含。 7.3 延迟队列延迟队列存储的对象是对应的延迟消息；所谓“延迟消息” 是指当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。 在RabbitMQ中延迟队列可以通过 过期时间 + 死信队列 来实现；具体如下流程图所示： 在上图中；分别设置了两个5秒、10秒的过期队列，然后等到时间到了则会自动将这些消息转移投递到对应的死信队列中，然后消费者再从这些死信队列接收消息就可以实现消息的延迟接收。 延迟队列的应用场景；如： 在电商项目中的支付场景；如果在用户下单之后的几十分钟内没有支付成功；那么这个支付的订单算是支付失败，要进行支付失败的异常处理（将库存加回去），这时候可以通过使用延迟队列来处理 在系统中如有需要在指定的某个时间之后执行的任务都可以通过延迟队列处理 7.4 消息确认机制确认并且保证消息被送达，提供了两种方式：发布确认和事务。(两者不可同时使用)在channel为事务时，不可引入确认模式；同样channel为确认模式下，不可使用事务。 7.4.1 发布确认有两种方式：消息发送成功确认和消息发送失败回调。 7.4.1.1 消息发送成功确认在springboot-rabbitmq-producer\\src\\main\\resources\\spring\\spring-rabbitmq.xml connectionFactory 中启用消息确认： 1234567&lt;!-- publisher-confirms=\"true\" 表示：启用了消息确认 --&gt;&lt;rabbit:connection-factory id=\"connectionFactory\" host=\"192.168.142.128\" port=\"5672\" username=\"wgy\" password=\"123456\" virtual-host=\"/test\"/&gt; publisher-confirms=\"true\"/&gt; 配置消息确认回调方法如下： 1234567&lt;!-- 消息回调处理类 --&gt;&lt;bean id=\"confirmCallback\" class=\"com.wgy.rabbitmq.MsgSendConfirmCallBack\"/&gt;&lt;!--定义rabbitTemplate对象操作可以在代码中方便发送消息--&gt;&lt;!-- confirm-callback=\"confirmCallback\" 表示：消息确认回调 --&gt;&lt;rabbit:template id=\"rabbitTemplate\" connection-factory=\"connectionFactory\" confirm-callback=\"confirmCallback\"/&gt; 消息确认回调方法com.wgy.rabbitmq.MsgSendConfirmCallBack如下： 1234567891011121314151617/** * 消息确认回调方法 * * @author wgy */public class MsgSendConfirmCallBack implements RabbitTemplate.ConfirmCallback &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; if (ack) &#123; System.out.println(\"消息确认成功....\"); &#125; else &#123; //处理丢失的消息 System.out.println(\"消息确认失败,\" + cause); &#125; &#125;&#125; 功能测试如下： 发送消息 12345678/** * 消息确认 */@Testpublic void queueTest() &#123; //路由键与队列同名 rabbitTemplate.convertAndSend(\"spring_queue\", \"只发队列spring_queue的消息。\");&#125; 管理界面确认消息发送成功 消息确认回调 7.4.1.2 消息发送失败回调在springboot-rabbitmq-producer\\src\\main\\resources\\spring\\spring-rabbitmq.xml connectionFactory 中启用回调： 1234567&lt;!-- publisher-returns=\"true\" 表示：启用了失败回调 --&gt;&lt;rabbit:connection-factory id=\"connectionFactory\" host=\"192.168.142.128\" port=\"5672\" username=\"wgy\" password=\"123456\" virtual-host=\"/test\"/&gt; publisher-returns=\"true\"/&gt; 配置消息失败回调方法如下： 注意：同时需配置mandatory=”true”，否则消息则丢失 1234567&lt;!-- 消息失败回调处理类 --&gt;&lt;bean id=\"sendReturnCallback\" class=\"com.wgy.rabbitmq.MsgSendReturnCallback\"/&gt;&lt;!--定义rabbitTemplate对象操作可以在代码中方便发送消息--&gt;&lt;!-- return-callback=\"sendReturnCallback\" 表示：消息失败回调 ,同时需配置mandatory=\"true\"，否则消息则丢失--&gt;&lt;rabbit:template id=\"rabbitTemplate\" connection-factory=\"connectionFactory\" confirm-callback=\"confirmCallback\" return-callback=\"sendReturnCallback\" mandatory=\"true\"/&gt; 消息失败回调方法com.wgy.rabbitmq.MsgSendReturnCallback如下： 12345678910111213/** * 消息失败回调方法 * * @author wgy */public class MsgSendReturnCallback implements RabbitTemplate.ReturnCallback &#123; @Override public void returnedMessage(Message message, int i, String s, String s1, String s2) &#123; String msgJson = new String(message.getBody()); System.out.println(\"Returned Message：\" + msgJson); &#125;&#125; 功能测试如下： 模拟消息发送失败 12345@Testpublic void testFailQueueTest() throws InterruptedException &#123; //exchange 正确,queue 错误 ,confirm被回调, ack=true; return被回调 replyText:NO_ROUTE rabbitTemplate.convertAndSend(\"test_fail_exchange\", \"\", \"测试消息发送失败进行确认应答。\");&#125; 失败回调结果如下： 7.4.2 事务支持场景：业务处理伴随消息的发送，业务处理失败（事务回滚）后要求消息不发送。rabbitmq 使用调用者的外部事务，通常是首选，因为它是非侵入性的（低耦合）。 外部事务的配置：springboot-rabbitmq-producer\\src\\main\\resources\\spring\\spring-rabbitmq.xml 12345678&lt;!-- channel-transacted=\"true\" 表示：支持事务操作 --&gt;&lt;rabbit:template id=\"rabbitTemplate\" connection-factory=\"connectionFactory\" confirm-callback=\"confirmCallback\" return-callback=\"sendReturnCallback\" channel-transacted=\"true\"/&gt;&lt;!--平台事务管理器--&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.amqp.rabbit.transaction.RabbitTransactionManager\"&gt; &lt;property name=\"connectionFactory\" ref=\"connectionFactory\"/&gt;&lt;/bean&gt; 模拟业务处理失败的场景： 测试类或者测试方法上加入@Transactional注解 1234567891011@Test@Transactional //开启事务//@Rollback(false)//在测试的时候，需要手动的方式制定回滚的策略public void queueTest2() &#123; //路由键与队列同名 rabbitTemplate.convertAndSend(\"spring_queue\", \"只发队列spring_queue的消息--01。\"); System.out.println(\"----------------dosoming:可以是数据库的操作，也可以是其他业务类型的操作---------------\"); //模拟业务处理失败 System.out.println(1 / 0); rabbitTemplate.convertAndSend(\"spring_queue\", \"只发队列spring_queue的消息--02。\");&#125; 测试结果： 7.5 消息追踪消息中心的消息追踪需要使用Trace实现，Trace是Rabbitmq用于记录每一次发送的消息，方便使用Rabbitmq的开发者调试、排错。可通过插件形式提供可视化界面。Trace启动后会自动创建系统Exchange：amq.rabbitmq.trace ,每个队列会自动绑定该Exchange，绑定后发送到队列的消息都会记录到Trace日志。 7.5.1 消息追踪启用与查看以下是trace的相关命令和使用（要使用需要先rabbitmq启用插件，再打开开关才能使用）： 命令集 描述 rabbitmq-plugins list 查看插件列表 rabbitmq-plugins enable rabbitmq_tracing rabbitmq启用trace插件 rabbitmqctl trace_on 打开trace的开关 rabbitmqctl trace_on -p itcast 打开trace的开关(itcast为需要日志追踪的vhost) rabbitmqctl trace_off 关闭trace的开关 rabbitmq-plugins disable rabbitmq_tracing rabbitmq关闭Trace插件 rabbitmqctl set_user_tags heima administrator 只有administrator的角色才能查看日志界面 安装插件并开启 trace_on 之后，会发现多个 exchange：amq.rabbitmq.trace ，类型为：topic。 7.5.2 日志追踪第一步：发送消息 1rabbitTemplate.convertAndSend(&quot;spring_queue&quot;, &quot;只发队列spring_queue的消息--01。&quot;); 发送成功，web查看多了一条消息 第二步：查看trace 第三步：点击Tracing查看Trace log files 第四步：点击itcast-trace.log确认消息轨迹正确性 url：http://127.0.0.1:15672/api/trace-files/itcast-trace.log 8. RabbitMQ集群架构模式8.1 主备模式用来实现RabbitMQ的高可用集群,一般是在并发和数据不是特别多的时候使用,当主节点挂掉以后会从备份节点中选择一个节点出来作为主节点对外提供服务。 8.2 远程模式主要用来实现双活,简称为Shovel模式,所谓的Shovel模式就是让我们可以把消息复制到不同的数据中心,让两个跨地域的集群互联。 8.3 镜像队列模式镜像队列也被称为Mirror队列,主要是用来保证mq消息的可靠性的,他通过消息复制的方式能够保证我们的消息100%不丢失,同时该集群模式也是企业中使用最多的模式。 8.4 多活模式多活模式主要是用来实现异地数据复制，Shovel模式其实也可以实现,但是他的配置及其繁琐同时还要受到版本的限制,所以如果做异地多活我们更加推荐使用多活模式,使用多活模式我们需要借助federation插件来实现集群与集群之间或者节点与节点之前的消息复制,该模式被广泛应用于饿了么、美团、滴滴等企业。 8.5 集群模式总结主备模式下主节点提供读写，从节点不提供读写服务，只是负责提供备份服务,备份节点的主要功能是在主节点宕机时，完成自动切换 从–&gt;主,同时因为主备模式下始终只有一个对外提供服务那么对于高并发的情况下该模式并不合适. 远程模式可以让我们实现异地多活的mq，但是现在已经有了更好的异地多活解决方案,所以在实际的项目中已经不推荐使用了 镜像队列模式可以让我们的消息100%不丢失,同时可以结合HAProxy来实现高并发的业务场景所以在项目中使用得最多 9. RabbitMQ 应用9.1 消息堆积当消息生产的速度长时间，远远大于消费的速度时。就会造成消息堆积。 消息堆积的影响 可能导致新消息无法进入队列 可能导致旧消息无法丢失 消息等待消费的时间过长，超出了业务容忍范围。 产生堆积的情况 生产者突然大量发布消息 消费者消费失败 消费者出现性能瓶颈。 消费者挂掉 解决办法 排查消费者的消费性能瓶颈 增加消费者的多线程处理 部署增加多个消费者 9.2 消息丢失在实际的生产环境中有可能出现一条消息因为一些原因丢失，导致消息没有消费成功，从而造成数据不一致等问题，造成严重的影响，比如：在一个商城的下单业务中，需要生成订单信息和扣减库存两个动作，如果使用RabbitMQ来实现该业务，那么在订单服务下单成功后需要发送一条消息到库存服务进行扣减库存，如果在此过程中，一条消息因为某些原因丢失，那么就会出现下单成功但是库存没有扣减，从而导致超卖的情况，也就是库存已经没有了，但是用户还能下单，这个问题对于商城系统来说是致命的。 消息丢失的场景主要分为：消息在生产者丢失，消息在RabbitMQ丢失，消息在消费者丢失。 9.2.1 消息在生产者丢失9.2.1.1 场景介绍消息生产者发送消息成功，但是MQ没有收到该消息，消息在从生产者传输到MQ的过程中丢失，一般是由于网络不稳定的原因。 9.2.1.2 解决方案采用RabbitMQ 发送方消息确认机制，当消息成功被MQ接收到时，会给生产者发送一个确认消息，表示接收成功。RabbitMQ 发送方消息确认模式有以下三种：普通确认模式，批量确认模式，异步监听确认模式。spring整合RabbitMQ后只使用了异步监听确认模式。 说明 异步监听模式，可以实现边发送消息边进行确认，不影响主线程任务执行。 步骤 1、生产者发送3000条消息 2、在发送消息前开启开启发送方确认模式 1234567&lt;!-- publisher-confirms=\"true\" 表示：启用了消息确认 --&gt;&lt;rabbit:connection-factory id=\"connectionFactory\" host=\"192.168.142.128\" port=\"5672\" username=\"wgy\" password=\"123456\" virtual-host=\"/test\"/&gt; publisher-confirms=\"true\"/&gt; 3、在发送消息前添加异步确认监听器 123456789101112131415161718/** * 消息确认回调方法 * * @author wgy */public class MsgSendConfirmCallBack implements RabbitTemplate.ConfirmCallback &#123; @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; if (ack) &#123; System.out.println(\"消息确认成功....\"); &#125; else &#123; //处理丢失的消息 System.out.println(\"消息确认失败,\" + cause); //重发 &#125; &#125;&#125; 9.2.2 消息在RabbitMQ丢失9.2.2.1 场景介绍消息成功发送到MQ，消息还没被消费却在MQ中丢失，比如MQ服务器宕机或者重启会出现这种情况 9.2.2.2 解决方案持久化交换机，队列，消息，确保MQ服务器重启时依然能从磁盘恢复对应的交换机，队列和消息。 spring整合后默认开启了交换机，队列，消息的持久化，所以不修改任何设置就可以保证消息不在RabbitMQ丢失。但是为了以防万一，还是可以申明下。 9.2.3 消息在消费者丢失9.2.3.1 场景介绍消息费者消费消息时，如果设置为自动回复MQ，消息者端收到消息后会自动回复MQ服务器，MQ则会删除该条消息，如果消息已经在MQ被删除但是消费者的业务处理出现异常或者消费者服务宕机，那么就会导致该消息没有处理成功从而导致该条消息丢失。 9.2.3.2 解决方案设置为手动回复MQ服务器，当消费者出现异常或者服务宕机时，MQ服务器不会删除该消息，而是会把消息重发给绑定该队列的消费者，如果该队列只绑定了一个消费者，那么该消息会一直保存在MQ服务器，直到消息者能正常消费为止。本解决方案以一个队列绑定多个消费者为例来说明，一般在生产环境上也会让一个队列绑定多个消费者也就是工作队列模式来减轻压力，提高消息处理效率 MQ重发消息场景： 1.消费者未响应ACK，主动关闭频道或者连接 2.消费者未响应ACK，消费者服务挂掉 9.3 有序消费消息9.3.1 场景介绍9.3.1.1 场景1当RabbitMQ采用work Queue模式，此时只会有一个Queue但是会有多个Consumer,同时多个Consumer直接是竞争关系，此时就会出现MQ消息乱序的问题。 9.3.1.2 场景2当RabbitMQ采用简单队列模式的时候,如果消费者采用多线程的方式来加速消息的处理,此时也会出现消息乱序的问题。 9.3.1.3 场景1解决 9.3.1.4 场景2解决 9.4 重复消费9.4.1 场景介绍为了防止消息在消费者端丢失，会采用手动回复MQ的方式来解决，同时也引出了一个问题，消费者处理消息成功，手动回复MQ时由于网络不稳定，连接断开，导致MQ没有收到消费者回复的消息，那么该条消息还会保存在MQ的消息队列，由于MQ的消息重发机制，会重新把该条消息发给和该队列绑定的消息者处理，这样就会导致消息重复消费。而有些操作是不允许重复消费的，比如下单，减库存，扣款等操作。 MQ重发消息场景： 1.消费者未响应ACK，主动关闭频道或者连接 2.消费者未响应ACK，消费者服务挂掉 9.4.2 解决方案如果消费消息的业务是幂等性操作（同一个操作执行多次，结果不变）就算重复消费也没问题，可以不做处理，如果不支持幂等性操作，如：下单，减库存，扣款等，那么可以在消费者端每次消费成功后将该条消息id保存到数据库，每次消费前查询该消息id，如果该条消息id已经存在那么表示已经消费过就不再消费否则就消费。本方案采用redis存储消息id，因为redis是单线程的，并且性能也非常好，提供了很多原子性的命令，本方案使用setnx命令存储消息id。 setnx(key,value):如果key不存在则插入成功且返回1,如果key存在,则不进行任何操作,返回0","tags":[{"name":"服务器中间件","slug":"服务器中间件","permalink":"https://wgy1993.gitee.io/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://wgy1993.gitee.io/tags/RabbitMQ/"}]},{"title":"Docker(一)","date":"2020-10-04T14:30:59.000Z","path":"archives/4021575d.html","text":"1. 什么是虚拟化1.1 概念在计算机中，虚拟化（英语：Virtualization）是一种资源管理技术，是将计算机的各种实体资源，如服务器、网络、内存及存储等，予以抽象、转换后呈现出来，打破实体结构间的不可切割的障碍，使用户可以比原本的组态更好的方式来应用这些资源。这些资源的新虚拟部份是不受现有资源的架设方式，地域或物理组态所限制。一般所指的虚拟化资源包括计算能力和资料存储。 在实际的生产环境中，虚拟化技术主要用来解决高性能的物理硬件产能过剩和老的旧的硬件产能过低的重组重用，透明化底层物理硬件，从而最大化的利用物理硬件。(物理资源的充分利用) 虚拟化技术种类很多，例如：软件虚拟化、硬件虚拟化、内存虚拟化、网络虚拟化、桌面虚拟化、服务虚拟化、虚拟机等等。 1.2 虚拟化架构1.2.1 全虚拟化在全虚拟化的虚拟平台中，GuestOS并不知道自己是一台虚拟机，它会认为自己就是运行在计算机物理硬件设备上的HostOS。因为全虚拟化的VMM会将一个OS所能够操作的CPU、内存、外设等物理设备逻辑抽象成为虚拟CPU、虚拟内存、虚拟外设等虚拟设备后，再交由GuestOS来操作使用。这样的GuestOS会将底层硬件平台视为自己所有的，但是实际上，这些都是VMM为GuestOS制造了这种假象。 1.2.2 操作系统虚拟化操作系统层虚拟化是指通过划分一个宿主操作系统的特定部分，产生一个个隔离的操作执行环境。操作系统层的虚拟化是操作系统内核直接提供的虚报化，虚拟出的操作系统之间共享底层宿主操作系统内核和底层的硬件资源。操作系统虚拟化的关键点在于将操作系统与上层应用隔离开，将对操作系统资源的访问进行虚报化，使上层应用觉得自己独占操作系统。 1.2.3 平台虚拟化平台虚拟化表现为在一个给定的硬件平台上宿主机创造一个模拟的计算机环境虚拟机提供给客户机。客户机软件对于用户应用程序没有限制;许多宿主机允许运行真实的操作系统。客户机就好像直接运行在计算机硬件上，伴随着几个明显的警告。虚拟机对硬件资源(如网络，显示器，键盘，硬盘)的访问被统一管理在一个比处理器和系统内存更有限制性的层次上。客户软件经常被限制访问计算机周边设备，或者被限制在较低的设备性能上，这取决于宿主机硬件访问策略设定。 1.3 hypervisorHypervisor是一种运行在物理服务器和操作系统之间的中间软件层,可允许多个操作系统和应用共享一套基础物理硬件，因此也可以看作是虚拟环境中的“元”操作系统，它可以协调访问服务器上的所有物理设备和虚拟机，也叫虚拟机监视器（Virtual Machine Monitor，VMM）。Hypervisor是所有虚拟化技术的核心。当服务器启动并执行Hypervisor时，它会给每一台虚拟机分配适量的内存、CPU、网络和磁盘，并加载所有虚拟机的客户操作系统。 Hypervisor是所有虚拟化技术的核心，软硬件架构和管理更高效、更灵活，硬件的效能能够更好地发挥出来。常见的产品有：VMware、KVM、Xen等等。 2. Docker 介绍2.1 容器技术在计算机的世界中，容器拥有一段漫长且传奇的历史。容器与管理程序虚拟化（hypervisor virtualization，HV）有所不同，管理程序虚拟化通过中间层将一台或者多台独立的机器虚拟运行与物理硬件之上，而容器则是直接运行在操作系统内核之上的用户空间。因此，容器虚拟化也被称为“操作系统级虚拟化”，容器技术可以让多个独立的用户空间运行在同一台宿主机上。 由于 “客居”于操作系统，容器只能运行与底层宿主机相同或者相似的操作系统，这看起来并不是非常灵活。例如：可以在Ubuntu服务中运行Redhat Enterprise Linux，但无法再Ubuntu服务器上运行Microsoft Windows。 相对于彻底隔离的管理程序虚拟化，容器被认为是不安全的。而反对这一观点的人则认为，由于虚拟容器所虚拟的是一个完整的操作系统，这无疑增大了攻击范围，而且还要考虑管理程序层潜在的暴露风险。 尽管有诸多局限性，容器还是被广泛部署于各种各样的应用场合。在超大规模的多租户服务部署、轻量级沙盒以及对安全要求不太高的隔离环境中，容器技术非常流行。最常见的一个例子就是“权限隔离监牢”（chroot jail），它创建一个隔离的目录环境来运行进程。如果权限隔离监牢正在运行的进程被入侵者攻破，入侵者便会发现自己“身陷囹圄”，因为权限不足被困在容器所创建的目录中，无法对宿主机进一步破坏。 最新的容器技术引入了OpenVZ、Solaris Zones以及Linux容器（LXC）。使用这些新技术，容器不在仅仅是一个单纯的运行环境。在自己的权限类内，容器更像是一个完整的宿主机。容器和宿主机之间的隔离更加彻底，容器有独立的网络和存储栈，还拥有自己的资源管理能力，使得同一台宿主机中的多个容器可以友好的共存。 容器被认为是精益技术，因为容器需要的开销有限。和传统虚拟化以及半虚拟化相比，容器不需要模拟层（emulation layer）和管理层（hypervisor layer），而是使用操作系统的系统调用接口。这降低了运行单个容器所需的开销，也使得宿主机中可以运行更多的容器。 尽管有着光辉的历史，容器仍未得到广泛的认可。一个很重要的原因就是容器技术的复杂性：容器本身就比较复杂，不易安装，管理和自动化也很困难。而Docker就是为了改变这一切而生的。 2.2 Docker 介绍Docker 是一个开发，运输和运行应用程序的开放平台。 Docker使您可以将应用程序与基础架构分离，以便快速交付软件。 使用Docker，您可以像管理应用程序一样管理基础架构（OS）。 通过利用Docker的方法快速发送，测试和部署代码，您可以显着减少编写代码和在生产中运行代码之间的延迟。（代码改了） 2.3 Docker 好处容器提供了隔离性，结论是，容器可以为各种测试提供很好的沙盒环境。并且，容器本身就具有“标准性”的特征，非常适合为服务创建构建块。Docker的一些应用场景如下： 加速本地开发和构建流程，使其更加高效、更加轻量化。本地开发人员可以构建、运行并分享Docker容器。容器可以在开发环境中构建，然后轻松的提交到测试环境中，并最终进入生产环境。 开发人员与运维人员进行职责的逻辑分离 能够让独立的服务或应用程序在不同的环境中，得到相同的运行结果。这一点在面向服务的架构和重度依赖微型服务的部署由其实用。 用 Docker创建隔离的环境来进行测试。例如，用Jenkins CI这样的持续集成工具启动一个用于测试的容器。（持续化集成 war 实际部署：jenkins持续化集成 Jenkins+git ） Docker 可以让开发者先在本机上构建一个复杂的程序或架构来进行测试，而不是一开始就在生产环境部署、测试。 构建一个多用户的平台即服务（ PaaS）基础设施 为开发、测试提供一个轻量级的独立的沙盒环境 提供软件即服务（ SaaS）应用程序，例如Memcached即服务 高性能、超大规模的宿主机部署（可以很多容器） 沙盒：在计算机安全领域，沙盒（英语：sandbox，又译为沙箱）是一种安全机制，为运行中的程序提供的隔离环境。通常是作为一些来源不可信、具破坏力或无法判定程序意图的程序提供实验之用。 2.4 集装箱思想Docker借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Dock将这个模型运用到自己的设计中，唯一不同的是：集装箱运输货物，而Docker运输软件、应用程序。 和集装箱一样，Docker在执行上述操作时，并不关心容器中到底装了什么，它不管是web服务器，还是数据库，或者是应用程序服务器什么的。所有的容器都按照相同的方式将内容“装载”进去。 Docker也不关心你要把容器运到何方：我们可以在自己的笔记本中构建容器，上传到Registry，然后下载到一个物理的或者虚拟的服务器来测试，在把容器部署到具体的主机中。像标准集装箱一样，Docker容器方便替换，可以叠加，易于分发，并且尽量通用。 2.5 container 与vm区别物理机： 虚拟机： 容器：最大化的利用资源。 通过上面这三张抽象图，我们大概可以通过类比概括出： 容器虚拟化的是操作系统而不是硬件，容器之间是共享同一套操作系统资源的。虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统。 容器和虚拟机具有相似的资源隔离和分配优势，但功能有所不同，因为容器虚拟化的是操作系统，而不是硬件，因此容器更容易移植，效率也更高。 容器在Linux上本机运行，并与其他容器共享主机的内核。 它运行一个独立的进程，不占用任何其他可执行文件的内存，使其轻量级。相比之下，虚拟机（VM）运行一个完整的“客户”操作系统，通过虚拟机管理程序对主机资源进行虚拟访问。 通常，VM提供的环境比大多数应用程序需要的资源更多。 2.6 虚拟机已死 容器才是未来 容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行 。与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），瞬间就能完成启动 。 虚拟机（ VM）是一个物理硬件层抽象，用于将一台服务器变成多台服务器。 管理程序允许多个VM在一台机器上运行。每个VM都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，因此占用大量空间。而且VM启动也十分缓慢 。 3. Docker 版本以及安装3.1 Docker 版本介绍Docker CE 在 17.03 版本之前叫 Docker Engine, Docker Engine 的版本号范围: 0.1.0 ~1.13.1 在 2017 年 3 月 2 日, docker 团队宣布企业版 Docker Enterprise Edition ( EE ) 发布. 为了一致, 免费的 Docker Engine 改名为 Docker Community Edition ( CE ), 并且采用基于时间的版本号方案. 就在这一天, Docker EE 和 Docker CE 的 17.03 版本发布, 这也是第一个采用新的版本号方案的版本. Docker CE/EE 每个季度发布一次 季度版本, 也就是说每年会发布 4 个季度版本, 17.03,17.06, 17.09, 17.12 就是 2017 年的 4 个季度版本的版本号, 同时 Docker CE 每个月还会发布一个 EDGE 版本，比如 17.04, 17.05, 17.07, 17.08, 17.10, 17.11 …… Docker CE 季度版本自发布后会有 4 个月的维护期. 在基于时间的发布方案中，版本号格式为: YY.MM.，YY.MM 代表年月，patch 代表补丁号，从 0 开始，在季度版本 (如 17.03) 的维护期内，bug 修复相关的更新会以 patch 递增的方式发布, 比如 17.03.0 -&gt; 17.03.1 -&gt; 17.03.2 Docker is available in two editions: Community Edition (CE) ：社区版 Enterprise Edition (EE) ：企业版 3.2 Docker 安装3.2.1 版本要求Docker对Ubuntu的支持是最好的。如果是 CentOS：安装Docker 建议7.x及以上版本。 Docker支持在多种平台上使用，包括Mac、Windows、Cloud以及Linux系统上等。由于Docker是基于Ubuntu发布的，所以官方更推荐在Ubuntu上使用Docker，开发者也可以根据自己的实际开发环境选择合适的开发。在不同的平台上安装Docker必须满足不同的先决条件。 3.2.2 安装步骤3.2.2.1 安装需要的软件包yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的 1yum install -y yum-utils device-mapper-persistent-data lvm2 3.2.2.2 设置yum源12345#中央仓库yum-config-manager --add-repo http://download.docker.com/linux/centos/docker-ce.repo#阿里仓库yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3.2.2.3 安装 Docker安装最新版本的 Docker Engine-Community 和 containerd 1yum install docker-ce docker-ce-cli containerd.io 安装特定版本的 Docker： 1、列出并排序您存储库中可用的版本。此示例按版本号（从高到低）对结果进行排序。 123456yum list docker-ce --showduplicates | sort -rdocker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable 2、通过其完整的软件包名称安装特定版本，该软件包名称是软件包名称（docker-ce）加上版本字符串（第二列），从第一个冒号（:）一直到第一个连字符，并用连字符（-）分隔。例如：docker-ce-18.09.1。 1yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io 3.2.2.4 启动Docker1234systemctl start docker#开机启动systemctl enable docker 3.2.2.5 配置国内镜像加速器阿里云镜像加速器地址 直接支付宝登陆，然后进入后台，点击镜像加速器，根据自己的Linux系统输入对应的命令，完成阿里云国内镜像源的配置。 3.2.2.6 测试1docker run hello‐world 3.3 常用Docker命令12345678910111213141516171819202122#查看docker详细信息docker info#查看docker版本docker version#启动/停止/重启docker：systemctl start/stop/restart docker#Ubuntu：开机自动启动#CentOS：手动启动#开机启动：systemctl enable docker#查看docker运行状态sytemctl status docker#查看当前正在运行的容器docker ps#查看所有容器的状态docker ps -a 4. Docker 架构Docker使用客户端 - 服务器架构。 Docker客户端与Docker守护进程通信，后者负责构建，运行和分发Docker容器。 Docker客户端和守护程序可以在同一系统上运行，也可以将Docker客户端连接到远程Docker守护程序。 Docker客户端和守护程序使用REST API，通过UNIX套接字或网络接口进行通信。 镜像（Image）：Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:16.04 就包含了完整的一套 Ubuntu16.04 最小系统的 root 文件系统。 容器（Container）：镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 仓库（Repository）：仓库可看成一个代码控制中心，用来保存镜像。 5. Docker 镜像操作Docker 镜像是容器的基础。镜像是一个有序集合，其中包含根文件系统更改和在容器运行时中使用的相应执行参数。镜像通常 包含堆叠在彼此之上的联合分层文件系统。镜像没有状态并且始终不会发生更改。 当运行容器时，使用的镜像如果在本地中不存在，docker 就会自动从 docker 镜像仓库中下载，默认是从 Docker Hub 公共镜像源下载。 5.1 列出镜像1234docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEubuntu latest 9140108b62dc 8 days ago 72.9MB REPOSITORY ：表示镜像的仓库源 TAG ：镜像的标签（版本），同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本，如ubuntu仓库源里，有15.10、14.04等多个不同的版本。 IMAGE ID ：镜像ID CREATED ：镜像创建时间 SIZE ：镜像大小 5.2 查找镜像123#docker search 镜像名称docker search tomcat NAME ：镜像仓库源的名称 DESCRIPTION：镜像的描述 starts：用户评价，反应一个镜像的受欢迎程度 OFFICIAL：是否docker官方发布 auto commit：自动构建，表示该镜像由Docker Hub自动构建流程创建的 5.3 拉取镜像1docker pull 镜像名称[:version] Docker镜像首页，包括官方镜像和其它公开镜像。Docker Hub上最受欢迎的10大镜像（通过Docker registry API获取不了镜像被pull的个数，只能通过镜像的stars数量，来衡量镜像的流行度。毫无疑问，拥有最高stars数量的库都是官方库。国情的原因，国内下载 Docker HUB 官方的相关镜像比较慢，可以使用国内（docker.io）的一些镜像加速器，镜像保持和官方一致，关键是速度块，推荐使用。配置镜像加速器： 123456789101112131415PS：配置镜像加速器（参考该网站具体的文档操作）‐ 阿里云（先加入阿里云开发者平台：https:&#x2F;&#x2F;dev.aliyun.com）‐ docker中国加速器（https:&#x2F;&#x2F;www.docker‐cn.com)‐ USTC加速器（https:&#x2F;&#x2F;lug.ustc.edu.cn&#x2F;wiki&#x2F; ） 真正的公共服务（无需任何操作）‐ daocloud、网易蜂巢加速器：略步骤：sudo vim &#x2F;etc&#x2F;docker&#x2F;daemon.json配置内容： &#123; &quot;registry‐mirrors&quot;: [&quot;https:&#x2F;&#x2F;cs913o6k.mirror.aliyuncs.com&quot;] &#125;sudo systemctl daemon‐reloadsudo systemctl restart docker 5.4 删除镜像12345678#1、删除一个镜像docker rmi 镜像名称/id#2、删除多个镜像docker rmi 镜像名称1/id1 镜像名称2/id2 ...#3、删除所有镜像docker rmi `docker images ‐q` 6. Docker 容器操作容器是 Docker 镜像的运行时实例。 6.1 创建容器12345678910docker run [options] image command [ARG...]options选项: -i、-t、-d、--name-i：交互式容器-t：tty，终端-d:后台运行，并且打印容器ideg:创建的容器名称不能重复docker run --name=u1 ubuntudocker run -i -t --name=u1 ubuntu /bin/bashdocker run -i -t -d --name=u3 ubuntu /bin/bash 6.2 进入容器1234567方式一：docker attach 容器名称/id (ps:exit,容器停止) eg：docker attach u3方式二：docker exec -it 容器名称/id /bin/bash （ps:exit,容器不会停止）eg：docker exec -it u3 /bin/bash 6.3 查看容器123docker ps：查看正在运行的容器docker ps -a：查看运行过的容器（历史）docker ps -l：最后一次运行的容器 6.4 停止/启动容器12docker start 容器名称/iddocker stop 容器名称/id 6.5 获取容器/镜像的元数据123456查看容器/镜像全部信息：docker inspect 容器/镜像#查看容器/镜像部分信息：docker inspect -f='&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;' 容器/镜像-f：可通过--format代替 6.6 删除容器12345678910#删除一个容器：docker rm 容器名称/id#删除多个容器：docker rm 容器名称1/id1 容器名称2/id2 ...#删除所有容器docker rm `docker ps -a -q`PS：无法删除正在运行的容器 6.7 查看容器日志1docker logs 容器名称/id 6.8 文件拷贝如果我们需要将文件拷贝到容器内可以使用cp命令 123docker cp 需要拷贝的文件或目录 容器名称:容器目录例如：docker cp 1.txt c2:/root 也可以将文件从容器内拷贝出来 123docker cp 容器名称:容器目录 需要拷贝的文件或目录例如：docker cp c2:/root/2.txt /root 6.9 目录挂载我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。 创建容器 添加-v参数 后边为宿主机目录:容器目录 1docker run -id --name=c4 -v /opt/:/usr/local/myhtml centos 如果你共享的是多级的目录，可能会出现权限不足的提示 这是因为 CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数 –privileged=true 来解决挂载的目录没有权限的问题 1docker run -id --privileged=true --name=c4 -v /opt/:/usr/local/myhtml centos","tags":[{"name":"Docker","slug":"Docker","permalink":"https://wgy1993.gitee.io/tags/Docker/"},{"name":"容器技术","slug":"容器技术","permalink":"https://wgy1993.gitee.io/tags/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/"}]},{"title":"网络爬虫(三)","date":"2020-10-04T04:48:31.000Z","path":"archives/1f29f91c.html","text":"1. 案例扩展1.1 定时任务在案例中我们使用的是Spring内置的Spring Task，这是Spring3.0加入的定时任务功能。我们使用注解的方式定时启动爬虫进行数据爬取。 我们使用的是@Scheduled注解，其属性如下： cron：cron表达式，指定任务在特定时间执行； fixedDelay：上一次任务执行完后多久再执行，参数类型为long，单位ms fixedDelayString：与fixedDelay含义一样，只是参数类型变为String fixedRate：按一定的频率执行任务，参数类型为long，单位ms fixedRateString: 与fixedRate的含义一样，只是将参数类型变为String initialDelay：延迟多久再第一次执行任务，参数类型为long，单位ms initialDelayString：与initialDelay的含义一样，只是将参数类型变为String zone：时区，默认为当前时区，一般没有用到 我们这里的使用比较简单，固定的间隔时间来启动爬虫。例如可以实现项目启动后，每隔一小时启动一次爬虫。 但是有可能业务要求更高，并不是定时定期处理，而是在特定的时间进行处理，这个时候我们之前的使用方式就不能满足需求了。例如我要在工作日（周一到周五）的晚上八点执行。这时我们就需要Cron表达式了。 1.1.1 Cron表达式cron的表达式是字符串，实际上是由七子表达式，描述个别细节的时间表。这些子表达式是分开的空白，代表： 1、Seconds 2、Minutes 3、Hours 4、Day-of-Month 5、Month 6、Day-of-Week 7、Year (可选字段) 例 “0 0 12 ? * WED” 在每星期三下午12:00 执行, “*” 代表整个时间段 每一个字段都有一套可以指定有效值，如 Seconds (秒) ：可以用数字0－59 表示， Minutes(分) ：可以用数字0－59 表示， Hours(时) ：可以用数字0-23表示, Day-of-Month(天) ：可以用数字1-31 中的任一一个值，但要注意一些特别的月份 Month(月) ：可以用0-11 或用字符串:JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC Day-of-Week(天) ：可以用数字1-7表示（1 ＝ 星期日）或用字符口串:SUN, MON, TUE, WED, THU, FRI, SAT “/”：为特别单位，表示为“每”如“0/15”表示每隔15分钟执行一次,“0”表示为从“0”分开始, “3/20”表示表示每隔20分钟执行一次，“3”表示从第3分钟开始执行 “?”：表示每月的某一天，或第周的某一天 “L”：用于每月，或每周，表示为每月的最后一天，或每个月的最后星期几如“6L”表示“每月的最后一个星期五” 1.1.2 Cron测试12345678910111213/** * 定时任务 * * @author wgy */@Componentpublic class TaskTest &#123; @Scheduled(cron = \"0/5 * * * * *\") public void test() &#123; System.out.println(LocalDateTime.now() + \"任务执行了\"); &#125;&#125; 1.2 网页去重之前我们对下载的url地址进行了去重操作，避免同样的url下载多次。其实不光url需要去重，我们对下载的内容也需要去重。 在网上我们可以找到许多内容相似的文章。但是实际我们只需要其中一个即可，同样的内容没有必要下载多次，那么如何进行去重就需要进行处理了 1.2.1 去重方案介绍 指纹码对比 最常见的去重方案是生成文档的指纹门。例如对一篇文章进行MD5加密生成一个字符串，我们可以认为这是文章的指纹码，再和其他的文章指纹码对比，一致则说明文章重复。 但是这种方式是完全一致则是重复的，如果文章只是多了几个标点符号，那仍旧被认为是重复的，这种方式并不合理。 BloomFilter 这种方式就是我们之前对url进行去重的方式，使用在这里的话，也是对文章进行计算得到一个数，再进行对比，缺点和方法1是一样的，如果只有一点点不一样，也会认为不重复，这种方式不合理。 KMP算法 KMP算法是一种改进的字符串匹配算法。KMP算法的关键是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。能够找到两个文章有哪些是一样的，哪些不一样。 这种方式能够解决前面两个方式的“只要一点不一样就是不重复”的问题。但是它的时空复杂度太高了，不适合大数据量的重复比对。 还有一些其他的去重方式：最长公共子串、后缀数组、字典树、DFA等等，但是这些方式的空复杂度并不适合数据量较大的工业应用场景。我们需要找到一款性能高速度快，能够进行相似度对比的去重方案 Google 的 simhash 算法产生的签名，可以满足上述要求。这个算法并不深奥，比较容易理解。这种算法也是目前Google搜索引擎所目前所使用的网页去重算法。 1.2.2 SimHash1.2.2.1 流程介绍simhash是由 Charikar 在2002年提出来的，为了便于理解尽量不使用数学公式，分为这几步： 1、分词，把需要判断文本分词形成这个文章的特征单词。 2、hash，通过hash算法把每个词变成hash值，比如“美国”通过hash算法计算为 100101,“51区”通过hash算法计算为 101011。这样我们的字符串就变成了一串串数字。 3、加权，通过 2步骤的hash生成结果，需要按照单词的权重形成加权数字串，“美国”的hash值为“100101”，通过加权计算为“4 -4 -4 4 -4 4”，“51区”计算为 “ 5 -5 5 -5 5 5”。 4、合并，把上面各个单词算出来的序列值累加，变成只有一个序列串。 “美国”的 “4 -4 -4 4 -4 4”，“51区”的 “ 5 -5 5 -5 5 5”，把每一位进行累加， “4+5 -4+-5 -4+5 4+-5 -4+5 4+5”–&gt;“9 -9 1 -1 1 9” 5、降维，把算出来的 “9 -9 1 -1 1 9”变成 0 1 串，形成最终的simhash签名。 1.2.2.2 签名距离计算我们把库里的文本都转换为simhash签名，并转换为long类型存储，空间大大减少。现在我们虽然解决了空间，但是如何计算两个simhash的相似度呢？ 我们通过海明距离（Hamming distance）就可以计算出两个simhash到底相似不相似。两个simhash对应二进制（01串）取值不同的数量称为这两个simhash的海明距离。 举例如下： 10101 和 00110 从第一位开始依次有第一位、第四、第五位不同，则海明距离为3。对于二进制字符串的a和b，海明距离为等于在a XOR b运算结果中1的个数（普遍算法）。 1.2.2.3 导入simhash工程参考项目：https://github.com/CreekLou/simhash.git 导入工程simhash，并打开测试用例。 1.2.2.4 案例整合需要先把simhash安装到本地仓库 在案例的pom.xml中加入以下依赖 123456&lt;!--simhash网页去重--&gt;&lt;dependency&gt; &lt;groupId&gt;com.lou&lt;/groupId&gt; &lt;artifactId&gt;simhasher&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 修改代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 定时任务 * * @author wgy */@Componentpublic class TaskTest &#123; /** * 测试用 * * @param filename 名字 * @return */ public static String readAllFile(String filename) &#123; String everything = \"\"; try &#123; FileInputStream inputStream = new FileInputStream(filename); everything = IOUtils.toString(inputStream); inputStream.close(); &#125; catch (IOException e) &#123; &#125; return everything; &#125; @Scheduled(cron = \"0/5 * * * * *\") public void test() &#123; String str1 = readAllFile(\"D:/test/testin.txt\"); SimHasher hash1 = new SimHasher(str1); //打印simhash签名 System.out.println(hash1.getSignature()); System.out.println(\"============================\"); String str2 = readAllFile(\"D:/test/testin2.txt\"); //打印simhash签名 SimHasher hash2 = new SimHasher(str2); System.out.println(hash2.getSignature()); System.out.println(\"============================\"); //打印海明距离 System.out.println(hash1.getHammingDistance(hash2.getSignature())); &#125;&#125; 1.3 代理的使用有些网站不允许爬虫进行数据爬取，因为会加大服务器的压力。其中一种最有效的方式是通过ip+时间进行鉴别，因为正常人不可能短时间开启太多的页面，发起太多的请求。 我们使用的WebMagic可以很方便的设置爬取数据的时间（参考第二天的的爬虫的配置、启动和终止）。但是这样会大大降低我们爬取数据的效率，如果不小心ip被禁了，会让我们无法爬去数据，那么我们就有必要使用代理服务器来爬取数据。 1.3.1 代理服务器代理（英语：Proxy），也称网络代理，是一种特殊的网络服务，允许一个网络终端（一般为客户端）通过这个服务与另一个网络终端（一般为服务器）进行非直接的连接。 提供代理服务的电脑系统或其它类型的网络终端称为代理服务器（英文：Proxy Server）。一个完整的代理请求过程为：客户端首先与代理服务器创建连接，接着根据代理服务器所使用的代理协议，请求对目标服务器创建连接、或者获得目标服务器的指定资源。 我们就需要知道代理服务器在哪里（ip和端口号）才可以使用。网上有很多代理服务器的提供商，但是大多是免费的不好用，付费的还行。 米扑代理：https://proxy.mimvp.com/free.php 1.3.2 使用代理WebMagic使用的代理APIProxyProvider。因为相对于Site的“配置”，ProxyProvider定位更多是一个“组件”，所以代理不再从Site设置，而是由HttpClientDownloader设置。 API 说明 HttpClientDownloader.setProxyProvider(ProxyProvider proxyProvider) 设置代理 ProxyProvider有一个默认实现：SimpleProxyProvider。它是一个基于简单Round-Robin的、没有失败检查的ProxyProvider。可以配置任意个候选代理，每次会按顺序挑选一个代理使用。它适合用在自己搭建的比较稳定的代理的场景。 如果需要根据实际使用情况对代理服务器进行管理（例如校验是否可用，定期清理、添加代理服务器等），只需要自己实现APIProxyProvider即可。 12345678910111213141516171819202122232425262728293031323334/** * 代理测试 * * @author wgy */@Componentpublic class ProxyTest implements PageProcessor &#123; private Site site = Site.me(); @Scheduled(fixedDelay = 1000) public void Process() &#123; //创建下载器Downloader HttpClientDownloader httpClientDownloader = new HttpClientDownloader(); //给下载器设置代理服务器信息 httpClientDownloader.setProxyProvider(SimpleProxyProvider.from(new Proxy(\"112.109.198.105\", 3128))); Spider.create(new ProxyTest()) .addUrl(\"http://ip.chinaz.com/\") .setDownloader(httpClientDownloader)//设置下载器 .run(); &#125; @Override public void process(Page page) &#123; //打印获取到的结果以测试代理服务器是否生效 System.out.println(page.getHtml().toString()); &#125; @Override public Site getSite() &#123; return site; &#125;&#125; 2. 查询案例实现把上一次上课抓取到的招聘数据作为数据源，实现招聘信息查询功能。首先需要把MySQL的数据添加到索引库中，然后再实现查询功能。我们这里使用的是SpringBoot，需要把Spring Data ElasticSearch 和项目进行整合。 2.1 开发准备需要修改之前的配置，网页去重排除lucene依赖，同时去重的依赖必须放在pom.xml的最下部。因为现在要使用ElasticSearch，需要用到新的lucene依赖。 添加ES依赖和单元测试依赖，并修改以前的去重依赖，pom.xml效果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;dependencies&gt; &lt;!--SpringMVC--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--SpringData Jpa--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--MySQL连接包--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--WebMagic核心包--&gt; &lt;dependency&gt; &lt;groupId&gt;us.codecraft&lt;/groupId&gt; &lt;artifactId&gt;webmagic-core&lt;/artifactId&gt; &lt;version&gt;0.7.3&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--WebMagic扩展--&gt; &lt;dependency&gt; &lt;groupId&gt;us.codecraft&lt;/groupId&gt; &lt;artifactId&gt;webmagic-extension&lt;/artifactId&gt; &lt;version&gt;0.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;!--WebMagic对布隆过滤器的支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;16.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--ElasticSearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--单元测试--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--simhash--&gt; &lt;dependency&gt; &lt;groupId&gt;com.lou&lt;/groupId&gt; &lt;artifactId&gt;simhasher&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 修改配置文件application.properties，添加以下内容 123456789101112131415#DB Configuration:spring.datasource.driverClassName=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://127.0.0.1:3306/crawlerspring.datasource.username=rootspring.datasource.password=root#JPA Configuration:spring.jpa.database=MySQLspring.jpa.show-sql=trueserver.port=80#ElasticSearch Configurationspring.data.elasticsearch.cluster-nodes=192.168.142.128:9700spring.data.elasticsearch.cluster-name=wgy-es 2.2 导入数据到索引库2.2.1 编写pojo12345678910111213141516171819202122232425262728293031323334/** * 招聘信息实体类es * * @author wgy */@Document(indexName = \"jobinfo\", type = \"JobInfoField\")public class JobInfoField &#123; @Id @Field(index = true, store = true, type = FieldType.Long) private Long id; @Field(index = true, store = true, analyzer = \"ik_smart\", searchAnalyzer = \"ik_smart\", type = FieldType.Text) private String companyName; @Field(index = true, store = true, analyzer = \"ik_smart\", searchAnalyzer = \"ik_smart\", type = FieldType.Text) private String companyAddr; @Field(index = true, store = true, analyzer = \"ik_smart\", searchAnalyzer = \"ik_smart\", type = FieldType.Text) private String companyInfo; @Field(index = true, store = true, analyzer = \"ik_smart\", searchAnalyzer = \"ik_smart\", type = FieldType.Text) private String jobName; @Field(index = true, store = true, analyzer = \"ik_smart\", searchAnalyzer = \"ik_smart\", type = FieldType.Text) private String jobAddr; @Field(index = true, store = true, analyzer = \"ik_smart\", searchAnalyzer = \"ik_smart\", type = FieldType.Text) private String jobInfo; @Field(index = true, store = true, type = FieldType.Integer) private Integer salaryMin; @Field(index = true, store = true, type = FieldType.Integer) private Integer salaryMax; @Field(index = true, store = true, type = FieldType.Text) private String url; @Field(index = true, store = true, type = FieldType.Text) private String time; //get/set/toString&#125; 2.2.2 编写dao1234567/** * 招聘信息持久层接口es * * @author wgy */public interface JobRepository extends ElasticsearchRepository&lt;JobInfoField, Long&gt; &#123;&#125; 2.2.3 编写Service编写Service接口 1234567891011121314151617181920212223/** * 招聘信息业务接口es * * @author wgy */public interface JobRepositoryService &#123; /** * 保存一条数据 * * @param jobInfoField */ public void save(JobInfoField jobInfoField); /** * 批量保存数据 * * @param list */ public void saveAll(List&lt;JobInfoField&gt; list); &#125; 编写Service实现类 123456789101112131415161718192021/** * 招聘信息业务实现es * * @author wgy */@Servicepublic class JobRepositoryServiceImpl implements JobRepositoryService &#123; @Autowired private JobRepository jobRepository; @Override public void save(JobInfoField jobInfoField) &#123; this.jobRepository.save(jobInfoField); &#125; @Override public void saveAll(List&lt;JobInfoField&gt; list) &#123; this.jobRepository.saveAll(list); &#125;&#125; 2.2.4 编写测试用例先执行createIndex()方法创建索引，再执行jobData()导入数据到索引库 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * es测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@SpringBootTest(classes = Application.class)public class ESTest &#123; @Autowired private ElasticsearchTemplate elasticsearchTemplate; @Autowired private JobInfoService jobInfoService; @Autowired private JobRepositoryService jobRepositoryService; /** * 创建索引和映射 */ @Test public void createIndex() &#123; this.elasticsearchTemplate.createIndex(JobInfoField.class); this.elasticsearchTemplate.putMapping(JobInfoField.class); &#125; /** * 导入数据到索引库 */ @Test public void jobInfoData() &#123; //声明页码数，从1开始 int p = 1; //声明查询到的数据条数 int pageSize = 0; do &#123; //从数据库中查询数据 Page&lt;JobInfo&gt; page = this.jobInfoService.findJobInfoByPage(p, 2); //声明容器存放JobInfoField List&lt;JobInfoField&gt; list = new ArrayList&lt;&gt;(); //把查询到的数据封装为JobInfoField for (JobInfo jobInfo : page.getContent()) &#123; //声明对象 JobInfoField jobInfoField = new JobInfoField(); //封装数据,复制数据 BeanUtils.copyProperties(jobInfo, jobInfoField); //把封装好数据的对象放到list容器中 list.add(jobInfoField); &#125; //把封装好的数据保存到索引库中 this.jobRepositoryService.saveAll(list); //页码数加一 p++; //获取查询结果集的数据条数 pageSize = page.getContent().size(); &#125; while (pageSize == 2); &#125;&#125; 2.3 查询案例实现2.3.1 页面跳转实现添加静态资源到项目中 2.3.2 编写pojo12345678910111213/** * 查询结果封装类 * * @author wgy */public class JobResult &#123; private List&lt;JobInfoField&gt; rows; private Integer pageTotal; //get/set/toString&#125; 2.3.3 编写Controller12345678910111213141516171819202122232425262728293031323334/** * 查询controller * * @author wgy */@RestControllerpublic class SearchController &#123; @Autowired private JobRepositoryService jobRepositoryService; //salary: *-* //page: 1 //jobaddr: 北京 //keyword: java //Request URL: http://127.0.0.1:80/search //Request Method: POST /** * 根据条件分页查询招聘信息 * * @param salary * @param jobaddr * @param keyword * @param page * @return */ @RequestMapping(value = \"search\", method = RequestMethod.POST) public JobResult search(String salary, String jobaddr, String keyword, Integer page) &#123; JobResult jobResult = this.jobRepositoryService.search(salary, jobaddr, keyword, page); return jobResult; &#125;&#125; 2.3.4 编写Service在JobRepositoryService编写接口方法 12345678910/** * 根据条件分页查询招聘信息 * * @param salary * @param jobaddr * @param keyword * @param page * @return */JobResult search(String salary, String jobaddr, String keyword, Integer page); 在JobRepositoryServiceImpl实现接口方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//salary: *-*//page: 1//jobaddr: 北京//keyword: java@Overridepublic JobResult search(String salary, String jobaddr, String keyword, Integer page) &#123; //解析参数薪资 String[] salarys = salary.split(\"-\"); int salaryMin = 0, salaryMax = 0; //获取最低薪资 if (\"*\".equals(salarys[0])) &#123; //如果最小值是*，表示最低薪资是0 &#125; else &#123; //如果最小值不是*，需要转为数字类型，乘以10000 salaryMin = Integer.parseInt(salarys[0]) * 10000; &#125; //获取最高薪资 if (\"*\".equals(salarys[1])) &#123; //如果最大值是*，代表最大的数也包含,设置为1000万 salaryMax = 10000000; &#125; else &#123; //如果最大值不是*，需要转为数字类型，乘以10000 salaryMax = Integer.parseInt(salarys[0]) * 10000; &#125; //判断工作地点是否为空 if (StringUtils.isBlank(jobaddr)) &#123; //如果为空，设置为* jobaddr = \"*\"; &#125; //判断查询关键词是否为空 if (StringUtils.isBlank(keyword)) &#123; //如果为空，设置为* keyword = \"*\"; &#125; //调用dao的方法执行查询 Page&lt;JobInfoField&gt; pages = this.jobRepository.findBySalaryMinBetweenAndSalaryMaxBetweenAndJobAddrAndJobNameAndJobInfo(salaryMin, salaryMax, salaryMin, salaryMax, jobaddr, keyword, keyword, PageRequest.of(page - 1, 2)); //封装结果对象jobResult JobResult jobResult = new JobResult(); //设置结果集 jobResult.setRows(pages.getContent()); //设置总页数 jobResult.setPageTotal(pages.getTotalPages()); return jobResult;&#125; 2.3.5 编写Dao在JobRepository编写接口方法 1234567891011121314/** * 根据条件分页查询数据 * * @param salaryMin 薪资下限最小值 * @param salaryMax 薪资下限最高值 * @param salaryMin1 薪资上限最小值 * @param salaryMax1 薪资上限最大值 * @param jobaddr 工作地 * @param keyword 职位名称 * @param keyword1 职位信息 * @param pageable 分页数据 * @return JobInfoField */Page&lt;JobInfoField&gt; findBySalaryMinBetweenAndSalaryMaxBetweenAndJobAddrAndJobNameAndJobInfo(int salaryMin, int salaryMax, int salaryMin1, int salaryMax1, String jobaddr, String keyword, String keyword1, Pageable pageable); 2.3.6 测试","tags":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"https://wgy1993.gitee.io/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"}]},{"title":"网络爬虫(二)","date":"2020-10-03T05:40:53.000Z","path":"archives/61dad937.html","text":"1. WebMagic介绍昨天完成了爬虫的入门的学习，是一个最基本的爬虫案例，今天我们要学习一款爬虫框架的使用就是WebMagic。其底层用到了我们上一天课程所使用的HttpClient和Jsoup，让我们能够更方便的开发爬虫。 WebMagic项目代码分为核心和扩展两部分。核心部分(webmagic-core)是一个精简的、模块化的爬虫实现，而扩展部分则包括一些便利的、实用性的功能。 WebMagic的设计目标是尽量的模块化，并体现爬虫的功能特点。这部分提供非常简单、灵活的API，在基本不改变开发模式的情况下，编写一个爬虫。 扩展部分(webmagic-extension)提供一些便捷的功能，例如注解模式编写爬虫等。同时内置了一些常用的组件，便于爬虫开发。 1.1 架构介绍WebMagic的结构分为Downloader、PageProcessor、Scheduler、Pipeline四大组件，并由Spider将它们彼此组织起来。这四大组件对应爬虫生命周期中的下载、处理、管理和持久化等功能。WebMagic的设计参考了Scapy，但是实现方式更Java化一些。 而Spider则将这几个组件组织起来，让它们可以互相交互，流程化的执行，可以认为Spider是一个大的容器，它也是WebMagic逻辑的核心。 WebMagic总体架构图如下： 1.1.1 WebMagic的四个组件 Downloader Downloader负责从互联网上下载页面，以便后续处理。WebMagic默认使用了Apache HttpClient作为下载工具。 PageProcessor PageProcessor负责解析页面，抽取有用信息，以及发现新的链接。WebMagic使用Jsoup作为HTML解析工具，并基于其开发了解析XPath的工具Xsoup。 在这四个组件中，PageProcessor对于每个站点每个页面都不一样，是需要使用者定制的部分。 Scheduler Scheduler负责管理待抓取的URL，以及一些去重的工作。WebMagic默认提供了JDK的内存队列来管理URL，并用集合来进行去重。也支持使用Redis进行分布式管理。 Pipeline Pipeline负责抽取结果的处理，包括计算、持久化到文件、数据库等。WebMagic默认提供了“输出到控制台”和“保存到文件”两种结果处理方案。 Pipeline定义了结果保存的方式，如果你要保存到指定数据库，则需要编写对应的Pipeline。对于一类需求一般只需编写一个Pipeline。 1.1.2 用于数据流转的对象 Request Request是对URL地址的一层封装，一个Request对应一个URL地址。 它是PageProcessor与Downloader交互的载体，也是PageProcessor控制Downloader唯一方式。 除了URL本身外，它还包含一个Key-Value结构的字段extra。你可以在extra中保存一些特殊的属性，然后在其他地方读取，以完成不同的功能。例如附加上一个页面的一些信息等。 Page Page代表了从Downloader下载到的一个页面——可能是HTML，也可能是JSON或者其他文本格式的内容。 Page是WebMagic抽取过程的核心对象，它提供一些方法可供抽取、结果保存等。 ResultItems ResultItems相当于一个Map，它保存PageProcessor处理的结果，供Pipeline使用。它的API与Map很类似，值得注意的是它有一个字段skip，若设置为true，则不应被Pipeline处理。 1.2 入门案例1.2.1 加入依赖创建Maven工程，并加入以下依赖 123456789101112131415161718192021222324&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;crawler-webmagic&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!--WebMagic--&gt; &lt;dependency&gt; &lt;groupId&gt;us.codecraft&lt;/groupId&gt; &lt;artifactId&gt;webmagic-core&lt;/artifactId&gt; &lt;version&gt;0.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;us.codecraft&lt;/groupId&gt; &lt;artifactId&gt;webmagic-extension&lt;/artifactId&gt; &lt;version&gt;0.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 注意：0.7.3版本对SSL的并不完全，如果是直接从Maven中央仓库下载依赖，在爬取只支持SSL v1.2的网站会有SSL的异常抛出。 解决方案： 等作者的0.7.4的版本发布 直接从github上下载最新的代码，安装到本地仓库 也可以参考以下资料自己修复 https://github.com/code4craft/webmagic/issues/701 1.2.2 加入配置文件WebMagic使用slf4j-log4j12作为slf4j的实现。 添加log4j.properties配置文件 12345log4j.rootLogger=INFO,A1log4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c]-[%p] %m%n 1.2.3 案例实现12345678910111213141516171819202122232425262728293031323334353637/** * WebMagic入门案例 * * @author wgy */public class JobProcessor implements PageProcessor &#123; private Site site = Site.me(); /** * 主函数，执行爬虫 * * @param args */ public static void main(String[] args) &#123; Spider.create(new JobProcessor()) .addUrl(\"https://group.jd.com/index/20000001.htm\") //设置爬取数据的页面 .run();//执行爬虫 &#125; /** * 解析页面 * * @param page page */ public void process(Page page) &#123; //解析返回的数据page，并且把解析的结果放到ResultItems中 //css选择器 page.putField(\"div\", page.getHtml().css(\"div.mt h3\").all()); &#125; public Site getSite() &#123; return site; &#125;&#125; 打印结果： 2. WebMagic功能2.1 实现PageProcessor2.1.1 抽取元素SelectableWebMagic里主要使用了三种抽取技术：XPath、正则表达式和CSS选择器。另外，对于JSON格式的内容，可使用JsonPath进行解析。 2.1.1.1 XPath获取属性class=mt的div标签，里面的h1标签的内容 1page.getHtml().xpath(\"//div[@class=mt]/h1/text()\") 2.1.1.2 CSS选择器CSS选择器是与XPath类似的语言。在上一次的课程中，我们已经学习过了Jsoup的选择器，它比XPath写起来要简单一些，但是如果写复杂一点的抽取规则，就相对要麻烦一点。 div.mt&gt;h1表示class为mt的div标签下的直接子元素h1标签 1page.getHtml().css(\"div.mt&gt;h1\").toString() 可是使用:nth-child(n)选择第几个元素，如下选择第一个元素 1page.getHtml().css(\"div#news_div &gt; ul &gt; li:nth-child(1) a\").toString() 注意：需要使用&gt;，就是直接子元素才可以选择第几个元素 2.1.1.3 正则表达式正则表达式则是一种通用的文本抽取语言。在这里一般用于获取url地址。 1page.getHtml().css(\"div#news_div a\").regex(\".*江苏.*\").all() 2.1.2 抽取元素APISelectable相关的抽取元素链式API是WebMagic的一个核心功能。使用Selectable接口，可以直接完成页面元素的链式抽取，也无需去关心抽取的细节。 在刚才的例子中可以看到，page.getHtml()返回的是一个Html对象，它实现了Selectable接口。这个接口包含的方法分为两类：抽取部分和获取结果部分。 方法 说明 示例 xpath(String xpath) 使用XPath选择 html.xpath(“//div[@class=’title’]”) $(String selector) 使用Css选择器选择 html.$(“div.title”) $(String selector,String attr) 使用Css选择器选择 html.$(“div.title”,”text”) css(String selector) 功能同$()，使用Css选择器选择 html.css(“div.title”) links() 选择所有链接 html.links() regex(String regex) 使用正则表达式抽取 html.regex(“(.*?)&quot;) 这部分抽取API返回的都是一个Selectable接口，意思是说，是支持链式调用的。例如访问https://www.jd.com/moreSubject.aspx页面 12345//先获取class为news_div的div//再获取里面的所有包含文明的元素List&lt;String&gt; list = page.getHtml() .css(\"div#news_div\") .regex(\".*文明.*\").all(); 2.1.3 获取结果API当链式调用结束时，我们一般都想要拿到一个字符串类型的结果。这时候就需要用到获取结果的API了。 我们知道，一条抽取规则，无论是XPath、CSS选择器或者正则表达式，总有可能抽取到多条元素。WebMagic对这些进行了统一，可以通过不同的API获取到一个或者多个元素。 方法 说明 示例 get() 返回一条String类型的结果 String link= html.links().get() toString() 同get()，返回一条String类型的结果 String link= html.links().toString() all() 返回所有抽取结果 List links= html.links().all() 当有多条数据的时候，使用get()和toString()都是获取第一个url地址。 1234567String str = page.getHtml() .css(\"div#news_div\") .links().regex(\".*[0-3]$\").toString();String get = page.getHtml() .css(\"div#news_div\") .links().regex(\".*[0-3]$\").get(); 测试结果： 这里selectable.toString()采用了toString()这个接口，是为了在输出以及和一些框架结合的时候，更加方便。因为一般情况下，我们都只需要选择一个元素！selectable.all()则会获取到所有元素。 2.1.4 获取链接有了处理页面的逻辑，我们的爬虫就接近完工了，但是现在还有一个问题：一个站点的页面是很多的，一开始我们不可能全部列举出来，于是如何发现后续的链接，是一个爬虫不可缺少的一部分。 下面的例子就是获取https://www.jd.com/moreSubject.aspx这个页面中所有符合[https://www.jd.com/news.\\\\w+?.*](https://www.jd.com/news./w+?.*)正则表达式的url地址并将这些链接加入到待抓取的队列中去。 12345678910111213public void process(Page page) &#123; page.addTargetRequests(page.getHtml().links() .regex(\"(https://www.jd.com/news.\\\\w+?.*)\").all()); System.out.println(page.getHtml().css(\"div.mt&gt;h1\").all());&#125;public static void main(String[] args) &#123; Spider.create(new JobProcessor()) .addUrl(\"https://www.jd.com/moreSubject.aspx\") .run();&#125; 2.2 使用Pipeline保存结果WebMagic用于保存结果的组件叫做Pipeline。我们现在通过“控制台输出结果”这件事也是通过一个内置的Pipeline完成的，它叫做ConsolePipeline。 那么，我现在想要把结果用保存到文件中，怎么做呢？只将Pipeline的实现换成”FilePipeline”就可以了。 123456789public static void main(String[] args) &#123; Spider.create(new JobProcessor()) //初始访问url地址 .addUrl(\"https://www.jd.com/moreSubject.aspx\") .addPipeline(new FilePipeline(\"C:\\\\Users\\\\wgy\\\\Desktop\\\\result\"))//Pipeline保存结果到文件 .thread(5)//设置线程数 .run();&#125; 2.3 爬虫的配置、启动和终止2.3.1 SpiderSpider是爬虫启动的入口。在启动爬虫之前，我们需要使用一个PageProcessor创建一个Spider对象，然后使用run()进行启动。 同时Spider的其他组件（Downloader、Scheduler、Pipeline）都可以通过set方法来进行设置。 方法 说明 示例 create(PageProcessor) 创建Spider Spider.create(new GithubRepoProcessor()) addUrl(String…) 添加初始的URL spider .addUrl(“http://webmagic.io/docs/&quot;) thread(n) 开启n个线程 spider.thread(5) run() 启动，会阻塞当前线程执行 spider.run() start()/runAsync() 异步启动，当前线程继续执行 spider.start() stop() 停止爬虫 spider.stop() addPipeline(Pipeline) 添加一个Pipeline，一个Spider可以有多个Pipeline spider .addPipeline(new ConsolePipeline()) setScheduler(Scheduler) 设置Scheduler，一个Spider只能有个一个Scheduler spider.setScheduler(new RedisScheduler()) setDownloader(Downloader) 设置Downloader，一个Spider只能有个一个Downloader spider .setDownloader( new SeleniumDownloader()) get(String) 同步调用，并直接取得结果 ResultItems result = spider.get(“http://webmagic.io/docs/&quot;) getAll(String…) 同步调用，并直接取得一堆结果 List results = spider.getAll(“http://webmagic.io/docs/&quot;, “http://webmagic.io/xxx&quot;) 2.3.2 爬虫配置SiteSite.me()可以对爬虫进行一些配置配置，包括编码、抓取间隔、超时时间、重试次数等。在这里我们先简单设置一下：重试次数为3次，抓取间隔为一秒。 12345678910private Site site = Site.me() .setCharset(\"UTF-8\")//编码 .setSleepTime(1)//抓取间隔时间 .setTimeOut(1000*10)//超时时间 .setRetrySleepTime(3000)//重试时间 .setRetryTimes(3);//重试次数public Site getSite() &#123; return site;&#125; 站点本身的一些配置信息，例如编码、HTTP头、超时时间、重试策略等、代理等，都可以通过设置Site对象来进行配置。 方法 说明 示例 setCharset(String) 设置编码 site.setCharset(“utf-8”) setUserAgent(String) 设置UserAgent site.setUserAgent(“Spider”) setTimeOut(int) 设置超时时间， 单位是毫秒 site.setTimeOut(3000) setRetryTimes(int) 设置重试次数 site.setRetryTimes(3) setCycleRetryTimes(int) 设置循环重试次数 site.setCycleRetryTimes(3) addCookie(String,String) 添加一条cookie site.addCookie(“dotcomt_user”,”code4craft”) setDomain(String) 设置域名，需设置域名后，addCookie才可生效 site.setDomain(“github.com”) addHeader(String,String) 添加一条addHeader site.addHeader(“Referer”,”https://github.com“) setHttpProxy(HttpHost) 设置Http代理 site.setHttpProxy(new HttpHost(“127.0.0.1”,8080)) 3. 爬虫分类网络爬虫按照系统结构和实现技术，大致可以分为以下几种类型：通用网络爬虫、聚焦网络爬虫、增量式网络爬虫、深层网络爬虫。 实际的网络爬虫系统通常是几种爬虫技术相结合实现的 3.1 通用网络爬虫通用网络爬虫又称全网爬虫（Scalable Web Crawler），爬行对象从一些种子 URL 扩充到整个 Web，主要为门户站点搜索引擎和大型 Web 服务提供商采集数据。 这类网络爬虫的爬行范围和数量巨大，对于爬行速度和存储空间要求较高，对于爬行页面的顺序要求相对较低，同时由于待刷新的页面太多，通常采用并行工作方式，但需要较长时间才能刷新一次页面。 简单的说就是互联网上抓取所有数据。 3.2 聚焦网络爬虫聚焦网络爬虫（Focused Crawler），又称主题网络爬虫（Topical Crawler），是指选择性地爬行那些与预先定义好的主题相关页面的网络爬虫。 和通用网络爬虫相比，聚焦爬虫只需要爬行与主题相关的页面，极大地节省了硬件和网络资源，保存的页面也由于数量少而更新快，还可以很好地满足一些特定人群对特定领域信息的需求 。 简单的说就是互联网上只抓取某一种数据。 3.3 增量式网络爬虫增量式网络爬虫（Incremental Web Crawler）是 指 对 已 下 载 网 页 采 取 增量式更新和只爬行新产生的或者已经发生变化网页的爬虫，它能够在一定程度上保证所爬行的页面是尽可能新的页面。 和周期性爬行和刷新页面的网络爬虫相比，增量式爬虫只会在需要的时候爬行新产生或发生更新的页面 ，并不重新下载没有发生变化的页面，可有效减少数据下载量，及时更新已爬行的网页，减小时间和空间上的耗费，但是增加了爬行算法的复杂度和实现难度。 简单的说就是互联网上只抓取刚刚更新的数据。 3.4 Deep Web 爬虫Web 页面按存在方式可以分为表层网页（Surface Web）和深层网页（Deep Web，也称 Invisible Web Pages 或 Hidden Web）。 表层网页是指传统搜索引擎可以索引的页面，以超链接可以到达的静态网页为主构成的 Web 页面。 Deep Web 是那些大部分内容不能通过静态链接获取的、隐藏在搜索表单后的，只有用户提交一些关键词才能获得的 Web 页面。 4. 案例开发分析我们已经学完了WebMagic的基本使用方法，现在准备使用WebMagic实现爬取数据的功能。这里是一个比较完整的实现。 在这里我们实现的是聚焦网络爬虫，只爬取招聘的相关数据。 4.1 业务分析今天要实现的是爬取https://www.51job.com/上的招聘信息。只爬取“计算机软件”和“互联网电子商务”两个行业的信息。 首先访问页面并搜索两个行业。结果如下 点击职位详情页，我们分析发现详情页还有一些数据需要抓取： 职位、公司名称、工作地点、薪资、发布时间、职位信息、公司联系方式、公司信息 4.2 数据库表根据以上信息，设计数据库表 1234567891011121314CREATE TABLE &#96;job_info&#96; ( &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;主键id&#39;, &#96;company_name&#96; varchar(100) DEFAULT NULL COMMENT &#39;公司名称&#39;, &#96;company_addr&#96; varchar(200) DEFAULT NULL COMMENT &#39;公司联系方式&#39;, &#96;company_info&#96; text COMMENT &#39;公司信息&#39;, &#96;job_name&#96; varchar(100) DEFAULT NULL COMMENT &#39;职位名称&#39;, &#96;job_addr&#96; varchar(50) DEFAULT NULL COMMENT &#39;工作地点&#39;, &#96;job_info&#96; text COMMENT &#39;职位信息&#39;, &#96;salary_min&#96; int(10) DEFAULT NULL COMMENT &#39;薪资范围，最小&#39;, &#96;salary_max&#96; int(10) DEFAULT NULL COMMENT &#39;薪资范围，最大&#39;, &#96;url&#96; varchar(150) DEFAULT NULL COMMENT &#39;招聘信息详情页&#39;, &#96;time&#96; varchar(10) DEFAULT NULL COMMENT &#39;职位最近发布时间&#39;, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8 COMMENT&#x3D;&#39;招聘信息&#39;; 4.3 实现流程我们需要解析职位列表页，获取职位的详情页，再解析页面获取数据。 获取url地址的流程如下 但是在这里有个问题：在解析页面的时候，很可能会解析出相同的url地址(例如商品标题和商品图片超链接，而且url一样)，如果不进行处理，同样的url会解析处理多次，浪费资源。所以我们需要有一个url去重的功能 4.3.1 Scheduler组件WebMagic提供了Scheduler可以帮助我们解决以上问题。 Scheduler是WebMagic中进行URL管理的组件。一般来说，Scheduler包括两个作用： 对待抓取的URL队列进行管理。 对已抓取的URL进行去重。 WebMagic内置了几个常用的Scheduler。如果你只是在本地执行规模比较小的爬虫，那么基本无需定制Scheduler，但是了解一下已经提供的几个Scheduler还是有意义的。 类 说明 备注 DuplicateRemovedScheduler 抽象基类，提供一些模板方法 继承它可以实现自己的功能 QueueScheduler 使用内存队列保存待抓取URL PriorityScheduler 使用带有优先级的内存队列保存待抓取URL 耗费内存较QueueScheduler更大，但是当设置了request.priority之后，只能使用PriorityScheduler才可使优先级生效 FileCacheQueueScheduler 使用文件保存抓取URL，可以在关闭程序并下次启动时，从之前抓取到的URL继续抓取 需指定路径，会建立.urls.txt和.cursor.txt两个文件 RedisScheduler 使用Redis保存抓取队列，可进行多台机器同时合作抓取 需要安装并启动redis 去重部分被单独抽象成了一个接口：DuplicateRemover，从而可以为同一个Scheduler选择不同的去重方式，以适应不同的需要，目前提供了两种去重方式。 类 说明 HashSetDuplicateRemover 使用HashSet来进行去重，占用内存较大 BloomFilterDuplicateRemover 使用BloomFilter来进行去重，占用内存较小，但是可能漏抓页面 RedisScheduler是使用Redis的set进行去重，其他的Scheduler默认都使用HashSetDuplicateRemover来进行去重。 如果要使用BloomFilter，必须要加入以下依赖： 123456&lt;!--WebMagic对布隆过滤器的支持--&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;16.0&lt;/version&gt;&lt;/dependency&gt; 修改代码，添加布隆过滤器 12345678910public static void main(String[] args) &#123; Spider.create(new JobProcessor()) //初始访问url地址 .addUrl(\"https://www.jd.com/moreSubject.aspx\") .addPipeline(new FilePipeline(\"D:/webmagic/\")) .setScheduler(new QueueScheduler().setDuplicateRemover(new BloomFilterDuplicateRemover(10000000)));//设置布隆去重过滤器，指定最多对1000万数据进行去重操作 .thread(5)//设置线程数 .run();&#125; 4.3.2 三种去重方式去重就有三种实现方式，那有什么不同呢？ HashSet 使用java中的HashSet不能重复的特点去重。优点是容易理解。使用方便。 缺点：占用内存大，性能较低。 Redis去重 使用Redis的set进行去重。优点是速度快（Redis本身速度就很快），而且去重不会占用爬虫服务器的资源，可以处理更大数据量的数据爬取。 缺点：需要准备Redis服务器，增加开发和使用成本。 布隆过滤器（BloomFilter） 使用布隆过滤器也可以实现去重。优点是占用的内存要比使用HashSet要小的多，也适合大量数据的去重操作。 缺点：有误判的可能。没有重复可能会判定重复，但是重复数据一定会判定重复。 布隆过滤器 (Bloom Filter)是由Burton Howard Bloom于1970年提出，它是一种space efficient的概率型数据结构，用于判断一个元素是否在集合中。在垃圾邮件过滤的黑白名单方法、爬虫(Crawler)的网址判重模块中等等经常被用到。 哈希表也能用于判断元素是否在集合中，但是布隆过滤器只需要哈希表的1/8或1/4的空间复杂度就能完成同样的问题。布隆过滤器可以插入元素，但不可以删除已有元素。其中的元素越多，误报率越大，但是漏报是不可能的。 原理： 布隆过滤器需要的是一个位数组(和位图类似)和K个映射函数(和Hash表类似)，在初始状态时，对于长度为m的位数组array，它的所有位被置0。 对于有n个元素的集合S={S1,S2…Sn},通过k个映射函数{f1,f2,……fk}，将集合S中的每个元素Sj(1&lt;=j&lt;=n)映射为K个值{g1,g2…gk}，然后再将位数组array中相对应的array[g1],array[g2]……array[gk]置为1： 如果要查找某个元素item是否在S中，则通过映射函数{f1,f2,…fk}得到k个值{g1,g2…gk}，然后再判断array[g1],array[g2]…array[gk]是否都为1，若全为1，则item在S中，否则item不在S中。 布隆过滤器会造成一定的误判，因为集合中的若干个元素通过映射之后得到的数值恰巧包括g1,g2,…gk，在这种情况下可能会造成误判，但是概率很小。 5. 案例实现5.1 开发准备5.1.1 创建工程创建Maven工程，并加入依赖。pom.xml为： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;crawler-job&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!--SpringMVC--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--SpringData Jpa--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--MySQL连接包--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--WebMagic核心包--&gt; &lt;dependency&gt; &lt;groupId&gt;us.codecraft&lt;/groupId&gt; &lt;artifactId&gt;webmagic-core&lt;/artifactId&gt; &lt;version&gt;0.7.3&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--WebMagic扩展--&gt; &lt;dependency&gt; &lt;groupId&gt;us.codecraft&lt;/groupId&gt; &lt;artifactId&gt;webmagic-extension&lt;/artifactId&gt; &lt;version&gt;0.7.3&lt;/version&gt; &lt;/dependency&gt; &lt;!--WebMagic对布隆过滤器的支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;16.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 5.1.2 加入配置文件添加application.properties配置文件 1234567891011#DB Configuration:spring.datasource.driverClassName=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://127.0.0.1:3306/crawlerspring.datasource.username=rootspring.datasource.password=root#JPA Configuration:spring.jpa.database=MySQLspring.jpa.show-sql=trueserver.port=80 5.1.3 编写Pojo123456789101112131415161718192021222324/** * 招聘信息实体类 * * @author wgy */@Entitypublic class JobInfo &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String companyName; private String companyAddr; private String companyInfo; private String jobName; private String jobAddr; private String jobInfo; private Integer salaryMin; private Integer salaryMax; private String url; private String time; //get/set/toString&#125; 5.1.4 编写Dao1234567/** * 招聘信息持久层接口 * * @author wgy */public interface JobInfoDao extends JpaRepository&lt;JobInfo, Long&gt; &#123;&#125; 5.1.5 编写Service编写Service接口 1234567891011121314151617181920212223/** * 招聘信息业务接口 * * @author wgy */public interface JobInfoService &#123; /** * 保存工作信息 * * @param jobInfo */ public void save(JobInfo jobInfo); /** * 根据条件查询工作信息 * * @param jobInfo * @return list */ public List&lt;JobInfo&gt; findJobInfo(JobInfo jobInfo);&#125; 编写Service实现类 1234567891011121314151617181920212223242526272829303132333435363738394041/** * 招聘信息业务实现 * * @author wgy */@Servicepublic class JobInfoServiceImpl implements JobInfoService &#123; @Autowired private JobInfoDao jobInfoDao; @Override @Transactional public void save(JobInfo jobInfo) &#123; //根据url和发布时间查询数据 JobInfo param = new JobInfo(); param.setUrl(jobInfo.getUrl()); param.setTime(jobInfo.getTime()); //执行查询 List&lt;JobInfo&gt; list = this.findJobInfo(param); //判断查询结果是否为空 if (list.size() == 0) &#123; //如果查询结果为空，表示招聘信息数据不存在，或者已经更新了，需要新增或者更新数据库 this.jobInfoDao.saveAndFlush(jobInfo); &#125; &#125; @Override public List&lt;JobInfo&gt; findJobInfo(JobInfo jobInfo) &#123; //设置查询条件 Example example = Example.of(jobInfo); //执行查询 List list = this.jobInfoDao.findAll(example); return list; &#125;&#125; 5.1.6 编写引导类12345678910111213/** * 引导类 * * @author wgy */@SpringBootApplication@EnableScheduling//开启定时任务public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 5.2 功能实现5.2.1 编写url解析功能123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 招聘信息解析 * * @author wgy */@Componentpublic class JobProcessor implements PageProcessor &#123; private String url = \"https://search.51job.com/list/060000,000000,0000,32%252c01,9,99,Java,2,1.html?lang=c&amp;postchannel=0000&amp;workyear=03&amp;cotype=99&amp;degreefrom=99&amp;jobterm=99&amp;companysize=99&amp;ord_field=0&amp;dibiaoid=0&amp;line=&amp;welfare=\"; private Site site = Site.me() .setCharset(\"GBK\")//设置编码 .setTimeOut(10 * 1000)//设置超时时间 .setRetrySleepTime(3000)//设置重试的间隔时间 .setRetryTimes(3);//设置重试的次数 @Autowired private SpringDataPipeline springDataPipeline; /** * 解析页面 * * @param page page */ @Override public void process(Page page) &#123; //解析页面，获取招聘信息详情的url地址 List&lt;Selectable&gt; list = page.getHtml().css(\"div.j_joblist div.e\").nodes(); //判断获取到的集合是否为空 if (list.size() == 0) &#123; // 如果为空，表示这是招聘详情页,解析页面，获取招聘详情信息，保存数据 this.saveJobInfo(page); &#125; else &#123; //如果不为空，表示这是列表页,解析出详情页的url地址，放到任务队列中 for (Selectable selectable : list) &#123; //获取url地址 String jobInfoUrl = selectable.links().toString(); //把获取到的url地址放到任务队列中 page.addTargetRequest(jobInfoUrl); &#125; &#125; //获取下一页的url String bkUrl = page.getHtml().css(\"div.p_in li.bk\").nodes().get(1).links().toString(); //把url放到任务队列中 page.addTargetRequest(bkUrl); &#125; @Override public Site getSite() &#123; return site; &#125; /** * initialDelay当任务启动后，等等多久执行方法 * &lt;p&gt; * fixedDelay每个多久执行方法 */ @Scheduled(initialDelay = 1000, fixedDelay = 100 * 1000) public void process() &#123; Spider.create(new JobProcessor()) .addUrl(url) .setScheduler(new QueueScheduler().setDuplicateRemover(new BloomFilterDuplicateRemover(100000))) .thread(10) .run(); &#125;&#125; 5.2.2 编写页面解析功能薪水的计算工具类MathSalary 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * 计算薪水工具类 * * @author wgy */public class MathSalary &#123; /** * 获取薪水范围 * * @param salaryStr * @return */ public static Integer[] getSalary(String salaryStr) &#123; //声明存放薪水范围的数组 Integer[] salary = new Integer[2]; //\"500/天\" //0.8-1.2万/月 //5-8千/月 //5-6万/年 String date = salaryStr.substring(salaryStr.length() - 1, salaryStr.length()); //如果是按天，则直接乘以240进行计算 if (!\"月\".equals(date) &amp;&amp; !\"年\".equals(date)) &#123; salaryStr = salaryStr.substring(0, salaryStr.length() - 2); salary[0] = salary[1] = str2Num(salaryStr, 240); return salary; &#125; String unit = salaryStr.substring(salaryStr.length() - 3, salaryStr.length() - 2); String[] salarys = salaryStr.substring(0, salaryStr.length() - 3).split(\"-\"); salary[0] = mathSalary(date, unit, salarys[0]); salary[1] = mathSalary(date, unit, salarys[1]); return salary; &#125; //根据条件计算薪水 private static Integer mathSalary(String date, String unit, String salaryStr) &#123; Integer salary = 0; //判断单位是否是万 if (\"万\".equals(unit)) &#123; //如果是万，薪水乘以10000 salary = str2Num(salaryStr, 10000); &#125; else &#123; //否则乘以1000 salary = str2Num(salaryStr, 1000); &#125; //判断时间是否是月 if (\"月\".equals(date)) &#123; //如果是月，薪水乘以12 salary = str2Num(salary.toString(), 12); &#125; return salary; &#125; private static int str2Num(String salaryStr, int num) &#123; try &#123; // 把字符串转为小数，必须用Number接受，否则会有精度丢失的问题 Number result = Float.parseFloat(salaryStr) * num; return result.intValue(); &#125; catch (Exception e) &#123; &#125; return 0; &#125;&#125; 解析页面，获取招聘详情信息，保存数据 123456789101112131415161718192021222324252627282930313233/** * 解析页面，获取招聘详情信息，保存数据 * * @param page */private void saveJobInfo(Page page) &#123; //创建招聘详情对象 JobInfo jobInfo = new JobInfo(); //解析页面 Html html = page.getHtml(); //获取数据，封装到对象中 jobInfo.setCompanyName(html.css(\"div.cn p.cname a\", \"text\").toString()); jobInfo.setCompanyAddr(Jsoup.parse(html.css(\"div.bmsg\").nodes().get(1).toString()).text()); jobInfo.setCompanyInfo(Jsoup.parse(html.css(\"div.tmsg\").toString()).text()); jobInfo.setJobName(html.css(\"div.cn h1\", \"text\").toString()); jobInfo.setJobAddr(html.css(\"div.cn span.lname\", \"text\").toString()); jobInfo.setJobInfo(Jsoup.parse(html.css(\"div.job_msg\").toString()).text()); jobInfo.setUrl(page.getUrl().toString()); //获取薪资 Integer[] salary = MathSalary.getSalary(html.css(\"div.cn strong\", \"text\").toString()); jobInfo.setSalaryMin(salary[0]); jobInfo.setSalaryMax(salary[1]); //获取发布时间 String time = Jsoup.parse(html.css(\"div.t1 span\").regex(\".*发布\").toString()).text(); jobInfo.setTime(time.substring(0, time.length() - 2)); //把结果保存起来 page.putField(\"jobInfo\", jobInfo);&#125; 5.3 使用和定制Pipeline在WebMagic中，Pileline是抽取结束后，进行处理的部分，它主要用于抽取结果的保存，也可以定制Pileline可以实现一些通用的功能。在这里我们会定制Pipeline实现数据导入到数据库中 5.3.1 Pipeline输出Pipeline的接口定义如下： 1234567public interface Pipeline &#123; //ResultItems保存了抽取结果，它是一个Map结构， //在page.putField(key,value)中保存的数据， //可以通过ResultItems.get(key)获取 public void process(ResultItems resultItems, Task task);&#125; 可以看到，Pipeline其实就是将PageProcessor抽取的结果，继续进行了处理的，其实在Pipeline中完成的功能，你基本上也可以直接在PageProcessor实现，那么为什么会有Pipeline？有几个原因： 为了模块分离 “页面抽取”和“后处理、持久化”是爬虫的两个阶段，将其分离开来，一个是代码结构比较清晰，另一个是以后也可能将其处理过程分开，分开在独立的线程以至于不同的机器执行。 Pipeline的功能比较固定，更容易做成通用组件 每个页面的抽取方式千变万化，但是后续处理方式则比较固定，例如保存到文件、保存到数据库这种操作，这些对所有页面都是通用的。 在WebMagic里，一个Spider可以有多个Pipeline，使用Spider.addPipeline()即可增加一个Pipeline。这些Pipeline都会得到处理，例如可以使用 1spider.addPipeline(new ConsolePipeline()).addPipeline(new FilePipeline()) 实现输出结果到控制台，并且保存到文件的目标。 5.3.2 已有的PipelineWebMagic中就已经提供了控制台输出、保存到文件、保存为JSON格式的文件几种通用的Pipeline。 类 说明 备注 ConsolePipeline 输出结果到控制台 抽取结果需要实现toString方法 FilePipeline 保存结果到文件 抽取结果需要实现toString方法 JsonFilePipeline JSON格式保存结果到文件 ConsolePageModelPipeline (注解模式)输出结果到控制台 FilePageModelPipeline (注解模式)保存结果到文件 JsonFilePageModelPipeline (注解模式)JSON格式保存结果到文件 想持久化的字段需要有getter方法 5.3.3 案例自定义Pipeline导入数据自定义SpringDataPipeline 1234567891011121314151617181920212223/** * 自定义Pipeline导入数据 * * @author wgy */@Componentpublic class SpringDataPipeline implements Pipeline &#123; @Autowired private JobInfoService jobInfoService; @Override public void process(ResultItems resultItems, Task task) &#123; //获取封装好的招聘详情对象 JobInfo jobInfo = resultItems.get(\"jobInfo\"); //判断数据是否不为空 if (jobInfo != null) &#123; //如果不为空把数据保存到数据库中 this.jobInfoService.save(jobInfo); &#125; &#125;&#125; 在JobProcessor中修改process()启动的逻辑，添加代码 1234567891011121314151617@Autowiredprivate SpringDataPipeline springDataPipeline;/** * initialDelay当任务启动后，等等多久执行方法 * &lt;p&gt; * fixedDelay每个多久执行方法 */@Scheduled(initialDelay = 1000, fixedDelay = 100 * 1000)public void process() &#123; Spider.create(new JobProcessor()) .addUrl(url) .setScheduler(new QueueScheduler().setDuplicateRemover(new BloomFilterDuplicateRemover(100000))) .thread(10) .addPipeline(this.springDataPipeline) .run();&#125;","tags":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"https://wgy1993.gitee.io/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"}]},{"title":"网络爬虫(一)","date":"2020-10-02T05:04:34.000Z","path":"archives/ceeb4255.html","text":"1. 网络爬虫1.1 网络爬虫介绍在大数据时代，信息的采集是一项重要的工作，而互联网中的数据是海量的，如果单纯靠人力进行信息采集，不仅低效繁琐，搜集的成本也会提高。如何自动高效地获取互联网中我们感兴趣的信息并为我们所用是一个重要的问题，而爬虫技术就是为了解决这些问题而生的。 网络爬虫（Web crawler）也叫做网络机器人，可以代替人们自动地在互联网中进行数据信息的采集与整理。它是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本，可以自动采集所有其能够访问到的页面内容，以获取相关数据。 从功能上来讲，爬虫一般分为数据采集，处理，储存三个部分。爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列,直到满足系统的一定停止条件。 1.2 为什么学网络爬虫我们初步认识了网络爬虫，但是为什么要学习网络爬虫呢？只有清晰地知道我们的学习目的，才能够更好地学习这一项知识。在此，总结了4种常见的学习爬虫的原因： 可以实现搜索引擎 我们学会了爬虫编写之后，就可以利用爬虫自动地采集互联网中的信息，采集回来后进行相应的存储或处理，在需要检索某些信息的时候，只需在采集回来的信息中进行检索，即实现了私人的搜索引擎。 大数据时代，可以让我们获取更多的数据源。 在进行大数据分析或者进行数据挖掘的时候，需要有数据源进行分析。我们可以从某些提供数据统计的网站获得，也可以从某些文献或内部资料中获得，但是这些获得数据的方式，有时很难满足我们对数据的需求，而手动从互联网中去寻找这些数据，则耗费的精力过大。此时就可以利用爬虫技术，自动地从互联网中获取我们感兴趣的数据内容，并将这些数据内容爬取回来，作为我们的数据源，再进行更深层次的数据分析，并获得更多有价值的信息。 可以更好地进行搜索引擎优化（SEO）。 对于很多SEO从业者来说，为了更好的完成工作，那么就必须要对搜索引擎的工作原理非常清楚，同时也需要掌握搜索引擎爬虫的工作原理。而学习爬虫，可以更深层次地理解搜索引擎爬虫的工作原理，这样在进行搜索引擎优化时，才能知己知彼，百战不殆。 有利于就业。 从就业来说，爬虫工程师方向是不错的选择之一，因为目前爬虫工程师的需求越来越大，而能够胜任这方面岗位的人员较少，所以属于一个比较紧缺的职业方向，并且随着大数据时代和人工智能的来临，爬虫技术的应用将越来越广泛，在未来会拥有很好的发展空间。 2.1 爬虫入门程序2.1.1 环境准备 JDK1.8 IntelliJ IDEA IDEA自带的Maven 2.1.2 创建工程添加依赖123456789101112131415&lt;dependencies&gt; &lt;!-- HttpClient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.1.3 加入log4j.properties123456log4j.rootLogger=DEBUG,A1log4j.logger.com.wgy = DEBUGlog4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c]-[%p] %m%n 2.1.4 编写代码123456789101112131415161718192021222324252627282930/** * 爬虫入门程序 * * @author wgy */public class CrawlerFirst &#123; public static void main(String[] args) throws Exception &#123; //1. 打开浏览器,创建HttpClient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //2. 输入网址,发起get请求创建HttpGet对象 HttpGet httpGet = new HttpGet(\"http://www.itcast.cn\"); //使用User-Agent防止HttpClient发送http请求时403 Forbidden和安全拦截 //String userAgent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"; //httpGet.setHeader(\"User-Agent\", userAgent); //3.按回车，发起请求，返回响应，使用HttpClient对象发起请求 CloseableHttpResponse response = httpClient.execute(httpGet); //4. 解析响应，获取数据 //判断状态码是否是200 if (response.getStatusLine().getStatusCode() == 200) &#123; HttpEntity entity = response.getEntity(); String content = EntityUtils.toString(entity, \"UTF-8\"); System.out.println(content); &#125; &#125;&#125; 2. HttpClient网络爬虫就是用程序帮助我们访问网络上的资源，我们一直以来都是使用HTTP协议访问互联网的网页，网络爬虫需要编写程序，在这里使用同样的HTTP协议访问网页。 这里我们使用Java的HTTP协议客户端 HttpClient这个技术，来实现抓取网页数据。 2.1 GET请求1234567891011121314151617181920212223242526272829303132333435363738394041/** * GET请求 * * @author wgy */public class HttpGetTest &#123; public static void main(String[] args) &#123; //创建HttpClient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //创建HttpGet对象，设置url访问地址 HttpGet httpGet = new HttpGet(\"http://www.itcast.cn\"); CloseableHttpResponse response = null; try &#123; //使用HttpClient发起请求，获取response response = httpClient.execute(httpGet); //解析响应 if (response.getStatusLine().getStatusCode() == 200) &#123; String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content.length()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭response try &#123; response.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; httpClient.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 请求结果： 2.2 带参数的GET请求12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 带参数的GET请求 * * @author wgy */public class HttpGetParamTest &#123; public static void main(String[] args) throws Exception &#123; //创建HttpClient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //设置请求地址是：http://yun.itheima.com/search?keys=Java //创建URIBuilder URIBuilder uriBuilder = new URIBuilder(\"http://yun.itheima.com/search\"); //设置参数 uriBuilder.setParameter(\"keys\", \"Java\"); //创建HttpGet对象，设置url访问地址 HttpGet httpGet = new HttpGet(uriBuilder.build()); System.out.println(\"发起请求的信息：\" + httpGet); CloseableHttpResponse response = null; try &#123; //使用HttpClient发起请求，获取response response = httpClient.execute(httpGet); //解析响应 if (response.getStatusLine().getStatusCode() == 200) &#123; String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content.length()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭response try &#123; response.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; httpClient.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 请求结果： 2.3 POST请求123456789101112131415161718192021222324252627282930313233343536373839404142/** * POST请求 * * @author wgy */public class HttpPostTest &#123; public static void main(String[] args) &#123; //创建HttpClient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //创建HttpPost对象，设置url访问地址 HttpPost httpPost = new HttpPost(\"http://www.itcast.cn\"); CloseableHttpResponse response = null; try &#123; //使用HttpClient发起请求，获取response response = httpClient.execute(httpPost); //解析响应 if (response.getStatusLine().getStatusCode() == 200) &#123; String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content.length()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; //关闭response try &#123; response.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; httpClient.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 请求结果： 2.4 带参数的POST请求123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 带参数的POST请求 * * @author wgy */public class HttpPostParamTest &#123; public static void main(String[] args) throws Exception &#123; //创建HttpClient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //创建HttpPost对象，设置url访问地址 HttpPost httpPost = new HttpPost(\"http://yun.itheima.com/search\"); //声明List集合，封装表单中的参数 List&lt;NameValuePair&gt; params = new ArrayList&lt;NameValuePair&gt;(); //设置请求地址是：http://yun.itheima.com/search?keys=Java params.add(new BasicNameValuePair(\"keys\", \"Java\")); //创建表单的Entity对象,第一个参数就是封装好的表单数据，第二个参数就是编码 UrlEncodedFormEntity formEntity = new UrlEncodedFormEntity(params, \"UTF-8\"); //设置表单的Entity对象到Post请求中 httpPost.setEntity(formEntity); CloseableHttpResponse response = null; try &#123; //使用HttpClient发起请求，获取response response = httpClient.execute(httpPost); //解析响应 if (response.getStatusLine().getStatusCode() == 200) &#123; String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content.length()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭response try &#123; response.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; httpClient.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 请求结果： 2.5 连接池如果每次请求都要创建HttpClient，会有频繁创建和销毁的问题，可以使用连接池来解决这个问题。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * HttpClient连接池 * * @author wgy */public class HttpClientPoolTest &#123; public static void main(String[] args) &#123; //创建连接池管理器 PoolingHttpClientConnectionManager cm = new PoolingHttpClientConnectionManager(); //设置最大连接数 cm.setMaxTotal(100); //设置每个主机的最大连接数 cm.setDefaultMaxPerRoute(10); //使用连接池管理器发起请求 doGet(cm); doGet(cm); &#125; private static void doGet(PoolingHttpClientConnectionManager cm) &#123; //不是每次创建新的HttpClient，而是从连接池中获取HttpClient对象 CloseableHttpClient httpClient = HttpClients.custom().setConnectionManager(cm).build(); HttpGet httpGet = new HttpGet(\"http://www.itcast.cn\"); CloseableHttpResponse response = null; try &#123; response = httpClient.execute(httpGet); if (response.getStatusLine().getStatusCode() == 200) &#123; String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content.length()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (response != null) &#123; try &#123; response.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; //不能关闭HttpClient，由连接池管理HttpClient //httpClient.close(); &#125; &#125; &#125;&#125; 2.6 请求参数有时候因为网络，或者目标服务器的原因，请求需要更长的时间才能完成，我们需要自定义相关时间 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 请求参数 * * @author wgy */public class HttpConfigTest &#123; public static void main(String[] args) &#123; //创建HttpClient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //创建HttpGet对象，设置url访问地址 HttpGet httpGet = new HttpGet(\"http://www.itcast.cn\"); //配置请求信息 RequestConfig config = RequestConfig.custom().setConnectTimeout(1000) //创建连接的最长时间，单位是毫秒 .setConnectionRequestTimeout(500) //设置获取连接的最长时间，单位是毫秒 .setSocketTimeout(10 * 1000) //设置数据传输的最长时间，单位是毫秒 .build(); //给请求设置请求信息 httpGet.setConfig(config); CloseableHttpResponse response = null; try &#123; //使用HttpClient发起请求，获取response response = httpClient.execute(httpGet); //解析响应 if (response.getStatusLine().getStatusCode() == 200) &#123; String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); System.out.println(content.length()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭response try &#123; response.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; httpClient.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 3. Jsoup我们抓取到页面之后，还需要对页面进行解析。可以使用字符串处理工具解析页面，也可以使用正则表达式，但是这些方法都会带来很大的开发成本，所以我们需要使用一款专门解析html页面的技术。 3.1 Jsoup介绍Jsoup 是一款Java 的HTML解析器，可直接解析某个URL地址、HTML文本内容。它提供了一套非常省力的API，可通过DOM，CSS以及类似于jQuery的操作方法来取出和操作数据。 Jsoup的主要功能如下： 从一个URL，文件或字符串中解析HTML； 使用DOM或CSS选择器来查找、取出数据； 可操作HTML元素、属性、文本； 3.2 Jsoup解析Jsoup依赖： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;dependencies&gt; &lt;!-- HttpClient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;!--&lt;scope&gt;test&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;!--Jsoup--&gt; &lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;/groupId&gt; &lt;artifactId&gt;jsoup&lt;/artifactId&gt; &lt;version&gt;1.10.2&lt;/version&gt; &lt;/dependency&gt; &lt;!--测试--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--工具--&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.7&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.2.1 解析urlJsoup可以直接输入url，它会发起请求并获取数据，封装为Document对象 123456789101112131415161718192021222324/** * jsoup测试 * * @author wgy */public class JsoupFirstTest &#123; /** * 解析url * * @throws Exception */ @Test public void testUrl() throws Exception &#123; //解析url地址,第一个参数是访问的url，第二个参数是访问时候的超时时间 Document doc = Jsoup.parse(new URL(\"http://www.itcast.cn\"), 1000); //使用标签选择器，获取title标签中的内容 String title = doc.getElementsByTag(\"title\").first().text(); //打印 System.out.println(title); &#125;&#125; PS：虽然使用Jsoup可以替代HttpClient直接发起请求解析数据，但是往往不会这样用，因为实际的开发过程中，需要使用到多线程，连接池，代理等等方式，而jsoup对这些的支持并不是很好，所以我们一般把jsoup仅仅作为Html解析工具使用 3.2.2 解析字符串先准备以下html文件 1234567891011121314151617181920212223242526272829303132333435&lt;html&gt; &lt;head&gt; &lt;title&gt;传智播客官网-一样的教育,不一样的品质&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=\"city\"&gt; &lt;h3 id=\"city_bj\"&gt;北京中心&lt;/h3&gt; &lt;fb:img src=\"/2018czgw/images/slogan.jpg\" class=\"slogan\"/&gt; &lt;div class=\"city_in\"&gt; &lt;div class=\"city_con\" style=\"display: none;\"&gt; &lt;ul&gt; &lt;li id=\"test\" class=\"class_a class_b\"&gt; &lt;a href=\"http://www.itcast.cn\" target=\"_blank\"&gt; &lt;span class=\"s_name\"&gt;北京&lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=\"http://sh.itcast.cn\" target=\"_blank\"&gt; &lt;span class=\"s_name\"&gt;上海&lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;li&gt; &lt;a href=\"http://gz.itcast.cn\" target=\"_blank\"&gt; &lt;span abc=\"123\" class=\"s_name\"&gt;广州&lt;/span&gt; &lt;/a&gt; &lt;/li&gt; &lt;ul&gt; &lt;li&gt;天津&lt;/li&gt; &lt;/ul&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; Jsoup可以直接输入字符串，并封装为Document对象 12345678910111213141516171819202122232425/** * jsoup测试 * * @author wgy */public class JsoupFirstTest &#123; /** * 解析字符串 * * @throws Exception */ @Test public void testString() throws Exception &#123; //使用工具类读取文件，获取字符串 String content = FileUtils.readFileToString(new File(\"C:\\\\Users\\\\wgy\\\\Desktop\\\\test.html\"), \"UTF-8\"); //解析字符串 Document doc = Jsoup.parse(content); String title = doc.getElementsByTag(\"title\").first().text(); System.out.println(title); &#125;&#125; 3.2.3 解析文件Jsoup可以直接解析文件，并封装为Document对象 1234567891011121314151617181920212223/** * jsoup测试 * * @author wgy */public class JsoupFirstTest &#123; /** * 解析文件 * * @throws Exception */ @Test public void testFile() throws Exception &#123; //解析文件 Document doc = Jsoup.parse(new File(\"C:\\\\Users\\\\wgy\\\\Desktop\\\\test.html\"), \"UTF-8\"); String title = doc.getElementsByTag(\"title\").first().text(); System.out.println(title); &#125;&#125; 3.2.4 使用dom方式遍历文档3.2.4.1 元素获取 根据id查询元素getElementById 根据标签获取元素getElementsByTag 根据class获取元素getElementsByClass 根据属性获取元素getElementsByAttribute 123456789101112131415161718192021222324252627282930313233343536373839/** * jsoup测试 * * @author wgy */public class JsoupFirstTest &#123; /** * 元素获取 * * @throws Exception */ @Test public void testDOM() throws Exception &#123; //解析文件，获取Document对象 Document doc = Jsoup.parse(new File(\"C:\\\\Users\\\\wgy\\\\Desktop\\\\test.html\"), \"UTF-8\"); //获取元素 //1. 根据id查询元素getElementById //Element element = doc.getElementById(\"city_bj\"); //2. 根据标签获取元素getElementsByTag //Element element = doc.getElementsByTag(\"span\").first(); //3. 根据class获取元素getElementsByClass //Element element = doc.getElementsByClass(\"class_a class_b\").first(); //Element element = doc.getElementsByClass(\"class_a\").first(); //Element element = doc.getElementsByClass(\"class_b\").first(); //4. 根据属性获取元素getElementsByAttribute //Element element = doc.getElementsByAttribute(\"abc\").first(); Element element = doc.getElementsByAttributeValue(\"href\", \"http://sh.itcast.cn\").first(); //打印元素的内容 System.out.println(\"获取到的元素内容是：\" + element.text()); &#125;&#125; 3.2.4.2 元素中获取数据 从元素中获取id 从元素中获取className 从元素中获取属性的值attr 从元素中获取所有属性attributes 从元素中获取文本内容text 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * jsoup测试 * * @author wgy */public class JsoupFirstTest &#123; /** * 元素中获取数据 * * @throws Exception */ @Test public void testData() throws Exception &#123; //解析文件，获取Document Document doc = Jsoup.parse(new File(\"C:\\\\Users\\\\wgy\\\\Desktop\\\\test.html\"), \"UTF-8\"); //根据id获取元素 Element element = doc.getElementById(\"test\"); String str = \"\"; //元素中获取数据 //1. 从元素中获取id str = element.id(); //2. 从元素中获取className str = element.className(); //Set&lt;String&gt; classSet = element.classNames(); //for (String s : classSet ) &#123; // System.out.println(s); //&#125; //3. 从元素中获取属性的值attr //str = element.attr(\"id\"); str = element.attr(\"class\"); //4. 从元素中获取所有属性attributes Attributes attributes = element.attributes(); System.out.println(attributes.toString()); //5. 从元素中获取文本内容text str = element.text(); //打印获取到的内容 System.out.println(\"获取到的数据是：\" + str); &#125;&#125; 3.2.5 使用选择器语法查找元素Jsoup elements对象支持类似于CSS (或jquery)的选择器语法，来实现非常强大和灵活的查找功能。这个select 方法在Document, Element,或Elements对象中都可以使用。且是上下文相关的，因此可实现指定元素的过滤，或者链式选择访问。 Select方法将返回一个Elements集合，并提供一组方法来抽取和处理结果。 3.2.5.1 Selector选择器概述 tagname: 通过标签查找元素，比如：span #id: 通过ID查找元素，比如：# city_bj .class: 通过class名称查找元素，比如：.class_a [attribute]: 利用属性查找元素，比如：[abc] [attr=value]: 利用属性值来查找元素，比如：[class=s_name] 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * jsoup测试 * * @author wgy */public class JsoupFirstTest &#123; /** * Selector选择器 * * @throws Exception */ @Test public void testSelector() throws Exception &#123; //解析html文件，获取Document对象 Document doc = Jsoup.parse(new File(\"C:\\\\Users\\\\wgy\\\\Desktop\\\\test.html\"), \"UTF-8\"); //tagname: 通过标签查找元素，比如：span Elements elements = doc.select(\"span\"); //for (Element element : elements) &#123; // System.out.println(element.text()); //&#125; //#id: 通过ID查找元素，比如：#city_bj //Element element = doc.select(\"#city_bj\").first(); //.class: 通过class名称查找元素，比如：.class_a //Element element = doc.select(\".class_a\").first(); //[attribute]: 利用属性查找元素，比如：[abc] Element element = doc.select(\"[abc]\").first(); //[attr=value]: 利用属性值来查找元素，比如：[class=s_name] Elements elements1 = doc.select(\"[class=s_name]\"); for (Element element1 : elements1) &#123; System.out.println(element1.text()); &#125; //打印结果 System.out.println(\"获取到的结果是：\" + element.text()); &#125;&#125; 3.2.5.2 Selector选择器组合使用 el#id: 元素+ID，比如： h3#city_bj el.class: 元素+class，比如： li.class_a el[attr]: 元素+属性名，比如： span[abc] 任意组合: 比如：span[abc].s_name ancestor child: 查找某个元素下子元素，比如：.city_con li 查找”city_con”下的所有li parent &gt; child: 查找某个父元素下的直接子元素，比如：.city_con &gt; ul &gt; li 查找city_con第一级（直接子元素）的ul，再找所有ul下的第一级li parent &gt; *: 查找某个父元素下所有直接子元素 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * jsoup测试 * * @author wgy */public class JsoupFirstTest &#123; /** * Selector选择器组合使用 * * @throws Exception */ @Test public void testSelector2() throws Exception &#123; //解析html文件，获取Document对象 Document doc = Jsoup.parse(new File(\"C:\\\\Users\\\\wgy\\\\Desktop\\\\test.html\"), \"UTF-8\"); //el#id: 元素+ID，比如： h3#city_bj Element element = doc.select(\"h3#city_bj\").first(); //el.class: 元素+class，比如： li.class_a element = doc.select(\"li.class_a\").first(); //el[attr]: 元素+属性名，比如： span[abc] element = doc.select(\"span[abc]\").first(); //任意组合: 比如：span[abc].s_name element = doc.select(\"span[abc].s_name\").first(); //ancestor child: 查找某个元素下子元素，比如：.city_con li 查找\"city_con\"下的所有li Elements elements = doc.select(\".city_con li\"); //parent &gt; child: 查找某个父元素下的直接子元素，比如： //.city_con &gt; ul &gt; li 查找city_con第一级（直接子元素）的ul，再找所有ul下的第一级li elements = doc.select(\".city_con &gt; ul &gt; li\"); //parent &gt; *: 查找某个父元素下所有直接子元素 elements = doc.select(\".city_con &gt; ul &gt; *\"); System.out.println(\"获取到的内容是：\" + element.text()); for (Element element1 : elements) &#123; System.out.println(\"遍历的结果：\" + element1.text()); &#125; &#125;&#125; 4. 爬虫案例学习了HttpClient和Jsoup，就掌握了如何抓取数据和如何解析数据，接下来，我们做一个小练习，把京东的手机数据抓取下来。 主要目的是HttpClient和Jsoup的学习。 4.1 需求分析首先访问京东，搜索手机，分析页面，我们抓取以下商品数据：商品图片、价格、标题、商品详情页 4.1.1 SPU和SKU除了以上四个属性以外，我们发现上图中的苹果手机有四种产品，我们应该每一种都要抓取。那么这里就必须要了解spu和sku的概念 SPU = Standard Product Unit （标准产品单位） SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。通俗点讲，属性值、特性相同的商品就可以称为一个SPU。 例如上图中的苹果手机就是SPU，包括红色、深灰色、金色、银色 SKU=stock keping unit(库存量单位) SKU即库存进出计量的单位， 可以是以件、盒、托盘等为单位。SKU是物理上不可分割的最小存货单元。在使用时要根据不同业态，不同管理模式来处理。在服装、鞋类商品中使用最多最普遍。 例如上图中的苹果手机有几个款式，红色苹果手机，就是一个sku 查看页面的源码也可以看出区别 4.2 开发准备4.2.1 数据库表12345678910111213CREATE TABLE &#96;jd_item&#96; ( &#96;id&#96; bigint(10) NOT NULL AUTO_INCREMENT COMMENT &#39;主键id&#39;, &#96;spu&#96; bigint(15) DEFAULT NULL COMMENT &#39;商品集合id&#39;, &#96;sku&#96; bigint(15) DEFAULT NULL COMMENT &#39;商品最小品类单元id&#39;, &#96;title&#96; varchar(100) DEFAULT NULL COMMENT &#39;商品标题&#39;, &#96;price&#96; bigint(10) DEFAULT NULL COMMENT &#39;商品价格&#39;, &#96;pic&#96; varchar(200) DEFAULT NULL COMMENT &#39;商品图片&#39;, &#96;url&#96; varchar(200) DEFAULT NULL COMMENT &#39;商品详情地址&#39;, &#96;created&#96; datetime DEFAULT NULL COMMENT &#39;创建时间&#39;, &#96;updated&#96; datetime DEFAULT NULL COMMENT &#39;更新时间&#39;, PRIMARY KEY (&#96;id&#96;), KEY &#96;sku&#96; (&#96;sku&#96;) USING BTREE) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8 COMMENT&#x3D;&#39;京东商品表&#39;; 4.2.2 添加依赖使用Spring Boot+Spring Data JPA和定时任务进行开发，需要创建Maven工程并添加以下依赖 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;crawler-jd&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!--SpringMVC--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--SpringData Jpa--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--MySQL连接包--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- HttpClient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--Jsoup--&gt; &lt;dependency&gt; &lt;groupId&gt;org.jsoup&lt;/groupId&gt; &lt;artifactId&gt;jsoup&lt;/artifactId&gt; &lt;version&gt;1.10.3&lt;/version&gt; &lt;/dependency&gt; &lt;!--工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 4.2.3 添加配置文件加入application.properties配置文件 1234567891011#DB Configuration:spring.datasource.driverClassName=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://127.0.0.1:3306/crawlerspring.datasource.username=rootspring.datasource.password=root#JPA Configuration:spring.jpa.database=MySQLspring.jpa.show-sql=trueserver.port=80 4.3 代码实现4.3.1 编写pojo12345678910111213141516171819202122232425262728293031/** * 京东商品实体类 * * @author wgy */@Entity@Table(name = \"jd_item\")public class Item &#123; //主键 @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; //标准产品单位（商品集合） private Long spu; //库存量单位（最小品类单元） private Long sku; //商品标题 private String title; //商品价格 private Double price; //商品图片 private String pic; //商品详情地址 private String url; //创建时间 private Date created; //更新时间 private Date updated; //get/set/toString...&#125; 4.3.2 编写dao1234567/** * dao接口 * * @author wgy */public interface ItemDao extends JpaRepository&lt;Item, Long&gt; &#123;&#125; 4.3.3 编写ServiceItemService接口 12345678910111213141516171819202122/** * service接口 * * @author wgy */public interface ItemService &#123; /** * 保存商品 * * @param item */ public void save(Item item); /** * 根据条件查询商品 * * @param item * @return */ public List&lt;Item&gt; findAll(Item item);&#125; ItemServiceImpl实现类 12345678910111213141516171819202122232425262728/** * service实现类 * * @author wgy */@Servicepublic class ItemServiceImpl implements ItemService &#123; @Autowired private ItemDao itemDao; @Override @Transactional public void save(Item item) &#123; this.itemDao.save(item); &#125; @Override public List&lt;Item&gt; findAll(Item item) &#123; //声明查询条件 Example&lt;Item&gt; example = Example.of(item); //根据查询条件进行查询数据 List&lt;Item&gt; list = this.itemDao.findAll(example); return list; &#125;&#125; 4.3.4 编写引导类1234567891011121314/** * 引导类 * * @author wgy */@SpringBootApplication//使用定时任务，需要先开启定时任务，需要添加注解@EnableSchedulingpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 4.3.5 封装HttpClient123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147/** * HttpClient工具类 * * @author wgy */@Componentpublic class HttpUtils &#123; private PoolingHttpClientConnectionManager cm; public HttpUtils() &#123; this.cm = new PoolingHttpClientConnectionManager(); //设置最大连接数 this.cm.setMaxTotal(100); //设置每个主机的最大连接数 this.cm.setDefaultMaxPerRoute(10); &#125; /** * 根据请求地址下载页面数据 * * @param url * @return 页面数据 */ public String doGetHtml(String url) &#123; //获取HttpClient对象 CloseableHttpClient httpClient = HttpClients.custom().setConnectionManager(this.cm).build(); //创建httpGet请求对象，设置url地址 HttpGet httpGet = new HttpGet(url); //使用User-Agent防止HttpClient发送http请求时403 Forbidden和安全拦截 String userAgent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"; httpGet.setHeader(\"User-Agent\", userAgent); //设置请求信息 httpGet.setConfig(this.getConfig()); CloseableHttpResponse response = null; try &#123; //使用HttpClient发起请求，获取响应 response = httpClient.execute(httpGet); //解析响应，返回结果 if (response.getStatusLine().getStatusCode() == 200) &#123; //判断响应体Entity是否不为空，如果不为空就可以使用EntityUtils if (response.getEntity() != null) &#123; String content = EntityUtils.toString(response.getEntity(), \"UTF-8\"); return content; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭response if (response != null) &#123; try &#123; response.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; //返回空串 return \"\"; &#125; /** * 下载图片 * * @param url * @return 图片名称 */ public String doGetImage(String url) &#123; //获取HttpClient对象 CloseableHttpClient httpClient = HttpClients.custom().setConnectionManager(this.cm).build(); //创建httpGet请求对象，设置url地址 HttpGet httpGet = new HttpGet(url); //使用User-Agent防止HttpClient发送http请求时403 Forbidden和安全拦截 String userAgent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"; httpGet.setHeader(\"User-Agent\", userAgent); //设置请求信息 httpGet.setConfig(this.getConfig()); CloseableHttpResponse response = null; try &#123; //使用HttpClient发起请求，获取响应 response = httpClient.execute(httpGet); //解析响应，返回结果 if (response.getStatusLine().getStatusCode() == 200) &#123; //判断响应体Entity是否不为空 if (response.getEntity() != null) &#123; //下载图片 //获取图片的后缀 String extName = url.substring(url.lastIndexOf(\".\")); //创建图片名，重命名图片 String picName = UUID.randomUUID().toString() + extName; //下载图片 //声明OutPutStream OutputStream outputStream = new FileOutputStream(new File(\"C:\\\\Users\\\\wgy\\\\Desktop\\\\images\\\\\" + picName)); response.getEntity().writeTo(outputStream); //返回图片名称 return picName; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; //关闭response if (response != null) &#123; try &#123; response.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; //如果下载失败，返回空串 return \"\"; &#125; /** * 设置请求信息 * * @return */ private RequestConfig getConfig() &#123; RequestConfig config = RequestConfig.custom() .setConnectTimeout(1000) //创建连接的最长时间 .setConnectionRequestTimeout(500) // 获取连接的最长时间 .setSocketTimeout(10000) //数据传输的最长时间 .build(); return config; &#125;&#125; 4.3.6 实现数据抓取使用定时任务，可以定时抓取最新的数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102/** * 定时任务：京东商品手机信息下载 * * @author wgy */@Componentpublic class ItemTask &#123; private static final ObjectMapper MAPPER = new ObjectMapper(); @Autowired private HttpUtils httpUtils; @Autowired private ItemService itemService; /** * 下载任务 * 当下载任务完成后，间隔100秒进行下一次的任务 * * @throws Exception */ @Scheduled(fixedDelay = 100 * 1000) public void itemTask() throws Exception &#123; //声明需要解析的初始地址 String url = \"https://search.jd.com/Search?keyword=%E6%89%8B%E6%9C%BA&amp;wq=%E6%89%8B%E6%9C%BA&amp;s=51&amp;click=0&amp;page=\"; //按照页面对手机的搜索结果进行遍历解析 for (int i = 1; i &lt; 10; i = i + 2) &#123; String html = httpUtils.doGetHtml(url + i); //解析页面，获取商品数据并存储 this.parse(html); &#125; System.out.println(\"手机数据抓取完成！\"); &#125; /** * 解析页面，获取商品数据并存储 * * @param html * @throws Exception */ private void parse(String html) throws Exception &#123; //解析html获取Document Document doc = Jsoup.parse(html); //获取spu信息 Elements spuEles = doc.select(\"div#J_goodsList &gt; ul &gt; li\"); for (Element spuEle : spuEles) &#123; //获取spu long spu = Long.parseLong(StringUtils.isEmpty(spuEle.attr(\"data-spu\")) ? \"0\" : spuEle.attr(\"data-spu\")); //获取sku信息 Elements skuEles = spuEle.select(\"li.ps-item\"); for (Element skuEle : skuEles) &#123; //获取sku long sku = Long.parseLong(skuEle.select(\"[data-sku]\").attr(\"data-sku\")); //根据sku查询商品数据 Item item = new Item(); item.setSku(sku); List&lt;Item&gt; list = this.itemService.findAll(item); if (list.size() &gt; 0) &#123; //如果商品存在，就进行下一个循环，该商品不保存，因为已存在 continue; &#125; //设置商品的spu item.setSpu(spu); //获取商品的详情的url String itemUrl = \"https://item.jd.com/\" + sku + \".html\"; item.setUrl(itemUrl); //获取商品的图片 String picUrl = \"https:\" + skuEle.select(\"img[data-sku]\").first().attr(\"data-lazy-img\"); picUrl = picUrl.replace(\"/n7/\", \"/n1/\"); String picName = this.httpUtils.doGetImage(picUrl); item.setPic(picName); //获取商品的价格 String priceJson = this.httpUtils.doGetHtml(\"https://p.3.cn/prices/mgets?skuIds=J_\" + sku); double price = MAPPER.readTree(priceJson).get(0).get(\"p\").asDouble(); item.setPrice(price); //获取商品的标题 String itemInfo = this.httpUtils.doGetHtml(item.getUrl()); String title = Jsoup.parse(itemInfo).select(\"div.sku-name\").text(); item.setTitle(title); item.setCreated(new Date()); item.setUpdated(item.getCreated()); //保存商品数据到数据库中 this.itemService.save(item); &#125; &#125; &#125;&#125;","tags":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"https://wgy1993.gitee.io/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"}]},{"title":"Mybatis-Plus(二)","date":"2020-10-01T03:32:11.000Z","path":"archives/1ea41e8e.html","text":"1. ActiveRecordActiveRecord（简称AR）一直广受动态语言（ PHP 、 Ruby 等）的喜爱，而 Java 作为准静态语言，对于ActiveRecord 往往只能感叹其优雅，所以我们也在 AR 道路上进行了一定的探索，喜欢大家能够喜欢。 什么是ActiveRecord？ ActiveRecord也属于ORM（对象关系映射）层，由Rails最早提出，遵循标准的ORM模型：表映射到记录，记录映射到对象，字段映射到对象属性。配合遵循的命名和配置惯例，能够很大程度的快速实现模型的操作，而且简洁易懂。 ActiveRecord的主要思想是： 每一个数据库表对应创建一个类，类的每一个对象实例对应于数据库中表的一行记录；通常表的每个字段在类中都有相应的Field； ActiveRecord 同时负责把自己持久化，在ActiveRecord中封装了对数据库的访问，即CURD;； ActiveRecord 是一种领域模型(Domain Model)，封装了部分业务逻辑； 1.1 开启AR之旅在MP中，开启AR非常简单，只需要将实体对象继承Model即可。 12345678910111213141516171819/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructorpublic class User extends Model&lt;User&gt; &#123; private Long id; private String userName; @TableField(select = false) //查询时不返回该字段的值 private String password; private String name; private Integer age; @TableField(value = \"email\") //指定数据表中字段名 private String mail;&#125; 1.2 根据主键查询123456789101112131415161718192021/** * ActiveRecord测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapper2Test &#123; /** * 根据主键查询 */ @Test public void testSelectById() &#123; User user = new User(); user.setId(2L); User user1 = user.selectById(); System.out.println(user1); &#125;&#125; 1.3 新增数据1234567891011121314151617181920212223242526/** * ActiveRecord测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapper2Test &#123; /** * 新增数据 */ @Test public void testInsert() &#123; User user = new User(); user.setUserName(\"diaochan\"); user.setPassword(\"123456\"); user.setAge(20); user.setName(\"貂蝉\"); user.setMail(\"diaochan@test.cn\"); // 调用AR的insert方法进行插入数据 boolean insert = user.insert(); System.out.println(\"result =&gt; \" + insert); &#125;&#125; 1.4 更新操作12345678910111213141516171819202122/** * ActiveRecord测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapper2Test &#123; /** * 更新操作 */ @Test public void testUpdate() &#123; User user = new User(); user.setId(13L);// 查询条件 user.setAge(31); // 更新的数据 boolean result = user.updateById(); System.out.println(\"result =&gt; \" + result); &#125;&#125; 1.5 删除操作123456789101112131415161718192021/** * ActiveRecord测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapper2Test &#123; /** * 删除操作 */ @Test public void testDelete() &#123; User user = new User(); user.setId(13L); boolean delete = user.deleteById(); System.out.println(\"result =&gt; \" + delete); &#125;&#125; 1.6 根据条件查询12345678910111213141516171819202122232425/** * ActiveRecord测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapper2Test &#123; /** * 根据条件查询 */ @Test public void testSelect() &#123; User user = new User(); QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); wrapper.ge(\"age\", 30); //大于等于30岁的用户查询出来 List&lt;User&gt; users = user.selectList(wrapper); for (User user1 : users) &#123; System.out.println(user1); &#125; &#125;&#125; 2. Oracle 主键Sequence在mysql中，主键往往是自增长的，这样使用起来是比较方便的，如果使用的是Oracle数据库，那么就不能使用自增长了，就得使用Sequence 序列生成id值了。 2.1 部署Oracle环境为了简化环境部署，这里使用Docker环境进行部署安装Oracle。 1234567891011# 拉取镜像docker pull sath89/oracle-12c#创建容器docker create --name oracle -p 1521:1521 sath89/oracle-12c#启动docker start oracle &amp;&amp; docker logs -f oracle#通过用户名密码即可登录用户名和密码为： system/oracle 下面使用navicat12进行连接并操作oracle。 需要注意的是：由于安装的Oracle是64位版本，所以navicat也是需要使用64为版本，否则连接不成功。 2.2 创建表以及序列123456789101112-- 创建表，表名以及字段名都要大写CREATE TABLE \"TB_USER\" ( \"ID\" NUMBER(20) VISIBLE NOT NULL , \"USER_NAME\" VARCHAR2(255 BYTE) VISIBLE , \"PASSWORD\" VARCHAR2(255 BYTE) VISIBLE , \"NAME\" VARCHAR2(255 BYTE) VISIBLE , \"AGE\" NUMBER(10) VISIBLE , \"EMAIL\" VARCHAR2(255 BYTE) VISIBLE)--创建序列CREATE SEQUENCE SEQ_USER START WITH 1 INCREMENT BY 1 2.3 jdbc驱动包由于版权原因，我们不能直接通过maven的中央仓库下载oracle数据库的jdbc驱动包，所以我们需要将驱动包安装到本地仓库。 12#ojdbc8.jarmvn install:install-file -DgroupId=com.oracle -DartifactId=ojdbc8 -Dversion=12.1.0.1 -Dpackaging=jar -Dfile=ojdbc8.jar 安装完成后的坐标： 12345&lt;dependency&gt; &lt;groupId&gt;com.oracle&lt;/groupId&gt; &lt;artifactId&gt;ojdbc8&lt;/artifactId&gt; &lt;version&gt;12.1.0.1&lt;/version&gt;&lt;/dependency&gt; 2.4 修改application.properties对于application.properties的修改，需要修改2个位置，分别是： 1234567# 数据库连接配置spring.datasource.driver-class-name=oracle.jdbc.OracleDriverspring.datasource.url=jdbc:oracle:thin:@192.168.142.128:1521:xespring.datasource.username=systemspring.datasource.password=oracle#id生成策略mybatis-plus.global-config.db-config.id-type=input 2.5 配置序列使用Oracle的序列需要做2件事情： 1、需要配置MP的序列生成器到Spring容器： 12345678910111213141516171819/** * 配置 * * @author wgy */@Configuration@MapperScan(\"com.wgy.mapper\") //设置mapper接口的扫描包public class MybatisPlusConfig &#123; /** * Oracle的序列生成器 * * @return */ @Bean public OracleKeyGenerator oracleKeyGenerator() &#123; return new OracleKeyGenerator(); &#125;&#125; 2、在实体对象中指定序列的名称： 1234567891011121314151617181920/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructor@KeySequence(value = \"SEQ_USER\", clazz = Long.class)public class User extends Model&lt;User&gt; &#123; private Long id; private String userName; @TableField(select = false) //查询时不返回该字段的值 private String password; private String name; private Integer age; @TableField(value = \"email\") //指定数据表中字段名 private String mail;&#125; 2.6 测试123456789101112131415161718192021222324252627282930313233/** * Oracle 主键Sequence * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapper2Test &#123; @Autowired private UserMapper userMapper; @Test public void testInsert() &#123; User user = new User(); user.setAge(20); user.setEmail(\"test@itcast.cn\"); user.setName(\"曹操\"); user.setUserName(\"caocao\"); user.setPassword(\"123456\"); int result = this.userMapper.insert(user); //返回的result是受影响的行数，并不是自增后的id System.out.println(\"result = \" + result); System.out.println(user.getId()); //自增后的id会回填到对象中 &#125; @Test public void testSelectById() &#123; User user = this.userMapper.selectById(8L); System.out.println(user); &#125;&#125; 3. 插件3.1 mybatis的插件机制MyBatis 允许你在已映射语句执行过程中的某一点进行拦截调用。默认情况下，MyBatis 允许使用插件来拦截的方法调用包括： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 我们看到了可以拦截Executor接口的部分方法，比如update，query，commit，rollback等方法，还有其他接口的一些方法等。 总体概括为： 拦截执行器的方法 拦截参数的处理 拦截结果集的处理 拦截Sql语法构建的处理 拦截器示例： 12345678910111213141516171819202122232425262728/** * 拦截器 * * @author wgy */@Intercepts(&#123;@Signature( type = Executor.class, method = \"update\", args = &#123;MappedStatement.class, Object.class&#125;)&#125;)public class MyInterceptor implements Interceptor &#123; @Override public Object intercept(Invocation invocation) throws Throwable &#123; //拦截方法，具体业务逻辑编写的位置 return invocation.proceed(); &#125; @Override public Object plugin(Object target) &#123; //创建target对象的代理对象,目的是将当前拦截器加入到该对象中 return Plugin.wrap(target, this); &#125; @Override public void setProperties(Properties properties) &#123; //属性设置 &#125;&#125; 注入到Spring容器： 12345678910111213141516171819/** * 配置 * * @author wgy */@Configuration@MapperScan(\"com.wgy.mapper\") //设置mapper接口的扫描包public class MybatisPlusConfig &#123; /** * 注入自定义的拦截器（插件） * * @return */ @Bean public MyInterceptor myInterceptor() &#123; return new MyInterceptor(); &#125;&#125; 或者通过xml配置，mybatis-config.xml： 12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;plugins&gt; &lt;!-- 自定义拦截器 --&gt; &lt;plugin interceptor=\"com.wgy.plugins.MyInterceptor\"&gt;&lt;/plugin&gt; &lt;/plugins&gt;&lt;/configuration&gt; 3.2 执行分析插件在MP中提供了对SQL执行的分析的插件，可用作阻断全表更新、删除的操作，注意：该插件仅适用于开发环境，不适用于生产环境。 SpringBoot配置： 12345678910111213141516171819202122232425262728/** * 配置 * * @author wgy */@Configuration@MapperScan(\"com.wgy.mapper\") //设置mapper接口的扫描包public class MybatisPlusConfig &#123; /** * SQL分析插件 * * @return */ @Bean public SqlExplainInterceptor sqlExplainInterceptor() &#123; SqlExplainInterceptor sqlExplainInterceptor = new SqlExplainInterceptor(); List&lt;ISqlParser&gt; list = new ArrayList&lt;&gt;(); //全表更新、删除的阻断器 list.add(new BlockAttackSqlParser()); sqlExplainInterceptor.setSqlParserList(list); return sqlExplainInterceptor; &#125;&#125; 测试： 123456789101112131415161718192021/** * 插件测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapper2Test &#123; /** * 测试全表更新，SQL分析器阻断效果 */ @Test public void testUpdateAll() &#123; User user = new User(); user.setAge(31); // 更新的数据 boolean result = user.update(null); //全表更新 System.out.println(\"result =&gt; \" + result); &#125;&#125; 结果： 12345678910111213Caused by: com.baomidou.mybatisplus.core.exceptions.MybatisPlusException: Prohibition of table update operation at com.baomidou.mybatisplus.core.toolkit.ExceptionUtils.mpe(ExceptionUtils.java:49) at com.baomidou.mybatisplus.core.toolkit.Assert.isTrue(Assert.java:38) at com.baomidou.mybatisplus.core.toolkit.Assert.notNull(Assert.java:72) at com.baomidou.mybatisplus.extension.parsers.BlockAttackSqlParser.processUpdate(BlockAttackSqlParser.java:45) at com.baomidou.mybatisplus.core.parser.AbstractJsqlParser.processParser(AbstractJsqlParser.java:92) at com.baomidou.mybatisplus.core.parser.AbstractJsqlParser.parser(AbstractJsqlParser.java:67) at com.baomidou.mybatisplus.extension.handlers.AbstractSqlParserHandler.sqlParser(AbstractSqlParserHandler.java:76) at com.baomidou.mybatisplus.extension.plugins.SqlExplainInterceptor.intercept(SqlExplainInterceptor.java:63) at org.apache.ibatis.plugin.Plugin.invoke(Plugin.java:61) at com.sun.proxy.$Proxy67.update(Unknown Source) at org.apache.ibatis.session.defaults.DefaultSqlSession.update(DefaultSqlSession.java:197) ... 32 more 可以看到，当执行全表更新时，会抛出异常，这样有效防止了一些误操作。 3.3 性能分析插件性能分析拦截器，用于输出每条 SQL 语句及其执行时间，可以设置最大执行时间，超过时间会抛出异常。 该插件只用于开发环境，不建议生产环境使用。 配置： 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;plugins&gt; &lt;!-- 性能分析插件 --&gt; &lt;plugin interceptor=\"com.baomidou.mybatisplus.extension.plugins.PerformanceInterceptor\"&gt; &lt;!--最大的执行时间，单位为毫秒--&gt; &lt;property name=\"maxTime\" value=\"100\"/&gt; &lt;!--对输出的SQL做格式化，默认为false--&gt; &lt;property name=\"format\" value=\"true\"/&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/configuration&gt; 执行结果： 123456789101112Time：11 ms - ID：com.wgy.mapper.UserMapper.selectByIdExecute SQL： SELECT id, user_name, name, age, email AS mail FROM tb_user WHERE id&#x3D;2 可以看到，执行时间为11ms。如果将maxTime设置为1，那么，该操作会抛出异常。 1234Caused by: com.baomidou.mybatisplus.core.exceptions.MybatisPlusException: The SQL execution time is too large, please optimize ! at com.baomidou.mybatisplus.core.toolkit.ExceptionUtils.mpe(ExceptionUtils.java:49) at com.baomidou.mybatisplus.core.toolkit.Assert.isTrue(Assert.java:38) ................ 3.4 乐观锁插件3.4.1 主要适用场景意图： 当要更新一条记录的时候，希望这条记录没有被别人更新 乐观锁实现方式： 取出记录时，获取当前 version 更新时，带上这个 version 执行更新时， set version = newVersion where version = oldVersion 如果 version不对，就更新失败 3.4.2 插件配置spring xml: 1&lt;bean class=\"com.baomidou.mybatisplus.extension.plugins.OptimisticLockerInterceptor\"/&gt; spring boot: 123456789101112131415161718/** * 配置 * * @author wgy */@Configuration@MapperScan(\"com.wgy.mapper\") //设置mapper接口的扫描包public class MybatisPlusConfig &#123; /** * 乐观锁插件 * @return */ @Bean public OptimisticLockerInterceptor optimisticLockerInterceptor() &#123; return new OptimisticLockerInterceptor(); &#125;&#125; 或者通过xml配置，mybatis-config.xml： 12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;plugins&gt; &lt;!--乐观锁插件--&gt; &lt;plugin interceptor=\"com.baomidou.mybatisplus.extension.plugins.OptimisticLockerInterceptor\"/&gt; &lt;/plugins&gt;&lt;/configuration&gt; 3.4.3 注解实体字段需要为实体字段添加@Version注解。 1、为表添加version字段，并且设置初始值为1： 123ALTER TABLE &#96;tb_user&#96; ADD COLUMN &#96;version&#96; int(10) NULL AFTER &#96;email&#96;;UPDATE &#96;tb_user&#96; SET &#96;version&#96;&#x3D;&#39;1&#39;; 2、为User实体对象添加version字段，并且添加@Version注解： 12345678910111213141516171819202122/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructorpublic class User extends Model&lt;User&gt; &#123; private Long id; private String userName; @TableField(select = false) //查询时不返回该字段的值 private String password; private String name; private Integer age; @TableField(value = \"email\") //指定数据表中字段名 private String mail; @Version //乐观锁的版本字段 private Integer version;&#125; 3.4.4 测试1234567891011121314151617181920212223242526/** * 插件测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapper2Test &#123; /** * 测试乐观锁 */ @Test public void testUpdateVersion() &#123; User user = new User(); user.setId(2L);// 查询条件 User userVersion = user.selectById(); user.setAge(21); // 更新的数据 user.setVersion(userVersion.getVersion()); // 当前的版本信息 boolean result = user.updateById(); System.out.println(\"result =&gt; \" + result); &#125;&#125; 更新的条件中有version条件，并且更新的version为2。 如果再次执行，更新则不成功。这样就避免了多人同时更新时导致数据的不一致。 3.4.5 特别说明 支持的数据类型只有 :int,Integer,long,Long,Date,Timestamp,LocalDateTime 整数类型下 newVersion = oldVersion + 1 newVersion 会回写到 entity 中 仅支持 updateById(id) 与 update(entity, wrapper) 方法 在 update(entity, wrapper) 方法下, wrapper 不能复用!!! 4. Sql 注入器我们已经知道，在MP中，通过AbstractSqlInjector将BaseMapper中的方法注入到了Mybatis容器，这样这些方法才可以正常执行。 那么，如果我们需要扩充BaseMapper中的方法，又该如何实现呢？ 下面我们以扩展findAll方法为例进行学习。 4.1 编写MyBaseMapper12345678910111213141516/** * 扩充BaseMapper中的方法 * * @author wgy */public interface MyBaseMapper&lt;T&gt; extends BaseMapper&lt;T&gt; &#123; /** * 查询所有 * * @return */ List&lt;T&gt; findAll(); // 扩展其他的方法&#125; 其他的Mapper都可以继承该Mapper，这样实现了统一的扩展。 如： 12345678/** * 用户mapper接口 * * @author wgy */public interface UserMapper extends MyBaseMapper&lt;User&gt; &#123;&#125; 4.2 编写MySqlInjector如果直接继承AbstractSqlInjector的话，原有的BaseMapper中的方法将失效，所以我们选择继承DefaultSqlInjector进行扩展。 1234567891011121314151617181920/** * SQL 注入器 * * @author wgy */public class MySqlInjector extends DefaultSqlInjector &#123; @Override public List&lt;AbstractMethod&gt; getMethodList() &#123; List&lt;AbstractMethod&gt; list = new ArrayList&lt;&gt;(); // 获取父类中的集合 list.addAll(super.getMethodList()); // 再扩充自定义的方法 list.add(new FindAll()); return list; &#125;&#125; 4.3 编写FindAll12345678910111213141516/** * 自定义的查询所有方法 * * @author wgy */public class FindAll extends AbstractMethod &#123; @Override public MappedStatement injectMappedStatement(Class&lt;?&gt; mapperClass, Class&lt;?&gt; modelClass, TableInfo tableInfo) &#123; String sql = \"select * from \" + tableInfo.getTableName(); SqlSource sqlSource = languageDriver.createSqlSource(configuration, sql, modelClass); return this.addSelectMappedStatement(mapperClass, \"findAll\", sqlSource, modelClass, tableInfo); &#125;&#125; 4.4 注册到Spring容器12345678910111213141516171819/** * 配置 * * @author wgy */@Configuration@MapperScan(\"com.wgy.mapper\") //设置mapper接口的扫描包public class MybatisPlusConfig &#123; /** * 注入自定义的SQL注入器 * * @return */ @Bean public MySqlInjector mySqlInjector() &#123; return new MySqlInjector(); &#125;&#125; 4.5 测试1234567891011121314151617181920212223/** * Sql注入器测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 查询所有 */ @Test public void testFindAll() &#123; List&lt;User&gt; users = this.userMapper.findAll(); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 5. 自动填充功能有些时候我们可能会有这样的需求，插入或者更新数据时，希望有些字段可以自动填充数据，比如密码、version等。在MP中提供了这样的功能，可以实现自动填充。 5.1 添加@TableField注解12345678910111213141516171819202122232425262728/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructorpublic class User extends Model&lt;User&gt; &#123; private Long id; private String userName; // 插入数据时进行填充 @TableField(select = false, fill = FieldFill.INSERT) //查询时不返回该字段的值 private String password; private String name; private Integer age; @TableField(value = \"email\") //指定数据表中字段名 private String mail; @TableField(exist = false) private String address; //在数据库表中是不存在的 @Version //乐观锁的版本字段 private Integer version;&#125; 为password添加自动填充功能，在新增数据时有效。 FieldFill提供了多种模式选择： 123456789101112131415161718public enum FieldFill &#123; /** * 默认不处理 */ DEFAULT, /** * 插入时填充字段 */ INSERT, /** * 更新时填充字段 */ UPDATE, /** * 插入和更新时填充字段 */ INSERT_UPDATE&#125; 5.2 编写MyMetaObjectHandler1234567891011121314151617181920212223242526272829303132/** * 自动填充功能 * * @author wgy */@Componentpublic class MyMetaObjectHandler implements MetaObjectHandler &#123; /** * 插入数据时填充 * * @param metaObject */ @Override public void insertFill(MetaObject metaObject) &#123; // 先获取到password的值，再进行判断，如果为空，就进行填充，如果不为空，就不做处理 Object password = getFieldValByName(\"password\", metaObject); if (null == password) &#123; setFieldValByName(\"password\", \"888888\", metaObject); &#125; &#125; /** * 更新数据时填充 * * @param metaObject */ @Override public void updateFill(MetaObject metaObject) &#123; &#125;&#125; 5.3 测试123456789101112@Testpublic void testInsert() &#123; User user = new User(); user.setName(\"关羽\"); user.setUserName(\"guanyu\"); user.setAge(30); user.setMail(\"guanyu@itast.cn\"); user.setVersion(1); int result = this.userMapper.insert(user); System.out.println(\"result = \" + result);&#125; 6. 逻辑删除开发系统时，有时候在实现功能时，删除操作需要实现逻辑删除，所谓逻辑删除就是将数据标记为删除，而并非真正的物理删除（非DELETE操作），查询时需要携带状态条件，确保被标记的数据不被查询到。这样做的目的就是避免数据被真正的删除。 6.1 修改表结构为tb_user表增加deleted字段，用于表示数据是否被删除，1代表删除，0代表未删除。 1ALTER TABLE &#96;tb_user&#96; ADD COLUMN &#96;deleted&#96; int(1) NULL DEFAULT 0 COMMENT &#39;1代表删除，0代表未删除&#39; AFTER &#96;version&#96;; 同时，也修改User实体，增加deleted属性并且添加@TableLogic注解： 12345678910111213141516171819202122232425262728293031/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructorpublic class User extends Model&lt;User&gt; &#123; private Long id; private String userName; // 插入数据时进行填充 @TableField(select = false, fill = FieldFill.INSERT) //查询时不返回该字段的值 private String password; private String name; private Integer age; @TableField(value = \"email\") //指定数据表中字段名 private String mail; @TableField(exist = false) private String address; //在数据库表中是不存在的 @Version //乐观锁的版本字段 private Integer version; @TableLogic // 逻辑删除字段 ，1-删除，0-未删除 private Integer deleted;&#125; 6.2 配置application.properties： 1234# 删除状态的值为：1mybatis-plus.global-config.db-config.logic-delete-value=1# 未删除状态的值为：0mybatis-plus.global-config.db-config.logic-not-delete-value=0 6.3 测试1234567891011/** * 删除操作 */@Testpublic void testDelete() &#123; User user = new User(); user.setId(2L); boolean delete = user.deleteById(); System.out.println(\"result =&gt; \" + delete);&#125; 执行的SQL： 123[main] [com.wgy.mapper.UserMapper.deleteById]-[DEBUG] &#x3D;&#x3D;&gt; Preparing: UPDATE tb_user SET deleted&#x3D;1 WHERE id&#x3D;? AND deleted&#x3D;0[main] [com.wgy.mapper.UserMapper.deleteById]-[DEBUG] &#x3D;&#x3D;&gt; Parameters: 2(Long)[main] [com.wgy.mapper.UserMapper.deleteById]-[DEBUG] &lt;&#x3D;&#x3D; Updates: 1 测试查询： 1234567891011/** * 根据主键查询 */@Testpublic void testSelectById() &#123; User user = new User(); user.setId(2L); User user1 = user.selectById(); System.out.println(user1);&#125; 执行的SQL： 1234[main] [com.wgy.mapper.UserMapper.selectById]-[DEBUG] &#x3D;&#x3D;&gt; Preparing: SELECTid,user_name,password,name,age,email,version,deleted FROM tb_user WHERE id&#x3D;? AND deleted&#x3D;0[main] [com.wgy.mapper.UserMapper.selectById]-[DEBUG] &#x3D;&#x3D;&gt; Parameters: 2(Long)[main] [com.wgy.mapper.UserMapper.selectById]-[DEBUG] &lt;&#x3D;&#x3D; Total: 0 可见，已经实现了逻辑删除。 7. 通用枚举解决了繁琐的配置，让 mybatis 优雅的使用枚举属性！ 7.1 修改表结构1ALTER TABLE &#96;tb_user&#96; ADD COLUMN &#96;sex&#96; int(1) NULL DEFAULT 1 COMMENT &#39;1-男，2-女&#39; AFTER &#96;deleted&#96;; 7.2 定义枚举1234567891011121314151617181920212223242526272829/** * 定义枚举 * * @author wgy */public enum SexEnum implements IEnum&lt;Integer&gt; &#123; MAN(1, \"男\"), WOMAN(2, \"女\"); private int value; private String desc; SexEnum(int value, String desc) &#123; this.value = value; this.desc = desc; &#125; @Override public Integer getValue() &#123; return this.value; &#125; @Override public String toString() &#123; return this.desc; &#125;&#125; 7.3 配置12# 枚举包扫描mybatis-plus.type-enums-package=com.wgy.enums 7.4 修改实体123456789101112131415161718192021222324252627282930313233/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructorpublic class User extends Model&lt;User&gt; &#123; private Long id; private String userName; // 插入数据时进行填充 @TableField(select = false, fill = FieldFill.INSERT) //查询时不返回该字段的值 private String password; private String name; private Integer age; @TableField(value = \"email\") //指定数据表中字段名 private String mail; @TableField(exist = false) private String address; //在数据库表中是不存在的 @Version //乐观锁的版本字段 private Integer version; @TableLogic // 逻辑删除字段 ，1-删除，0-未删除 private Integer deleted; private SexEnum sex; //性别，枚举类型&#125; 7.5 测试123456789101112131415161718/** * 新增数据 */@Testpublic void testInsert() &#123; User user = new User(); user.setUserName(\"diaochan\"); user.setPassword(\"123456\"); user.setAge(20); user.setName(\"貂蝉\"); user.setMail(\"diaochan@itast.cn\"); user.setVersion(1); user.setSex(SexEnum.WOMAN); //使用的是枚举 // 调用AR的insert方法进行插入数据 boolean insert = user.insert(); System.out.println(\"result =&gt; \" + insert);&#125; 查询： 1234567891011/** * 根据主键查询 */@Testpublic void testSelectById() &#123; User user = new User(); user.setId(2L); User user1 = user.selectById(); System.out.println(user1);&#125; 结果： 12345[main] [com.wgy.mapper.UserMapper.selectById]-[DEBUG] &#x3D;&#x3D;&gt; Preparing: SELECT id,user_name,password,name,age,email,version,deleted,sex FROM tb_user WHERE id&#x3D;? ANDdeleted&#x3D;0[main] [com.wgy.mapper.UserMapper.selectById]-[DEBUG] &#x3D;&#x3D;&gt; Parameters: 2(Long)[main] [com.wgy.mapper.UserMapper.selectById]-[DEBUG] &lt;&#x3D;&#x3D; Total: 1User(id&#x3D;2, userName&#x3D;lisi, password&#x3D;123456, name&#x3D;李四, age&#x3D;30, email&#x3D;test2@itcast.cn,address&#x3D;null, version&#x3D;2, deleted&#x3D;0, sex&#x3D;女) 从测试可以看出，可以很方便的使用枚举了。 查询条件时也是有效的： 123456789101112131415/** * 使用枚举查询 */@Testpublic void testSelectBySex() &#123; User user = new User(); QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); wrapper.eq(\"sex\", SexEnum.WOMAN); //查询性别为女的数据 List&lt;User&gt; users = user.selectList(wrapper); for (User user1 : users) &#123; System.out.println(user1); &#125;&#125; SQL： 1234[main] [com.wgy.mapper.UserMapper.selectList]-[DEBUG] &#x3D;&#x3D;&gt; Preparing: SELECT id,user_name,password,name,age,email,version,deleted,sex FROM tb_user WHERE deleted&#x3D;0AND sex &#x3D; ?[main] [com.wgy.mapper.UserMapper.selectList]-[DEBUG] &#x3D;&#x3D;&gt; Parameters: 2(Integer)[main] [com.wgy.mapper.UserMapper.selectList]-[DEBUG] &lt;&#x3D;&#x3D; Total: 3 8. 代码生成器AutoGenerator 是 MyBatis-Plus 的代码生成器，通过 AutoGenerator 可以快速生成 Entity、Mapper、Mapper XML、Service、Controller 等各个模块的代码，极大的提升了开发效率。 8.1 创建工程pom.xml： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;mp-generator&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--mybatis-plus的springboot支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-generator&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-freemarker&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 8.2 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * &lt;p&gt; * mysql 代码生成器演示例子 * &lt;/p&gt; * * @author wgy */public class MysqlGenerator &#123; /** * &lt;p&gt; * 读取控制台内容 * &lt;/p&gt; */ public static String scanner(String tip) &#123; Scanner scanner = new Scanner(System.in); StringBuilder help = new StringBuilder(); help.append(\"请输入\" + tip + \"：\"); System.out.println(help.toString()); if (scanner.hasNext()) &#123; String ipt = scanner.next(); if (StringUtils.isNotEmpty(ipt)) &#123; return ipt; &#125; &#125; throw new MybatisPlusException(\"请输入正确的\" + tip + \"！\"); &#125; /** * RUN THIS */ public static void main(String[] args) &#123; // 代码生成器 AutoGenerator mpg = new AutoGenerator(); // 全局配置 GlobalConfig gc = new GlobalConfig(); String projectPath = System.getProperty(\"user.dir\"); gc.setOutputDir(projectPath + \"/src/main/java\"); gc.setAuthor(\"wgy\"); gc.setOpen(false); mpg.setGlobalConfig(gc); // 数据源配置 DataSourceConfig dsc = new DataSourceConfig(); dsc.setUrl(\"jdbc:mysql://127.0.0.1:3306/mp?useUnicode=true&amp;useSSL=false&amp;characterEncoding=utf8\"); // dsc.setSchemaName(\"public\"); dsc.setDriverName(\"com.mysql.jdbc.Driver\"); dsc.setUsername(\"root\"); dsc.setPassword(\"root\"); mpg.setDataSource(dsc); // 包配置 PackageConfig pc = new PackageConfig(); pc.setModuleName(scanner(\"模块名\")); pc.setParent(\"com.wgy.generator\"); mpg.setPackageInfo(pc); // 自定义配置 InjectionConfig cfg = new InjectionConfig() &#123; @Override public void initMap() &#123; // to do nothing &#125; &#125;; List&lt;FileOutConfig&gt; focList = new ArrayList&lt;&gt;(); focList.add(new FileOutConfig(\"/templates/mapper.xml.ftl\") &#123; @Override public String outputFile(TableInfo tableInfo) &#123; // 自定义输入文件名称 return projectPath + \"/mp-generator/src/main/resources/mapper/\" + pc.getModuleName() + \"/\" + tableInfo.getEntityName() + \"Mapper\" + StringPool.DOT_XML; &#125; &#125;); cfg.setFileOutConfigList(focList); mpg.setCfg(cfg); mpg.setTemplate(new TemplateConfig().setXml(null)); // 策略配置 StrategyConfig strategy = new StrategyConfig(); strategy.setNaming(NamingStrategy.underline_to_camel); strategy.setColumnNaming(NamingStrategy.underline_to_camel);// strategy.setSuperEntityClass(\"com.baomidou.mybatisplus.samples.generator.common.BaseEntity\"); strategy.setEntityLombokModel(true);// strategy.setSuperControllerClass(\"com.baomidou.mybatisplus.samples.generator.common.BaseController\"); strategy.setInclude(scanner(\"表名\")); strategy.setSuperEntityColumns(\"id\"); strategy.setControllerMappingHyphenStyle(true); strategy.setTablePrefix(pc.getModuleName() + \"_\"); mpg.setStrategy(strategy); // 选择 freemarker 引擎需要指定如下加，注意 pom 依赖必须有！ mpg.setTemplateEngine(new FreemarkerTemplateEngine()); mpg.execute(); &#125;&#125; 8.3 测试 代码已生成： 9. MybatisX 快速开发插件MybatisX 是一款基于 IDEA 的快速开发插件，为效率而生。 安装方法：打开 IDEA，进入 File -&gt; Settings -&gt; Plugins -&gt; Browse Repositories，输入 mybatisx 搜索并安装。 功能： Java 与 XML 来回跳转 Mapper 方法自动生成 XML","tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://wgy1993.gitee.io/tags/MyBatis/"},{"name":"Mybatis-Plus","slug":"Mybatis-Plus","permalink":"https://wgy1993.gitee.io/tags/Mybatis-Plus/"}]},{"title":"Mybatis-Plus(一)","date":"2020-09-29T10:24:33.000Z","path":"archives/b19585ec.html","text":"1. 了解Mybatis-Plus1.1 Mybatis-Plus介绍MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 官网： https://mybatis.plus/ 或 https://mp.baomidou.com/ 愿景我们的愿景是成为 MyBatis 最好的搭档，就像 魂斗罗 中的 1P、2P，基友搭配，效率翻倍。 1.2 代码以及文档文档地址：https://mybatis.plus/guide/ 源码地址：https://github.com/baomidou/mybatis-plus 1.3 特性 无侵入 ：只做增强不做改变，引入它不会对现有工程产生影响，如丝般顺滑 损耗小 ：启动即会自动注入基本 CURD，性能基本无损耗，直接面向对象操作 强大的 CRUD 操作：内置通用 Mapper、通用 Service，仅仅通过少量配置即可实现单表大部分 CRUD 操作，更有强大的条件构造器，满足各类使用需求 支持 Lambda 形式调用：通过 Lambda 表达式，方便的编写各类查询条件，无需再担心字段写错 支持多种数据库 ：支持 MySQL、MariaDB、Oracle、DB2、H2、HSQL、SQLite、Postgre、SQLServer2005、SQLServer 等多种数据库 支持主键自动生成 ：支持多达 4 种主键策略（内含分布式唯一 ID 生成器 - Sequence），可自由配置，完美解决主键问题 支持 XML 热加载：Mapper 对应的 XML 支持热加载，对于简单的 CRUD 操作，甚至可以无 XML 启动 支持 ActiveRecord 模式：支持 ActiveRecord 形式调用，实体类只需继承 Model 类即可进行强大的 CRUD 操作 支持自定义全局通用操作 ：支持全局通用方法注入（ Write once, use anywhere ） 支持关键词自动转义 ：支持数据库关键词（order、key……）自动转义，还可自定义关键词 内置代码生成器 ：采用代码或者 Maven 插件可快速生成 Mapper 、 Model 、 Service 、 Controller 层代码，支持模板引擎，更有超多自定义配置等您来使用 内置分页插件 ：基于 MyBatis 物理分页，开发者无需关心具体操作，配置好插件之后，写分页等同于普通 List查询 内置性能分析插件 ：可输出 Sql 语句以及其执行时间，建议开发测试时启用该功能，能快速揪出慢查询 内置全局拦截插件 ：提供全表 delete 、 update 操作智能分析阻断，也可自定义拦截规则，预防误操作 内置 Sql 注入剥离器：支持 Sql 注入剥离，有效预防 Sql 注入攻击 1.4 架构 1.5 作者Mybatis-Plus是由baomidou（苞米豆）组织开发并且开源的，目前该组织大概有30人左右。 码云地址：https://gitee.com/organizations/baomidou 2. 快速开始对于 Mybatis整合MP有常常有三种用法，分别是Mybatis+MP、Spring+Mybatis+MP、Spring Boot+Mybatis+MP。 2.1 创建数据库以及表 12345678910111213141516171819202122-- 创建测试表CREATE TABLE &#96;tb_user&#96; ( &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;主键ID&#39;, &#96;user_name&#96; varchar(20) NOT NULL COMMENT &#39;用户名&#39;, &#96;password&#96; varchar(20) NOT NULL COMMENT &#39;密码&#39;, &#96;name&#96; varchar(30) DEFAULT NULL COMMENT &#39;姓名&#39;, &#96;age&#96; int(11) DEFAULT NULL COMMENT &#39;年龄&#39;, &#96;email&#96; varchar(50) DEFAULT NULL COMMENT &#39;邮箱&#39;, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8;-- 插入测试数据INSERT INTO &#96;tb_user&#96; (&#96;id&#96;, &#96;user_name&#96;, &#96;password&#96;, &#96;name&#96;, &#96;age&#96;, &#96;email&#96;) VALUES(&#39;1&#39;, &#39;zhangsan&#39;, &#39;123456&#39;, &#39;张三&#39;, &#39;18&#39;, &#39;test1@test.cn&#39;);INSERT INTO &#96;tb_user&#96; (&#96;id&#96;, &#96;user_name&#96;, &#96;password&#96;, &#96;name&#96;, &#96;age&#96;, &#96;email&#96;) VALUES(&#39;2&#39;, &#39;lisi&#39;, &#39;123456&#39;, &#39;李四&#39;, &#39;20&#39;, &#39;test2@test.cn&#39;);INSERT INTO &#96;tb_user&#96; (&#96;id&#96;, &#96;user_name&#96;, &#96;password&#96;, &#96;name&#96;, &#96;age&#96;, &#96;email&#96;) VALUES(&#39;3&#39;, &#39;wangwu&#39;, &#39;123456&#39;, &#39;王五&#39;, &#39;28&#39;, &#39;test3@test.cn&#39;);INSERT INTO &#96;tb_user&#96; (&#96;id&#96;, &#96;user_name&#96;, &#96;password&#96;, &#96;name&#96;, &#96;age&#96;, &#96;email&#96;) VALUES(&#39;4&#39;, &#39;zhaoliu&#39;, &#39;123456&#39;, &#39;赵六&#39;, &#39;21&#39;, &#39;test4@test.cn&#39;);INSERT INTO &#96;tb_user&#96; (&#96;id&#96;, &#96;user_name&#96;, &#96;password&#96;, &#96;name&#96;, &#96;age&#96;, &#96;email&#96;) VALUES(&#39;5&#39;, &#39;sunqi&#39;, &#39;123456&#39;, &#39;孙七&#39;, &#39;24&#39;, &#39;test5@test.cn&#39;); 2.2 创建工程导入依赖12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;mybatis-plus-simple&lt;/module&gt; &lt;module&gt;mybatis-plus-spring&lt;/module&gt; &lt;/modules&gt; &lt;dependencies&gt; &lt;!-- mybatis-plus插件依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 连接池 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.11&lt;/version&gt; &lt;/dependency&gt; &lt;!--简化bean代码的工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;version&gt;1.18.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.6.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2.3 Mybatis + MP下面演示，通过纯Mybatis与Mybatis-Plus整合。 2.3.1 创建子Module12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;mybatis-plus&lt;/artifactId&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;mybatis-plus-simple&lt;/artifactId&gt;&lt;/project&gt; log4j.properties： 1234log4j.rootLogger =DEBUG,A1log4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=[%t] [%c]-[%p] %m%n 2.3.2 Mybatis实现查询User2.3.2.1 编写mybatis-config.xml文件123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql://127.0.0.1:3306/mp?useUnicode=true&amp;amp;characterEncoding=utf8&amp;amp;autoReconnect=true&amp;amp;allowMultiQueries=true&amp;amp;useSSL=false\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=\"UserMapper.xml\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 2.3.2.2 编写User对象使用lombok进行了进化bean操作。注意：注解不生效需要Idea安装lombok插件 1234567891011121314151617/** * 用户实体类 * * @author wgy */@Data//注解包含包含getter、setter、NoArgsConstructor注解@NoArgsConstructor//注解会生成对应的无参构造方法@AllArgsConstructor//注解会生成对应的有参构造方法public class User &#123; private Long id; private String userName; private String password; private String name; private Integer age; private String email;&#125; 2.3.2.3 编写UserMapper接口1234567891011121314/** * 用户mapper接口 * * @author wgy */public interface UserMapper &#123; /** * 查询所有用户信息 * * @return list */ List&lt;User&gt; findAll();&#125; 2.3.2.4 编写UserMapper.xml文件1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wgy.mapper.UserMapper\"&gt; &lt;select id=\"findAll\" resultType=\"com.wgy.pojo.User\"&gt; select * from tb_user &lt;/select&gt;&lt;/mapper&gt; 2.3.2.5 测试123456789101112131415161718192021222324/** * Mybatis测试 * * @author wgy */public class MybatisTest &#123; @Test public void testFindAll() throws Exception &#123; String config = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(config); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); SqlSession sqlSession = sqlSessionFactory.openSession(); UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //测试查询 List&lt;User&gt; users = userMapper.findAll(); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 2.3.3 Mybatis+MP实现查询User2.3.3.1 Mapper继承BaseMapper将UserMapper继承BaseMapper，将拥有了BaseMapper中的所有方法 1234567891011121314/** * 用户mapper接口 * * @author wgy */public interface UserMapper extends BaseMapper&lt;User&gt; &#123; /** * 查询所有用户信息 * * @return list */ List&lt;User&gt; findAll();&#125; 2.3.3.2 MybatisSqlSessionFactoryBuilder进程构建123456789101112131415161718192021222324252627/** * MybatisPlus测试 * * @author wgy */public class MybatisPlusTest &#123; @Test public void testFindAll() throws Exception &#123; String config = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(config); //这里使用的是MP中的MybatisSqlSessionFactoryBuilder SqlSessionFactory sqlSessionFactory = new MybatisSqlSessionFactoryBuilder().build(inputStream); SqlSession sqlSession = sqlSessionFactory.openSession(); UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //测试查询 //List&lt;User&gt; users = userMapper.findAll(); //可以调用BaseMapper中定义的方法 List&lt;User&gt; users = userMapper.selectList(null); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 运行报错： 解决：在 User对象中添加@TableName，指定数据库表名 简单说明： 由于使用了 MybatisSqlSessionFactoryBuilder进行了构建，继承的BaseMapper中的方法就载入到了SqlSession中，所以就可以直接使用相关的方法； 2.4 Spring + Mybatis + MP引入了Spring框架，数据源、构建等工作就交给了Spring管理。 2.4.1 创建子Module1234567891011121314151617181920212223242526272829303132333435&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;mybatis-plus&lt;/artifactId&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;mybatis-plus-spring&lt;/artifactId&gt; &lt;properties&gt; &lt;spring.version&gt;5.1.6.RELEASE&lt;/spring.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.4.2 实现查询User2.4.2.1 编写jdbc.properties1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/mp?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&amp;useSSL=falsejdbc.username=rootjdbc.password=root 2.4.2.2 编写applicationContext.xml12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;context:property-placeholder location=\"classpath:*.properties\"/&gt; &lt;!-- 定义数据源 --&gt; &lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" destroy-method=\"close\"&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"/&gt; &lt;property name=\"driverClassName\" value=\"$&#123;jdbc.driver&#125;\"/&gt; &lt;property name=\"maxActive\" value=\"10\"/&gt; &lt;property name=\"minIdle\" value=\"5\"/&gt; &lt;/bean&gt; &lt;!--这里使用MP提供的sqlSessionFactory，完成了Spring与MP的整合--&gt; &lt;bean id=\"sqlSessionFactory\" class=\"com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;/bean&gt; &lt;!--扫描mapper接口，使用的依然是Mybatis原生的扫描器--&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.wgy.mapper\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 2.4.2.3 编写User对象123456789101112131415161718/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructor@TableName(\"tb_user\")public class User &#123; private Long id; private String userName; private String password; private String name; private Integer age; private String email;&#125; 2.4.2.4 编写UserMapper接口1234567/** * 用户mapper接口 * * @author wgy */public interface UserMapper extends BaseMapper&lt;User&gt; &#123;&#125; 2.4.2.5 测试1234567891011121314151617181920/** * Spring + Mybatis + MP测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = \"classpath:applicationContext.xml\")public class MybatisSpringTest &#123; @Autowired private UserMapper userMapper; @Test public void testSelectList() &#123; List&lt;User&gt; users = this.userMapper.selectList(null); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 注意： 测试报错需将jdbc.properties、applicationContext.xml复制到test/resources下 2.5 SpringBoot + Mybatis + MP使用SpringBoot将进一步的简化MP的整合，需要注意的是，由于使用SpringBoot需要继承parent，所以需要重新创建工程，并不是创建子Module。 2.5.1 创建工程导入依赖12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;mp-springboot&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--简化代码的工具包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;!--mybatis-plus的springboot支持--&gt; &lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; log4j.properties： 1234log4j.rootLogger=DEBUG,A1log4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=[%t] [%c]-[%p] %m%n 2.5.2 编写application.properties12345spring.application.name=mp-springbootspring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://127.0.0.1:3306/mp?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true&amp;allowMultiQueries=true&amp;useSSL=falsespring.datasource.username=rootspring.datasource.password=root 2.5.3 编写User对象123456789101112131415161718/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructor@TableName(\"tb_user\")public class User &#123; private Long id; private String userName; private String password; private String name; private Integer age; private String email;&#125; 2.5.4 编写UserMapper接口1234567/** * 用户mapper接口 * * @author wgy */public interface UserMapper extends BaseMapper&lt;User&gt; &#123;&#125; 2.5.5 编写启动类12345678910111213/** * SpringBoot启动类 * * @author wgy */@SpringBootApplication@MapperScan(\"com.wgy.mapper\") //设置mapper接口的扫描包public class MyApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(MyApplication.class, args); &#125;&#125; 2.5.6 测试1234567891011121314151617181920/** * SpringBoot + Mybatis + MP测试 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class MybatisSpringBootTest &#123; @Autowired private UserMapper userMapper; @Test public void testSelectList() &#123; List&lt;User&gt; users = this.userMapper.selectList(null); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 3. 通用CRUD通过前面的学习，我们了解到通过继承BaseMapper就可以获取到各种各样的单表操作，接下来我们将详细讲解这些操作。 3.1 插入操作12345678910111213141516171819202122232425262728293031/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 插入一条记录 */ @Test public void testInsert() &#123; User user = new User(); user.setAge(20); user.setMail(\"1@test.cn\"); user.setName(\"曹操\"); user.setUserName(\"caocao\"); user.setPassword(\"123456\"); int result = this.userMapper.insert(user); //result数据库受影响的行数 System.out.println(\"result =&gt; \" + result); //获取自增长后的id值, 自增长后的id值会回填到user对象中 System.out.println(\"id =&gt; \" + user.getId()); &#125;&#125; 数据已经写入到了数据库，但是， id的值不正确，我们期望的是数据库自增长，实际是MP生成了id的值写入到了数据库。 如何设置id的生成策略呢？ 修改User对象： 12345678910111213141516171819/** * 用户实体类 * * @author wgy */@Data@NoArgsConstructor@AllArgsConstructor@TableName(\"tb_user\")public class User &#123; @TableId(type = IdType.AUTO) //指定id类型为自增长 private Long id; private String userName; private String password; private String name; private Integer age; private String email;&#125; 3.1.1 @TableField在MP中通过@TableField注解可以指定字段的一些属性，常常解决的问题有2个： 对象中的属性名和字段名不一致的问题（非驼峰） 对象中的属性字段在表中不存在的问题 其他用法，如大字段不加入查询字段： 效果： 3.2 更新操作在MP中，更新操作有2种，一种是根据id更新，另一种是根据条件更新。 3.2.1 根据id更新1234567891011121314151617181920212223242526/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 ID 修改 */ @Test public void testUpdateById() &#123; User user = new User(); user.setId(1L); //条件，根据id更新 user.setAge(19); //更新的字段 user.setPassword(\"666666\"); int result = this.userMapper.updateById(user); System.out.println(\"result =&gt; \" + result); &#125;&#125; 3.2.2 根据条件更新1234567891011121314151617181920212223242526272829/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 whereEntity 条件，更新记录 */ @Test public void testUpdate() &#123; User user = new User(); user.setAge(20); //更新的字段 user.setPassword(\"8888888\"); QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); wrapper.eq(\"user_name\", \"zhangsan\"); //匹配user_name = zhangsan 的用户数据 //根据条件做更新 int result = this.userMapper.update(user, wrapper); System.out.println(\"result =&gt; \" + result); &#125;&#125; 或者，通过UpdateWrapper进行更新： 123456789101112131415161718192021222324252627/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 whereEntity 条件，更新记录 */ @Test public void testUpdate2() &#123; UpdateWrapper&lt;User&gt; wrapper = new UpdateWrapper&lt;&gt;(); wrapper.set(\"age\", 21).set(\"password\", \"999999\") //更新的字段 .eq(\"user_name\", \"zhangsan\"); //更新的条件 //根据条件做更新 int result = this.userMapper.update(null, wrapper); System.out.println(\"result =&gt; \" + result); &#125;&#125; 3.3 删除操作3.3.1 deleteById12345678910111213141516171819202122/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 ID 删除 */ @Test public void testDeleteById() &#123; // 根据id删除数据 int result = this.userMapper.deleteById(9L); System.out.println(\"result =&gt; \" + result); &#125;&#125; 3.3.2 deleteByMap123456789101112131415161718192021222324252627/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 columnMap 条件，删除记录 */ @Test public void testDeleteByMap() &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"user_name\", \"zhangsan\"); map.put(\"password\", \"999999\"); // 根据map删除数据，多条件之间是and关系 int result = this.userMapper.deleteByMap(map); System.out.println(\"result =&gt; \" + result); &#125;&#125; 3.3.3 delete1234567891011121314151617181920212223242526272829303132333435/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 entity 条件，删除记录 */ @Test public void testDelete() &#123; //用法一：// QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;();// wrapper.eq(\"user_name\", \"caocao1\")// .eq(\"password\", \"123456\"); //用法二： User user = new User(); user.setPassword(\"123456\"); user.setUserName(\"caocao\"); QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(user); // 根据包装条件做删除 int result = this.userMapper.delete(wrapper); System.out.println(\"result =&gt; \" + result); &#125;&#125; 3.3.4 deleteBatchIds12345678910111213141516171819202122/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据ID 批量删除 */ @Test public void testDeleteBatchIds() &#123; // 根据id批量删除数据 int result = this.userMapper.deleteBatchIds(Arrays.asList(10L, 11L)); System.out.println(\"result =&gt; \" + result); &#125;&#125; 3.4 查询操作MP提供了多种查询操作，包括根据id查询、批量查询、查询单条数据、查询列表、分页查询等操作 3.4.1 selectById123456789101112131415161718192021/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 ID 查询 */ @Test public void testSelectById() &#123; User user = this.userMapper.selectById(2L); System.out.println(user); &#125;&#125; 3.4.2 selectBatchIds123456789101112131415161718192021222324/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据ID 批量查询 */ @Test public void testSelectBatchIds() &#123; // 根据id批量查询数据 List&lt;User&gt; users = this.userMapper.selectBatchIds(Arrays.asList(2L, 3L, 4L, 100L)); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 3.4.3 selectOne12345678910111213141516171819202122232425/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 entity 条件，查询一条记录 */ @Test public void testSelectOne() &#123; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); //查询条件 wrapper.eq(\"password\", \"123456\"); // 查询的数据超过一条时，会抛出异常 User user = this.userMapper.selectOne(wrapper); System.out.println(user); &#125;&#125; 3.4.4 selectCount1234567891011121314151617181920212223242526/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 Wrapper 条件，查询总记录数 */ @Test public void testSelectCount() &#123; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); wrapper.gt(\"age\", 20); // 条件：年龄大于20岁的用户 // 根据条件查询数据条数 Integer count = this.userMapper.selectCount(wrapper); System.out.println(\"count =&gt; \" + count); &#125;&#125; 3.4.5 selectList123456789101112131415161718192021222324252627/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 根据 entity 条件，查询全部记录 */ @Test public void testSelectList() &#123; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); //设置查询条件 wrapper.like(\"email\", \"itcast\"); List&lt;User&gt; users = this.userMapper.selectList(wrapper); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 3.4.6 selectPage配置分页插件： 1234567891011121314/** * 配置分页插件 * * @author wgy */@Configuration@MapperScan(\"com.wgy.mapper\") //设置mapper接口的扫描包public class MybatisPlusConfig &#123; @Bean public PaginationInterceptor paginationInterceptor() &#123; return new PaginationInterceptor(); &#125;&#125; 或者config配置分页插件 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;plugins&gt; &lt;plugin interceptor=\"com.baomidou.mybatisplus.extension.plugins.PaginationInterceptor\"&gt;&lt;/plugin&gt; &lt;/plugins&gt;&lt;/configuration&gt; 测试： 1234567891011121314151617181920212223242526272829303132333435/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 测试分页查询 */ @Test public void testSelectPage() &#123; Page&lt;User&gt; page = new Page&lt;&gt;(1, 1); //查询第一页，查询1条数据 QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); //设置查询条件 wrapper.like(\"email\", \"test\"); IPage&lt;User&gt; iPage = this.userMapper.selectPage(page, wrapper); System.out.println(\"数据总条数： \" + iPage.getTotal()); System.out.println(\"数据总页数： \" + iPage.getPages()); System.out.println(\"当前页数： \" + iPage.getCurrent()); List&lt;User&gt; records = iPage.getRecords(); for (User record : records) &#123; System.out.println(record); &#125; &#125;&#125; 3.5 SQL注入的原理前面我们已经知道，MP在启动后会将BaseMapper中的一系列的方法注册到mappedStatements中，那么究竟是如何注入的呢？流程又是怎么样的？下面我们将一起来分析下。 在MP中，ISqlInjector负责SQL的注入工作，它是一个接口，AbstractSqlInjector是它的实现类，实现关系如下： 在 AbstractSqlInjector中，主要是由inspectInject()方法进行注入的，如下： 12345678910111213141516171819@Overridepublic void inspectInject(MapperBuilderAssistant builderAssistant, Class&lt;?&gt; mapperClass) &#123; Class&lt;?&gt; modelClass = extractModelClass(mapperClass); if (modelClass != null) &#123; String className = mapperClass.toString(); Set&lt;String&gt; mapperRegistryCache = GlobalConfigUtils.getMapperRegistryCache(builderAssistant.getConfiguration()); if (!mapperRegistryCache.contains(className)) &#123; List&lt;AbstractMethod&gt; methodList = this.getMethodList(); if (CollectionUtils.isNotEmpty(methodList)) &#123; TableInfo tableInfo = TableInfoHelper.initTableInfo(builderAssistant, modelClass); // 循环注入自定义方法 methodList.forEach(m -&gt; m.inject(builderAssistant, mapperClass, modelClass, tableInfo)); &#125; else &#123; logger.debug(mapperClass.toString() + \", No effective injection method was found.\"); &#125; mapperRegistryCache.add(className); &#125; &#125;&#125; 在实现方法中， methodList.forEach(m -&gt; m.inject(builderAssistant, mapperClass, modelClass,tableInfo)); 是关键，循环遍历方法，进行注入。 最终调用抽象方法injectMappedStatement进行真正的注入： 123456789/** * 注入自定义 MappedStatement * * @param mapperClass mapper 接口 * @param modelClass mapper 泛型 * @param tableInfo 数据库表反射信息 * @return MappedStatement */public abstract MappedStatement injectMappedStatement(Class&lt;?&gt; mapperClass, Class&lt;?&gt; modelClass, TableInfo tableInfo); 查看该方法的实现： 以 SelectById为例查看： 123456789101112131415161718/** * 根据ID 查询一条数据 * * @author hubin * @since 2018-04-06 */public class SelectById extends AbstractMethod &#123; @Override public MappedStatement injectMappedStatement(Class&lt;?&gt; mapperClass, Class&lt;?&gt; modelClass, TableInfo tableInfo) &#123; SqlMethod sqlMethod = SqlMethod.LOGIC_SELECT_BY_ID; SqlSource sqlSource = new RawSqlSource(configuration, String.format(sqlMethod.getSql(), sqlSelectColumns(tableInfo, false), tableInfo.getTableName(), tableInfo.getKeyColumn(), tableInfo.getKeyProperty(), tableInfo.getLogicDeleteSql(true, false)), Object.class); return this.addSelectMappedStatement(mapperClass, sqlMethod.getMethod(), sqlSource, modelClass, tableInfo); &#125;&#125; 可以看到，生成了SqlSource对象，再将SQL通过addSelectMappedStatement方法添加到meppedStatements中 4. 配置在MP中有大量的配置，其中有一部分是Mybatis原生的配置，另一部分是MP的配置，详情：https://mybatis.plus/config/ 下面我们对常用的配置做讲解。 4.1 基本配置4.1.1 configLocationMyBatis 配置文件位置，如果您有单独的 MyBatis 配置，请将其路径配置到 configLocation 中。 MyBatis Configuration 的具体内容请参考MyBatis 官方文档 Spring Boot： 12#指定全局的配置文件mybatis-plus.config-location=classpath:mybatis-config.xml Spring MVC： 12345&lt;!--这里使用MP提供的sqlSessionFactory，完成了Spring与MP的整合--&gt;&lt;bean id=\"sqlSessionFactory\" class=\"com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;property name=\"configLocation\" value=\"classpath:mybatis-config.xml\"/&gt;&lt;/bean&gt; 4.1.2 mapperLocationsMyBatis Mapper 所对应的 XML 文件位置，如果您在 Mapper 中有自定义方法（XML 中有自定义实现），需要进行该配置，告诉 Mapper 所对应的 XML 文件位置。 Spring Boot： 12# 指定Mapper.xml文件的路径mybatis-plus.mapper-locations=classpath*:mybatis/*.xml Spring MVC： 12345&lt;!--这里使用MP提供的sqlSessionFactory，完成了Spring与MP的整合--&gt;&lt;bean id=\"sqlSessionFactory\" class=\"com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;property name=\"mapperLocations\" value=\"classpath*:mybatis/*.xml\"/&gt;&lt;/bean&gt; Maven 多模块项目的扫描路径需以 classpath*: 开头 （即加载多个 jar 包下的 XML 文件） 测试： UserMapper.xml： 123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wgy.mapper.UserMapper\"&gt; &lt;select id=\"findById\" resultType=\"com.wgy.pojo.User\"&gt; select * from tb_user where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 123456789101112131415161718192021/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 自定义的方法 */ @Test public void testFindById() &#123; User user = this.userMapper.findById(2L); System.out.println(user); &#125;&#125; 4.1.3 typeAliasesPackageMyBaits 别名包扫描路径，通过该属性可以给包中的类注册别名，注册后在 Mapper 对应的 XML 文件中可以直接使用类名，而不用使用全限定的类名（即 XML 中调用的时候不用包含包名）。 Spring Boot： 12# 实体对象的扫描包mybatis-plus.type-aliases-package=com.wgy.pojo Spring MVC： 12345&lt;!--这里使用MP提供的sqlSessionFactory，完成了Spring与MP的整合--&gt;&lt;bean id=\"sqlSessionFactory\" class=\"com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;property name=\"typeAliasesPackage\" value=\"com.baomidou.mybatisplus.samples.quickstart.entity\"/&gt;&lt;/bean&gt; 4.2 进阶配置本部分（Configuration）的配置大都为 MyBatis 原生支持的配置，这意味着您可以通过 MyBatis XML 配置文件的形式进行配置。 4.2.1 mapUnderscoreToCamelCase类型： boolean 默认值： true 是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN（下划线命名） 到经典 Java 属性名 aColumn（驼峰命名） 的类似映射。 注意： 此属性在 MyBatis 中原默认值为 false，在 MyBatis-Plus 中，此属性也将用于生成最终的 SQL 的 select body 如果您的数据库命名符合规则无需使用 @TableField 注解指定数据库字段名 12# 关闭自动驼峰映射，该参数不能和mybatis-plus.config-location同时存在mybatis-plus.configuration.map-underscore-to-camel-case=false 4.2.2 cacheEnabled类型： boolean 默认值： true 全局地开启或关闭配置文件中的所有映射器已经配置的任何缓存，默认为 true。 12# 禁用缓存mybatis-plus.configuration.cache-enabled=false 4.3 DB 策略配置4.3.1 idType类型： com.baomidou.mybatisplus.annotation.IdType 默认值： ID_WORKER 全局默认主键类型，设置后，即可省略实体对象中的@TableId(type = IdType.AUTO)配置。 SpringBoot： 12# 全局的id生成策略mybatis-plus.global-config.db-config.id-type=auto SpringMVC： 1234567891011121314&lt;!--这里使用MP提供的sqlSessionFactory，完成了Spring与MP的整合--&gt;&lt;bean id=\"sqlSessionFactory\" class=\"com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;property name=\"configLocation\" value=\"classpath:mybatis-config.xml\"/&gt; &lt;property name=\"globalConfig\"&gt; &lt;bean class=\"com.baomidou.mybatisplus.core.config.GlobalConfig\"&gt; &lt;property name=\"dbConfig\"&gt; &lt;bean class=\"com.baomidou.mybatisplus.core.config.GlobalConfig$DbConfig\"&gt; &lt;property name=\"idType\" value=\"AUTO\"/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 4.3.2 tablePrefix类型： String 默认值： null 表名前缀，全局配置后可省略 @TableName()配置。 SpringBoot： 12# 全局的表名的前缀mybatis-plus.global-config.db-config.table-prefix=tb_ SpringMVC： 123456789101112131415&lt;!--这里使用MP提供的sqlSessionFactory，完成了Spring与MP的整合--&gt;&lt;bean id=\"sqlSessionFactory\" class=\"com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;property name=\"configLocation\" value=\"classpath:mybatis-config.xml\"/&gt; &lt;property name=\"globalConfig\"&gt; &lt;bean class=\"com.baomidou.mybatisplus.core.config.GlobalConfig\"&gt; &lt;property name=\"dbConfig\"&gt; &lt;bean class=\"com.baomidou.mybatisplus.core.config.GlobalConfig$DbConfig\"&gt; &lt;property name=\"idType\" value=\"AUTO\"/&gt; &lt;property name=\"tablePrefix\" value=\"tb_\"/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 5. 条件构造器在MP中，Wrapper接口的实现类关系如下： 可以看到， AbstractWrapper和AbstractChainWrapper是重点实现，接下来我们重点学习AbstractWrapper以及其子类。 说明: QueryWrapper(LambdaQueryWrapper) 和 UpdateWrapper(LambdaUpdateWrapper) 的父类 用于生成 sql的 where 条件, entity 属性也用于生成 sql 的 where 条件 注意: entity 生成的 where 条件与 使用各个 api 生成的 where 条件没有任何关联行为 官网文档地址： https://mybatis.plus/guide/wrapper.html 5.1 allEq123allEq (Map&lt;R, V&gt; params)allEq(Map&lt;R, V&gt; params, boolean null2IsNull)allEq(boolean condition, Map&lt;R, V&gt; params, boolean null2IsNull) 全部eq(或个别isNull ) 个别参数说明: params : key 为数据库字段名, value 为字段值 null2IsNull : 为 true 则在 map 的 value 为null 时调用 isNull 方法,为 false 时则忽略 value 为 null 的 例 1: allEq({id:1,name:&quot; 老王&quot;,age:null}) ---&gt; id = 1 and name = &#39; 老王&#39; and age is null例 2: allEq({id:1,name:&quot; 老王&quot;,age:null}, false) ---&gt; id = 1 and name = &#39; 老王&#39; 123allEq (BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params)allEq(BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params, boolean null2IsNull)allEq(boolean condition, BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params, boolean null2IsNull) 个别参数说明: filter : 过滤函数,是否允许字段传入比对条件中 params 与 null2IsNull : 同上 例 1: allEq((k,v) -&gt; k.indexOf(“a”) &gt; 0, {id:1,name:”老王”,age:null}) —&gt; name = ‘老王’ and age is null例 2: allEq((k,v) -&gt; k.indexOf(“a”) &gt; 0, {id:1,name:”老王”,age:null}, false) —&gt; name =’老王’ 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * allEq条件 */ @Test public void testAllEq() &#123; Map&lt;String, Object&gt; params = new HashMap&lt;&gt;(); params.put(\"name\", \"李四\"); params.put(\"age\", \"20\"); params.put(\"password\", null); QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;();// null2IsNull : 为 true 则在 map 的 value 为null 时调用 isNull// SELECT id,user_name,name,age,email AS mail FROM tb_user WHERE password IS NULL AND name = ? AND age = ?// wrapper.allEq(params);// 为 false 时则忽略 value 为 null 的// SELECT id, user_name, name, age, email AS mail FROM tb_user WHERE name = ?AND age = ?// wrapper.allEq(params, false);// SELECT id, user_name, name, age, email AS mail FROM tb_user WHERE age = ?// wrapper.allEq((k, v) -&gt; (k.equals(\"age\") || k.equals(\"id\")) , params);// SELECT id, user_name, name, age, email AS mail FROM tb_user WHERE name = ?AND age = ? wrapper.allEq((k, v) -&gt; (k.equals(\"age\") || k.equals(\"id\") || k.equals(\"name\")), params); List&lt;User&gt; users = this.userMapper.selectList(wrapper); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 5.2 基本比较操作 操作 说明 eq 等于 = ne 不等于 &lt;&gt; gt 大于 &gt; ge 大于等于 &gt;= lt 小于 &lt; le 小于等于 &lt;= between BETWEEN 值1 AND 值2 notBetween NOT BETWEEN 值1 AND 值2 in 字段 IN (value.get(0), value.get(1), …) notIn 字段 NOT IN (v0, v1, …) 123456789101112131415161718192021222324252627282930/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 比较操作 */ @Test public void testEq() &#123; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); //SELECT id,user_name,password,name,age,email FROM tb_user WHERE password = ? AND age &gt;= ? AND name IN (?,?,?) wrapper.eq(\"password\", \"123456\") .ge(\"age\", 20) .in(\"name\", \"李四\", \"王五\", \"赵六\"); List&lt;User&gt; users = this.userMapper.selectList(wrapper); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 5.3 模糊查询 操作 说明 like LIKE ‘% 值%’例 : like(“name”, “ 王”) —&gt; name like ‘% 王%’ notLike NOT LIKE ‘% 值%’例 : notLike(“name”, “ 王”) —&gt; name not like ‘% 王%’ likeLeft LIKE ‘% 值’例 : likeLeft(“name”, “ 王”) —&gt; name like ‘% 王’ likeRight LIKE ‘ 值%’例 : likeRight(“name”, “ 王”) —&gt; name like ‘ 王%’ 12345678910111213141516171819202122232425262728/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 模糊查询 */ @Test public void testLike() &#123; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); // SELECT id,user_name,name,age,email AS mail FROM tb_user WHERE name LIKE ? // 参数：%五(String) wrapper.likeLeft(\"name\", \"五\"); List&lt;User&gt; users = this.userMapper.selectList(wrapper); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 5.4 排序 操作 说明 orderBy 排序： ORDER BY 字段, …例 : orderBy(true, true, “id”, “name”) —&gt; order by id ASC,name ASC orderByAsc 排序： ORDER BY 字段, … ASC例 : orderByAsc(“id”, “name”) —&gt; order by id ASC,name ASC orderByDesc 排序： ORDER BY 字段, … DESC例 : orderByDesc(“id”, “name”) —&gt; order by id DESC,name DESC 12345678910111213141516171819202122232425262728/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 排序 */ @Test public void testOrderByAgeDesc() &#123; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); //按照年龄倒序排序 // SELECT id,user_name,name,age,email AS mail FROM tb_user ORDER BY age DESC wrapper.orderByDesc(\"age\"); List&lt;User&gt; users = this.userMapper.selectList(wrapper); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 5.5 逻辑查询 操作 说明 or 拼接 OR主动调用 or 表示紧接着下一个方法不是用 and 连接!(不调用 or 则默认为使用 and 连接) and AND 嵌套例 : and(i -&gt; i.eq(“name”, “李白”).ne(“status”, “活着”)) —&gt; and (name = ‘李白’ and status&lt;&gt; ‘活着’) 123456789101112131415161718192021222324252627/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * 逻辑查询 */ @Test public void testOr() &#123; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); // SELECT id,user_name,name,age,email AS mail FROM tb_user WHERE name = ? OR age = ? wrapper.eq(\"name\", \"王五\").or().eq(\"age\", 21); List&lt;User&gt; users = this.userMapper.selectList(wrapper); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125; 2.6 select在MP查询中，默认查询所有的字段，如果有需要也可以通过select方法进行指定字段。 123456789101112131415161718192021222324252627282930/** * 通用CRUD * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class UserMapperTest &#123; @Autowired private UserMapper userMapper; /** * select */ @Test public void testSelect() &#123; QueryWrapper&lt;User&gt; wrapper = new QueryWrapper&lt;&gt;(); //SELECT id,name,age FROM tb_user WHERE name = ? OR age = ? wrapper.eq(\"name\", \"王五\") .or() .eq(\"age\", 21) .select(\"id\", \"name\", \"age\"); //指定查询的字段 List&lt;User&gt; users = this.userMapper.selectList(wrapper); for (User user : users) &#123; System.out.println(user); &#125; &#125;&#125;","tags":[{"name":"MyBatis","slug":"MyBatis","permalink":"https://wgy1993.gitee.io/tags/MyBatis/"},{"name":"Mybatis-Plus","slug":"Mybatis-Plus","permalink":"https://wgy1993.gitee.io/tags/Mybatis-Plus/"}]},{"title":"SpringData(二)","date":"2020-09-27T11:25:48.000Z","path":"archives/dc35f26.html","text":"1. SpringData Redis1.1 SpringData Redis 简介Redis是一个基于内存的数据结构存储系统，它可以用作数据库或者缓存。它支持多种类型的数据结构，这些数据结构类型分别为String（字符串）、List（列表）、Set（集合）、Hash（散列）和Zset（有序集合）。 SpringData Redis的作用是通过一段简单的配置即可访问redis服务，它的底层是对java提供的redis开发包(比如jedis等)进行了高度封装，主要提供了如下功能： 连接池自动管理，提供了一个高度封装的 RedisTemplate类,基于这个类的对象可以对redis进行各种操作 针对 jedis客户端中大量api进行了归类封装,将同一类型操作封装为operation接口 ValueOperations ：简单字符串类型数据操作 SetOperations ：set类型数据操作 ZSetOperations ：zset类型数据操作 HashOperations ：map类型的数据操作 ListOperations ：list类型的数据操作 1.2 Redis 环境搭建1.2.1 安装redis的依赖环境1yum -y install gcc automake autoconf libtool make 1.2.2 上传安装包获取到安装包,并将它上传到linux的/usr/local/src/目录下 1.2.3 解压解压安装包,得到一个redis-5.0.4目录 1tar -zxvf redis-5.0.4.tar.gz 1.2.4 编译进入redis目录,在目录下执行make命令 123cd redis-5.0.4make 1.2.5 安装执行安装命令,注意此处指定了安装目录为/usr/local/redis 1make PREFIX=/usr/local/redis install 1.2.6 复制配置文件将配置文件复制到redis的安装目录的bin目录下 123cd /usr/local/redis/bin/cp /usr/local/src/redis-5.0.4/redis.conf ./ 1.2.7 修改 redis的配置文件修改redis的配置文件,将注解绑定和保护模式关闭,方便我们从客户端连接测试 1vim redis.conf 1.2.8 启动 redis服务1.&#x2F;src&#x2F;redis-server redis.conf 1.3 SpringData Redis 入门案例1.3.1 创建工程，引入坐标123456789101112131415161718192021222324252627282930&lt;dependencies&gt; &lt;!--jedis--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.8&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.3.2 创建配置文件123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--配置Jedis连接池参数--&gt; &lt;bean id=\"jedisPoolConfig\" class=\"redis.clients.jedis.JedisPoolConfig\"&gt; &lt;!--最大连接数--&gt; &lt;property name=\"maxTotal\" value=\"30\"/&gt; &lt;!--最大空闲连接数--&gt; &lt;property name=\"maxIdle\" value=\"20\"/&gt; &lt;!--最小空闲连接数--&gt; &lt;property name=\"minIdle\" value=\"10\"/&gt; &lt;/bean&gt; &lt;!--配置Jedis连接工厂--&gt; &lt;bean id=\"jedisConnectionFactory\" class=\"org.springframework.data.redis.connection.jedis.JedisConnectionFactory\" p:hostName=\"192.168.142.128\" p:port=\"6379\" p:poolConfig-ref=\"jedisPoolConfig\" /&gt; &lt;!--配置Redis的模板--&gt; &lt;bean id=\"redisTemplate\" class=\"org.springframework.data.redis.core.RedisTemplate\"&gt; &lt;!--配置jedis的连接工厂,目的是为了获取jedis连接--&gt; &lt;property name=\"connectionFactory\" ref=\"jedisConnectionFactory\"/&gt; &lt;!--配置非hash类型的序列化器--&gt; &lt;!--&lt;property name=\"keySerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.StringRedisSerializer\"/&gt; &lt;/property&gt; &lt;property name=\"valueSerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.StringRedisSerializer\"/&gt; &lt;/property&gt;--&gt; &lt;/bean&gt;&lt;/beans&gt; 1.3.3 测试123456789101112131415161718192021222324/** * SpringData Redis 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-redis.xml\")public class RedisTest &#123; @Autowired private RedisTemplate redisTemplate; /** * 测试:向redis保存一条数据 */ @Test public void testSave() &#123; //存入数据 redisTemplate.opsForValue().set(\"name\", \"test\"); //查询数据 String name = (String) redisTemplate.opsForValue().get(\"name\"); System.out.println(name); &#125;&#125; 1.4 SpringData Redis 的序列化器通过Redis提供的客户端查看入门案例中存入redis的数据 这时候会发现，存入的数据并不是简单的字符串，而是一些类似于二进制的数据，这是怎么回事呢？ 原来，SpringData Redis在保存数据的时候，底层有一个序列化器在工作，它会将要保存的数据（键和值）按照一定的规则进行序列化操作后再进行存储。spring-data-redis提供如下几种序列化器： StringRedisSerializer: 简单的字符串序列化 GenericToStringSerializer: 可以将任何对象泛化为字符串并序列化 Jackson2JsonRedisSerializer: 序列化对象为json字符串 GenericJackson2JsonRedisSerializer: 功能同上,但是更容易反序列化 OxmSerializer: 序列化对象为xml字符串 JdkSerializationRedisSerializer: 序列化对象为二进制数据 RedisTemplate默认使用的是JdkSerializationRedisSerializer对数据进行序列化。 那么如何选择自己想要的序列化器呢？SpringData提供了两种方式： 1、通过配置文件配置 12345678910111213&lt;!--配置Redis的模板--&gt;&lt;bean id=\"redisTemplate\" class=\"org.springframework.data.redis.core.RedisTemplate\"&gt; &lt;!--配置jedis的连接工厂,目的是为了获取jedis连接--&gt; &lt;property name=\"connectionFactory\" ref=\"jedisConnectionFactory\"/&gt; &lt;!--配置非hash类型的序列化器--&gt; &lt;property name=\"keySerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.StringRedisSerializer\"/&gt; &lt;/property&gt; &lt;property name=\"valueSerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.StringRedisSerializer\"/&gt; &lt;/property&gt;&lt;/bean&gt; 2、通过RedisTemplate设定 12345678910111213@Testpublic void testSave2() &#123; redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new StringRedisSerializer()); //获取操作简单字符串类型数据的数据句柄 ValueOperations operations = redisTemplate.opsForValue(); operations.set(\"name3\", \"test\"); //查询数据 String name = (String) redisTemplate.opsForValue().get(\"name3\"); System.out.println(name);&#125; 1.5 SpringData Redis 运行原理分析我们从入门案例中已经知道SpringData Redis操作Redis服务器只要是通过RestTemplate实现的，那么RestTemplate底层到底是如何操作redis的呢，下面我们通过源码追踪的形式看一看。 1、首先看配置文件中关于RestTemplate的bean的配置，可以看到在RedisTemplate的bean声明中注入了一个JedisConnectionFactory实例，顾名思义，这个连接工厂是用来获取Jedis连接的，那么通过这种方式RedisTemplate就可以拿到操作Redis服务器的句柄了。 2、使用debug运行入门案例，观察创建好的RestTemplate实例，可以看到里面主要有序列化器和redis的连接信息，基于这些，我们就可以对redis进行操作了 3 、跟踪进入set方法，我们可以看到set方法中使用了一个connection来进行操作，这个connection的类型是JedisConnetion，而这个connection肯定是通过配置文件配置的JedisConnectionFactory产生的，也就是底层开始调用jedis的api了。 4 、继续追踪set方法，选择JedisStringCommands实现 5 、继续之宗set方法，可以看到底层已经获取到了jedis的实例，再调用set方法已经在调jedis的set了 6 、再追踪一步，就会发现，底层最终调用的是jedis的原生API，setCommand方法，这个方法就是jedis提供的对redis的各种操作命令了。 至此，我们的分析完毕。得到的结论就是： SpringData提供redisTemplate就是在原生的Jedis或者其他操作redis的技术上做的一层封装，它屏蔽掉了这些原生技术的实现细节，统一了调用接口，使得我们的操作更加简单明了。 1.6 SpringData Redis 常见操作本章节我们来学习如何使用SpringData Redis来操作Redis的各种数据类型. 在Redis中有五种常见类型,SpringData Redis对每一种数据类型都提供了一个xxxOperations的API,他们分别是: ValueOperations : 用来操作字符串类型数据 HashOperations: 用来操作hash类型数据 ListOperations: 用来操作list类型数据 SetOperations: 用来操作set类型数据 ZSetOperations: 用来操作zset类型数据 1.6.1 String类型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101/** * String类型 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-redis.xml\")public class RedisStringTest &#123; ValueOperations&lt;String, String&gt; operations = null; @Autowired private RedisTemplate redisTemplate; @Before public void init() &#123; redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new StringRedisSerializer()); operations = redisTemplate.opsForValue(); &#125; @Test public void testSet() &#123; //向数据库中保存name--heima operations.set(\"name\", \"heima\"); //相关数据库保存name1--heima1 有效时间为10s operations.set(\"name1\", \"heima1\", 10, TimeUnit.SECONDS); //替换 heima ---&gt; heXXa offset 索引位置是从0开始 operations.set(\"name\", \"XX\", 2); //当key不存在的时候,执行保存操作;当key存在的时候,什么都不做 operations.setIfAbsent(\"name1\", \"heima\"); //批量保存 Map map = new HashMap(); map.put(\"name2\", \"heima2\"); map.put(\"name3\", \"heima3\"); map.put(\"name4\", \"heima4\"); operations.multiSet(map); //追加 当key存在时,会执行追加操作;当key不存在时,会执行保存操作 operations.append(\"name5\", \"Heima\"); &#125; @Test public void testGet() &#123; //根据key获取value String value = operations.get(\"name\"); System.out.println(value);//heXXaHeima //首先根据key获取value,然后再根据value进行截取,从start位置截取到end位置[包含start和end] String value2 = operations.get(\"name\", 5, 7); System.out.println(value2);//heXXaHeima--&gt;Hei //批量获取 List&lt;String&gt; keys = new ArrayList&lt;&gt;(); keys.add(\"name2\"); keys.add(\"name3\"); keys.add(\"name4\"); List&lt;String&gt; values = operations.multiGet(keys); for (String s : values) &#123; System.out.println(s); &#125; //根据key获取value的长度 Long size = operations.size(\"name\"); System.out.println(size); &#125; //自增 @Test public void testIncrement() &#123; operations.set(\"age\", \"18\"); operations.increment(\"age\");//自增1---&gt;19 System.out.println(operations.get(\"age\")); operations.increment(\"age\", 5);//自增5 System.out.println(operations.get(\"age\"));//----&gt;24 //自减 operations.decrement(\"age\"); &#125; //删除 @Test public void testDelete() &#123; //单个删除 redisTemplate.delete(\"name\"); List&lt;String&gt; keys = new ArrayList&lt;&gt;(); keys.add(\"name2\"); keys.add(\"name3\"); keys.add(\"name4\"); //批量删除 redisTemplate.delete(keys); &#125;&#125; 1.6.2 Hash 类型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Hash类型 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-redis.xml\")public class RedisHashTest &#123; HashOperations&lt;String, String, Article&gt; operations = null; @Autowired private RedisTemplate redisTemplate; @Before public void init() &#123; redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setHashKeySerializer(new StringRedisSerializer()); redisTemplate.setHashValueSerializer(new JdkSerializationRedisSerializer()); operations = redisTemplate.opsForHash(); &#125; /** * 保存 */ @Test public void testPut() &#123; Article article = new Article(); article.setTitle(\"测试\"); article.setAuthor(\"程序员\"); article.setCreateTime(new Date()); operations.put(\"article\", \"3\", article); &#125; /** * 获取 */ @Test public void testGet() &#123; //判断hashkey是否存在 Boolean flag = operations.hasKey(\"article\", \"3\"); System.out.println(flag); //根据key和hashkay获取操作 Article article = operations.get(\"article\", \"2\"); System.out.println(article); //根据key获取所有的hashkey Set&lt;String&gt; set = operations.keys(\"article\"); for (String s : set) &#123; System.out.println(s); &#125; List&lt;Article&gt; articles = operations.values(\"article\"); for (Article art : articles) &#123; System.out.println(art); &#125; Map&lt;String, Article&gt; map = operations.entries(\"article\"); for (Map.Entry&lt;String, Article&gt; entry : map.entrySet()) &#123; System.out.println(entry.getKey() + \":\" + entry.getValue()); &#125; &#125; /** * 删除 */ @Test public void testDelete() &#123; //当hash中的数据全部被删除后,整个hash就没了 operations.delete(\"article\", \"2\", \"3\"); &#125;&#125; 1.6.3 List 类型1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * List类型 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-redis.xml\")public class RedisListTest &#123; ListOperations&lt;String, String&gt; operations = null; @Autowired private RedisTemplate redisTemplate; @Before public void init() &#123; redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new StringRedisSerializer()); operations = redisTemplate.opsForList(); &#125; /** * 增加 */ @Test public void testAdd() &#123; //从左边添加一个元素 operations.leftPush(\"students\", \"zhangsan\"); //从左边添加多个元素 operations.leftPushAll(\"students\", \"lisi\", \"wangwu\", \"zhaoliu\"); //从右边添加一个元素 operations.rightPush(\"students\", \"zhangsan1\"); //从右边添加多个元素 operations.rightPushAll(\"students\", \"lisi\", \"wangwu\", \"zhaoliu\"); &#125; /** * 查询 */ @Test public void testFind() &#123; //根据key和索引进行查询 //0和正数代表从左边开始 0 1 2 //负数代表从右边开始 -1 -2 -3 String student = operations.index(\"students\", 1); System.out.println(student); String student1 = operations.index(\"students\", -2); System.out.println(student1); //范围查询 //根据key [start,end] 包括首尾 List&lt;String&gt; students = operations.range(\"students\", 0, 2); for (String s : students) &#123; System.out.println(s); &#125; &#125; /** * 删除 */ @Test public void testRemove() &#123; //从左边删除第一个元素 //String s = operations.leftPop(\"students\"); //从右边删除第一个元素 //operations.rightPop(\"students\"); // count &gt; 0：删除左边起第几个等于指定值的元素 // count &lt; 0：删除右边起第几个等于指定值的元素 // count = 0：删除所有等于value的元素。 //删除左边起第二个wangwu operations.remove(\"students\", 2, \"wangwu\"); &#125;&#125; 1.6.4 Set 类型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394/** * Set类型 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-redis.xml\")public class RedisSetTest &#123; SetOperations&lt;String, String&gt; operations = null; @Autowired private RedisTemplate redisTemplate; @Before public void init() &#123; redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new StringRedisSerializer()); operations = redisTemplate.opsForSet(); &#125; /** * 增加 */ @Test public void testAdd() &#123; operations.add(\"students\", \"zhangsan\", \"lisi\", \"wangwu\", \"zhangsan\"); &#125; /** * 查询 */ @Test public void testFind() &#123; //查询所有元素 Set&lt;String&gt; students = operations.members(\"students\"); for (String student : students) &#123; System.out.println(student); &#125; //随机获取一个元素 String student = operations.randomMember(\"students\"); System.out.println(student); //随机多个元素[可能会重复] List&lt;String&gt; stus = operations.randomMembers(\"students\", 2); for (String stu : stus) &#123; System.out.println(stu); &#125; &#125; /** * 删除 */ @Test public void testRemove() &#123; //移除元素,并返回移除成功个数 Long count = operations.remove(\"students\", \"zhangsan\", \"wangwu\", \"sunliu\"); System.out.println(count); //随机移除指定集合中的多少个元素 List&lt;String&gt; students = operations.pop(\"students\", 2); for (String student : students) &#123; System.out.println(student); &#125; &#125; /** * 多集合操作 */ @Test public void testMoreSet() &#123; operations.add(\"names1\", \"zhangsan\", \"li\", \"wangwu\"); operations.add(\"names2\", \"zhangsan\", \"li\", \"zhaoliu\"); //取交集 Set&lt;String&gt; sets1 = operations.intersect(\"names1\", \"names2\"); for (String s : sets1) &#123; System.out.println(s); &#125; //取并集 Set&lt;String&gt; sets2 = operations.union(\"names1\", \"names2\"); for (String s : sets2) &#123; System.out.println(s); &#125; //取差集[第一个集合中存在,但是在第二个集合中不存在的元素] Set&lt;String&gt; sets3 = operations.difference(\"names2\", \"names1\"); for (String s : sets3) &#123; System.out.println(s); &#125; &#125;&#125; 1.6.5 ZSet 类型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116/** * ZSet类型 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-redis.xml\")public class RedisZSetTest &#123; ZSetOperations&lt;String, String&gt; operations = null; @Autowired private RedisTemplate redisTemplate; @Before public void init() &#123; redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new StringRedisSerializer()); operations = redisTemplate.opsForZSet(); &#125; /** * 增加 */ @Test public void testAdd() &#123; operations.add(\"students\", \"zhangsan\", 100); operations.add(\"students\", \"lisi\", 60); operations.add(\"students\", \"wangwu\", 80); &#125; /** * 分数的增减 */ @Test public void testScore() &#123; //incrementScore 可以用来增减分数 增加就用正数 减少用负数 //增加分数 operations.incrementScore(\"students\", \"wangwu\", 30); //减少分数 operations.incrementScore(\"students\", \"wangwu\", -70); &#125; /** * 查询一个元素的信息 */ @Test public void testFindOne() &#123; //查询一个元素的分数 Double score = operations.score(\"students\", \"wangwu\"); System.out.println(score); //查询一个元素在集合中的排名 排名从0开始 Long rank = operations.rank(\"students\", \"zhangsan\"); System.out.println(rank); &#125; /** * 根据一个区间获得一个列表 */ @Test public void testFindList() &#123; //根据排名区间来获取元素列表 Set&lt;String&gt; students = operations.range(\"students\", 0, 2); for (String student : students) &#123; System.out.println(student); &#125; System.out.println(\"=============\"); Set&lt;ZSetOperations.TypedTuple&lt;String&gt;&gt; set = operations.rangeWithScores(\"students\", 0, 2); for (ZSetOperations.TypedTuple&lt;String&gt; tuple : set) &#123; System.out.println(tuple.getValue() + \"同学,得了\" + tuple.getScore() + \"分\"); &#125; System.out.println(\"---------------------------------\"); //根据分数区间来获取列表 Set&lt;String&gt; students2 = operations.rangeByScore(\"students\", 60, 90); for (String student : students2) &#123; System.out.println(student); &#125; System.out.println(\"=============\"); Set&lt;ZSetOperations.TypedTuple&lt;String&gt;&gt; set2 = operations.rangeByScoreWithScores(\"students\", 60, 90); for (ZSetOperations.TypedTuple&lt;String&gt; tuple : set2) &#123; System.out.println(tuple.getValue() + \"同学,得了\" + tuple.getScore() + \"分\"); &#125; &#125; /** * 统计 */ @Test public void testCount() &#123; //统计一个集合中元素 Long zCard = operations.zCard(\"students\"); System.out.println(zCard); //根据一个分数区间统计元素数量 Long count = operations.count(\"students\", 50, 100); System.out.println(count); &#125; /** * 删除 */ @Test public void testRemove() &#123; //根据key-value删除 value允许传入多个 operations.remove(\"students\", \"zhangsan\", \"lisi\"); //根据排名区间删除 operations.removeRange(\"students\", 0, 1); //根据分数区间删除 operations.removeRangeByScore(\"students\", 70, 100); &#125;&#125; 2. Repository和Template的选用经过前面的章节，我们学习了SpringData家族中jpa和redis的使用，在感受到SpringData技术使用方便的同时，也隐隐约约感觉有点问题，那就是jpa和redis的使用思路好像不是很一致。 我们使用SpringDataJpa的时候，采用了继承SpringData提供的一个接口的形式，即 public interface ArticleDao extends JpaRepository&lt;Article,Integer&gt;,JpaSpecificationExecutor&lt;Article&gt; ，但是使用SpingData Redis的时候，却是使用了在实现类中注入一个 redisTemplate 的方式，那么这两种方式到底有什么关系，用哪个更好呢？ 其实这两种方式都可以完成我们对持久层的操作，但是对比两种方式的使用，就会发现： 第一种方式，直接继承xxxRepository接口，可以不必自己去写实现类，而轻松实现简单的增删改查、分页、排序操作，但是对于非常复杂的查询，用起来就比较的费力了； 第二种方式，直接使用xxxTemplate，就需要自己写实现类，但是这样增删改查可以自己控制，对于复杂查询来说，用起来更加得心应手。 所以，两种方式在企业开发中都可能用到，甚至有的项目开发中会同时使用两种方式:对于简单的操作，直接继承Repository接口，对于复杂操作，使用Template完成。所以我们用的时候也要根据实际场景进行灵活选用。 3. SpringData ElasticSearch3.1 SpringData ElasticSearch 简介Elasticsearch是一个实时的分布式搜索和分析引擎。它底层封装了Lucene框架,可以提供分布式多用户的全文搜索服务。 Spring Data ElasticSearch是SpringData技术对ElasticSearch原生API封装之后的产物,它通过对原生API的封装,使得程序员可以简单的对ElasticSearch进行各种操作。 3.2 ElasticSearch 环境搭建3.2.1 安装ElasticSearch3.2.1.1 准备工作修改进程限制,编辑/etc/security/limits.conf,添加下面的代码 1234* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 修改系统变量的最大值,编辑/etc/sysctl.conf,添加下面的配置 1vm.max_map_count = 655360 保存修改之后 ,执行 sysctl -p 命令 1sysctl -p 3.2.1.2 文件上传上传文件到src目录下 3.2.1.3 文件解压直接将软件解压到安装目录 1tar -zxvf elasticsearch-5.6.8.tar.gz -C /usr/local 3.2.1.4 添加用户新增加一个es用户,并将elasticsearch-5.6.8目录的所属用户和用户组改成es 123useradd eschown es:es -R ../elasticsearch-5.6.8 切换到新创建的 es用户,执行后续操作 3.2.1.5 修改配置编辑配置文件,修改数据文件和日志文件的存储位置以及es的绑定地址 123cd /usr/local/elasticsearch-5.6.8/config/vim elasticsearch.yml 3.2.1.6 启动 elasticSearch123cd /usr/local/elasticsearch-5.6.8/bin/./elasticsearch 3.2.1.7 访问测试通过服务器的9200端口访问,得到下面的结果,证明安装成功. 3.2.2 安装 Head插件3.2.2.1 安装nodeJS将nodeJS的安装包上传到/usr/local/src下,然后解压到/usr/local下,然后将npm和node建立软连接到/usr/local/bin/下 1234567tar -zxvf node-v10.16.0-linux-x64.tar.gz -C /usr/local/ln -s /usr/local/node-v10.16.0-linux-x64/bin/npm /usr/local/bin/ln -s /usr/local/node-v10.16.0-linux-x64/bin/node /usr/local/bin/node -v 3.2.2.2 安装 cnpm12345npm install -g cnpm --registry=https://registry.npm.taobao.orgln -s /usr/local/node-v10.16.0-linux-x64/bin/cnpm /usr/local/bin/cnpm -v 3.2.2.3 安装 grunt123npm install -g grunt-cliln -s /usr/local/node-v10.16.0-linux-x64/bin/grunt /usr/local/bin 3.2.2.4 安装 head插件上传head插件到/usr/local/src/下,然后解压到/usr/local下 123unzip elasticsearch-head-master.zipcp -R elasticsearch-head-master /usr/local/ 3.2.2.5 安装 head插件所需依赖123cd /usr/local/elasticsearch-head-mastercnpm install 3.2.2.6 修改 elasticsearch的配置编辑配置文件:/usr/local/elasticsearch-5.6.8/config/elasticsearch.yml,添加跨域请求允许,即增加以下两行： 12http.cors.enabled: truehttp.cors.allow-origin: \"*\" 修改完毕之后 ,要对elasticsearch进行重启 3.2.2.7 启动head在head目录下启动插件 1grunt server 3.2.2.8 访问测试通过服务器的9100端口访问,得到下面的结果,证明安装成功. 3.2.3 安装 IK分词器3.2.3.1 说明ES默认的中文分词器是将每一个汉字作为一个词，这显然不合适，而IK分词是一款国人开发的相对简单的中文分词器，它包含大量的中文词，而且支持自定义分词。 ik分词器提供的分词规则： ik_max_word ：会将文本做最细粒度的拆分，比如会将“中华人民共和国人民大会堂”拆分为“中华人民共和国、中华人民、中华、华人、人民共和国、人民、共和国、大会堂、大会、会堂等词语。 ik_smart ：会做最粗粒度的拆分，比如会将“中华人民共和国人民大会堂”拆分为中华人民共和国、人民大会堂 两种分词器使用的最佳实践是：索引时用ik_max_word，在搜索时用ik_smart。即：索引时最大化的将文章内容分词，搜索时更精确的搜索到想要的结果。 3.2.3.2 安装第一步:下载得到ik分词器的安装包,将其解压得到有一个文件夹,并将文件夹重命名为ik 1unzip elasticsearch-analysis-ik-5.6.8.zip 第二步 :将elastaicsearch文件夹拷贝到elastaicsearch-5.6.8下的plugins目录下,并重命名为ik 1cp -r elasticsearch /usr/local/elasticsearch-5.6.8/plugins/ik 第三步 :重新启动elasticsearch即可加载IK分词器 第四部:测试 http://服务地址:9200/_analyze?analyzer=ik_smart&amp;pretty=true&amp;text=我是程序员 3.3 ElasticSearch 基础知识回顾3.3.1 核心概念3.3.1.1 索引 index一个索引就是一个拥有几分相似特征的文档的集合。索引就类似于关系型数据库中的库的概念。 3.3.1.2 类型 type一个类型是索引中的一个逻辑上的分类/分区。类型就类似于关系型数据库中的数据表的概念。 3.3.1.3 映射 mapping映射是对类型中的字段的限制。映射就类似于关系型数据库中的数据表结构的概念。 3.3.1.4 文档 document一个文档是一个可被索引的基础信息单元。文档就类似于关系型数据库中的行的概念。 123ElasticSearch跟关系型数据库中概念的对比：Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields 3.3.2 常见操作3.3.2.1 创建工程，引入坐标12345678910111213141516171819202122232425262728293031323334353637&lt;dependencies&gt; &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.5.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.5.4&lt;/version&gt; &lt;/dependency&gt; &lt;!--单元测试--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!--日志--&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt; &lt;version&gt;2.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.24&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.3.2.2 操作测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215/** * es测试 * * @author wgy */public class EsTest &#123; TransportClient client = null; /** * 创建连接 */ @Before public void initClient() &#123; try &#123; //设置集群名称 Settings settings = Settings.builder().put(\"cluster.name\", \"wgy-es\").build();// 集群名 client = new PreBuiltTransportClient(settings) .addTransportAddress( new TransportAddress( InetAddress.getByName(\"192.168.142.128\"), 9700 ) ); &#125; catch (UnknownHostException e) &#123; System.out.println(\"初始化失败....\"); e.printStackTrace(); &#125; &#125; /** * 关闭连接 */ @After public void closeClient() &#123; client.close(); &#125; /** * 创建索引 * * @throws UnknownHostException */ @Test public void testCreateIndex() throws UnknownHostException &#123; client.admin().indices().prepareCreate(\"test\").get(); &#125; /** * 删除索引,可以一次性删除多个 * * @throws UnknownHostException */ @Test public void testDeleteIndex() throws UnknownHostException &#123; client.admin().indices().prepareDelete(\"test\", \"springData\").get(); &#125; /** * 创建映射 * * @throws Exception */ @Test public void testCreateMappping() throws Exception &#123; XContentBuilder builder = XContentFactory.jsonBuilder() .startObject() .startObject(\"article\") .startObject(\"properties\") .startObject(\"title\") .field(\"type\", \"text\") .field(\"store\", \"true\") .field(\"analyzer\", \"ik_smart\") .endObject() .startObject(\"content\") .field(\"type\", \"text\") .field(\"store\", \"true\") .field(\"analyzer\", \"ik_smart\") .endObject() .startObject(\"hits\") .field(\"type\", \"long\") .field(\"store\", \"true\") .endObject() .endObject() .endObject() .endObject(); // 创建映射(表结构) PutMappingRequest mapping = Requests.putMappingRequest(\"test\")//指定索引(库) .type(\"article\")//指定类型(表) .source(builder); client.admin().indices().putMapping(mapping).get(); &#125; /** * 创建文档 */ @Test public void testCreateDocuments() &#123; //组装数据 Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"title\", \"程序员\"); map.put(\"content\", \"程序员其实很低调\"); map.put(\"hits\", 100); //创建文档 client.prepareIndex(\"test\", \"article\", \"1\").setSource(map).get(); &#125; /** * 修改文档 */ @Test public void testUpdateDocuments() &#123; //组装数据 Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"title\", \"程序员2\"); map.put(\"content\", \"程序员很低调2\"); map.put(\"hits\", 200); //修改文档 client.prepareUpdate(\"test\", \"article\", \"1\").setDoc(map).get(); &#125; /** * 删除文档 */ @Test public void testDeleteDocuments() &#123; client.prepareDelete(\"test\", \"article\", \"1\").get(); &#125; /** * 查询所有 */ @Test public void testFindAll() &#123; //1 设置index type 查询条件,返回一个查询结果对象 SearchResponse searchResponse = client.prepareSearch(\"test\").setTypes(\"article\")//设置index和type,允许传入多个 .setQuery(QueryBuilders.matchAllQuery())//设置查询条件 :查询所有 .get(); //2 检索的命中对象 SearchHits hits = searchResponse.getHits(); //3 获取查询结果数 System.out.println(\"总共查询到\" + hits.getTotalHits() + \"条记录\"); //4 获取结果 Iterator&lt;SearchHit&gt; iterator = hits.iterator(); while (iterator.hasNext()) &#123; SearchHit searchHit = iterator.next(); System.out.println(searchHit.getSourceAsString()); &#125; &#125; /** * 根据title查询 */ @Test public void testFindByTitle() &#123; //QueryBuilders.termQuery(\"属性\",\"值[分词之后存在]\") //1 设置index type 查询条件,返回一个查询结果对象 SearchResponse searchResponse = client.prepareSearch(\"test\").setTypes(\"article\")//设置index和type,允许传入多个 .setQuery(QueryBuilders.termQuery(\"title\", \"员\"))//设置查询条件 :根据title查询 .get(); //2 检索的命中对象 SearchHits hits = searchResponse.getHits(); //3 获取查询结果数 System.out.println(\"总共查询到\" + hits.getTotalHits() + \"条记录\"); //4 获取结果 Iterator&lt;SearchHit&gt; iterator = hits.iterator(); while (iterator.hasNext()) &#123; SearchHit searchHit = iterator.next(); System.out.println(searchHit.getSourceAsString()); &#125; &#125; /** * 分页和排序 */ @Test public void testFindAllWithPageAndSort() &#123; //1 设置index type 查询条件,返回一个查询结果对象 SearchResponse searchResponse = client.prepareSearch(\"test\").setTypes(\"article\")//设置index和type,允许传入多个 .setQuery(QueryBuilders.matchAllQuery())//设置查询条件 :查询所有 .setFrom(0).setSize(20)//设置分页的条件 .setFrom(从第几行开始查).setSize(查多少行) .addSort(\"hits\", SortOrder.ASC)//设置分页条件 .addSort(属性, 排序规则) .get(); //2 检索的命中对象 SearchHits hits = searchResponse.getHits();//20 //3 获取查询结果数 System.out.println(\"总共查询到\" + hits.getTotalHits() + \"条记录\"); //4 获取结果 Iterator&lt;SearchHit&gt; iterator = hits.iterator(); while (iterator.hasNext()) &#123; SearchHit searchHit = iterator.next(); System.out.println(searchHit.getSourceAsString()); &#125; &#125;&#125; 3.4 SpringData ElasticSearch 入门案例3.4.1 目标通过SpringData ES技术向ElasticSearch数据库存储一条数据 3.4.2 创建工程，引入坐标12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;dependencies&gt; &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;!--spring-data-elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt; &lt;version&gt;3.1.20.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!--日志--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt; &lt;version&gt;2.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.24&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;!--单元测试--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.4.3 添加配置文件1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:elasticsearch=\"http://www.springframework.org/schema/data/elasticsearch\" xsi:schemaLocation=\"http://www.springframework.org/schema/data/elasticsearch http://www.springframework.org/schema/data/elasticsearch/spring-elasticsearch.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 扫描dao包 --&gt; &lt;elasticsearch:repositories base-package=\"com.wgy\"/&gt; &lt;!-- 配置Client --&gt; &lt;elasticsearch:transport-client id=\"client\" cluster-nodes=\"192.168.142.128:9700\" cluster-name=\"wgy-es\"/&gt; &lt;!-- 配置搜索模板 --&gt; &lt;bean id=\"elasticsearchTemplate\" class=\"org.springframework.data.elasticsearch.core.ElasticsearchTemplate\"&gt; &lt;constructor-arg name=\"client\" ref=\"client\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 3.4.4 创建实体类1234567891011121314151617181920212223242526272829/** * 文章实体类 * * @author wgy *///indexName指定索引名称 type 指定类型名称@Document(indexName = \"test-sd\", type = \"article\")public class Article &#123; @Id @Field(index = false, type = FieldType.Integer) private Integer id; /** * index：是否设置分词 默认是true * analyzer：存储时使用的分词器 * searchAnalyze：搜索时使用的分词器 * store：是否存储 默认是false * type: 数据类型 默认值是FieldType.Auto */ @Field(analyzer = \"ik_smart\", searchAnalyzer = \"ik_smart\", store = true, type = FieldType.Text) private String title; @Field(analyzer = \"ik_smart\", searchAnalyzer = \"ik_smart\", store = true, type = FieldType.Text) private String context; @Field(store = true, type = FieldType.Integer) private Integer hits; //省略set和get方法。。。 //省略toString方法。。&#125; 3.4.5 自定义 dao接口12345678/** * dao接口 * 自定义的接口需要继承ElasticsearchRepository&lt;实体类型,主键类型&gt; 基本的crud 分页 * * @author wgy */public interface ArticleDao extends ElasticsearchRepository&lt;Article, Integer&gt; &#123;&#125; 3.4.6 测试12345678910111213141516171819202122232425262728293031323334/** * SpringData ElasticSearch 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-es.xml\")public class EsTest &#123; @Autowired private ElasticsearchTemplate template; @Autowired private ArticleDao articleDao; //通过SpringData ES技术向ElasticSearch数据库存储一条数据 @Test public void testSave() &#123; //创建索引 template.createIndex(Article.class); //创建映射 template.putMapping(Article.class); //创建文档 Article article = new Article(); article.setId(1); article.setTitle(\"sd-程序员\"); article.setContext(\"sd-程序员很棒\"); //保存文档 articleDao.save(article); &#125;&#125; 3.5 SpringData ElasticSearch 实现CRUD操作3.5.1 增删改12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * SpringData ElasticSearch 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-es.xml\")public class EsTest &#123; @Autowired private ElasticsearchTemplate template; @Autowired private ArticleDao articleDao; //通过SpringData ES技术向ElasticSearch数据库存储一条数据 @Test public void testSave() &#123; //创建索引 template.createIndex(Article.class); //创建映射 template.putMapping(Article.class); //创建文档 Article article = new Article(); article.setId(1); article.setTitle(\"sd-程序员\"); article.setContext(\"sd-程序员很棒\"); //保存文档 articleDao.save(article); &#125; /** * 修改 */ @Test public void testUpdate() &#123; //判断数据库中是否有你指定的id的文档,如果没有,就进行保存,如果有,就进行更新 Article article = new Article(); article.setId(1); article.setTitle(\"sd-程序员1\"); article.setContext(\"sd-程序员很棒1\"); articleDao.save(article); &#125; /** * 删除 */ @Test public void testDelete() &#123; //根据主键删除 articleDao.deleteById(1); &#125;&#125; 3.5.2 接口方法查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * SpringData ElasticSearch 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-es.xml\")public class EsTest &#123; @Autowired private ElasticsearchTemplate template; @Autowired private ArticleDao articleDao; /** * 查询所有 */ @Test public void testFindAll() &#123; Iterable&lt;Article&gt; all = articleDao.findAll(); for (Article article : all) &#123; System.out.println(article); &#125; &#125; /** * 主键查询 */ @Test public void testFindById() &#123; Optional&lt;Article&gt; opt = articleDao.findById(1); System.out.println(opt.get()); &#125; /** * 分页查询 */ @Test public void testFindAllWithPage() &#123; //设置分页条件 Pageable pageable = PageRequest.of(1, 3);//page代表的页码,从0开始 Page&lt;Article&gt; page = articleDao.findAll(pageable); for (Article article : page.getContent()) &#123; System.out.println(article); &#125; &#125; /** * 排序查询 */ @Test public void testFindAllWithSort() &#123; //设置排序条件 Sort sort = Sort.by(Sort.Order.desc(\"hits\")); Iterable&lt;Article&gt; all = articleDao.findAll(sort); for (Article article : all) &#123; System.out.println(article); &#125; &#125; /** * 分页+排序查询 */ @Test public void testFindAllWithPageAndSort() &#123; //设置排序条件 Sort sort = Sort.by(Sort.Order.desc(\"hits\")); //设置分页条件 Pageable pageable = PageRequest.of(1, 3, sort);//page代表的页码,从0开始 Page&lt;Article&gt; page = articleDao.findAll(pageable); for (Article article : page.getContent()) &#123; System.out.println(article); &#125; &#125;&#125; 3.5.3 命名规则查询es的命名规则跟jpa基本一致，常见的如下： 关键字 命名规则 解释 示例 and findByField1AndField2 根据Field1和Field2获得数据 findByTitleAndContent or findByField1OrField2 根据Field1或Field2获得数据 findByTitleOrContent is findByField 根据Field获得数据 findByTitle not findByFieldNot 根据Field获得补集数据 findByTitleNot between findByFieldBetween 获得指定范围的数据 findByPriceBetween lessThanEqual findByFieldLessThan 获得小于等于指定值的数据 findBy 下面,我们在dao接口中按照规则进行自定义查询方法 123456789101112131415161718192021222324252627282930313233343536/** * dao接口 * 自定义的接口需要继承ElasticsearchRepository&lt;实体类型,主键类型&gt; 基本的crud 分页 * * @author wgy */public interface ArticleDao extends ElasticsearchRepository&lt;Article, Integer&gt; &#123; /** * 根据标题查询 * * @param title * @return */ List&lt;Article&gt; findByTitle(String title); /** * 根据标题或内容查询 * * @param title * @param context * @return */ List&lt;Article&gt; findByTitleOrContext(String title, String context); /** * 根据标题或内容查询(含分页) * * @param title * @param context * @param pageable * @return */ List&lt;Article&gt; findByTitleOrContext(String title, String context, Pageable pageable);&#125; 添加测试方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * SpringData ElasticSearch 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext-es.xml\")public class EsTest &#123; @Autowired private ArticleDao articleDao; /** * 根据标题查询 */ @Test public void testFindByTitle() &#123; List&lt;Article&gt; articles = articleDao.findByTitle(\"员\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据标题或内容查询 */ @Test public void testFindByTitleOrContext() &#123; List&lt;Article&gt; articles = articleDao.findByTitleOrContext(\"程序员\", \"程序员\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据标题或内容查询(含分页) */ @Test public void testFindByTitleOrContextWithPage() &#123; //设置排序条件 Sort sort = Sort.by(Sort.Order.desc(\"hits\")); //设置分页条件 Pageable pageable = PageRequest.of(1, 3, sort);//page代表的页码,从0开始 List&lt;Article&gt; articles = articleDao.findByTitleOrContext(\"程序员\", \"程序员\", pageable); for (Article article : articles) &#123; System.out.println(article); &#125; &#125;&#125; 4. SpringData MongoDB4.1 SpringData MongoDB 简介MongoDB 是一个跨平台的，面向文档的数据库，是非关系数据库当中功能最丰富，最像关系数据库的产品。它支持的数据结构非常松散，是类似 JSON 的一种格式，因此可以存储比较复杂的数据类型。 MongoDB主要由文档(document)、集合(collection)、数据库(database)三部分组成 文档（ document）就相当于关系数据库中的一行记录 多个文档组成一个集合（ collection），相当于关系数据库的表 多个集合组织在一起，就是数据库（ database），一个 MongoDB 实例支持多个数据库 SpringData MongoDB是SpringData技术封装了mongodb-driver技术之后的产物,它可以用更加简单的方式操作MongoDB。 4.2 MongoDB 环境搭建4.2.1 解压上传文件到服务器，然后将其解压到/usr/local下 1tar -zxvf mongodb-linux-x86_64-rhel70-4.0.10.tgz -C /usr/local/ 4.2.2 创建需要的目录进入软件的安装目录下,创建数据存储和日志存储目录 1234cd /usr/local/mongodb-linux-x86_64-rhel70-4.0.10/mkdir datatouch log 4.2.3 创建配置文件进入bin目录,创建mongodb的配置文件mongo.conf ,文件内容如下: 12345dbpath=/usr/local/mongodb-linux-x86_64-rhel70-4.0.10/datalogpath=/usr/local/mongodb-linux-x86_64-rhel70-4.0.10/logbind_ip=0.0.0.0port=27017fork=true 4.2.4 启动 mongodb使用./mongod -f mongo.conf 启动服务,见到类似如下提示,证明启动成功 1./mongod -f mongo.conf 4.3 SpringData MongoDB 入门案例4.3.1 目标通过SpringData技术向Mongodb数据库存储一条数据 4.3.2 创建工程，引入坐标12345678910111213141516171819&lt;dependencies&gt; &lt;!--spring-data-mongodb--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-mongodb&lt;/artifactId&gt; &lt;version&gt;2.2.10.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!--测试--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.3.3 创建配置文件123456789101112131415161718192021222324252627282930&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:mongo=\"http://www.springframework.org/schema/data/mongo\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/data/mongo http://www.springframework.org/schema/data/mongo/spring-mongo.xsd\"&gt; &lt;!--包扫描--&gt; &lt;mongo:repositories base-package=\"com.wgy\"/&gt; &lt;!-- spring连接mongodb数据库的配置 --&gt; &lt;!-- host=\"192.168.142.128\" 指定mongodb服务所在主机地址 port=\"27017\" 指定mongodb服务所在主机端口号 dbname=\"springdata\" 数据名称 --&gt; &lt;mongo:mongo-client host=\"192.168.142.128\" port=\"27017\" id=\"mongo\" credentials=\"bobo:123456@springdata\"&gt; &lt;mongo:client-options write-concern=\"SAFE\"/&gt; &lt;/mongo:mongo-client&gt; &lt;mongo:db-factory id=\"mongoDbFactory\" dbname=\"springdata\" mongo-ref=\"mongo\"/&gt; &lt;!--mongoTemplate--&gt; &lt;bean id=\"mongoTemplate\" class=\"org.springframework.data.mongodb.core.MongoTemplate\"&gt; &lt;constructor-arg name=\"mongoDbFactory\" ref=\"mongoDbFactory\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 4.3.4 创建实体类123456789101112131415161718192021/** * 文章实体类 * * @author wgy *///使用@Document建立的是实体类和collection的关系@Document(\"article\")public class Article &#123; @Id//用来标识主键 private Integer id; //使用@Field建立实体类中属性跟collection中字段的映射关系,如果省略,代表两个名称一致 //@Field private String name; private String content; private Integer hits; //省略set和get方法。。。 //省略toString方法。。&#125; 4.3.5 自定义 dao接口1234567/** * dao接口 * * @author wgy */public interface ArticleDao extends MongoRepository&lt;Article, Integer&gt; &#123;&#125; 4.3.6 测试1234567891011121314151617181920212223242526/** * SpringData MongoDB 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class ArticleDaoTest &#123; @Autowired private ArticleDao articleDao; /** * 保存 */ @Test public void testSave() &#123; Article article = new Article(); article.setId(1); article.setName(\"程序员\"); article.setContent(\"程序员很低调\"); article.setHits(100); articleDao.save(article); &#125;&#125; 4.4 SpringData MongoDB 实现CRUD操作4.4.1 增删改123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * SpringData MongoDB 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class ArticleDaoTest &#123; @Autowired private ArticleDao articleDao; /** * 保存 */ @Test public void testSave() &#123; Article article = new Article(); article.setId(1); article.setName(\"程序员\"); article.setContent(\"程序员很低调\"); article.setHits(100); articleDao.save(article); &#125; /** * 修改 */ @Test public void testUpdate() &#123; Article article = new Article(); article.setId(1); article.setName(\"程序员2\"); article.setContent(\"程序员很低调2\"); article.setHits(200); articleDao.save(article); &#125; /** * 删除 */ @Test public void testDelete() &#123; articleDao.deleteById(1); &#125;&#125; 4.4.2 简单查询1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * SpringData MongoDB 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class ArticleDaoTest &#123; @Autowired private ArticleDao articleDao; /** * 查询所有 */ @Test public void testFindAll() &#123; List&lt;Article&gt; articles = articleDao.findAll(); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 主键查询 */ @Test public void testFindById() &#123; Optional&lt;Article&gt; opt = articleDao.findById(1); System.out.println(opt.get()); &#125; /** * 分页和排序 */ @Test public void testFindAllWithPageAndSort() &#123; //设置排序条件 Sort sort = Sort.by(Sort.Order.desc(\"hits\")); //设置分页条件 Pageable pageable = PageRequest.of(1, 3, sort); Page&lt;Article&gt; page = articleDao.findAll(pageable); for (Article article : page.getContent()) &#123; System.out.println(article); &#125; &#125;&#125; 4.4.3 命名规则查询定义接口 1234567891011121314151617181920212223/** * dao接口 * * @author wgy */public interface ArticleDao extends MongoRepository&lt;Article, Integer&gt; &#123; /** * 根据标题查询 * * @param name * @return */ List&lt;Article&gt; findByNameLike(String name); /** * 根据点击量查询 * * @param hits * @return */ List&lt;Article&gt; findByHitsGreaterThan(Integer hits);&#125; 测试方法 12345678910111213141516171819202122232425262728293031323334/** * SpringData MongoDB 入门案例 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class ArticleDaoTest &#123; @Autowired private ArticleDao articleDao; /** * 根据标题查询 */ @Test public void testFindByName() &#123; List&lt;Article&gt; articles = articleDao.findByNameLike(\"程序员1\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据点击量查询 */ @Test public void testFindByHitsGreaterThan() &#123; List&lt;Article&gt; articles = articleDao.findByHitsGreaterThan(105); for (Article article : articles) &#123; System.out.println(article); &#125; &#125;&#125; 5. 综合案例5.1 案例说明及思路分析5.1.1 案例目标通过一个【文章】案例来综合使用SpringData技术，案例中将涉及到jpa、redis、es、mongo的使用，可以很好的将前面章节所学知识点加以练习巩固。 5.1.2 涉及技术分析案例以常见网站中的文章管理和查询为背景，涉及到文章内容、最新文章列表、文章评论、文章检索等功能，下面具体分析： 文章内容分为文章基础和文章详情两部分，分别存储在 mysql的文章表和文章详情表中 最新文章列表展示的是热点数据，访问量比较大，采用 redis存储 文章评论数据量大，数据价值较低，存放在 mongodb中 全文检索使用 ES实现，本次直接向ES中插入测试数据测试，后期可以考虑使用logstash从数据库同步 5.1.3 功能分析5.1.3.1 数据后台管理 功能 mysql redis mongodb ES 添加文章 添加数据 清空缓存 添加数据 修改文章 修改数据 清空缓存 修改数据 删除文章 删除数据 清空缓存 删除文章评论 删除数据 添加评论 添加文章评论 删除评论 删除文章评论 5.1.3.2 数据查询功能 功能 mysql redis mongodb ES 查询最新文章 当redis中不存在时来数据库查 先从redis查，没有再去数据库查，查到以后放入redis 查询文章评论 根据文章标识从mongo中获取评论数据 文章检索 使用es实现文章检索功能 5.2 代码实现5.2.1 项目结构说明本次案例采用Spring为核心骨架，使用SpringData实现持久层操作，采用junit进行功能测试，完整项目结构如下图所示 5.2.2 创建工程，引入坐标123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133&lt;dependencies&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 单元测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring框架相关jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.7&lt;/version&gt; &lt;/dependency&gt; &lt;!--mvc--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--jpa--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;2.2.0.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;5.0.7.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--redis--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.8&lt;/version&gt; &lt;/dependency&gt; &lt;!--es--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;transport&lt;/artifactId&gt; &lt;version&gt;6.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt; &lt;version&gt;3.1.20.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-to-slf4j&lt;/artifactId&gt; &lt;version&gt;2.9.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.24&lt;/version&gt; &lt;/dependency&gt; &lt;!--mongo--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-mongodb&lt;/artifactId&gt; &lt;version&gt;2.2.0.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!--工具--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.55&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 5.2.3 加入配置文件5.2.3.1 jpa 配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:jdbc=\"http://www.springframework.org/schema/jdbc\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:jpa=\"http://www.springframework.org/schema/data/jpa\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd\"&gt; &lt;!-- 配置要扫描的包 --&gt; &lt;context:component-scan base-package=\"com.wgy\"/&gt; &lt;!-- 1.dataSource --&gt; &lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql:///case\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt; &lt;/bean&gt; &lt;!-- 2.EntityManagerFactory --&gt; &lt;bean id=\"entityManagerFactory\" class=\"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean\"&gt; &lt;!-- 注入数据源 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!-- 指定实体类所在的包 --&gt; &lt;property name=\"packagesToScan\" value=\"com.wgy.domain\"/&gt; &lt;!-- 指定jpa的实现提供者 --&gt; &lt;property name=\"persistenceProvider\"&gt; &lt;bean class=\"org.hibernate.jpa.HibernatePersistenceProvider\"/&gt; &lt;/property&gt; &lt;!--JPA供应商适配器 --&gt; &lt;property name=\"jpaVendorAdapter\"&gt; &lt;bean class=\"org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter\"&gt; &lt;!-- 是否生成DDL语句 是否自动建表 --&gt; &lt;property name=\"generateDdl\" value=\"true\"/&gt; &lt;!-- 数据库厂商名称 --&gt; &lt;property name=\"database\" value=\"MYSQL\"/&gt; &lt;!-- 数据库方言 --&gt; &lt;property name=\"databasePlatform\" value=\"org.hibernate.dialect.MySQLDialect\"/&gt; &lt;!-- 是否显示SQL --&gt; &lt;property name=\"showSql\" value=\"true\"/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 3.事务管理器 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.orm.jpa.JpaTransactionManager\"&gt; &lt;property name=\"entityManagerFactory\" ref=\"entityManagerFactory\"/&gt; &lt;/bean&gt; &lt;!-- 整合spring data jpa --&gt; &lt;!--spring 通过代理的方式为dao接口提供实现类，需要指明为哪些接口去产生代理类--&gt; &lt;jpa:repositories base-package=\"com.wgy.dao\" transaction-manager-ref=\"transactionManager\" entity-manager-factory-ref=\"entityManagerFactory\"/&gt; &lt;!-- 4.txAdvice --&gt; &lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"get*\" read-only=\"true\" propagation=\"SUPPORTS\"/&gt; &lt;tx:method name=\"find*\" read-only=\"true\" propagation=\"SUPPORTS\"/&gt; &lt;tx:method name=\"*\" propagation=\"REQUIRED\" read-only=\"false\"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 5.aop --&gt; &lt;aop:config&gt; &lt;aop:pointcut id=\"pointcut\" expression=\"execution(* com.wgy.service.impl.*.*(..))\"/&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"pointcut\"/&gt; &lt;/aop:config&gt;&lt;/beans&gt; 5.2.3.2 redis 配置文件12345678910111213141516171819202122232425262728293031323334&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- redis 相关配置 --&gt; &lt;bean id=\"poolConfig\" class=\"redis.clients.jedis.JedisPoolConfig\"&gt; &lt;!--最大空闲数--&gt; &lt;property name=\"maxIdle\" value=\"300\"/&gt; &lt;!--连接时的最大等待毫秒数--&gt; &lt;property name=\"maxWaitMillis\" value=\"3000\"/&gt; &lt;!--在提取一个jedis实例时，是否提前进行验证操作；如果为true，则得到的jedis实例均是可用的--&gt; &lt;property name=\"testOnBorrow\" value=\"false\"/&gt; &lt;/bean&gt; &lt;!--连接工厂--&gt; &lt;bean id=\"jedisConnectionFactory\" class=\"org.springframework.data.redis.connection.jedis.JedisConnectionFactory\" p:host-name=\"192.168.142.128\" p:port=\"6379\" p:pool-config-ref=\"poolConfig\" p:password=\"123456\"&gt; &lt;/bean&gt; &lt;!--redisTemplate--&gt; &lt;bean id=\"redisTemplate\" class=\"org.springframework.data.redis.core.RedisTemplate\"&gt; &lt;property name=\"connectionFactory\" ref=\"jedisConnectionFactory\"/&gt; &lt;!--指定数据序列化器--&gt; &lt;property name=\"keySerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.StringRedisSerializer\"/&gt; &lt;/property&gt; &lt;property name=\"valueSerializer\"&gt; &lt;bean class=\"org.springframework.data.redis.serializer.StringRedisSerializer\"/&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 5.2.3.3 es 配置文件1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:elasticsearch=\"http://www.springframework.org/schema/data/elasticsearch\" xsi:schemaLocation=\"http://www.springframework.org/schema/data/elasticsearch http://www.springframework.org/schema/data/elasticsearch/spring-elasticsearch.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 扫描dao包 --&gt; &lt;elasticsearch:repositories base-package=\"com.wgy.es\"/&gt; &lt;!-- 配置Client --&gt; &lt;elasticsearch:transport-client id=\"client\" cluster-nodes=\"192.168.142.128:9700\" cluster-name=\"wgy-es\"/&gt; &lt;!-- 配置搜索模板 --&gt; &lt;bean id=\"elasticsearchTemplate\" class=\"org.springframework.data.elasticsearch.core.ElasticsearchTemplate\"&gt; &lt;constructor-arg name=\"client\" ref=\"client\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 5.2.3.4 mongo 配置文件123456789101112131415161718192021222324&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:mongo=\"http://www.springframework.org/schema/data/mongo\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/data/mongo http://www.springframework.org/schema/data/mongo/spring-mongo.xsd\"&gt; &lt;!--包扫描--&gt; &lt;mongo:repositories base-package=\"com.wgy.mongo\"/&gt; &lt;!-- spring连接mongodb数据库的配置 --&gt; &lt;mongo:mongo-client host=\"192.168.142.128\" port=\"27017\" id=\"mongo\" credentials=\"bobo:123456@case\"&gt; &lt;mongo:client-options write-concern=\"SAFE\"/&gt; &lt;/mongo:mongo-client&gt; &lt;mongo:db-factory id=\"mongoDbFactory\" dbname=\"case\" mongo-ref=\"mongo\"/&gt; &lt;!--mongoTemplate--&gt; &lt;bean id=\"mongoTemplate\" class=\"org.springframework.data.mongodb.core.MongoTemplate\"&gt; &lt;constructor-arg name=\"mongoDbFactory\" ref=\"mongoDbFactory\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 5.2.3.5 汇总配置文件123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--汇总其它配置文件--&gt; &lt;import resource=\"applicationContext-jpa.xml\"/&gt; &lt;import resource=\"applicationContext-redis.xml\"/&gt; &lt;import resource=\"applicationContext-es.xml\"/&gt; &lt;import resource=\"applicationContext-mongo.xml\"/&gt;&lt;/beans&gt; 5.2.4 创建实体类5.2.4.1 Article（对应数据库文章表）1234567891011121314151617181920212223/** * 文章实体类 * * @author wgy */@Entity@Table(name = \"article\")public class Article implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer aid;//主键 private String title;//文章标题 private String author;//文章作者 private Date createTime;//创建时间 //建立从Article到ArticleData的一对一关系 @OneToOne(mappedBy = \"article\") private ArticleData articleData; //省略set和get方法。。。 //省略toString方法。。&#125; 5.2.4.2 ArticleData （对应数据库文章表）1234567891011121314151617181920212223/** * 文章详情实体类 * * @author wgy */@Entity@Table(name = \"article_data\")public class ArticleData implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id;//主键 private String content;//文章内容 //建立从ArticleData到Article的一对一关系 @OneToOne //@JoinColumn(name=当前表的外键字段名称,referencedColumnName 指向对方表的主键) @JoinColumn(name = \"articleId\", referencedColumnName = \"aid\", unique = true) private Article article; //省略set和get方法。。。 //省略toString方法。。&#125; 5.2.4.3 EsArticle ( 对应ES中的文章)123456789101112131415161718192021222324252627/** * es文章实体类 * * @author wgy */@Document(indexName = \"case\", type = \"article\")public class EsArticle implements Serializable &#123; @Id @Field(type = FieldType.Integer) private Integer id;//主键标识 @Field(type = FieldType.Text, analyzer = \"ik_max_word\", searchAnalyzer = \"ik_smart\", store = true) private String title;//标题 @Field(type = FieldType.Text, analyzer = \"ik_max_word\", searchAnalyzer = \"ik_smart\", store = true) private String content;//内容 @Field(type = FieldType.Text) private String author;//作者 @Field(type = FieldType.Date) private Date createTime;//创建时间 //省略set和get方法。。。 //省略toString方法。。&#125; 5.2.4.4 Comment ( 对应mongodb中的评论)1234567891011121314151617/** * 文章评论实体类 * * @author wgy */@Document(collection = \"comment\")public class Comment implements Serializable &#123; @Id private String cid;//主键 private Integer aid;//文章标识 private String comment;//评论 private String nickname;//评论者昵称 //省略set和get方法。。。 //省略toString方法。。&#125; 5.2.5 创建 dao层接口5.2.5.1 ArticleDao1234567/** * 文章dao接口 * * @author wgy */public interface ArticleDao extends JpaRepository&lt;Article, Integer&gt; &#123;&#125; 5.2.5.2 ArticleDataDao123456789101112131415161718192021222324252627/** * 文章详情dao接口 * * @author wgy */public interface ArticleDataDao extends JpaRepository&lt;ArticleData, Integer&gt; &#123; /** * 根据aid修改ArticleData中的content * Jpa规定如果想使用JPQL进行更新或者删除操作,必须要使用@Modifying显示声明 * * @param content * @param aid */ @Modifying @Query(\"update ArticleData ad set ad.content = ?1 where ad.article.aid = ?2\") void updateContentByAid(String content, Integer aid); /** * 根据aid删除ArticleData中的文章详情 * * @param aid */ @Modifying @Query(\"delete from ArticleData ad where ad.article.aid = ?1\") void deleteByAid(Integer aid);&#125; 5.2.5.3 EsArticleDao1234567891011121314151617/** * es文章dao接口 * * @author wgy */public interface EsArticleDao extends ElasticsearchRepository&lt;EsArticle, Integer&gt; &#123; /** * 根据title或者content进行查询 * * @param title * @param content * @param pageable * @return */ List&lt;EsArticle&gt; findByTitleOrContent(String title, String content, Pageable pageable);&#125; 5.2.5.4 CommentDao123456789101112131415/** * 文章评论dao接口 * * @author wgy */public interface CommentDao extends MongoRepository&lt;Comment, String&gt; &#123; /** * 命名规则查询:按照aid查询到一个文章的所有评论 * * @param aid * @return */ List&lt;Comment&gt; findByAid(Integer aid);&#125; 5.2.6 创建 service层5.2.6.1 ArticleService接口1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 文章业务接口 * * @author wgy */public interface ArticleService &#123; /** * 保存文章 * * @param article */ void saveArticle(Article article); /** * 更新文章 * * @param article */ void updateArticle(Article article); /** * 删除文章 * * @param aid */ void deleteByAid(Integer aid); /** * 最新文章列表 * * @return */ List&lt;Article&gt; findNewArticleList(); /** * 文章检索 * * @param pageNum * @param pageSize * @param keyword * @return */ List&lt;EsArticle&gt; search(Integer pageNum, Integer pageSize, String keyword);&#125; 5.2.6.2 ArticleServiceImpl 实现类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119/** * 文章业务实现类 * * @author wgy */@Servicepublic class ArticleServiceImpl implements ArticleService &#123; @Autowired private ArticleDao articleDao; @Autowired private ArticleDataDao articleDataDao; @Autowired private RedisTemplate&lt;String, String&gt; redisTemplate; @Autowired private EsArticleDao esArticleDao; @Autowired private CommentDao commentDao; @Override public void saveArticle(Article article) &#123; // 向mysql保存文章和文章详情 articleDataDao.save(article.getArticleData()); articleDao.save(article); // 清空redis缓存 redisTemplate.delete(\"articles\"); // 向ES中保存数据 EsArticle esArticle = new EsArticle(); esArticle.setCreateTime(article.getCreateTime()); esArticle.setContent(article.getArticleData().getContent()); esArticle.setTitle(article.getTitle()); esArticle.setAuthor(article.getAuthor()); esArticle.setId(article.getAid()); esArticleDao.save(esArticle); &#125; @Override public void updateArticle(Article article) &#123; //更新article Article articleParam = new Article(); articleParam.setTitle(article.getTitle()); articleParam.setAid(article.getAid()); articleParam.setAuthor(article.getAuthor()); articleParam.setCreateTime(article.getCreateTime()); articleDao.save(articleParam); //更新articleData articleDataDao.updateContentByAid(article.getArticleData().getContent(), article.getAid()); //清空redis缓存 redisTemplate.delete(\"articles\"); //向ES中保存数据 EsArticle esArticle = new EsArticle(); esArticle.setId(article.getAid()); esArticle.setCreateTime(article.getCreateTime()); esArticle.setContent(article.getArticleData().getContent()); esArticle.setTitle(article.getTitle()); esArticle.setAuthor(article.getAuthor()); esArticleDao.save(esArticle); &#125; @Override public void deleteByAid(Integer aid) &#123; //删除articleData articleDataDao.deleteByAid(aid); //删除article articleDao.deleteById(aid); //删除mongodb中相关的评论 //1---先根据aid查询到一个comment列表 List&lt;Comment&gt; comments = commentDao.findByAid(aid); //2---删除一个评论列表 commentDao.deleteAll(comments); //清空redis redisTemplate.delete(\"articles\"); //删除ES中的数据 esArticleDao.deleteById(aid); &#125; @Override public List&lt;Article&gt; findNewArticleList() &#123; //1 先从redis中获取 String value = redisTemplate.opsForValue().get(\"articles\"); //2 如果redis中没有,去数据库中查询,查询到以后,要存入redis if (StringUtils.isEmpty(value)) &#123; //设置分页排序条件 Pageable pageable = PageRequest.of(0, 10, Sort.by(Sort.Order.desc(\"createTime\"))); Page&lt;Article&gt; page = articleDao.findAll(pageable); List&lt;Article&gt; content = page.getContent(); //将结果转成String,存入redis if (content != null &amp;&amp; content.size() &gt; 0) &#123; value = JSONObject.toJSONString(content); redisTemplate.opsForValue().set(\"articles\", value); &#125; &#125; //3 将结果转成List返回 return JSONObject.parseArray(value, Article.class); &#125; @Override public List&lt;EsArticle&gt; search(Integer pageNum, Integer pageSize, String keyword) &#123; //设置分页条件 Pageable pageable = PageRequest.of(pageNum, pageSize); return esArticleDao.findByTitleOrContent(keyword, keyword, pageable); &#125;&#125; 5.2.6.3 CommentService 接口1234567891011121314151617181920212223242526272829/** * 文章评论业务接口 * * @author wgy */public interface CommentService &#123; /** * 保存 * * @param comment */ void saveComment(Comment comment); /** * 删除 * * @param cid */ void deleteByCid(String cid); /** * 根据文章标识查询评论数据 * * @param aid * @return */ List&lt;Comment&gt; findCommentsByAid(Integer aid);&#125; 5.2.6.4 CommentServiceImpl 实现类1234567891011121314151617181920212223242526272829/** * 文章评论业务实现类 * * @author wgy */@Servicepublic class CommentServiceImpl implements CommentService &#123; @Autowired private CommentDao commentDao; @Autowired private MongoTemplate mongoTemplate; @Override public void saveComment(Comment comment) &#123; commentDao.save(comment); &#125; @Override public void deleteByCid(String cid) &#123; commentDao.deleteById(cid); &#125; @Override public List&lt;Comment&gt; findCommentsByAid(Integer aid) &#123; return commentDao.findByAid(aid); &#125;&#125; 5.2.7 测试5.2.7.1 后台管理功能测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/** * 测后台管理功能测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class ManageTest &#123; @Autowired private ArticleService articleService; @Autowired private CommentService commentService; /** * 保存文章 */ @Test public void testSaveArticle() &#123; //准备测试数据 ArticleData articleData = new ArticleData(); articleData.setContent(\"1程序员专注于IT培训,Java培训,人工智能培训,Python培训,大数据培训,区\\n\" + \"块链培训,UI设计培训,PHP培训,Web前端培训,软件测试培训,产品经理培训，并提供Java培训,大数据培训,区块链培训,UI\\n\" + \"设计培训,PHP培训,软件测试培训,产品经理培训等服务。\"); Article article = new Article(); article.setTitle(\"1程序员介绍\"); article.setAuthor(\"1程序员\"); article.setCreateTime(new Date()); //建立两者关系 article.setArticleData(articleData); articleData.setArticle(article); articleService.saveArticle(article); &#125; /** * 更新文章 */ @Test public void testUpdateArticle() &#123; //准备测试数据 ArticleData articleData = new ArticleData(); articleData.setContent(\"2程序员专注于IT培训,Java培训,人工智能培训,Python培训,大数据培训,区\\n\" + \"块链培训,UI设计培训,PHP培训,Web前端培训,软件测试培训,产品经理培训，并提供Java培训,大数据培训,区块链培训,UI\\n\" + \"设计培训,PHP培训,软件测试培训,产品经理培训等服务。\"); Article article = new Article(); article.setAid(6); article.setTitle(\"6程序员介绍\"); article.setAuthor(\"6程序员\"); article.setCreateTime(new Date()); article.setArticleData(articleData); articleService.updateArticle(article); &#125; /** * 删除文章 */ @Test public void testDeleteArticle() &#123; articleService.deleteByAid(6); &#125; /** * 添加评论 */ @Test public void testSaveComment() &#123; Comment comment = new Comment(); comment.setCid(UUID.randomUUID().toString()); comment.setAid(3); comment.setComment(\"2程序员真棒!!!\"); comment.setNickname(\"2程序员\"); commentService.saveComment(comment); &#125; /** * 删除评论 */ @Test public void testDeleteComment() &#123; commentService.deleteByCid(\"3974c94a-384e-4b1f-9eb2-64d72896054d\"); &#125;&#125; 5.2.7.2 前台查看测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 前台查看测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class ViewTest &#123; @Autowired private ArticleService articleService; @Autowired private CommentService commentService; /** * 最新文章列表 */ @Test public void testFindNewArticleList() &#123; List&lt;Article&gt; list = articleService.findNewArticleList(); for (Article article : list) &#123; System.out.println(article); &#125; &#125; /** * 根据文章获取评论 */ @Test public void testFindCommentsByAid() &#123; List&lt;Comment&gt; comments = commentService.findCommentsByAid(3); for (Comment comment : comments) &#123; System.out.println(comment); &#125; &#125; /** * 文章全文检索 */ @Test public void testSearch() &#123; List&lt;EsArticle&gt; esArticles = articleService.search(0, 10, \"程序员\"); for (EsArticle esArticle : esArticles) &#123; System.out.println(esArticle); &#125; &#125;&#125;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"MySQL","slug":"MySQL","permalink":"https://wgy1993.gitee.io/tags/MySQL/"},{"name":"Redis","slug":"Redis","permalink":"https://wgy1993.gitee.io/tags/Redis/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://wgy1993.gitee.io/tags/MongoDB/"},{"name":"SpringData","slug":"SpringData","permalink":"https://wgy1993.gitee.io/tags/SpringData/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://wgy1993.gitee.io/tags/Elasticsearch/"}]},{"title":"SpringData(一)","date":"2020-09-23T14:19:01.000Z","path":"archives/a2f2c444.html","text":"1. SpringData概述1.1 持久层开发的问题随着互联网技术的发展，现在的企业开发中用到的用于数据存储的产品，不再仅仅是关系型数据库，而是要根据场景需要选择不同的存储技术，比如用于缓存热点数据的redis，用于存储文档数据的mongodb，用于支持强大搜索功能的elasticsearch等等。 在Java中，对于上面所说的产品都提供了优秀的访问技术。比如针对关系型数据库的mybatis、jpa等技术，针对于redis的jedis技术等等….. 这些技术虽然可以很好的针对各个存储产品进行访问操作，但同时也带来了新的问题，那就是不同的持久层技术的API是不一样的。 这样一来，开发人员就必须同时掌握多种数据访问技术，这无疑增加了开发成本。那么我们会想，有没有这样一种技术，它可以使用一套API支持各个不同的存储的访问呢？就在这样的需求下，SpringData产生了。 1.2 SpringData 简介1.2.1 什么是SpringDataSpringData是一个用来简化dao层开发的框架。它在保证了各个底层存储特性的同时，提供了一套统一的数据访问API。它可以很好的支持常用的关系型数据库和非关系型数据库。 使用SpringData作为dao层开发技术，将大大简化代码量，而且其API比各个技术的原生API更加简单易用。 1.2.2 SpringData的主要模块SpringData支持的持久层技术非常多，我们只介绍几个常见的： Spring Data common SpringData 的核心模块，定义了SpringData的核心功能 Spring Data JDBC 对JDBC的Spring Data存储库支持 Spring Data JPA 对JPA的Spring Data存储库支持 Spring Data MongoDB 对MongoDB的基于Spring对象文档的存储库支持 Spring Data Redis 封装Jedis技术，对redis实现访问操作 Spring Data Elasticsearch 对Elasticsearch实现访问操作 2. JPA回顾2.1 JPA 基础Hibernate 是一个全自动的ORM框架，是对 JDBC技术的封装。它在实体类和数据库表之间建立了映射关系，使得程序员可以使用面向对象编程思维来操纵数据库，而Hibernate会自动给我们生成 SQL语句。 JPA 的全称是 Java Persistence API，即 Java 持久化 API，是 SUN 公司推出的一套基于 ORM 的规范，注意不是 ORM 框架——因为 JPA 并未提供 ORM 实现，它只是提供了一些编程的 API 接口。 2.2 JPA 实战2.2.1 目标搭建Jpa环境，并实现一条数据的增删改查。 2.2.2 准备数据库环境12345678--准备数据库，创建一张文章表备用CREATE TABLE &#96;article&#96; ( &#96;aid&#96; int(11) NOT NULL auto_increment COMMENT &#39;主键&#39;, &#96;author&#96; varchar(255) default NULL COMMENT &#39;作者&#39;, &#96;createTime&#96; datetime default NULL COMMENT &#39;创建时间&#39;, &#96;title&#96; varchar(255) default NULL COMMENT &#39;标题&#39;, PRIMARY KEY (&#96;aid&#96;)); 2.2.3 创建 java工程，导入坐标1234567891011121314151617181920212223242526&lt;dependencies&gt; &lt;!--mysql--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--Jpa的支撑框架hibernate--&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;5.0.7.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 单元测试junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志log4j --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.2.4 创建实体类12345678public class Article implements Serializable &#123; private Integer aid; private String title; private String author; private Date createTime; //省略set和get方法。。。 //省略toString方法。。。&#125; 2.2.5 在实体类中配置映射关系1234567891011121314151617181920212223/** * 文章实体类 * * @author wgy */@Entity//告诉jpa这是一个实体类，需要把它跟数据库中的表做映射//使用注解建立实体类和数据表之间的对应关系@Table(name = \"article\")//@Table建立了实体类和数据表的关系 name指向表名public class Article implements Serializable &#123; @Id//标识这是主键字段 //指定主键生成策略，GenerationType.IDENTITY就是对应到mysql中的数据自增策略 @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer aid; //使用@Column映射类的属性和数据表的字段关系 name指定表中的字段名 //当类的属性名和数据表的字段名一致时，此注解可省略 @Column(name = \"author\") private String author; private Date createTime; private String title; //省略set和get方法。。。 //省略toString方法。。。&#125; 2.2.6 加入 JPA 的核心配置文件在maven工程的resources路径下创建一个名为META-INF的文件夹，在文件夹下创建一个名为persistence.xml的配置文件。注意： META-INF文件夹名称不能修改,persistence.xml文件名称不能改。 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;persistence xmlns=\"http://java.sun.com/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd\" version=\"2.0\"&gt; &lt;!--持久化单元 name 持久化单元的名称 唯一 transaction-type 事务类型 RESOURCE_LOCAL 本地事务 JTA 分布式事务 --&gt; &lt;persistence-unit name=\"springdata\" transaction-type=\"RESOURCE_LOCAL\"&gt; &lt;!--配置 JPA 规范的服务提供商,当项目中只有一个JPA的实现时,此选项可省略--&gt; &lt;provider&gt;org.hibernate.jpa.HibernatePersistenceProvider&lt;/provider&gt; &lt;!--指定实体类,此选项可省略--&gt; &lt;class&gt;com.wgy.domain.Article&lt;/class&gt; &lt;properties&gt; &lt;!--跟数据库相关的信息 驱动 url 用户名 密码--&gt; &lt;property name=\"javax.persistence.jdbc.driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"javax.persistence.jdbc.url\" value=\"jdbc:mysql:///springdata\"/&gt; &lt;property name=\"javax.persistence.jdbc.user\" value=\"root\"/&gt; &lt;property name=\"javax.persistence.jdbc.password\" value=\"root\"/&gt; &lt;!--jpa的核心配置中兼容hibernate的配置--&gt; &lt;!--是否显示SQL--&gt; &lt;property name=\"hibernate.show_sql\" value=\"true\"/&gt; &lt;!--是否格式化显示的SQL--&gt; &lt;property name=\"hibernate.format_sql\" value=\"true\"/&gt; &lt;!-- 自动建表 update 如果数据库存在数据表,就使用;不存在,就创建 create 不管数据库有没有数据表,每次SQL请求都会重新建表 --&gt; &lt;property name=\"hibernate.hbm2ddl.auto\" value=\"update\"/&gt; &lt;/properties&gt; &lt;/persistence-unit&gt;&lt;/persistence&gt; 2.2.7 测试2.2.7.1 实现保存操作123456789101112131415161718192021222324/** * 保存 */@Testpublic void testSave() &#123; //1 创建持久化管理器工厂 String persistenceUnitName = \"springdata\"; EntityManagerFactory factory = Persistence.createEntityManagerFactory(persistenceUnitName); //2 创建持久化管理器 EntityManager entityManager = factory.createEntityManager(); //3 获取事务 EntityTransaction transaction = entityManager.getTransaction(); transaction.begin(); //4 操作 Article article = new Article(); article.setTitle(\"测试文章标题\"); article.setAuthor(\"保存\"); article.setCreateTime(new Date()); entityManager.persist(article); //5 事务提交 transaction.commit(); //6 关闭资源 entityManager.close();&#125; 2.2.7.2 实现查询操作12345678910111213141516/** * 查询 */@Testpublic void testFindByAid() &#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory(\"springdata\"); EntityManager entityManager = factory.createEntityManager(); EntityTransaction transaction = entityManager.getTransaction(); transaction.begin(); Article article = entityManager.find(Article.class, 1); System.out.println(article); transaction.commit(); entityManager.close();&#125; 2.2.7.3 实现修改操作1234567891011121314151617/** * 更新 */@Testpublic void testUpdate() &#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory(\"springdata\"); EntityManager entityManager = factory.createEntityManager(); EntityTransaction transaction = entityManager.getTransaction(); transaction.begin(); Article article = entityManager.find(Article.class, 1); //修改 article.setAuthor(\"保存更新\"); transaction.commit(); entityManager.close();&#125; 2.2.7.4 实现删除操作1234567891011121314151617/** * 删除 */@Testpublic void testDelete() &#123; EntityManagerFactory factory = Persistence.createEntityManagerFactory(\"springdata\"); EntityManager entityManager = factory.createEntityManager(); EntityTransaction transaction = entityManager.getTransaction(); transaction.begin(); Article article = entityManager.find(Article.class, 1); //删除 entityManager.remove(article); transaction.commit(); entityManager.close();&#125; 2.3 JPA 的重要API介绍2.3.1 EntityManagerFactoryEntityManagerFactory接口主要用来创建EntityManager实例 EntityManagerFactory是一个线程安全的对象，并且其创建极其浪费资源，所以编程的时候要保持它是单例的。 2.3.2 EntityManager在JPA规范中,EntityManager是操作数据库的重要API，他是线程不安全的，需要保持线程独有。 重要方法说明： getTransaction: 获取事务对象 persist：保存操作 merge：更新操作 remove：删除操作 find/getReference：根据id查询 3. SpringData JPA基础3.1 SpringData JPA 简介SpringData JPA是Spring Data家族的一个成员，是Spring Data对JPA封装之后的产物，目的在于简化基于JPA的数据访问技术。使用SpringData JPA技术之后，开发者只需要声明Dao层的接口，不必再写实现类或其它代码，剩下的一切交给SpringData JPA来搞定 。 3.2 SpringData JPA 快速入门3.2.1 目标搭建SpringData JPA环境，并实现一条数据的增删改查。 3.2.2 准备数据环境下面的操作让JPA自动生成表结构 3.2.3 创建 java工程，导入坐标1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;dependencies&gt; &lt;!-- 日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 单元测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spring框架相关jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;5.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.7&lt;/version&gt; &lt;/dependency&gt; &lt;!--jpa--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;2.1.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;5.0.7.Final&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.2.4 创建实体类12345678public class Article implements Serializable &#123; private Integer aid; private String title; private String author; private Date createTime; //省略set和get方法。。。 //省略toString方法。。。&#125; 3.2.5 在实体类中配置映射关系1234567891011121314151617181920/** * 文章实体类 * * @author wgy */@Entity@Table(name = \"article\")public class Article implements Serializable &#123; @Id//声明当前私有属性为主键 @GeneratedValue(strategy = GenerationType.IDENTITY)//配置主键的生成策略 private Integer aid; //声明类的属性跟数据表字段的对应关系，如果属性名称和字段名称一致，可省略 @Column(name = \"author\") private String author; private Date createTime; private String title; //省略set和get方法。。。 //省略toString方法。。。&#125; 3.2.6 编写 dao接口使用 Spring Data JPA操作数据库，只需要按照框架的规范提供 dao 接口，不需要提供在接口中定义方法，也不需要为接口提供实现类就能完成基本的数据库的增删改查等功能。 在 Spring Data JPA 中，对于定义符合规范的 Dao 层接口，我们只需要遵循以下几点就可以了： 创建一个 Dao 层接口，并实现 JpaRepository 和 JpaSpecificationExecutor 提供相应的泛型 123456789/** * dao接口 * JpaRepository&lt;实体类类型，主键类型&gt;：用来完成基本 CRUD 操作 * JpaSpecificationExecutor&lt;实体类类型&gt;：用于复杂查询（分页等查询操作） * * @author wgy */public interface ArticleDao extends JpaRepository&lt;Article, Integer&gt;, JpaSpecificationExecutor&lt;Article&gt; &#123;&#125; 3.2.7 添加 Spring整合Jpa的配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:jdbc=\"http://www.springframework.org/schema/jdbc\" xmlns:jpa=\"http://www.springframework.org/schema/data/jpa\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/jdbc http://www.springframework.org/schema/jdbc/spring-jdbc.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd\"&gt; &lt;!-- 配置要扫描的包 --&gt; &lt;context:component-scan base-package=\"com.wgy\"/&gt; &lt;!-- 配置一个数据源 --&gt; &lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql:///springdata\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt; &lt;/bean&gt; &lt;!--配置EntityManagerFactory 可以产生entityManger--&gt; &lt;bean id=\"entityManagerFactory\" class=\"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean\"&gt; &lt;!--配置一个数据源--&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!--指定实体类--&gt; &lt;property name=\"packagesToScan\" value=\"com.wgy.domain\"/&gt; &lt;!--配置服务的提供商--&gt; &lt;property name=\"persistenceProvider\"&gt; &lt;bean class=\"org.hibernate.jpa.HibernatePersistenceProvider\"/&gt; &lt;/property&gt; &lt;!--SpringData Jpa 兼容Hibernate使用--&gt; &lt;property name=\"jpaVendorAdapter\"&gt; &lt;bean class=\"org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter\"&gt; &lt;!--配置数据库名称--&gt; &lt;property name=\"database\" value=\"MYSQL\"/&gt; &lt;!--是否自动建表 true 自动建表 false 不会自动建表--&gt; &lt;property name=\"generateDdl\" value=\"true\"/&gt; &lt;!--是否显示SQL--&gt; &lt;property name=\"showSql\" value=\"true\"/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!--声明事务管理器--&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.orm.jpa.JpaTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;/bean&gt; &lt;!--做一个jpa:repository的配置--&gt; &lt;!--base-package 配置dao包的包名 它会为这个包先所有的接口动态产生代理对象--&gt; &lt;jpa:repositories base-package=\"com.wgy.dao\" entity-manager-factory-ref=\"entityManagerFactory\" transaction-manager-ref=\"transactionManager\"/&gt;&lt;/beans&gt; 3.2.8 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * SpringData Jpa测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class SpringDataJpaTest &#123; @Autowired private ArticleDao articleDao; /** * 保存 */ @Test public void testSave() &#123; Article article = new Article(); article.setTitle(\"SpringData Jpa保存测试1\"); article.setAuthor(\"测试\"); article.setCreateTime(new Date()); articleDao.save(article); &#125; /** * 查询主键 */ @Test public void testFindByAid() &#123; Optional&lt;Article&gt; optional = articleDao.findById(1); System.out.println(optional.get()); &#125; /** * 查询所有 */ @Test public void testFindAll() &#123; List&lt;Article&gt; articles = articleDao.findAll(); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 修改 */ @Test public void testUpdate() &#123; Article article = new Article(); article.setAuthor(\"测试2\"); article.setAid(2); //Spingdata Jpa的保存和修改使用的都是save方法 //关键来看传入的实体是否有主键 //---如果有主键,代表要修改 //---如果没有主键,代表要保存 articleDao.save(article); &#125; /** * 删除 */ @Test public void testDelete() &#123; articleDao.deleteById(2); &#125;&#125; 3.3 SpringData Jpa 运行原理分析3.3.1 SpringData中的几个重要接口思考一个问题：自定义的接口中没有写任何的方法声明，那么测试类中调用的接口中的方法是哪来的呢？ 123456789101112131415自定义的接口继承了两个接口，方法肯定来自里面，追踪关系得到下面的继承关系 Repository 标记接口：继承了此接口后会被Spring识别，进而可以在接口中声明一些满足规范的方法 | | CrudRepository 实现了基本增删改查方法 | | PagingAndSortingRepository 实现了分页和排序的方法 | | JpaRepository 重写了几个查找和删除的方法 | | ArticleDao通过上面的继承关系，我们可以看到我们自定义的接口ArticleDao继承了一系列的Repository接口，而每一个接口都会给我们提供一部分的功能，这样继承下来，我们的ArticleDao不用任何的方法声明就拥有了很多的功能了。 3.3.2 SpringData Jpa 底层运行原理思考一个问题：我们找到了定义方法的接口，但并没有看到实现类，没有实现来就无法创建对象，那么真正干活的实现类到底在哪，它又是如何产生对象的呢？ 下面我们通过debug的形式，寻找答案： 1、在运行时，Spring会使用JdkDynamicAopProxy为dao接口生成一个代理对象 2、那么这个代理对象是根据那个类代理出来的呢？点击进入JdkDynamicAopProxy源码查看invoke方法，发现targetSource代理的是SimpleJpaRepository类 3、通过对SimpleJpaRepository中代码的分析，我们看到最终执行保存的是EntityManager对象 总结：使用 SpringData JPA开发底层还是用的JPA的API，SpringData JPA只是对标准 JPA 操作进行了进一步封装，已达到简化了Dao层代码开发的目的。 3.3.3 SpringData Jpa 与 Jpa 及 Hibernate的关系 3.3.4 SpringData Jpa CUD方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102/** * SpringData Jpa CUD测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class SpringDataJpaCUDTest &#123; @Autowired private ArticleDao articleDao; /** * 保存 */ @Test public void testSave() &#123; Article article = new Article(); article.setTitle(\"测试保存\"); article.setAuthor(\"测试\"); article.setCreateTime(new Date()); //保存一个实体 articleDao.save(article); //保存一个实体,并且立即刷新缓存 //articleDao.saveAndFlush(article); &#125; /** * 保存多个 */ @Test public void testSaveAll() &#123; Article article1 = new Article(); article1.setTitle(\"测试保存1\"); article1.setAuthor(\"测试\"); article1.setCreateTime(new Date()); Article article2 = new Article(); article2.setTitle(\"测试保存2\"); article2.setAuthor(\"测试\"); article2.setCreateTime(new Date()); Article article3 = new Article(); article3.setTitle(\"测试保存3\"); article3.setAuthor(\"测试\"); article3.setCreateTime(new Date()); List list = new ArrayList(); list.add(article1); list.add(article2); list.add(article3); //保存多个实体 articleDao.saveAll(list); &#125; /** * 删除 */ @Test public void testDeleteOne() &#123; //1 根据主键删除 //articleDao.deleteById(13); //2 根据实体删除,但是这个实体必须要有主键 Article article = new Article(); article.setAid(13); articleDao.delete(article); &#125; /** * 删除 */ @Test public void testDeleteAll() &#123; //1 删除所有 先查询--再一条条的删除 //articleDao.deleteAll(); //2 删除所有 一下子删除所有记录 //articleDao.deleteAllInBatch(); Article article1 = new Article(); article1.setAid(24); Article article2 = new Article(); article2.setAid(22); List list = new ArrayList(); list.add(article1); list.add(article2); //3 批量删除指定数据 一条语句搞定 //articleDao.deleteInBatch(list); //4 先一条条的查,然后再一条条的删除 articleDao.deleteAll(list); &#125;&#125; 4. SpringData JPA的多种查询方式4.1 父接口方法查询我们自定义的Dao接口可以使用它的父接口提供的方法，可以使用的方法如下图所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102/** * SpringData Jpa Query测试 * 父接口方法查询 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class Query1Test &#123; @Autowired private ArticleDao articleDao; /** * 根据主键查询 */ @Test public void testFindById() &#123; //根据一个主键查询 Optional&lt;Article&gt; optional = articleDao.findById(21); System.out.println(optional.get()); //根据多个主键查询 List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(21); list.add(23); list.add(25); List&lt;Article&gt; articles = articleDao.findAllById(list); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 查询所有 */ @Test public void testFindAll() &#123; List&lt;Article&gt; articles = articleDao.findAll(); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 查询所有--排序 */ @Test public void testFindAllWithSort() &#123; //按照aid倒序排列 Sort sort = Sort.by(Sort.Order.desc(\"aid\")); List&lt;Article&gt; articles = articleDao.findAll(sort); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 查询所有--分页 */ @Test public void testFindAllWithPage() &#123; //处理分页条件 //page 当前是第几页(从0开始) size 每页大小 Pageable pageable = PageRequest.of(0, 2); Page&lt;Article&gt; page = articleDao.findAll(pageable); //总记录数 总页数 每页多少 System.out.println(\"总记录数:\" + page.getTotalElements()); System.out.println(\"总页数:\" + page.getTotalPages()); System.out.println(\"每页多少:\" + page.getSize()); //当前页的元素 List&lt;Article&gt; content = page.getContent(); for (Article article : content) &#123; System.out.println(article); &#125; &#125; /** * 查询所有--分页+排序 */ @Test public void testFindAllWithPageAndPage() &#123; //按照aid倒序排列 Sort sort = Sort.by(Sort.Order.desc(\"aid\")); //处理分页条件 //page 当前是第几页(从0开始) size 每页大小 Pageable pageable = PageRequest.of(0, 2, sort); Page&lt;Article&gt; page = articleDao.findAll(pageable); //总记录数 总页数 每页多少 System.out.println(\"总记录数:\" + page.getTotalElements()); System.out.println(\"总页数:\" + page.getTotalPages()); System.out.println(\"每页多少:\" + page.getSize()); //当前页的元素 List&lt;Article&gt; content = page.getContent(); for (Article article : content) &#123; System.out.println(article); &#125; &#125;&#125; 4.2 方法命名规则查询顾名思义，方法命名规则查询就是根据方法的名字，就能创建查询。只需要按照SpringData JPA提供的方法命名规则定义方法的名称，就可以完成查询工作。 SpringData JPA在程序执行的时候会根据方法名称进行解析，并自动生成查询语句进行查询. 按照SpringData JPA定义的规则，查询方法以findBy开头，涉及条件查询时，条件的属性用条件关键字连接，要注意的是：条件属性首字母需大写。框架在进行方法名解析时，会先把方法名多余的前缀截取掉，然后对剩下部分进行解析。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * dao接口 * JpaRepository&lt;实体类类型，主键类型&gt;：用来完成基本 CRUD 操作 * JpaSpecificationExecutor&lt;实体类类型&gt;：用于复杂查询（分页等查询操作） * * @author wgy */public interface ArticleDao extends JpaRepository&lt;Article, Integer&gt;, JpaSpecificationExecutor&lt;Article&gt; &#123; /** * 根据标题查询 * * @param title * @return */ List&lt;Article&gt; findByTitle(String title); /** * 根据标题模糊查询 * * @param title * @return */ List&lt;Article&gt; findByTitleLike(String title); /** * 根据标题和作者查询 * * @param title * @param author * @return */ List&lt;Article&gt; findByTitleAndAuthor(String title, String author); /** * 根据ID范围查询 &lt; * * @param aid * @return */ List&lt;Article&gt; findByAidIsLessThan(Integer aid); /** * 根据ID范围查询 between * * @param startAid * @param endAid * @return */ List&lt;Article&gt; findByAidBetween(Integer startAid, Integer endAid); /** * 根据ID范围查询 in * * @param aids * @return */ List&lt;Article&gt; findByAidIn(List&lt;Integer&gt; aids); /** * 根据创建时间之后查询 * * @param createTime * @return */ List&lt;Article&gt; findByCreateTimeAfter(Date createTime);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/** * SpringData Jpa Query测试 * 方法命名规则查询 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class Query2Test &#123; @Autowired private ArticleDao articleDao; /** * 根据标题查询 */ @Test public void testFindByTitle() &#123; List&lt;Article&gt; articles = articleDao.findByTitle(\"测试\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据标题模糊查询 */ @Test public void testFindByTitleLike() &#123; List&lt;Article&gt; articles = articleDao.findByTitleLike(\"%测试%\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据标题和作者查询 */ @Test public void testFindByTitleAndAuthor() &#123; List&lt;Article&gt; articles = articleDao.findByTitleAndAuthor(\"测试\", \"测试\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据ID范围查询 &lt; */ @Test public void testFindByAidIsLessThan() &#123; List&lt;Article&gt; articles = articleDao.findByAidIsLessThan(25); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据ID范围查询 between */ @Test public void testFindByAidBetween() &#123; List&lt;Article&gt; articles = articleDao.findByAidBetween(25, 30); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据ID范围查询 in */ @Test public void testFindByAidIn() &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(29); list.add(30); List&lt;Article&gt; articles = articleDao.findByAidIn(list); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 根据创建时间之后查询 */ @Test public void testFindByCreateTimeAfter() &#123; List&lt;Article&gt; articles = articleDao.findByCreateTimeAfter(new Date()); for (Article article : articles) &#123; System.out.println(article); &#125; &#125;&#125; 关键字 例子 对应的JPQL语句 And findByLastnameAndFirstname … where x.lastname = ?1 and x.firstname = ? 2 Or findByLastnameOrFirstname … where x.lastname = ?1 or x.firstname = ?2 Is,Equals indByFirstnameIs,findByFirstnameEquals … where x.firstname = ?1 Between findByStartDateBetween … where x.startDate between ?1 and ?2 LessThan findByAgeLessThan … where x.age &lt; ?1 LessThanEqual findByAgeLessThanEqual … where x.age &lt;= ?1 GreaterThan findByAgeGreaterThan … where x.age &gt; ?1 GreaterThanEqual findByAgeGreaterThanEqual … where x.age &gt;= ?1 After findByStartDateAfter … where x.startDate &gt; ?1 Before findByStartDateBefore … where x.startDate &lt; ?1 IsNull findByAgeIsNull … where x.age is null IsNotNull,NotNull findByAge(Is)NotNull … where x.age not null Like findByFirstnameLike … where x.firstname like ?1 NotLike findByFirstnameNotLike … where x.firstname not like ?1 StartingWith findByFirstnameStartingWith … where x.firstname like ?1(parameter bound with appended %) EndingWith findByFirstnameEndingWith … where x.firstname like ?1(parameter bound with prepended %) Containing findByFirstnameContaining … where x.firstname like ?1(parameter bound wrapped in %) OrderBy findByAgeOrderByLastnameDesc … where x.age = ?1 order by x.lastname desc Not findByLastnameNot … where x.lastname &lt;&gt; ?1 In findByAgeIn(Collection ages) … where x.age in ?1 NotIn findByAgeNotIn(Collection age) … where x.age not in ?1 TRUE findByActiveTrue() … where x.active = true FALSE findByActiveFalse() … where x.active = false IgnoreCase findByFirstnameIgnoreCase … where UPPER(x.firstame) = U 4.3 JPQL 查询使用SpringData JPA提供的查询方法已经可以解决大部分的应用场景，但是对于某些业务来说，我们还需要灵活的构造查询条件，这时就可以使用@Query注解，结合JPQL的语句方式完成查询。 JPQL，全称是Java Persistence Query Language。JPQL语句是JPA中定义的一种查询语言，此种语言的用意是让开发者忽略数据库表和表中的字段，而关注实体类及实体类中的属性。 它的写法十分类似于SQL语句的写法，但是要把查询的表名换成实体类名称，把表中的字段名换成实体类的属性名称。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * dao接口 * JpaRepository&lt;实体类类型，主键类型&gt;：用来完成基本 CRUD 操作 * JpaSpecificationExecutor&lt;实体类类型&gt;：用于复杂查询（分页等查询操作） * * @author wgy */public interface ArticleDao extends JpaRepository&lt;Article, Integer&gt;, JpaSpecificationExecutor&lt;Article&gt; &#123; /** * 展示位置参数绑定[按照title和author查询] * 占位符从1开始 * * @param title * @param author * @return */ @Query(\"from Article a where a.title = ?1 and a.author=?2\") List&lt;Article&gt; findByCondition1(String title, String author); /** * 展示名字参数绑定 * * @param title * @param author * @return */ @Query(\"from Article a where a.title = :title and a.author = :authors\") List&lt;Article&gt; findByCondition2(@Param(\"title\") String title, @Param(\"authors\") String author); /** * 展示like模糊查询 * * @param title * @return */ @Query(\"from Article a where a.title like %:title%\") List&lt;Article&gt; findByCondition3(@Param(\"title\") String title); /** * 展示排序查询 * * @param title * @return */ @Query(\"from Article a where a.title like %:title% order by a.aid desc \") List&lt;Article&gt; findByCondition4(@Param(\"title\") String title); /** * 展示分页查询 * * @param pageable * @param title * @return */ @Query(\"from Article a where a.title like %:title%\") List&lt;Article&gt; findByCondition5(Pageable pageable, @Param(\"title\") String title); /** * 展示传入集合参数查询 * * @param aids * @return */ @Query(\"from Article a where a.aid in :aids\") List&lt;Article&gt; findByCondition6(@Param(\"aids\") List&lt;Integer&gt; aids); /** * 展示传入Bean进行查询（SPEL表达式查询） * * @param article * @return */ @Query(\"from Article a where a.title = :#&#123;#article.title&#125; and a.author = :#&#123;#article.author&#125;\") List&lt;Article&gt; findByCondition7(@Param(\"article\") Article article);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * SpringData Jpa Query测试 * JPQL 查询 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class Query3Test &#123; @Autowired private ArticleDao articleDao; /** * 展示位置参数绑定 */ @Test public void testFindByCondition1() &#123; List&lt;Article&gt; articles = articleDao.findByCondition1(\"测试保存1\", \"测试\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 展示名字参数绑定 */ @Test public void testFindByCondition2() &#123; List&lt;Article&gt; articles = articleDao.findByCondition2(\"测试保存1\", \"测试\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 展示like模糊查询 */ @Test public void testFindByCondition3() &#123; List&lt;Article&gt; articles = articleDao.findByCondition3(\"测试\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 展示排序查询 */ @Test public void testFindByCondition4() &#123; List&lt;Article&gt; articles = articleDao.findByCondition4(\"测试\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 展示分页查询 */ @Test public void testFindByCondition5() &#123; Pageable pageable = PageRequest.of(0, 3); List&lt;Article&gt; articles = articleDao.findByCondition5(pageable, \"测试\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 展示传入集合参数查询 */ @Test public void testFindByCondition6() &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(29); list.add(30); List&lt;Article&gt; articles = articleDao.findByCondition6(list); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 展示传入Bean进行查询 */ @Test public void testFindByCondition7() &#123; Article articleParam = new Article(); articleParam.setTitle(\"测试保存1\"); articleParam.setAuthor(\"测试\"); List&lt;Article&gt; articles = articleDao.findByCondition7(articleParam); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 本地SQL查询 */ @Test public void testFindByCondition8() &#123; List&lt;Article&gt; articles = articleDao.findByCondition8(\"测试保存1\", \"测试\"); for (Article article : articles) &#123; System.out.println(article); &#125; &#125;&#125; 4.4 本地 SQL查询123456789/** * 本地SQL查询 * * @param title * @param author * @return */@Query(value = \"select * from article a where a.title = ?1 and a.author =?2\", nativeQuery = true)List&lt;Article&gt; findByCondition8(String title, String author); 4.5 Specifications 动态查询有时我们在查询某个实体的时候，给定的条件是不固定的，这时就需要动态构建相应的查询语句，在 Spring Data JPA 中可以通过 JpaSpecificationExecutor 接口查询。相比 JPQL,其优势是类型安全,更加的面向对象，缺点是书写比较麻烦。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145/** * SpringData Jpa Query测试 * Specifications 动态查询 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class Query4Test &#123; @Autowired private ArticleDao articleDao; /** * 按照标题和作者进行查询,以不为空的属性作为查询条件 */ @Test public void testFindAll() &#123; //就模拟从从外边传入的变量 String title = \"测试保存1\"; String author = \"测试\"; List&lt;Article&gt; articles = articleDao.findAll(new Specification&lt;Article&gt;() &#123; /** * @param root 代表实体对象,我们可以通过它获取属性值 * @param cq 用于生成SQL语句 * @param cb 用于拼接查询条件 * @return */ @Override public Predicate toPredicate(Root&lt;Article&gt; root, CriteriaQuery&lt;?&gt; cq, CriteriaBuilder cb) &#123; List&lt;Predicate&gt; list = new ArrayList&lt;&gt;(); if (!StringUtils.isEmpty(title)) &#123; //拼接作为查询条件 Predicate predicate = cb.like(root.get(\"title\").as(String.class), \"%\" + title + \"%\"); list.add(predicate); &#125; if (!StringUtils.isEmpty(author)) &#123; //拼接作为查询条件 Predicate predicate = cb.equal(root.get(\"author\").as(String.class), author); list.add(predicate); &#125; return cb.and(list.toArray(new Predicate[]&#123;&#125;)); &#125; &#125;); for (Article article : articles) &#123; System.out.println(article); &#125; &#125; /** * 分页 */ @Test public void testFindAllWithPage() &#123; //就模拟从从外边传入的变量 String title = \"\"; String author = \"\"; //分页 Pageable pageable = PageRequest.of(0, 3); Page&lt;Article&gt; page = articleDao.findAll(new Specification&lt;Article&gt;() &#123; /** * @param root 代表实体对象,我们可以通过它获取属性值 * @param cq 用于生成SQL语句 * @param cb 用于拼接查询条件 * @return */ @Override public Predicate toPredicate(Root&lt;Article&gt; root, CriteriaQuery&lt;?&gt; cq, CriteriaBuilder cb) &#123; List&lt;Predicate&gt; list = new ArrayList&lt;&gt;(); if (!StringUtils.isEmpty(title)) &#123; //拼接作为查询条件 Predicate predicate = cb.like(root.get(\"title\").as(String.class), \"%\" + title + \"%\"); list.add(predicate); &#125; if (!StringUtils.isEmpty(author)) &#123; //拼接作为查询条件 Predicate predicate = cb.equal(root.get(\"author\").as(String.class), author); list.add(predicate); &#125; return cb.and(list.toArray(new Predicate[]&#123;&#125;)); &#125; &#125;, pageable); for (Article article : page.getContent()) &#123; System.out.println(article); &#125; &#125; /** * 分页排序 */ @Test public void testFindAllWithPageAndSort() &#123; //就模拟从从外边传入的变量 String title = \"\"; String author = \"\"; //分页 Pageable pageable = PageRequest.of(0, 3, Sort.by(Sort.Order.desc(\"aid\"))); Page&lt;Article&gt; page = articleDao.findAll(new Specification&lt;Article&gt;() &#123; /** * @param root 代表实体对象,我们可以通过它获取属性值 * @param cq 用于生成SQL语句 * @param cb 用于拼接查询条件 * @return */ @Override public Predicate toPredicate(Root&lt;Article&gt; root, CriteriaQuery&lt;?&gt; cq, CriteriaBuilder cb) &#123; List&lt;Predicate&gt; list = new ArrayList&lt;&gt;(); if (!StringUtils.isEmpty(title)) &#123; //拼接作为查询条件 Predicate predicate = cb.like(root.get(\"title\").as(String.class), \"%\" + title + \"%\"); list.add(predicate); &#125; if (!StringUtils.isEmpty(author)) &#123; //拼接作为查询条件 Predicate predicate = cb.equal(root.get(\"author\").as(String.class), author); list.add(predicate); &#125; return cb.and(list.toArray(new Predicate[]&#123;&#125;)); &#125; &#125;, pageable); for (Article article : page.getContent()) &#123; System.out.println(article); &#125; &#125;&#125; 5. SpringData JPA实现多表操作5.1 多表关系分析数据库中多表之间存在着三种关系，如图所示。 123456789从图可以看出，系统设计的三种实体关系分别为： 多对多、一对多和一对一关系。注意：一对多关系可以看为两种： 即一对多，多对一。所以说四种更精确。在实际开发中，我们数据库的表难免会有相互的关联关系，在操作表的时候就有可能会涉及到多张表的操作。而在这种实现了ORM思想的框架中（如 JPA），可以让我们通过操作实体类就实现对数据库表的操作。所以今天我们的学习重点是：掌握配置实体之间的关联关系第一步：首先确定两张表之间的关系第二步：在实体类中描述出两个实体的关系第三步：配置出实体类和数据库表的关系映射（重点） 5.2 案例表间关系 5.3 一对一关系5.3.1 数据环境article和article_data的一对一关系 5.3.2 创建实体类，并配置表间关系5.3.2.1 创建文章类1234567891011121314151617181920212223242526272829303132/** * 文章实体类 * * @author wgy */@Entity@Table(name = \"article\")public class Article implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer aid; @Column(name = \"author\") private String author; private Date createTime; private String title; //1声明类间关系 //声明主动放弃关系维护 mappedBy=\"当前类在对方类中的属性名\" //设置级联操作,当保存Article的时候,同时保存ArticleData @OneToOne(mappedBy = \"article\", cascade = CascadeType.PERSIST) private ArticleData articleData; //2 在类中使用注解再声明表间关系 // --书写注解 // --明确谁来维护关系(在多的一方维护关系) // ----在维护的一方主动声明维护策略,在不维护的一方声明主动放弃 //省略set和get方法。。。 //省略toString方法。。。&#125; 5.3.2.2 创建文章详情类123456789101112131415161718192021222324/** * 文章详情实体类 * * @author wgy */@Entity@Table(name = \"article_data\")public class ArticleData implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; private String content; //让这个实体维护关系 //name 当前表中的外键名 //referencedColumnName 指向的对方表中的主键名 @OneToOne @JoinColumn(name = \"articleId\", referencedColumnName = \"aid\", unique = true) private Article article; //省略set和get方法。。。 //省略toString方法。。。&#125; 5.3.3 添加 ArticleDao接口123456789/** * dao接口 * JpaRepository&lt;实体类类型，主键类型&gt;：用来完成基本 CRUD 操作 * JpaSpecificationExecutor&lt;实体类类型&gt;：用于复杂查询（分页等查询操作） * * @author wgy */public interface ArticleDao extends JpaRepository&lt;Article, Integer&gt;, JpaSpecificationExecutor&lt;Article&gt; &#123;&#125; 5.3.4 测试123456789101112131415161718192021222324252627282930313233343536/** * SpringData Jpa测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class One2OneTest &#123; @Autowired private ArticleDao articleDao; /** * 保存 */ @Test public void testSave() &#123; //创建文章对象 Article article = new Article(); article.setTitle(\"测试好文章\"); article.setAuthor(\"测试\"); article.setCreateTime(new Date()); //创建文章内容对象 ArticleData articleData = new ArticleData(); articleData.setContent(\"真是一篇好文章\"); //建立两个对象间的关系 article.setArticleData(articleData); articleData.setArticle(article); //保存操作 articleDao.save(article); &#125;&#125; 5.4 一对多关系5.4.1 数据环境article和comment的一对多关系 5.4.2 创建实体类，并配置表间关系5.4.2.1 修改文章类，添加文章跟评论的映射12345678910111213141516171819202122232425262728293031323334353637/** * 文章实体类 * * @author wgy */@Entity@Table(name = \"article\")public class Article implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer aid; @Column(name = \"author\") private String author; private Date createTime; private String title; //建立文章对评论的一对多关系 //在一的一方声明放弃维护关系 @OneToMany(mappedBy = \"article\") private Set&lt;Comment&gt; comments = new HashSet&lt;&gt;(0); //1声明类间关系 //声明主动放弃关系维护 mappedBy=\"当前类在对方类中的属性名\" //设置级联操作,当保存Article的时候,同时保存ArticleData @OneToOne(mappedBy = \"article\", cascade = CascadeType.PERSIST) private ArticleData articleData; //2 在类中使用注解再声明表间关系 // --书写注解 // --明确谁来维护关系(在多的一方维护关系) // ----在维护的一方主动声明维护策略,在不维护的一方声明主动放弃 //省略set和get方法。。。 //省略toString方法。。。&#125; 5.4.2.2 创建文章评论类1234567891011121314151617181920212223/** * 文章评论实体类 * * @author wgy */@Entity@Table(name = \"comment\")public class Comment implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer cid; private String comment; //建立评论到文章的多对一关系 //在多的一方维护关系 @ManyToOne @JoinColumn(name = \"aid\", referencedColumnName = \"aid\") private Article article; //省略set和get方法。。。 //省略toString方法。。。&#125; 5.4.3 添加 CommentDao接口123456789/** * dao接口 * JpaRepository&lt;实体类类型，主键类型&gt;：用来完成基本 CRUD 操作 * JpaSpecificationExecutor&lt;实体类类型&gt;：用于复杂查询（分页等查询操作） * * @author wgy */public interface CommentDao extends JpaRepository&lt;Comment, Integer&gt;, JpaSpecificationExecutor&lt;Comment&gt; &#123;&#125; 5.4.4 测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * SpringData Jpa测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class One2ManyTest &#123; @Autowired private ArticleDao articleDao; @Autowired private CommentDao commentDao; /** * 保存 */ @Test public void testSave() &#123; //创建文章对象 Article article = new Article(); article.setTitle(\"测试好文章\"); article.setAuthor(\"测试\"); article.setCreateTime(new Date()); //创建文章评论对象 Comment comment1 = new Comment(); comment1.setComment(\"真不错\"); Comment comment2 = new Comment(); comment2.setComment(\"挺好的\"); //建立两个对象间的关系 comment1.setArticle(article); comment2.setArticle(article); Set&lt;Comment&gt; comments = new HashSet&lt;&gt;(); comments.add(comment1); comments.add(comment2); article.setComments(comments); //保存操作 articleDao.save(article); commentDao.save(comment1); commentDao.save(comment2); &#125;&#125; 5.5 多对多关系5.5.1 数据环境article跟type之间的多对多关系 5.5.2 创建实体类，并配置表间关系5.5.2.1 修改文章类，添加文章跟评论用户的多对多关系12345678910111213141516171819202122232425262728293031323334353637383940/** * 文章实体类 * * @author wgy */@Entity@Table(name = \"article\")public class Article implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer aid; @Column(name = \"author\") private String author; private Date createTime; private String title; @ManyToMany(mappedBy = \"articles\") private Set&lt;Type&gt; types = new HashSet&lt;&gt;(0); //建立文章对评论的一对多关系 //在一的一方声明放弃维护关系 @OneToMany(mappedBy = \"article\") private Set&lt;Comment&gt; comments = new HashSet&lt;&gt;(0); //1声明类间关系 //声明主动放弃关系维护 mappedBy=\"当前类在对方类中的属性名\" //设置级联操作,当保存Article的时候,同时保存ArticleData @OneToOne(mappedBy = \"article\", cascade = CascadeType.PERSIST) private ArticleData articleData; //2 在类中使用注解再声明表间关系 // --书写注解 // --明确谁来维护关系(在多的一方维护关系) // ----在维护的一方主动声明维护策略,在不维护的一方声明主动放弃 //省略set和get方法。。。 //省略toString方法。。。&#125; 5.5.2.2 创建文章用户类12345678910111213141516171819202122232425262728/** * 用户实体类 * * @author wgy */@Entity@Table(name = \"type\")public class Type implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer tid; private String name; @ManyToMany @JoinTable( //代表中间表名称 name = \"article_type\", //中间表的外键对应到当前表的主键名称 joinColumns = &#123;@JoinColumn(name = \"tid\", referencedColumnName = \"tid\")&#125;, //中间表的外键对应到对方表的主键名称 inverseJoinColumns = &#123;@JoinColumn(name = \"aid\", referencedColumnName = \"aid\")&#125; ) private Set&lt;Article&gt; articles = new HashSet&lt;&gt;(0); //省略set和get方法。。。 //省略toString方法。。。&#125; 5.5.3 添加 TypeDao接口123456789/** * dao接口 * JpaRepository&lt;实体类类型，主键类型&gt;：用来完成基本 CRUD 操作 * JpaSpecificationExecutor&lt;实体类类型&gt;：用于复杂查询（分页等查询操作） * * @author wgy */public interface TypeDao extends JpaRepository&lt;Type, Integer&gt;, JpaSpecificationExecutor&lt;Type&gt; &#123;&#125; 5.5.4 测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * SpringData Jpa测试 * * @author wgy */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath:applicationContext.xml\")public class Many2ManyTest &#123; @Autowired private ArticleDao articleDao; @Autowired private TypeDao typeDao; /** * 保存 */ @Test public void testSave() &#123; //创建文章对象 Article article1 = new Article(); article1.setTitle(\"测试好文章11\"); article1.setAuthor(\"测试11\"); article1.setCreateTime(new Date()); Article article2 = new Article(); article2.setTitle(\"测试好文章22\"); article2.setAuthor(\"测试22\"); article2.setCreateTime(new Date()); //创建文章类型对象 Type type1 = new Type(); type1.setName(\"军事\"); Type type2 = new Type(); type2.setName(\"民政\"); //建立两个对象间的关系 Set&lt;Type&gt; types = new HashSet&lt;&gt;(); types.add(type1); types.add(type2); article1.setTypes(types); article2.setTypes(types); Set&lt;Article&gt; articles = new HashSet&lt;&gt;(); articles.add(article1); articles.add(article2); type1.setArticles(articles); type2.setArticles(articles); //保存操作 articleDao.save(article1); articleDao.save(article2); typeDao.save(type1); typeDao.save(type2); &#125;&#125;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"SpringData","slug":"SpringData","permalink":"https://wgy1993.gitee.io/tags/SpringData/"}]},{"title":"MongoDB(二)","date":"2020-09-18T08:54:22.000Z","path":"archives/afe7047c.html","text":"1. 副本集-Replica Sets1.1 简介MongoDB中的副本集（Replica Set）是一组维护相同数据集的mongod服务。 副本集可提供冗余和高可用性，是所有生产部署的基础。 也可以说，副本集类似于有自动故障恢复功能的主从集群。通俗的讲就是用多台机器进行同一数据的异步同步，从而使多台机器拥有同一数据的多个副本，并且当主库当掉时在不需要用户干预的情况下自动切换其他备份服务器做主库。而且还可以利用副本服务器做只读服务器，实现读写分离，提高负载。 （1）冗余和数据可用性 复制提供冗余并提高数据可用性。 通过在不同数据库服务器上提供多个数据副本，复制可提供一定级别的容错功能，以防止丢失单个数据库服务器。 在某些情况下，复制可以提供增加的读取性能，因为客户端可以将读取操作发送到不同的服务上， 在不同数据中心维护数据副本可以增加分布式应用程序的数据位置和可用性。 您还可以为专用目的维护其他副本，例如灾难恢复，报告或备份。 （2）MongoDB中的复制 副本集是一组维护相同数据集的mongod实例。 副本集包含多个数据承载节点和可选的一个仲裁节点。在承载数据的节点中，一个且仅一个成员被视为主节点，而其他节点被视为次要（从）节点。 主节点接收所有写操作。 副本集只能有一个主要能够确认具有{w：“most”}写入关注的写入; 虽然在某些情况下，另一个mongod实例可能暂时认为自己也是主要的。主要记录其操作日志中的数据集的所有更改，即oplog。 辅助(副本)节点复制主节点的oplog并将操作应用于其数据集，以使辅助节点的数据集反映主节点的数据集。 如果主要人员不在，则符合条件的中学将举行选举以选出新的主要人员。 （3）主从复制和副本集区别 主从集群和副本集最大的区别就是副本集没有固定的“主节点”；整个集群会选出一个“主节点”，当其挂掉后，又在剩下的从节点中选中其他节点为“主节点”，副本集总有一个活跃点(主、primary)和一个或多个备份节点(从、secondary)。 1.2 副本集的三个角色副本集有两种类型三种角色 两种类型： 主节点（ Primary）类型：数据操作的主要连接点，可读写。 次要（辅助、从）节点（ Secondaries）类型：数据冗余备份节点，可以读或选举。 三种角色： 主要成员（Primary）：主要接收所有写操作。就是主节点。 副本成员（Replicate）：从主节点通过复制操作以维护相同的数据集，即备份数据，不可写操作，但可以读操作（但需要配置）。是默认的一种从节点类型。 仲裁者（ Arbiter）：不保留任何数据的副本，只具有投票选举作用。当然也可以将仲裁服务器维护为副本集的一部分，即副本成员同时也可以是仲裁者。也是一种从节点类型。 关于仲裁者的额外说明： 您可以将额外的mongod实例添加到副本集作为仲裁者。 仲裁者不维护数据集。 仲裁者的目的是通过响应其他副本集成员的心跳和选举请求来维护副本集中的仲裁。 因为它们不存储数据集，所以仲裁器可以是提供副本集仲裁功能的好方法，其资源成本比具有数据集的全功能副本集成员更便宜。 如果您的副本集具有偶数个成员，请添加仲裁者以获得主要选举中的“大多数”投票。 仲裁者不需要专用硬件。 仲裁者将永远是仲裁者，而主要人员可能会退出并成为次要人员，而次要人员可能成为选举期间的主要人员。 如果你的副本+主节点的个数是偶数，建议加一个仲裁者，形成奇数，容易满足大多数的投票。 如果你的副本+主节点的个数是奇数，可以不加仲裁者。 1.3 副本集架构目标一主一副本一仲裁 1.4 副本集的创建1.4.1 第一步：创建主节点建立存放数据和日志的目录 1234#-----------myrs#主节点mkdir -p /mongodb/replica_sets/myrs_27017/log \\ &amp;mkdir -p /mongodb/replica_sets/myrs_27017/data/db 新建或修改配置文件： 1vim /mongodb/replica_sets/myrs_27017/mongod.conf myrs_27017 ： 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/replica_sets/myrs_27017/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/replica_sets/myrs_27017/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/replica_sets/myrs_27017/log/mongod.pid\"net: #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #bindIp #绑定的端口 port: 27017replication: #副本集的名称 replSetName: myrs 启动节点服务： 1234[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 54257child process started successfully, parent exiting 1.4.2 第二步：创建副本节点建立存放数据和日志的目录 1234#-----------myrs#副本节点mkdir -p /mongodb/replica_sets/myrs_27018/log \\ &amp;mkdir -p /mongodb/replica_sets/myrs_27018/data/db 新建或修改配置文件： 1vim /mongodb/replica_sets/myrs_27018/mongod.conf myrs_27018 ： 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/replica_sets/myrs_27018/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/replica_sets/myrs_27018/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/replica_sets/myrs_27018/log/mongod.pid\"net: #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #bindIp #绑定的端口 port: 27018replication: #副本集的名称 replSetName: myrs 启动节点服务： 1234[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 54361child process started successfully, parent exiting 1.4.3 第三步：创建仲裁节点建立存放数据和日志的目录 1234#-----------myrs#仲裁节点mkdir -p /mongodb/replica_sets/myrs_27019/log \\ &amp;mkdir -p /mongodb/replica_sets/myrs_27019/data/db 新建或修改配置文件： 1vim /mongodb/replica_sets/myrs_27019/mongod.conf myrs_27019 ： 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/replica_sets/myrs_27019/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/replica_sets/myrs_27019/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/replica_sets/myrs_27019/log/mongod.pid\"net: #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #bindIp #绑定的端口 port: 27019replication: #副本集的名称 replSetName: myrs 启动节点服务： 1234[root@bobohost replica_sets]# /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 54410child process started successfully, parent exiting 1.4.4 第四步：初始化配置副本集和主节点使用客户端命令连接任意一个节点，但这里尽量要连接主节点(27017节点)： 1/usr/local/mongodb/bin/mongo --host=180.76.159.126 --port=27017 结果，连接上之后，很多命令无法使用，，比如 show dbs 等，必须初始化副本集才行 准备初始化新的副本集： 语法： 1rs.initiate(configuration) 【示例】 使用默认的配置来初始化副本集： 1rs.initiate() 执行结果： 12345678910111213141516&gt; rs.initiate()&#123; &quot;info2&quot; : &quot;no configuration specified. Using a default configuration for the set&quot;, &quot;me&quot; : &quot;180.76.159.126:27017&quot;, &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1565760476, 1), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1565760476, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125;myrs:SECONDARY&gt;myrs:PRIMARY&gt; 提示： “ok”的值为1，说明创建成功。 命令行提示符发生变化，变成了一个从节点角色，此时默认不能读写。稍等片刻，回车，变成主节点。 1.4.5 第五步：查看副本集的配置内容返回包含当前副本集配置的文档。 语法： 1rs.conf(configuration) 提示： rs.config() 是该方法的别名。 configuration：可选，如果没有配置，则使用默认主节点配置。 【示例】 在27017上执行副本集中当前节点的默认节点配置 1234567891011121314151617181920212223242526272829303132333435363738myrs:PRIMARY&gt; rs.conf()&#123; &quot;_id&quot; : &quot;myrs&quot;, &quot;version&quot; : 1, &quot;protocolVersion&quot; : NumberLong(1), &quot;writeConcernMajorityJournalDefault&quot; : true, &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;host&quot; : &quot;180.76.159.126:27017&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 &#125; ], &quot;settings&quot; : &#123; &quot;chainingAllowed&quot; : true, &quot;heartbeatIntervalMillis&quot; : 2000, &quot;heartbeatTimeoutSecs&quot; : 10, &quot;electionTimeoutMillis&quot; : 10000, &quot;catchUpTimeoutMillis&quot; : -1, &quot;catchUpTakeoverDelayMillis&quot; : 30000, &quot;getLastErrorModes&quot; : &#123; &#125;, &quot;getLastErrorDefaults&quot; : &#123; &quot;w&quot; : 1, &quot;wtimeout&quot; : 0 &#125;, &quot;replicaSetId&quot; : ObjectId(&quot;5d539bdcd6a308e600d126bb&quot;) &#125;&#125; 说明： &quot;_id&quot; : &quot;myrs&quot; ：副本集的配置数据存储的主键值，默认就是副本集的名字 &quot;members&quot; ：副本集成员数组，此时只有一个： “host” : “180.76.159.126:27017” ，该成员不是仲裁节点：”arbiterOnly” : false ，优先级（权重值）： “priority” : 1, &quot;settings&quot; ：副本集的参数配置。 提示：副本集配置的查看命令，本质是查询的是 system.replset 的表中的数据： 123456789101112131415161718192021myrs:PRIMARY&gt; use localswitched to db localmyrs:PRIMARY&gt; show collectionsoplog.rsreplset.electionreplset.minvalidreplset.oplogTruncateAfterPointstartup_logsystem.replsetsystem.rollback.idmyrs:PRIMARY&gt; db.system.replset.find()&#123; &quot;_id&quot; : &quot;myrs&quot;, &quot;version&quot; : 1, &quot;protocolVersion&quot; : NumberLong(1),&quot;writeConcernMajorityJournalDefault&quot; : true, &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;host&quot; :&quot;180.76.159.126:27017&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; :false, &quot;priority&quot; : 1, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1&#125; ], &quot;settings&quot; : &#123; &quot;chainingAllowed&quot; : true, &quot;heartbeatIntervalMillis&quot; : 2000,&quot;heartbeatTimeoutSecs&quot; : 10, &quot;electionTimeoutMillis&quot; : 10000,&quot;catchUpTimeoutMillis&quot; : -1, &quot;catchUpTakeoverDelayMillis&quot; : 30000,&quot;getLastErrorModes&quot; : &#123; &#125;, &quot;getLastErrorDefaults&quot; : &#123; &quot;w&quot; : 1, &quot;wtimeout&quot; : 0&#125;, &quot;replicaSetId&quot; : ObjectId(&quot;5d539bdcd6a308e600d126bb&quot;) &#125; &#125;myrs:PRIMARY&gt; 1.4.6 第六步：查看副本集状态检查副本集状态。 说明： 返回包含状态信息的文档。此输出使用从副本集的其他成员发送的心跳包中获得的数据反映副本集的当前状态。 语法： 1rs.status() 【示例】 在27017上查看副本集状态： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263myrs:PRIMARY&gt; rs.status()&#123; &quot;set&quot; : &quot;myrs&quot;, &quot;date&quot; : ISODate(&quot;2019-08-14T05:29:45.161Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;optimes&quot; : &#123; &quot;lastCommittedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565760578, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;readConcernMajorityOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565760578, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;appliedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565760578, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;durableOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565760578, 1), &quot;t&quot; : NumberLong(1) &#125; &#125;, &quot;lastStableCheckpointTimestamp&quot; : Timestamp(1565760528, 1), &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;name&quot; : &quot;180.76.159.126:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 419, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1565760578, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2019-08-14T05:29:38Z&quot;), &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;could not find member to sync from&quot;, &quot;electionTime&quot; : Timestamp(1565760476, 2), &quot;electionDate&quot; : ISODate(&quot;2019-08-14T05:27:56Z&quot;), &quot;configVersion&quot; : 1, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; &#125; ], &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1565760578, 1), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1565760578, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 说明： &quot;set&quot; : &quot;myrs&quot; ：副本集的名字 &quot;myState&quot; : 1：说明状态正常 &quot;members&quot; ：副本集成员数组，此时只有一个： &quot;name&quot; : &quot;180.76.159.126:27017&quot; ，该成员的角色是 &quot;stateStr&quot; : &quot;PRIMARY&quot;, 该节点是健康的： &quot;health&quot; : 1 。 1.4.7 第七步：添加副本从节点在主节点添加从节点，将其他成员加入到副本集 语法： 1rs.add(host, arbiterOnly) 选项： Parameter Type Description host string or document 要添加到副本集的新成员。 指定为字符串或配置文档：1）如果是一个字符串，则需要指定新成员的主机名和可选的端口号；2）如果是一个文档，请指定在members数组中找到的副本集成员配置文档。 您必须在成员配置文档中指定主机字段。有关文档配置字段的说明，详见下方文档：“主机成员的配置文档” arbiterOnly boolean 可选的。 仅在 值为字符串时适用。 如果为true，则添加的主机是仲裁者。 主机成员的配置文档： 1234567891011&#123; _id: &lt;int&gt;, host: &lt;string&gt;, &#x2F;&#x2F; required arbiterOnly: &lt;boolean&gt;, buildIndexes: &lt;boolean&gt;, hidden: &lt;boolean&gt;, priority: &lt;number&gt;, tags: &lt;document&gt;, slaveDelay: &lt;int&gt;, votes: &lt;number&gt;&#125; 【示例】 将27018的副本节点添加到副本集中： 123456789101112myrs:PRIMARY&gt; rs.add(&quot;180.76.159.126:27018&quot;)&#123; &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1565761757, 1), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1565761757, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 说明： “ok” : 1 ：说明添加成功。 查看副本集状态： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091myrs:PRIMARY&gt; rs.status()&#123; &quot;set&quot; : &quot;myrs&quot;, &quot;date&quot; : ISODate(&quot;2019-08-14T05:50:05.738Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;optimes&quot; : &#123; &quot;lastCommittedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761798, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;readConcernMajorityOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761798, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;appliedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761798, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;durableOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761798, 1), &quot;t&quot; : NumberLong(1) &#125; &#125;, &quot;lastStableCheckpointTimestamp&quot; : Timestamp(1565761798, 1), &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;name&quot; : &quot;180.76.159.126:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 1639, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761798, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2019-08-14T05:49:58Z&quot;), &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;electionTime&quot; : Timestamp(1565760476, 2), &quot;electionDate&quot; : ISODate(&quot;2019-08-14T05:27:56Z&quot;), &quot;configVersion&quot; : 2, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; &#125;, &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;180.76.159.126:27018&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 48, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761798, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDurable&quot; : &#123; &quot;ts&quot; : Timestamp(1565761798, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2019-08-14T05:49:58Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2019-08-14T05:49:58Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2019-08-14T05:50:05.294Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2019-08- 14T05:50:05.476Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncingTo&quot; : &quot;180.76.159.126:27017&quot;, &quot;syncSourceHost&quot; : &quot;180.76.159.126:27017&quot;, &quot;syncSourceId&quot; : 0, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 2 &#125; ], &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1565761798, 1), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1565761798, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 说明： &quot;name&quot; : &quot;180.76.159.126:27018&quot; 是第二个节点的名字，其角色是 &quot;stateStr&quot; : &quot;SECONDARY&quot; 1.4.8 第八步：添加仲裁从节点添加一个仲裁节点到副本集 语法： 1rs.addArb(host) 将27019的仲裁节点添加到副本集中： 123456789101112myrs:PRIMARY&gt; rs.addArb(&quot;180.76.159.126:27019&quot;)&#123; &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1565761959, 1), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1565761959, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 说明： &quot;ok&quot; : 1 ：说明添加成功。 查看副本集状态： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107myrs:PRIMARY&gt; rs.status()&#123; &quot;set&quot; : &quot;myrs&quot;, &quot;date&quot; : ISODate(&quot;2019-08-14T05:53:27.198Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;optimes&quot; : &#123; &quot;lastCommittedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761998, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;readConcernMajorityOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761998, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;appliedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761998, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;durableOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761998, 1), &quot;t&quot; : NumberLong(1) &#125; &#125;, &quot;lastStableCheckpointTimestamp&quot; : Timestamp(1565761978, 1), &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;name&quot; : &quot;180.76.159.126:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 1841, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761998, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2019-08-14T05:53:18Z&quot;), &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;electionTime&quot; : Timestamp(1565760476, 2), &quot;electionDate&quot; : ISODate(&quot;2019-08-14T05:27:56Z&quot;), &quot;configVersion&quot; : 3, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; &#125;, &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;180.76.159.126:27018&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 249, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1565761998, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDurable&quot; : &#123; &quot;ts&quot; : Timestamp(1565761998, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2019-08-14T05:53:18Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2019-08-14T05:53:18Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2019-08-14T05:53:25.668Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2019-08-14T05:53:26.702Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncingTo&quot; : &quot;180.76.159.126:27017&quot;, &quot;syncSourceHost&quot; : &quot;180.76.159.126:27017&quot;, &quot;syncSourceId&quot; : 0, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 3 &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;180.76.159.126:27019&quot;, &quot;health&quot; : 1, &quot;state&quot; : 7, &quot;stateStr&quot; : &quot;ARBITER&quot;, &quot;uptime&quot; : 47, &quot;lastHeartbeat&quot; : ISODate(&quot;2019-08-14T05:53:25.668Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2019-08-14T05:53:25.685Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 3 &#125; ], &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1565761998, 1), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1565761998, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 说明： &quot;name&quot; : &quot;180.76.159.126:27019&quot; 是第二个节点的名字，其角色是 &quot;stateStr&quot; : &quot;ARBITER&quot; 1.5 副本集的数据读写操作目标：测试三个不同角色的节点的数据读写情况 登录主节点27017，写入和读取数据： 123456789[root@bobohost ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27017myrs:PRIMARY&gt; use articledbswitched to db articledbmyrs:PRIMARY&gt; dbarticledbmyrs:PRIMARY&gt; db.comment.insert(&#123;&quot;articleid&quot;:&quot;100000&quot;,&quot;content&quot;:&quot;今天天气真好，阳光明媚&quot;,&quot;userid&quot;:&quot;1001&quot;,&quot;nickname&quot;:&quot;Rose&quot;,&quot;createdatetime&quot;:new Date()&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)myrs:PRIMARY&gt; db.comment.find()&#123; &quot;_id&quot; : ObjectId(&quot;5d4d2ae3068138b4570f53bf&quot;), &quot;articleid&quot; : &quot;100000&quot;,&quot;content&quot; : &quot;今天天气真好，阳光明媚&quot;, &quot;userid&quot; : &quot;1001&quot;, &quot;nickname&quot; : &quot;Rose&quot;,&quot;createdatetime&quot; : ISODate(&quot;2019-08-09T08:12:19.427Z&quot;) &#125; 登录从节点27018 123456789101112131415161718192021[root@bobohost ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27018myrs:SECONDARY&gt; show dbs;2019-09-10T10:56:51.953+0800 E QUERY [js] Error: listDatabases failed:&#123; &quot;operationTime&quot; : Timestamp(1568084204, 1), &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master and slaveOk&#x3D;false&quot;, &quot;code&quot; : 13435, &quot;codeName&quot; : &quot;NotMasterNoSlaveOk&quot;, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1568084204, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; :_getErrorWithCode@src&#x2F;mongo&#x2F;shell&#x2F;utils.js:25:13Mongo.prototype.getDBs@src&#x2F;mongo&#x2F;shell&#x2F;mongo.js:139:1shellHelper.show@src&#x2F;mongo&#x2F;shell&#x2F;utils.js:882:13shellHelper@src&#x2F;mongo&#x2F;shell&#x2F;utils.js:766:15@(shellhelp2):1:1 发现，不能读取集合的数据。当前从节点只是一个备份，不是奴隶节点，无法读取数据，写当然更不行。 因为默认情况下，从节点是没有读写权限的，可以增加读的权限，但需要进行设置 设置读操作权限： 说明： 设置为奴隶节点，允许在从成员上运行读的操作 语法： 123rs.slaveOk()#或rs.slaveOk(true) 提示： 该命令是 db.getMongo().setSlaveOk() 的简化命令。 【示例】 在27018上设置作为奴隶节点权限，具备读权限： 1rs:SECONDARY&gt; rs.slaveOk() 此时，在执行查询命令，运行成功！ 但仍然不允许插入。 123456789101112131415161718192021222324252627myrs:SECONDARY&gt; rs.slaveOk()myrs:SECONDARY&gt; show dbs;admin 0.000GBarticledb 0.000GBconfig 0.000GBlocal 0.000GBmyrs:SECONDARY&gt; use articledbswitched to db articledbmyrs:SECONDARY&gt; show collectionscommentmyrs:SECONDARY&gt; db.comment.find()&#123; &quot;_id&quot; : ObjectId(&quot;5d7710c04cfd7eee2e3cdabe&quot;), &quot;articleid&quot; : &quot;100000&quot;,&quot;content&quot; : &quot;今天天气真好，阳光明媚&quot;, &quot;userid&quot; : &quot;1001&quot;, &quot;nickname&quot; : &quot;Rose&quot;,&quot;createdatetime&quot; : ISODate(&quot;2019-09-10T02:56:00.467Z&quot;) &#125;myrs:SECONDARY&gt; db.comment.insert(&#123;&quot;_id&quot;:&quot;1&quot;,&quot;articleid&quot;:&quot;100001&quot;,&quot;content&quot;:&quot;我们不应该把清晨浪费在手机上，健康很重要，k一杯温水幸福你我他。&quot;,&quot;userid&quot;:&quot;1002&quot;,&quot;nickname&quot;:&quot;相忘于江湖&quot;,&quot;createdatetime&quot;:new Date(&quot;2019-08-05T22:08:15.522Z&quot;),&quot;likenum&quot;:NumberInt(1000),&quot;state&quot;:&quot;1&quot;&#125;)WriteCommandError(&#123; &quot;operationTime&quot; : Timestamp(1568084434, 1), &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot;, &quot;code&quot; : 10107, &quot;codeName&quot; : &quot;NotMaster&quot;, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1568084434, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125;) 现在可实现了读写分离，让主插入数据，让从来读取数据。 如果要取消作为奴隶节点的读权限： 12345678910111213141516myrs:SECONDARY&gt; rs.slaveOk(false)myrs:SECONDARY&gt; db.comment.find()Error: error: &#123; &quot;operationTime&quot; : Timestamp(1568084459, 1), &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master and slaveOk&#x3D;false&quot;, &quot;code&quot; : 13435, &quot;codeName&quot; : &quot;NotMasterNoSlaveOk&quot;, &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1568084459, 1), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 仲裁者节点，不存放任何业务数据的，可以登录查看 12345678910111213[root@bobohost ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27019myrs:ARBITER&gt; rs.slaveOk()myrs:ARBITER&gt; show dbslocal 0.000GBmyrs:ARBITER&gt; use localswitched to db localmyrs:ARBITER&gt; show collectionsreplset.minvalidreplset.oplogTruncateAfterPointstartup_logsystem.replsetsystem.rollback.idmyrs:ARBITER&gt; 发现，只存放副本集配置等数据。 1.6 主节点的选举原则MongoDB在副本集中，会自动进行主节点的选举，主节点选举的触发条件： 主节点故障 主节点网络不可达（默认心跳信息为10秒） 人工干预（rs.stepDown(600)） 一旦触发选举，就要根据一定规则来选主节点。 选举规则是根据票数来决定谁获胜： 票数最高，且获得了 “大多数”成员的投票支持的节点获胜。 “大多数”的定义为：假设复制集内投票成员数量为N，则大多数为 N/2 + 1。例如：3个投票成员，则大多数的值是2。当复制集内存活成员数量不足大多数时，整个复制集将无法选举出Primary，复制集将无法提供写服务，处于只读状态。 若票数相同，且都获得了 “大多数”成员的投票支持的，数据新的节点获胜。 数据的新旧是通过操作日志oplog来对比的。 在获得票数的时候，优先级（priority）参数影响重大。 可以通过设置优先级（priority）来设置额外票数。优先级即权重，取值为0-1000，相当于可额外增加0-1000的票数，优先级的值越大，就越可能获得多数成员的投票（votes）数。指定较高的值可使成员更有资格成为主要成员，更低的值可使成员更不符合条件。 默认情况下，优先级的值是1 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364myrs:PRIMARY&gt; rs.conf()&#123; &quot;_id&quot; : &quot;myrs&quot;, &quot;version&quot; : 3, &quot;protocolVersion&quot; : NumberLong(1), &quot;writeConcernMajorityJournalDefault&quot; : true, &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;host&quot; : &quot;180.76.159.126:27017&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 &#125;, &#123; &quot;_id&quot; : 1, &quot;host&quot; : &quot;180.76.159.126:27018&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 &#125;, &#123; &quot;_id&quot; : 2, &quot;host&quot; : &quot;180.76.159.126:27019&quot;, &quot;arbiterOnly&quot; : true, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 0, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 &#125; ], &quot;settings&quot; : &#123; &quot;chainingAllowed&quot; : true, &quot;heartbeatIntervalMillis&quot; : 2000, &quot;heartbeatTimeoutSecs&quot; : 10, &quot;electionTimeoutMillis&quot; : 10000, &quot;catchUpTimeoutMillis&quot; : -1, &quot;catchUpTakeoverDelayMillis&quot; : 30000, &quot;getLastErrorModes&quot; : &#123; &#125;, &quot;getLastErrorDefaults&quot; : &#123; &quot;w&quot; : 1, &quot;wtimeout&quot; : 0 &#125;, &quot;replicaSetId&quot; : ObjectId(&quot;5d539bdcd6a308e600d126bb&quot;) &#125;&#125; 可以看出，主节点和副本节点的优先级各为 1，即，默认可以认为都已经有了一票。但选举节点，优先级是0，（要注意是，官方说了，选举节点的优先级必须是0，不能是别的值。即不具备选举权，但具有投票权） 【了解】修改优先级 比如，下面提升从节点的优先级： 1）先将配置导入cfg变量 1myrs:SECONDARY&gt; cfg&#x3D;rs.conf() 2 ）然后修改值（ID号默认从0开始） 12myrs:SECONDARY&gt; cfg.members[1].priority&#x3D;22 3 ）重新加载配置 12myrs:SECONDARY&gt; rs.reconfig(cfg)&#123; &quot;ok&quot; : 1 &#125; 稍等片刻会重新开始选举。 1.7 故障测试1.7.1 副本节点故障测试关闭27018副本节点： 发现，主节点和仲裁节点对27018的心跳失败。因为主节点还在，因此，没有触发投票选举。 如果此时，在主节点写入数据。 1db.comment.insert(&#123;&quot;_id&quot;:&quot;1&quot;,&quot;articleid&quot;:&quot;100001&quot;,&quot;content&quot;:&quot;我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。&quot;,&quot;userid&quot;:&quot;1002&quot;,&quot;nickname&quot;:&quot;相忘于江湖&quot;,&quot;createdatetime&quot;:new Date(&quot;2019-08-05T22:08:15.522Z&quot;),&quot;likenum&quot;:NumberInt(1000),&quot;state&quot;:&quot;1&quot;&#125;) 再启动从节点，会发现，主节点写入的数据，会自动同步给从节点。 1.7.2 主节点故障测试关闭27017节点 发现，从节点和仲裁节点对27017的心跳失败，当失败超过10秒，此时因为没有主节点了，会自动发起投票。 而副本节点只有27018，因此，候选人只有一个就是27018，开始投票。 27019向27018投了一票，27018本身自带一票，因此共两票，超过了“大多数” 27019是仲裁节点，没有选举权，27018不向其投票，其票数是0. 最终结果，27018成为主节点。具备读写功能。 在27018写入数据查看。 1db.comment.insert(&#123;&quot;_id&quot;:&quot;2&quot;,&quot;articleid&quot;:&quot;100001&quot;,&quot;content&quot;:&quot;我夏天空腹喝凉开水，冬天喝温开水&quot;,&quot;userid&quot;:&quot;1005&quot;,&quot;nickname&quot;:&quot;伊人憔悴&quot;,&quot;createdatetime&quot;:new Date(&quot;2019-08-05T23:58:51.485Z&quot;),&quot;likenum&quot;:NumberInt(888),&quot;state&quot;:&quot;1&quot;&#125;) 再启动 27017节点，发现27017变成了从节点，27018仍保持主节点。 登录27017节点，发现是从节点了，数据自动从27018同步。 从而实现了高可用。 1.7.3 仲裁节点和主节点故障先关掉仲裁节点27019， 关掉现在的主节点27018 登录27017后，发现，27017仍然是从节点，副本集中没有主节点了，导致此时，副本集是只读状态，无法写入。 为啥不选举了？因为27017的票数，没有获得大多数，即没有大于等于2，它只有默认的一票（优先级是1） 如果要触发选举，随便加入一个成员即可。 如果只加入 27019仲裁节点成员，则主节点一定是27017，因为没得选了，仲裁节点不参与选举，但参与投票。（不演示） 如果只加入 27018节点，会发起选举。因为27017和27018都是两票，则按照谁数据新，谁当主节点。 1.7.4 仲裁节点和从节点故障先关掉仲裁节点27019， 关掉现在的副本节点27018 10秒后，27017主节点自动降级为副本节点。（服务降级） 副本集不可写数据了，已经故障了。 1.8 Compass 连接副本集compass连接： 1.9 SpringDataMongoDB 连接副本集副本集语法： 1mongodb:&#x2F;&#x2F;host1,host2,host3&#x2F;articledb?connect&#x3D;replicaSet&amp;slaveOk&#x3D;true&amp;replicaSet&#x3D;副本集名字 其中： slaveOk=true ：开启副本节点读的功能，可实现读写分离。 connect=replicaSet ：自动到副本集中选择读写的主机。如果slaveOK是打开的，则实现了读写分离 【示例】 连接 replica set 三台服务器 (端口 27017, 27018, 和27019)，直接连接第一个服务器，无论是replica set一部分或者主服务器或者从服务器，写入操作应用在主服务器 并且分布查询到从服务器。 修改配置文件application.yml 1234567891011121314spring: #数据源配置 data: mongodb: # 主机地址 # host: 180.76.159.126 # 数据库 # database: articledb # 默认端口是27017 # port: 27017 #也可以使用uri连接 #uri: mongodb://192.168.40.134:27017/articledb # 副本集的连接字符串 uri: mongodb://180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaSet&amp;slaveOk=true&amp;replicaSet=myrs 注意： 主机必须是副本集中所有的主机，包括主节点、副本节点、仲裁节点。 SpringDataMongoDB自动实现了读写分离： 写操作时，只打开主节点连接；读操作是，同时打开主节点和从节点连接，但使用从节点获取数据。 完整的连接字符串的参考（了解）： MongoDB客户端连接语法： 1mongodb:&#x2F;&#x2F;[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][&#x2F;[database][?options]] mongodb:// 这是固定的格式，必须要指定。 username:password@ 可选项，如果设置，在连接数据库服务器之后，驱动都会尝试登陆这个数据库 host1 必须的指定至少一个host, host1 是这个URI唯一要填写的。它指定了要连接服务器的地址。如果要连接复制集，请指定多个主机地址。 portX 可选的指定端口，如果不填，默认为27017 /database 如果指定username:password@，连接并验证登陆指定数据库。若不指定，默认打开test 数据库。 ?options 是连接选项。如果不使用/database，则前面需要加上/。所有连接选项都是键值对name=value，键值对之间通过&amp;或;（分号）隔开 标准的连接格式包含了多个选项(options)，如下所示： 选项 描述 replicaSet=name 验证replica set的名称。 Impliesconnect=replicaSet. slaveOk=true|false true:在connect=direct模式下，驱动会连接第一台机器，即使这台服务器不是主。在connect=replicaSet模式下，驱动会发送所有的写请求到主并且把读取操作分布在其他从服务器。false: 在connect=direct模式下，驱动会自动找寻主服务器. 在connect=replicaSet 模式下，驱动仅仅连接主服务器，并且所有的读写命令都连接到主服务器。 safe=true|false true: 在执行更新操作之后，驱动都会发送getLastError命令来确保更新成功。(还要参考 wtimeoutMS).false: 在每次更新之后，驱动不会发送getLastError来确保更新成功。 w=n 驱动添加 { w : n } 到getLastError命令. 应用于safe=true。 wtimeoutMS=ms 驱动添加 { wtimeout : ms } 到 getlasterror 命令. 应用于 safe=true. fsync=true|false true: 驱动添加 { fsync : true } 到 getlasterror 命令.应用于safe=true.false: 驱动不会添加到getLastError命令中。 journal=true|false 如果设置为 true, 同步到 journal (在提交到数据库前写入到实体中).应用于 safe=true connectTimeoutMS=ms 可以打开连接的时间 socketTimeoutMS=ms 发送和接受sockets的时间 2. 分片集群-Sharded Cluster2.1 分片概念分片（sharding）是一种跨多台机器分布数据的方法， MongoDB使用分片来支持具有非常大的数据集和高吞吐量操作的部署。 换句话说：分片(sharding)是指将数据拆分，将其分散存在不同的机器上的过程。有时也用分区(partitioning)来表示这个概念。将数据分散到不同的机器上，不需要功能强大的大型计算机就可以储存更多的数据，处理更多的负载。 具有大型数据集或高吞吐量应用程序的数据库系统可以会挑战单个服务器的容量。例如，高查询率会耗尽服务器的CPU容量。工作集大小大于系统的RAM会强调磁盘驱动器的I / O容量。 有两种解决系统增长的方法：垂直扩展和水平扩展。 垂直扩展意味着增加单个服务器的容量，例如使用更强大的CPU，添加更多RAM或增加存储空间量。可用技术的局限性可能会限制单个机器对于给定工作负载而言足够强大。此外，基于云的提供商基于可用的硬件配置具有硬性上限。结果，垂直缩放有实际的最大值。 水平扩展意味着划分系统数据集并加载多个服务器，添加其他服务器以根据需要增加容量。虽然单个机器的总体速度或容量可能不高，但每台机器处理整个工作负载的子集，可能提供比单个高速大容量服务器更高的效率。扩展部署容量只需要根据需要添加额外的服务器，这可能比单个机器的高端硬件的总体成本更低。权衡是基础架构和部署维护的复杂性增加。 MongoDB支持通过分片进行水平扩展。 2.2 分片集群包含的组件MongoDB分片群集包含以下组件： 分片（存储）：每个分片包含分片数据的子集。 每个分片都可以部署为副本集。 mongos （路由）：mongos充当查询路由器，在客户端应用程序和分片集群之间提供接口。 config servers （“调度”的配置）：配置服务器存储群集的元数据和配置设置。 从MongoDB 3.4开始，必须将配置服务器部署为副本集（CSRS）。 下图描述了分片集群中组件的交互： MongoDB在集合级别对数据进行分片，将集合数据分布在集群中的分片上。 2.3 分片集群架构目标两个分片节点副本集（3+3）+一个配置节点副本集（3）+两个路由节点（2），共11个服务节点。 2.4 分片（存储）节点副本集的创建所有的的配置文件都直接放到 sharded_cluster 的相应的子目录下面，默认配置文件名字：mongod.conf 2.4.1 第一套副本集准备存放数据和日志的目录： 123456789#-----------myshardrs01mkdir -p /mongodb/sharded_cluster/myshardrs01_27018/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs01_27018/data/db \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs01_27118/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs01_27118/data/db \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs01_27218/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs01_27218/data/db 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf myshardrs01_27018 ： 1234567891011121314151617181920212223242526272829303132systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myshardrs01_27018/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myshardrs01_27018/log/mongod.pid\"net: #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #bindIp #绑定的端口 port: 27018replication: #副本集的名称 replSetName: myshardrs01sharding: #分片角色 clusterRole: shardsvr sharding.clusterRole： Value Description configsvr Start this instance as a config server. The instance starts on port 27019 by default. shardsvr Start this instance as a shard . The instance starts on port 27018 by default. 注意： 设置sharding.clusterRole需要mongod实例运行复制。 要将实例部署为副本集成员，请使用replSetName设置并指定副本集的名称。 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf myshardrs01_27118 ： 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myshardrs01_27118/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myshardrs01_27118/log/mongod.pid\"net: #服务实例绑定所有IP #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27118replication: replSetName: myshardrs01sharding: clusterRole: shardsvr 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf myshardrs01_27218 ： 123456789101112131415161718192021222324252627systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myshardrs01_27218/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myshardrs01_27218/log/mongod.pid\"net: #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27218replication: replSetName: myshardrs01sharding: clusterRole: shardsvr 启动第一套副本集：一主一副本一仲裁 依次启动三个mongod服务： 1234567891011121314[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123223child process started successfully, parent exiting[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123292child process started successfully, parent exiting[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123326child process started successfully, parent exiting 查看服务是否启动： 12345[root@bobohost bin]# ps -ef |grep mongodpolkitd 61622 61604 0 7月31 ? 00:04:29 mongod --bind_ip_allroot 123223 1 1 01:10 ? 00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.confroot 123292 1 4 01:11 ? 00:00:00 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.confroot 123326 1 6 01:11 ? 00:00:00 /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf 2.4.1.1 初始化副本集和创建主节点：使用客户端命令连接任意一个节点，但这里尽量要连接主节点： 1/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27018 执行初始化副本集命令： 1rs.initiate() 查看副本集情况 1rs.status() 主节点配置查看 1rs.conf() 2.4.1.2 添加副本节点1rs.add(\"180.76.159.126:27118\") 2.4.1.3 添加仲裁节点1rs.addArb(\"180.76.159.126:27218\") 查看副本集的配置情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647myshardrs01:PRIMARY&gt; rs.conf()&#123; &quot;_id&quot; : &quot;myshardrs01&quot;, &quot;version&quot; : 3, &quot;protocolVersion&quot; : NumberLong(1), &quot;writeConcernMajorityJournalDefault&quot; : true, &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;host&quot; : &quot;180.76.159.126:27018&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 &#125;, &#123; &quot;_id&quot; : 1, &quot;host&quot; : &quot;180.76.159.126:27118&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 &#125;, &#123; &quot;_id&quot; : 2, &quot;host&quot; : &quot;180.76.159.126:27218&quot;, &quot;arbiterOnly&quot; : true, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 0, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 &#125; ], 2.4.2 第二套副本集准备存放数据和日志的目录： 123456789#-----------myshardrs02mkdir -p /mongodb/sharded_cluster/myshardrs02_27318/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs02_27318/data/db \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs02_27418/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs02_27418/data/db \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs02_27518/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myshardrs02_27518/data/db 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf myshardrs02_27318 ： 123456789101112131415161718192021222324252627systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myshardrs02_27318/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myshardrs02_27318/log/mongod.pid\"net: #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27318replication: replSetName: myshardrs02sharding: clusterRole: shardsvr 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf myshardrs02_27418 ： 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myshardrs02_27418/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myshardrs02_27418/log/mongod.pid\"net: #服务实例绑定所有IP #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27418replication: replSetName: myshardrs02sharding: clusterRole: shardsvr 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf myshardrs02_27518 ： 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myshardrs02_27518/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myshardrs02_27518/log/mongod.pid\"net: #服务实例绑定所有IP #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27518replication: replSetName: myshardrs02sharding: clusterRole: shardsvr 启动第二套副本集：一主一副本一仲裁 依次启动三个mongod服务： 1234567891011121314[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123223child process started successfully, parent exiting[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123292child process started successfully, parent exiting[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123326child process started successfully, parent exiting 查看服务是否启动： 1[root@bobohost bin]# ps -ef |grep mongod 2.4.2.1 初始化副本集和创建主节点使用客户端命令连接任意一个节点，但这里尽量要连接主节点： 1/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27318 执行初始化副本集命令： 1rs.initiate() 查看副本集情况 (节选内容)： 1rs.status() 主节点配置查看： 1rs.conf() 2.4.2.2 添加副本节点1rs.add(&quot;180.76.159.126:27418&quot;) 2.4.2.3 添加仲裁节点1rs.addArb(&quot;180.76.159.126:27518&quot;) 查看副本集的配置情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107myshardrs02:PRIMARY&gt; rs.status()&#123; &quot;set&quot; : &quot;myshardrs02&quot;, &quot;date&quot; : ISODate(&quot;2019-07-31T21:38:22.463Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;optimes&quot; : &#123; &quot;lastCommittedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;readConcernMajorityOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;appliedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;durableOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125; &#125;, &quot;lastStableCheckpointTimestamp&quot; : Timestamp(1564609074, 1), &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;name&quot; : &quot;180.76.159.126:27318&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 5086, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2019-07-31T21:38:14Z&quot;), &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;electionTime&quot; : Timestamp(1564604032, 2), &quot;electionDate&quot; : ISODate(&quot;2019-07-31T20:13:52Z&quot;), &quot;configVersion&quot; : 3, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; &#125;, &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;180.76.159.126:27418&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 4452, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDurable&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2019-07-31T21:38:14Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2019-07-31T21:38:14Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2019-07-31T21:38:21.178Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2019-07-31T21:38:20.483Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncingTo&quot; : &quot;180.76.159.126:27518&quot;, &quot;syncSourceHost&quot; : &quot;180.76.159.126:27518&quot;, &quot;syncSourceId&quot; : 2, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 3 &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;180.76.159.126:27518&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 4448, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDurable&quot; : &#123; &quot;ts&quot; : Timestamp(1564609094, 1), &quot;t&quot; : NumberLong(1) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2019-07-31T21:38:14Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2019-07-31T21:38:14Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2019-07-31T21:38:21.178Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2019-07-31T21:38:22.096Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncingTo&quot; : &quot;180.76.159.126:27318&quot;, &quot;syncSourceHost&quot; : &quot;180.76.159.126:27318&quot;, &quot;syncSourceId&quot; : 0, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 3 &#125; ], 2.5 配置节点副本集的创建第一步：准备存放数据和日志的目录： 12345678910#-----------configrs#建立数据节点data和日志目录mkdir -p /mongodb/sharded_cluster/myconfigrs_27019/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myconfigrs_27019/data/db \\ &amp;mkdir -p /mongodb/sharded_cluster/myconfigrs_27119/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myconfigrs_27119/data/db \\ &amp;mkdir -p /mongodb/sharded_cluster/myconfigrs_27219/log \\ &amp;mkdir -p /mongodb/sharded_cluster/myconfigrs_27219/data/db 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf myconfigrs_27019 ： 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myconfigrs_27019/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myconfigrs_27019/log/mongod.pid\"net: #服务实例绑定所有IP #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27019replication: replSetName: myconfigrssharding: clusterRole: configsvr 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf myconfigrs_27119 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myconfigrs_27119/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myconfigrs_27119/log/mongod.pid\"net: #服务实例绑定所有IP #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27119replication: replSetName: myconfigrssharding: clusterRole: configsvr 新建或修改配置文件： 1vim /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf myconfigrs_27219 1234567891011121314151617181920212223242526272829systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/sharded_cluster/myconfigrs_27219/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/sharded_cluster/myconfigrs_27219/log/mongod.pid\"net: #服务实例绑定所有IP #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27219replication: replSetName: myconfigrssharding: clusterRole: configsvr 启动配置副本集：一主两副本 依次启动三个mongod服务： 1234567891011121314[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123223child process started successfully, parent exiting[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123292child process started successfully, parent exiting[root@bobohost bin]# /usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.confabout to fork child process, waiting until server is ready for connections.forked process: 123326child process started successfully, parent exiting 查看服务是否启动： 1[root@bobohost bin]# ps -ef |grep mongod 2.5.1 初始化副本集和创建主节点使用客户端命令连接任意一个节点，但这里尽量要连接主节点： 1/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27019 执行初始化副本集命令： 1rs.initiate() 查看副本集情况 (节选内容)： 1rs.status() 主节点配置查看： 1rs.conf() 2.5.2 添加两个副本节点12myshardrs01:PRIMARY&gt; rs.add(&quot;180.76.159.126:27119&quot;)myshardrs01:PRIMARY&gt; rs.add(&quot;180.76.159.126:27219&quot;) 查看副本集的配置情况： 12myshardrs01:PRIMARY&gt; rs.conf()myshardrs01:PRIMARY&gt; rs.status() 2.6 路由节点的创建和操作2.6.1 第一个路由节点的创建和连接第一步：准备存放数据和日志的目录： 12#-----------mongos01mkdir -p /mongodb/sharded_cluster/mymongos_27017/log mymongos_27017节点： 新建或修改配置文件： 1vi /mongodb/sharded_cluster/mymongos_27017/mongos.conf mongos.conf： 1234567891011121314151617181920212223systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/mymongos_27017/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: /mongodb/sharded_cluster/mymongos_27017/log/mongod.pid\"net: #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #bindIp #绑定的端口 port: 27017sharding: #指定配置节点副本集 configDB: myconfigrs/180.76.159.126:27019,180.76.159.126:27119,180.76.159.126:27219 启动mongos： 1234[root@bobohost bin]# /usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.confabout to fork child process, waiting until server is ready for connections.forked process: 129874child process started successfully, parent exiting 提示：启动如果失败，可以查看 log目录下的日志，查看失败原因。 客户端登录mongos 1/usr/local/mongodb/bin/mongo --host 180.76.159.126 --port 27017 此时，写不进去数据，如果写数据会报错： 1234567891011121314151617mongos&gt; use aadbswitched to db aadbmongos&gt; db.aa.insert(&#123;aa:&quot;aa&quot;&#125;)WriteCommandError(&#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;unable to initialize targeter for write op for collection aa.aa :: caused by :: Database aa not found :: caused by :: No shards found&quot;, &quot;code&quot; : 70, &quot;codeName&quot; : &quot;ShardNotFound&quot;, &quot;operationTime&quot; : Timestamp(1564600123, 2), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1564600123, 2), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125;) 原因：通过路由节点操作，现在只是连接了配置节点，还没有连接分片数据节点，因此无法写入业务数据。 2.6.2 在路由节点上进行分片配置操作使用命令添加分片： 2.6.2.1 添加分片：语法： 1sh.addShard(&quot;IP:Port&quot;) 将第一套分片副本集添加进来： 12345678910111213mongos&gt;sh.addShard(&quot;myshardrs01&#x2F;192.168.0.2:27018,180.76.159.126:27118,180.76.159.126:27218&quot;)&#123; &quot;shardAdded&quot; : &quot;myshardrs01&quot;, &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1564611970, 4), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1564611970, 4), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 查看分片状态情况： 12345678910111213141516171819202122mongos&gt; sh.status()--- Sharding Status --- sharding version: &#123; &quot;_id&quot; : 1, &quot;minCompatibleVersion&quot; : 5, &quot;currentVersion&quot; : 6, &quot;clusterId&quot; : ObjectId(&quot;5d4211b798f3f9a48522c68b&quot;) &#125; shards: &#123; &quot;_id&quot; : &quot;myshardrs01&quot;, &quot;host&quot; : &quot;myshardrs01&#x2F;180.76.159.126:27018,180.76.159.126:27118&quot;, &quot;state&quot; : 1 &#125; active mongoses: &quot;4.0.10&quot; : 1 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: &#123; &quot;_id&quot; : &quot;config&quot;, &quot;primary&quot; : &quot;config&quot;, &quot;partitioned&quot; : true &#125; 继续将第二套分片副本集添加进来： 12345678910111213mongos&gt;sh.addShard(&quot;myshardrs02&#x2F;192.168.0.2:27318,180.76.159.126:27418,180.76.159.126:27518&quot;)&#123; &quot;shardAdded&quot; : &quot;myshardrs02&quot;, &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1564612147, 5), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1564612147, 5), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 查看分片状态： 1234567891011121314151617181920212223mongos&gt; sh.status()--- Sharding Status --- sharding version: &#123; &quot;_id&quot; : 1, &quot;minCompatibleVersion&quot; : 5, &quot;currentVersion&quot; : 6, &quot;clusterId&quot; : ObjectId(&quot;5d4211b798f3f9a48522c68b&quot;) &#125; shards: &#123; &quot;_id&quot; : &quot;myshardrs01&quot;, &quot;host&quot; : &quot;myshardrs01&#x2F;180.76.159.126:27018,180.76.159.126:27118&quot;, &quot;state&quot; : 1 &#125; &#123; &quot;_id&quot; : &quot;myshardrs02&quot;, &quot;host&quot; : &quot;myshardrs02&#x2F;180.76.159.126:27318,180.76.159.126:27418&quot;, &quot;state&quot; : 1 &#125; active mongoses: &quot;4.0.10&quot; : 1 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: &#123; &quot;_id&quot; : &quot;config&quot;, &quot;primary&quot; : &quot;config&quot;, &quot;partitioned&quot; : true &#125; 提示：如果添加分片失败，需要先手动移除分片，检查添加分片的信息的正确性后，再次添加分片。 移除分片参考(了解)： 12use admindb.runCommand( &#123; removeShard: &quot;myshardrs02&quot; &#125; ) 注意：如果只剩下最后一个 shard，是无法删除的 移除时会自动转移分片数据，需要一个时间过程。 完成后，再次执行删除分片命令才能真正删除。 2.6.2.2 开启分片功能sh.enableSharding(“库名”)、sh.shardCollection(“库名.集合名”,{“key”:1}) 在mongos上的articledb数据库配置sharding: 123456789101112mongos&gt; sh.enableSharding(&quot;articledb&quot;)&#123; &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1564612296, 5), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1564612296, 5), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 查看分片状态： 12345678910111213141516171819202122232425262728293031mongos&gt; sh.status()--- Sharding Status --- sharding version: &#123; &quot;_id&quot; : 1, &quot;minCompatibleVersion&quot; : 5, &quot;currentVersion&quot; : 6, &quot;clusterId&quot; : ObjectId(&quot;5d4211b798f3f9a48522c68b&quot;) &#125; shards: &#123; &quot;_id&quot; : &quot;myshardrs01&quot;, &quot;host&quot; : &quot;myshardrs01&#x2F;180.76.159.126:27018,180.76.159.126:27118&quot;, &quot;state&quot; : 1 &#125; &#123; &quot;_id&quot; : &quot;myshardrs02&quot;, &quot;host&quot; : &quot;myshardrs02&#x2F;180.76.159.126:27318,180.76.159.126:27418&quot;, &quot;state&quot; : 1 &#125; active mongoses: &quot;4.0.10&quot; : 1 autosplit: Currently enabled: yes balancer: Currently enabled: yes Currently running: no Failed balancer rounds in last 5 attempts: 0 Migration Results for the last 24 hours: No recent migrations databases: &#123; &quot;_id&quot; : &quot;articledb&quot;, &quot;primary&quot; : &quot;myshardrs02&quot;, &quot;partitioned&quot; : true, &quot;version&quot; : &#123; &quot;uuid&quot; : UUID(&quot;788c9a3b-bb6a-4cc2-a597-974694772986&quot;), &quot;lastMod&quot; : 1 &#125; &#125; &#123; &quot;_id&quot; : &quot;config&quot;, &quot;primary&quot; : &quot;config&quot;, &quot;partitioned&quot; : true &#125; config.system.sessions shard key: &#123; &quot;_id&quot; : 1 &#125; unique: false balancing: true chunks: myshardrs01 1 &#123; &quot;_id&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;_id&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : myshardrs01 Timestamp(1, 0) 2.6.2.3 集合分片对集合分片，你必须使用 sh.shardCollection() 方法指定集合和分片键 语法： 1sh.shardCollection(namespace, key, unique) 参数： Parameter Type Description namespace string 要（分片）共享的目标集合的命名空间，格式： . key document 用作分片键的索引规范文档。shard键决定MongoDB如何在shard之间分发文档。除非集合为空，否则索引必须在shardcollection命令之前存在。如果集合为空，则MongoDB在对集合进行分片之前创建索引，前提是支持分片键的索引不存在。简单的说：由包含字段和该字段的索引遍历方向的文档组成。 unique boolean 当值为true情况下，片键字段上会限制为确保是唯一索引。哈希策略片键不支持唯一索引。默认是false。 对集合进行分片时,你需要选择一个 片键（Shard Key） , shard key 是每条记录都必须包含的,且建立了索引的单个字段或复合字段,MongoDB按照片键将数据划分到不同的 数据块 中,并将 数据块 均衡地分布到所有分片中.为了按照片键划分数据块,MongoDB使用 基于哈希的分片方式（随机平均分配）或者基于范围的分片方式（数值大小分配） 。 用什么字段当片键都可以，如：nickname作为片键，但一定是必填字段。 分片规则一：哈希策略 对于 基于哈希的分片 ,MongoDB计算一个字段的哈希值,并用这个哈希值来创建数据块. 在使用基于哈希分片的系统中,拥有”相近”片键的文档 很可能不会 存储在同一个数据块中,因此数据的分离性更好一些. 使用nickname作为片键，根据其值的哈希值进行数据分片 1234567891011121314mongos&gt; sh.shardCollection(&quot;articledb.comment&quot;,&#123;&quot;nickname&quot;:&quot;hashed&quot;&#125;)&#123; &quot;collectionsharded&quot; : &quot;articledb.comment&quot;, &quot;collectionUUID&quot; : UUID(&quot;ddea6ed8-ee61-4693-bd16-196acc3a45e8&quot;), &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1564612840, 28), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1564612840, 28), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA&#x3D;&quot;), &quot;keyId&quot; : NumberLong(0) &#125; &#125;&#125; 查看分片状态：sh.status() 12345678910111213141516171819202122 databases: &#123; &quot;_id&quot; : &quot;articledb&quot;, &quot;primary&quot; : &quot;myshardrs02&quot;, &quot;partitioned&quot; : true, &quot;version&quot; : &#123; &quot;uuid&quot; : UUID(&quot;251436b7-86c2-4cd8-9a88-70874af29364&quot;), &quot;lastMod&quot; : 1 &#125; &#125; articledb.comment shard key: &#123; &quot;nickname&quot; : &quot;hashed&quot; &#125; unique: false balancing: true chunks: myshardrs01 2 myshardrs02 2 &#123; &quot;nickname&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;nickname&quot; : NumberLong(&quot;-4611686018427387902&quot;) &#125; on : myshardrs01 Timestamp(1, 0) &#123; &quot;nickname&quot; : NumberLong(&quot;-4611686018427387902&quot;) &#125; --&gt;&gt; &#123; &quot;nickname&quot; : NumberLong(0) &#125; on : myshardrs01 Timestamp(1, 1) &#123; &quot;nickname&quot; : NumberLong(0) &#125; --&gt;&gt; &#123; &quot;nickname&quot; : NumberLong(&quot;4611686018427387902&quot;) &#125; on : myshardrs02 Timestamp(1, 2) &#123; &quot;nickname&quot; : NumberLong(&quot;4611686018427387902&quot;) &#125; --&gt;&gt; &#123; &quot;nickname&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : myshardrs02 Timestamp(1, 3)&#123; &quot;_id&quot; : &quot;config&quot;, &quot;primary&quot; : &quot;config&quot;, &quot;partitioned&quot; : true &#125; config.system.sessions shard key: &#123; &quot;_id&quot; : 1 &#125; unique: false balancing: true chunks: myshardrs01 1 &#123; &quot;_id&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;_id&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : myshardrs01 Timestamp(1, 0) 分片规则二：范围策略 对于 基于范围的分片 ,MongoDB按照片键的范围把数据分成不同部分.假设有一个数字的片键:想象一个从负无穷到正无穷的直线,每一个片键的值都在直线上画了一个点.MongoDB把这条直线划分为更短的不重叠的片段,并称之为 数据块 ,每个数据块包含了片键在一定范围内的数据. 在使用片键做范围划分的系统中,拥有”相近”片键的文档很可能存储在同一个数据块中,因此也会存储在同一个分片中. 如使用作者年龄字段作为片键，按照点赞数的值进行分片： 1234567891011121314mongos&gt; sh.shardCollection(&quot;articledb.author&quot;,&#123;&quot;age&quot;:1&#125;)&#123; &quot;collectionsharded&quot; : &quot;articledb.author&quot;, &quot;collectionUUID&quot; : UUID(&quot;9a47bdaa-213a-4039-9c18-e70bfc369df7&quot;), &quot;ok&quot; : 1, &quot;operationTime&quot; : Timestamp(1567512803, 13), &quot;$clusterTime&quot; : &#123; &quot;clusterTime&quot; : Timestamp(1567512803, 13), &quot;signature&quot; : &#123; &quot;hash&quot; : BinData(0,&quot;eE9QT5yE5sL1Tyr7+3U8GRy5+5Q&#x3D;&quot;), &quot;keyId&quot; : NumberLong(&quot;6732061237309341726&quot;) &#125; &#125;&#125; 注意的是： 一个集合只能指定一个片键，否则报错。 一旦对一个集合分片，分片键和分片值就不可改变。 如：不能给集合选择不同的分片键、不能更新分片键的值。 根据age索引进行分配数据。 查看分片状态： 1234567articledb.author shard key: &#123; &quot;age&quot; : 1 &#125; unique: false balancing: true chunks: myshardrs01 1 &#123; &quot;age&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;age&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : myshardrs01 Timestamp(1, 0) 基于范围的分片方式与基于哈希的分片方式性能对比： 基于范围的分片方式提供了更高效的范围查询,给定一个片键的范围,分发路由可以很简单地确定哪个数据块存储了请求需要的数据,并将请求转发到相应的分片中. 不过,基于范围的分片会导致数据在不同分片上的不均衡,有时候,带来的消极作用会大于查询性能的积极作用.比如,如果片键所在的字段是线性增长的,一定时间内的所有请求都会落到某个固定的数据块中,最终导致分布在同一个分片中.在这种情况下,一小部分分片承载了集群大部分的数据,系统并不能很好地进行扩展. 与此相比,基于哈希的分片方式以范围查询性能的损失为代价,保证了集群中数据的均衡.哈希值的随机性使数据随机分布在每个数据块中,因此也随机分布在不同分片中.但是也正由于随机性,一个范围查询很难确定应该请求哪些分片,通常为了返回需要的结果,需要请求所有分片. 如无特殊情况，一般推荐使用 Hash Sharding。 而使用 _id 作为片键是一个不错的选择，因为它是必有的，你可以使用数据文档 _id 的哈希作为片键。 这个方案能够是的读和写都能够平均分布，并且它能够保证每个文档都有不同的片键所以数据块能够很精细。 似乎还是不够完美，因为这样的话对多个文档的查询必将命中所有的分片。虽说如此，这也是一种比较好的方案了。 理想化的 shard key 可以让 documents 均匀地在集群中分布： 显示集群的详细信息： 1mongos&gt; db.printShardingStatus() 查看均衡器是否工作（需要重新均衡时系统才会自动启动，不用管它）： 12mongos&gt; sh.isBalancerRunning()false 查看当前 Balancer状态： 12mongos&gt; sh.getBalancerState()true 2.6.3 分片后插入数据测试测试一（哈希规则）：登录mongs后，向comment循环插入1000条数据做测试： 123456mongos&gt; use articledbswitched to db articledbmongos&gt; for(var i&#x3D;1;i&lt;&#x3D;1000;i++)&#123;db.comment.insert(&#123;_id:i+&quot;&quot;,nickname:&quot;BoBo&quot;+i&#125;)&#125;WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)mongos&gt; db.comment.count()1000 提示： js的语法，因为mongo的shell是一个JavaScript的shell。 注意：从路由上插入的数据，必须包含片键，否则无法插入。 分别登陆两个片的主节点，统计文档数量 第一个分片副本集： 123456&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27018myshardrs01:PRIMARY&gt; use articledbswitched to db articledbmyshardrs01:PRIMARY&gt; db.comment.count()507 第二个分片副本集： 123456&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27318myshardrs02:PRIMARY&gt; use articledbswitched to db articledbmyshardrs02:PRIMARY&gt; db.comment.count()493 可以看到， 1000条数据近似均匀的分布到了2个shard上。是根据片键的哈希值分配的。 这种分配方式非常易于水平扩展：一旦数据存储需要更大空间，可以直接再增加分片即可，同时提升了性能。 使用db.comment.stats()查看单个集合的完整情况，mongos执行该命令可以查看该集合的数据分片的情况。 使用sh.status()查看本库内所有集合的分片信息。 测试二（范围规则）：登录mongs后，向comment循环插入1000条数据做测试： 123456mongos&gt; use articledbswitched to db articledbmongos&gt; for(var i&#x3D;1;i&lt;&#x3D;20000;i++)&#123;db.author.save(&#123;&quot;name&quot;:&quot;BoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBoBo&quot;+i,&quot;age&quot;:NumberInt(i%120)&#125;)&#125;WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)mongos&gt; db.comment.count()20000 插入成功后，仍然要分别查看两个分片副本集的数据情况。 分片效果： 12345678910articledb.author shard key: &#123; &quot;age&quot; : 1 &#125; unique: false balancing: true chunks: myshardrs01 2 myshardrs02 1 &#123; &quot;age&quot; : &#123; &quot;$minKey&quot; : 1 &#125; &#125; --&gt;&gt; &#123; &quot;age&quot; : 0 &#125; on : myshardrs02 Timestamp(2, 0) &#123; &quot;age&quot; : 0 &#125; --&gt;&gt; &#123; &quot;age&quot; : 112 &#125; on : myshardrs01 Timestamp(2, 1) &#123; &quot;age&quot; : 112 &#125; --&gt;&gt; &#123; &quot;age&quot; : &#123; &quot;$maxKey&quot; : 1 &#125; &#125; on : myshardrs01 Timestamp(1, 3) 提示： 如果查看状态发现没有分片，则可能是由于以下原因造成了： 系统繁忙，正在分片中。 数据块（chunk）没有填满，默认的数据块尺寸（chunksize）是64M，填满后才会考虑向其他片的数据块填充数据，因此，为了测试，可以将其改小，这里改为1M，操作如下： 12use configdb.settings.save( &#123; _id:&quot;chunksize&quot;, value: 1 &#125; ) 测试完改回来： 1db.settings.save( &#123; _id:&quot;chunksize&quot;, value: 64 &#125; ) 注意：要先改小，再设置分片。为了测试，可以先删除集合，重新建立集合的分片策略，再插入数据测试即可。 2.6.4 再增加一个路由节点文件夹： 12#-----------mongos02mkdir -p /mongodb/sharded_cluster/mymongos_27117/log 新建或修改配置文件： 1vi /mongodb/sharded_cluster/mymongos_27117/mongos.conf mongos.conf： 12345678910111213141516171819202122systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/sharded_cluster/mymongos_27117/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: /mongodb/sharded_cluster/mymongos_27117/log/mongod.pid\"net: #服务实例绑定所有IP，有副作用，副本集初始化的时候，节点名字会自动设置为本地域名，而不是ip #bindIpAll: true #服务实例绑定的IP bindIp: localhost,192.168.0.2 #bindIp #绑定的端口 port: 27117sharding: configDB: myconfigrs/180.76.159.126:27019,180.76.159.126:27119,180.76.159.126:27219 启动mongos2： 1234[root@bobohost bin]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongos -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;mymongos_27117&#x2F;mongos.confabout to fork child process, waiting until server is ready for connections.forked process: 129874child process started successfully, parent exiting 使用mongo客户端登录27117，发现，第二个路由无需配置，因为分片配置都保存到了配置服务器中了。 2.7 Compass 连接分片集群compass连接： 提示：和连接单机 mongod一样。 连接成功后，上方有mongos和分片集群的提示： 2.8 SpringDataMongDB 连接分片集群Java客户端常用的是SpringDataMongoDB，其连接的是mongs路由，配置和单机mongod的配置是一样的。 多个路由的时候的SpringDataMongoDB的客户端配置参考如下： 12345678910111213141516spring: #数据源配置 data: mongodb: # 主机地址 # host: 180.76.159.126 # 数据库 # database: articledb # 默认端口是27017 # port: 27017 #也可以使用uri连接 # uri: mongodb://192.168.40.134:28017/articledb # 连接副本集字符串 # uri: mongodb://180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaSet&amp;slaveOk=true&amp;replicaSet=myrs #连接路由字符串 uri: mongodb://180.76.159.126:27017,180.76.159.126:27117/articledb 通过日志发现，写入数据的时候，会选择一个路由写入 2.9 清除所有的节点数据（备用）如果在搭建分片的时候有操作失败或配置有问题，需要重新来过的，可以进行如下操作： 第一步：查询出所有的测试服务节点的进程： 123456789101112[root@bobohost sharded_cluster]# ps -ef |grep mongoroot 10184 1 0 06:04 ? 00:01:25 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs01_27018&#x2F;mongod.confroot 10219 1 0 06:04 ? 00:01:25 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs01_27118&#x2F;mongod.confroot 10253 1 0 06:04 ? 00:00:46 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs01_27218&#x2F;mongod.confroot 10312 1 0 06:04 ? 00:01:23 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs02_27318&#x2F;mongod.confroot 10346 1 0 06:05 ? 00:01:23 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs02_27418&#x2F;mongod.confroot 10380 1 0 06:05 ? 00:00:44 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs02_27518&#x2F;mongod.confroot 10414 1 1 06:05 ? 00:01:36 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myconfigrs_27019&#x2F;mongod.confroot 10453 1 1 06:05 ? 00:01:37 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myconfigrs_27119&#x2F;mongod.confroot 10492 1 1 06:05 ? 00:01:38 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;myconfigrs_27219&#x2F;mongod.confroot 11392 1 0 06:15 ? 00:00:24 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongos -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;mymongos_27017&#x2F;mongos.confroot 14829 1 0 07:15 ? 00:00:13 &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongos -f &#x2F;mongodb&#x2F;sharded_cluster&#x2F;mymongos_27117&#x2F;mongos.conf 根据上述的进程编号，依次中断进程： 1kill -2 进程编号 第二步：清除所有的节点的数据： 1234567891011rm -rf /mongodb/sharded_cluster/myconfigrs_27019/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/myconfigrs_27119/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/myconfigrs_27219/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/myshardrs01_27018/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/myshardrs01_27118/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/myshardrs01_27218/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/myshardrs02_27318/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/myshardrs02_27418/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/myshardrs02_27518/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/mymongos_27017/data/db/*.* \\ &amp;rm -rf /mongodb/sharded_cluster/mymongos_27117/data/db/*.* 第三步：查看或修改有问题的配置 第四步：依次启动所有节点，不包括路由节点： 123456789/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf 第五步：对两个数据分片副本集和一个配置副本集进行初始化和相关配置 第六步：检查路由mongos的配置，并启动mongos 12/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/mymongos_27017/mongos.cfg/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/mymongos_27017/mongos.cfg 第七步：mongo登录mongos，在其上进行相关操作。 3. 安全认证3.1 MongoDB 的用户和角色权限简介默认情况下，MongoDB实例启动运行时是没有启用用户访问权限控制的，也就是说，在实例本机服务器上都可以随意连接到实例进行各种操作，MongoDB不会对连接客户端进行用户验证，这是非常危险的。 mongodb官网上说，为了能保障mongodb的安全可以做以下几个步骤： 使用新的端口，默认的27017端口如果一旦知道了ip就能连接上，不太安全。 设置mongodb的网络环境，最好将mongodb部署到公司服务器内网，这样外网是访问不到的。公司内部访问使用vpn等。 开启安全认证。认证要同时设置服务器之间的内部认证方式，同时要设置客户端连接到集群的账号密码认证方式。 为了强制开启用户访问控制(用户验证)，则需要在MongoDB实例启动时使用选项 – auth 或在指定启动配置文件中添加选项 auth=true 。 在开始之前需要了解一下概念 1）启用访问控制： MongoDB使用的是基于角色的访问控制(Role-Based Access Control,RBAC)来管理用户对实例的访问。通过对用户授予一个或多个角色来控制用户访问数据库资源的权限和数据库操作的权限，在对用户分配角色之前，用户无法访问实例。 在实例启动时添加选项 – auth 或指定启动配置文件中添加选项 auth=true 。 2）角色： 在MongoDB中通过角色对用户授予相应数据库资源的操作权限，每个角色当中的权限可以显式指定，也可以通过继承其他角色的权限，或者两都都存在的权限。 3）权限： 权限由指定的数据库资源(resource)以及允许在指定资源上进行的操作(action)组成。 资源(resource)包括：数据库、集合、部分集合和集群； 操作(action)包括：对资源进行的增、删、改、查(CRUD)操作。 在角色定义时可以包含一个或多个已存在的角色，新创建的角色会继承包含的角色所有的权限。在同一个数据库中，新创建角色可以继承其他角色的权限，在 admin 数据库中创建的角色可以继承在其它任意数据库中角色的权限。 关于角色权限的查看，可以通过如下命令查询（了解）： 1234567891011121314151617181920&#x2F;&#x2F; 查询所有角色权限(仅用户自定义角色)&gt; db.runCommand(&#123; rolesInfo: 1 &#125;)&#x2F;&#x2F; 查询所有角色权限(包含内置角色)&gt; db.runCommand(&#123; rolesInfo: 1, showBuiltinRoles: true &#125;)&#x2F;&#x2F; 查询当前数据库中的某角色的权限&gt; db.runCommand(&#123; rolesInfo: &quot;&lt;rolename&gt;&quot; &#125;)&#x2F;&#x2F; 查询其它数据库中指定的角色权限&gt; db.runCommand(&#123; rolesInfo: &#123; role: &quot;&lt;rolename&gt;&quot;, db: &quot;&lt;database&gt;&quot; &#125; &#125;&#x2F;&#x2F; 查询多个角色权限&gt; db.runCommand( &#123; rolesInfo: [ &quot;&lt;rolename&gt;&quot;, &#123; role: &quot;&lt;rolename&gt;&quot;, db: &quot;&lt;database&gt;&quot; &#125;, ... ] &#125;) 示例： 查看所有内置角色： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&gt; db.runCommand(&#123; rolesInfo: 1, showBuiltinRoles: true &#125;)&#123; &quot;roles&quot; : [ &#123; &quot;role&quot; : &quot;__queryableBackup&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;__system&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;backup&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;clusterAdmin&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;clusterManager&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;clusterMonitor&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;dbAdmin&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;dbAdminAnyDatabase&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;dbOwner&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;enableSharding&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;hostManager&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;read&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;readAnyDatabase&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;readWrite&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;readWriteAnyDatabase&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;restore&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;root&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;userAdmin&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125;, &#123; &quot;role&quot; : &quot;userAdminAnyDatabase&quot;, &quot;db&quot; : &quot;admin&quot;, &quot;isBuiltin&quot; : true, &quot;roles&quot; : [ ], &quot;inheritedRoles&quot; : [ ] &#125; ], &quot;ok&quot; : 1&#125; 常用的内置角色： 数据库用户角色： read、readWrite; 所有数据库用户角色： readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase 数据库管理角色： dbAdmin、dbOwner、userAdmin； 集群管理角色： clusterAdmin、clusterManager、clusterMonitor、hostManager； 备份恢复角色： backup、restore； 超级用户角色： root 内部角色： system 角色说明： 角色 权限描述 read 可以读取指定数据库中任何数据 readWrite 可以读写指定数据库中任何数据，包括创建、重命名、删除集合 readAnyDatabase 可以读取所有数据库中任何数据(除了数据库config和local之外) readWriteAnyDatabase 可以读写所有数据库中任何数据(除了数据库config和local之外) userAdminAnyDatabase 可以在指定数据库创建和修改用户(除了数据库config和local之外) dbAdminAnyDatabase 可以读取任何数据库以及对数据库进行清理、修改、压缩、获取统计信息、执行检查等操作(除了数据库config和local之外) dbAdmin 可以读取指定数据库以及对数据库进行清理、修改、压缩、获取统计信息、执行检查等操作 userAdmin 可以在指定数据库创建和修改用户 clusterAdmin 可以对整个集群或数据库系统进行管理操作 backup 备份MongoDB数据最小的权限 restore 从备份文件中还原恢复MongoDB数据(除了system.profile集合)的权限 root 超级账号，超级权限 3.2 单实例环境目标：对单实例的MongoDB服务开启安全认证，这里的单实例指的是未开启副本集或分片的MongoDB实例。 3.2.1 关闭已开启的服务增加mongod的单实例的安全认证功能，可以在服务搭建的时候直接添加，也可以在之前搭建好的服务上添加。 本文使用之前搭建好的服务，因此，先停止之前的服务 停止服务的方式有两种：快速关闭和标准关闭，下面依次说明： （1）快速关闭方法（快速，简单，数据可能会出错） 目标：通过系统的kill命令直接杀死进程： 杀完要检查一下，避免有的没有杀掉。 12#通过进程编号关闭节点kill -2 54410 【补充】 如果一旦是因为数据损坏，则需要进行如下操作（了解）： 1）删除lock文件： 1rm -f /mongodb/single/data/db/*.lock 2 ）修复数据： 1/usr/local/mongodb/bin/mongod --repair --dbpath=/mongodb/single/data/db （2）标准的关闭方法（数据不容易出错，但麻烦）： 目标：通过mongo客户端中的shutdownServer命令来关闭服务 主要的操作步骤参考如下： 123456//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。mongo --port 27017//#切换到admin库use admin//关闭服务db.shutdownServer() 3.2.2 添加用户和权限（1）先按照普通无授权认证的配置，来配置服务端的配置文件 /mongodb/single/mongod.conf 1234567891011121314151617181920212223systemLog: #MongoDB发送所有日志输出的目标指定为文件 destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/single/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 dbPath: \"/mongodb/single/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: true #指定用于保存mongos或mongod进程的进程ID的文件位置，其中mongos或mongod将写入其PID pidFilePath: \"/mongodb/single/log/mongod.pid\"net: #服务实例绑定的IP bindIp: localhost,192.168.0.2 #绑定的端口 port: 27017 （2）按之前未开启认证的方式（不添加 – auth 参数）来启动MongoDB服务 1/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf 提示： 在操作用户时，启动mongod服务时尽量不要开启授权。 （3）使用Mongo客户端登录 1&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27017 （4）创建两个管理员用户，一个是系统的超级管理员 myroot ，一个是admin库的管理用户myadmin 1234567891011121314151617181920&#x2F;&#x2F;切换到admin库&gt; use admin&#x2F;&#x2F;创建系统超级用户 myroot,设置密码123456，设置角色root&#x2F;&#x2F;&gt; db.createUser(&#123;user:&quot;myroot&quot;,pwd:&quot;123456&quot;,roles:[ &#123; &quot;role&quot; : &quot;root&quot;, &quot;db&quot; : &quot;admin&quot; &#125; ]&#125;)&#x2F;&#x2F;或&gt; db.createUser(&#123;user:&quot;myroot&quot;,pwd:&quot;123456&quot;,roles:[&quot;root&quot;]&#125;)&#x2F;&#x2F;创建专门用来管理admin库的账号myadmin，只用来作为用户权限的管理&gt; db.createUser(&#123;user:&quot;myadmin&quot;,pwd:&quot;123456&quot;,roles: [&#123;role:&quot;userAdminAnyDatabase&quot;,db:&quot;admin&quot;&#125;]&#125;)&#x2F;&#x2F;查看已经创建了的用户的情况：&gt; db.system.users.find()&#x2F;&#x2F;删除用户&gt; db.dropUser(&quot;myadmin&quot;)true&#x2F;&#x2F;修改密码&gt; db.changeUserPassword(&quot;myroot&quot;, &quot;123456&quot;) 提示： 本案例创建了两个用户，分别对应超管和专门用来管理用户的角色，事实上，你只需要一个用户即可。如果你对安全要求很高，防止超管泄漏，则不要创建超管用户。 和其它数据库（MySQL）一样，权限的管理都差不多一样，也是将用户和权限信息保存到数据库对应的表中。Mongodb存储所有的用户信息在admin 数据库的集合system.users中，保存用户名、密码和数据库信息。 如果不指定数据库，则创建的指定的权限的用户在所有的数据库上有效，如 {role: “userAdminAnyDatabase”, db:””} （5）认证测试 测试添加的用户是否正确 1234567891011&#x2F;&#x2F;切换到admin&gt; use admin&#x2F;&#x2F;密码输错&gt; db.auth(&quot;myroot&quot;,&quot;12345&quot;)Error: Authentication failed.0&#x2F;&#x2F;密码正确&gt; db.auth(&quot;myroot&quot;,&quot;123456&quot;)1 （6）创建普通用户 创建普通用户可以在没有开启认证的时候添加，也可以在开启认证之后添加，但开启认证之后，必须使用有操作admin库的用户登录认证后才能操作。底层都是将用户信息保存在了admin数据库的集合system.users中 123456789&#x2F;&#x2F;创建(切换)将来要操作的数据库articledb,&gt; use articledb&#x2F;&#x2F;创建用户，拥有articledb数据库的读写权限readWrite，密码是123456&gt; db.createUser(&#123;user: &quot;bobo&quot;, pwd: &quot;123456&quot;, roles: [&#123; role: &quot;readWrite&quot;, db: &quot;articledb&quot; &#125;]&#125;)&#x2F;&#x2F;&gt; db.createUser(&#123;user: &quot;bobo&quot;, pwd: &quot;123456&quot;, roles: [&quot;readWrite&quot;]&#125;)&#x2F;&#x2F;测试是否可用&gt; db.auth(&quot;bobo&quot;,&quot;123456&quot;) 提示： 如果开启了认证后，登录的客户端的用户必须使用admin库的角色，如拥有root角色的myadmin用户，再通过myadmin用户去创建其他角色的用户 3.2.3 服务端开启认证和客户端连接登录3.2.3.1 关闭已经启动的服务1）使用linux命令杀死进程： 123[root@bobohost single]# ps -ef |grep mongoroot 23482 1 0 08:08 ? 00:00:55 /usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf[root@bobohost single]# kill -2 23482 2 ）在mongo客户端中使用shutdownServer命令来关闭 1234567891011121314&gt; db.shutdownServer()shutdown command only works with the admin database; try &#39;use admin&#39;&gt; use adminswitched to db admin&gt; db.shutdownServer()2019-08-14T11:20:16.450+0800 E QUERY [js] Error: shutdownServer failed: &#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;shutdown must run from localhost when running db without auth&quot;, &quot;code&quot; : 13, &quot;codeName&quot; : &quot;Unauthorized&quot; &#125; :_getErrorWithCode@src&#x2F;mongo&#x2F;shell&#x2F;utils.js:25:13DB.prototype.shutdownServer@src&#x2F;mongo&#x2F;shell&#x2F;db.js:453:1@(shell):1:1 需要几个条件： 必须是在 admin库下执行该关闭服务命令。 如果没有开启认证，必须是从 localhost登陆的，才能执行关闭服务命令。 非 localhost的、通过远程登录的，必须有登录且必须登录用户有对admin操作权限才可以。 3.2.3.2 以开启认证的方式启动服务有两种方式开启权限认证启动服务：一种是参数方式，一种是配置文件方式 1）参数方式 在启动时指定参数 – auth ，如： 1/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf --auth 2）配置文件方式 在mongod.conf配置文件中加入：vim /mongodb/single/mongod.conf 123security: #开启授权认证 authorization: enabled 启动时可不加 – auth 参数： 1/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf 3.2.3.3 开启了认证的情况下的客户端登录有两种认证方式，一种是先登录，在mongo shell中认证；一种是登录时直接认证 1）先连接再认证 123456[root@bobohost bin]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27017MongoDB shell version v4.0.10connecting to: mongodb:&#x2F;&#x2F;180.76.159.126:27017&#x2F;?gssapiServiceName&#x3D;mongodbImplicit session: session &#123; &quot;id&quot; : UUID(&quot;53fef661-35d6-4d29-b07c-020291d62e1a&quot;)&#125;MongoDB server version: 4.0.10&gt; 提示： 开启认证后再登录，发现打印的日志比较少了。 相关操作需要认证才可以： 查询admin库中的system.users集合的用户： 123456789101112&gt; use adminswitched to db admin&gt; db.system.users.find()Error: error: &#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;command find requires authentication&quot;, &quot;code&quot; : 13, &quot;codeName&quot; : &quot;Unauthorized&quot;&#125;&gt; db.auth(&quot;myroot&quot;,&quot;123456&quot;)1&gt; db.system.users.find() 查询articledb库中的comment集合的内容： 123456789101112131415161718&gt; use articledbswitched to db articledb&gt; db.comment.find()Error: error: &#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not authorized on articledb to execute command &#123; find: \\&quot;comment\\&quot;, filter: &#123;&#125;, lsid: &#123; id: UUID(\\&quot;53fef661-35d6-4d29-b07c-020291d62e1a\\&quot;) &#125;, $db: \\&quot;articledb\\&quot; &#125;&quot;, &quot;code&quot; : 13, &quot;codeName&quot; : &quot;Unauthorized&quot;&#125;&gt; db.auth(&quot;bobo&quot;,&quot;123456&quot;)1&gt; db.comment.find()Error: error: &#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;too many users are authenticated&quot;, &quot;code&quot; : 13, &quot;codeName&quot; : &quot;Unauthorized&quot;&#125; 提示： 这里可能出现错误，说是太多的用户正在认证了。因为我们确实连续登录了两个用户了。 解决方案：退出shell，重新进来登录认证 12345678910111213141516&gt; exitbye[root@bobohost bin]# .&#x2F;mongo --host 180.76.159.126 --port 27017MongoDB shell version v4.0.10connecting to: mongodb:&#x2F;&#x2F;180.76.159.126:27017&#x2F;?gssapiServiceName&#x3D;mongodbImplicit session: session &#123; &quot;id&quot; : UUID(&quot;329c1897-566d-4231-bcb3-b2acda301863&quot;)&#125;MongoDB server version: 4.0.10&gt; db.auth(&quot;bobo&quot;,&quot;123456&quot;)Error: Authentication failed.0&gt; use articledbswitched to db articledb&gt; db.auth(&quot;bobo&quot;,&quot;123456&quot;)1&gt; db.comment.find() 2）连接时直接认证 对admin数据库进行登录认证和相关操作： 123456789101112131415161718192021222324252627[root@bobohost ~]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27017 --authenticationDatabase admin -u myroot -p 123456MongoDB shell version v4.0.10connecting to: mongodb:&#x2F;&#x2F;180.76.159.126:27017&#x2F;?authSource&#x3D;admin&amp;gssapiServiceName&#x3D;mongodbImplicit session: session &#123; &quot;id&quot; : UUID(&quot;f959b8d6-6994-44bc-9d35-09fc7cd00ba6&quot;)&#125;MongoDB server version: 4.0.10Server has startup warnings:2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] ** WARNING: You arerunning this process as the root user, which is not recommended.2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten]2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten]2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] ** WARNING:&#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled is &#39;always&#39;.2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] ** We suggestsetting it to &#39;never&#39;2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten]2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] ** WARNING:&#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;defrag is &#39;always&#39;.2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten] ** We suggestsetting it to &#39;never&#39;2019-09-10T15:23:40.102+0800 I CONTROL [initandlisten]&gt; show dbs;admin 0.000GBarticledb 0.000GBconfig 0.000GBlocal 0.000GB 对articledb数据库进行登录认证和相关操作： 12345678910[root@bobohost bin]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27017 --authenticationDatabase articledb -u bobo -p 123456MongoDB shell version v4.0.10connecting to: mongodb:&#x2F;&#x2F;180.76.159.126:27017&#x2F;?authSource&#x3D;articledb&amp;gssapiServiceName&#x3D;mongodbImplicit session: session &#123; &quot;id&quot; : UUID(&quot;e5d4148f-373b-45b8-9cff-a927ce617100&quot;)&#125;MongoDB server version: 4.0.10&gt; use articledbswitched to db articledb&gt; db.comment.find() 提示： -u ：用户名 -p ：密码 -- authenticationDatabase ：指定连接到哪个库。当登录是指定用户名密码时，必须指定对应的数据库！ 3.2.4 SpringDataMongoDB连接认证使用用户名和密码连接到 MongoDB 服务器，你必须使用&#39;username:password@hostname/dbname&#39; 格式，’username’为用户名，’password’ 为密码。 目标：使用用户bobo使用密码 123456 连接到MongoDB 服务上。 application.yml： 12345678910111213141516spring: #数据源配置 data: mongodb: # 主机地址 # host: 180.76.159.126 # 数据库 # database: articledb # 默认端口是27017 # port: 27017 #帐号 # username: bobo #密码 # password: 123456 #单机有认证的情况下，也使用字符串连接 uri: mongodb://bobo:123456@180.76.159.126:27017/articledb 3.3 副本集环境3.3.1 前言对于搭建好的mongodb副本集，为了安全，启动安全认证，使用账号密码登录。 副本集环境使用之前搭建好的，架构如下： 对副本集执行访问控制需要配置两个方面 : 副本集和共享集群的各个节点成员之间使用内部身份验证，可以使用密钥文件或x.509证书。密钥文件比较简单，本文使用密钥文件，官方推荐如果是测试环境可以使用密钥文件，但是正式环境，官方推荐x.509证书。原理就是，集群中每一个实例彼此连接的时候都检验彼此使用的证书的内容是否相同。只有证书相同的实例彼此才可以访问 使用客户端连接到mongodb集群时，开启访问授权。对于集群外部的访问。如通过可视化客户端，或者通过代码连接的时候，需要开启授权。 在keyfile身份验证中，副本集中的每个mongod实例都使用keyfile的内容作为共享密码，只有具有正确密钥文件的mongod或者mongos实例可以连接到副本集。密钥文件的内容必须在6到1024个字符之间，并且在unix/linux系统中文件所有者必须有对文件至少有读的权限。 3.3.2 关闭已开启的副本集服务增加副本集的安全认证和服务鉴权功能，可以在副本集搭建的时候直接添加，也可以在之前搭建好的副本集服务上添加。 本文使用之前搭建好的副本集服务，因此，先停止之前的集群服务 停止服务的方式有两种：快速关闭和标准关闭，下面依次说明： （1）快速关闭方法（快速，简单，数据可能会出错） 目标：通过系统的kill命令直接杀死进程： 依次杀死仲裁者、副本节点、主节点，直到所有成员都离线。建议主节点最后kill，以避免潜在的回滚。杀完要检查一下，避免有的没有杀掉。 12#通过进程编号关闭节点kill -2 54410 【补充】 如果一旦是因为数据损坏，则需要进行如下操作（了解）： 1）删除lock文件： 123rm -f /mongodb/replica_sets/myrs_27017/data/db/*.lock \\/mongodb/replica_sets/myrs_27018/data/db/*.lock \\/mongodb/replica_sets/myrs_27019/data/db/mongod.lock \\ 2 ）依次修复数据： 123&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;replica_sets&#x2F;myrs_27017&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;replica_sets&#x2F;myrs_27018&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;replica_sets&#x2F;myrs_27019&#x2F;data&#x2F;db （2）标准的关闭方法（数据不容易出错，但麻烦） 目标：通过mongo客户端中的shutdownServer命令来依次关闭各个服务 关闭副本集中的服务，建议依次关闭仲裁节点、副本节点、主节点。主要的操作步骤参考如下： 12345678&#x2F;&#x2F;客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。mongo --port 27017&#x2F;&#x2F;告知副本集说本机要下线rs.stepDown()&#x2F;&#x2F;#切换到admin库use admin&#x2F;&#x2F;关闭服务db.shutdownServer() 3.3.3 通过主节点添加一个管理员帐号只需要在主节点上添加用户，副本集会自动同步。 开启认证之前，创建超管用户：myroot，密码：123456 1234myrs:PRIMARY&gt; use adminswitched to db adminmyrs:PRIMARY&gt; db.createUser(&#123;user:&quot;myroot&quot;,pwd:&quot;123456&quot;,roles:[&quot;root&quot;]&#125;)Successfully added user: &#123; &quot;user&quot; : &quot;myroot&quot;, &quot;roles&quot; : [ &quot;root&quot; ] &#125; 该步骤也可以在开启认证之后，但需要通过localhost登录才允许添加用户，用户数据也会自动同步到副本集。 后续再创建其他用户，都可以使用该超管用户创建。 3.3.4 创建副本集认证的key文件第一步：生成一个key文件到当前文件夹中。 可以使用任何方法生成密钥文件。例如，以下操作使用openssl生成密码文件，然后使用chmod来更改文件权限，仅为文件所有者提供读取权限 1234[root@bobohost ~]# openssl rand -base64 90 -out .&#x2F;mongo.keyfile[root@bobohost ~]# chmod 400 .&#x2F;mongo.keyfile[root@bobohost ~]# ll mongo.keyfile-r--------. 1 root root 122 8月 14 14:23 mongo.keyfile 提示： 所有副本集节点都必须要用同一份keyfile，一般是在一台机器上生成，然后拷贝到其他机器上，且必须有读的权限，否则将来会报错： permissions on /mongodb/replica_sets/myrs_27017/mongo.keyfile are too open 一定要保证密钥文件一致，文件位置随便。但是为了方便查找，建议每台机器都放到一个固定的位置，都放到和配置文件一起的目录中。 这里将该文件分别拷贝到多个目录中： 123[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27017[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27018[root@bobohost ~]# cp mongo.keyfile /mongodb/replica_sets/myrs_27019 3.3.5 修改配置文件指定keyfile分别编辑几个服务的mongod.conf文件，添加相关内容 /mongodb/replica_sets/myrs_27017/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/replica_sets/myrs_27017/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/replica_sets/myrs_27018/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/replica_sets/myrs_27018/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/replica_sets/myrs_27019/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/replica_sets/myrs_27019/mongo.keyfile #开启认证方式运行 authorization: enabled 3.3.6 重新启动副本集如果副本集是开启状态，则先分别关闭关闭复本集中的每个mongod，从次节点开始。直到副本集的所有成员都离线，包括任何仲裁者。主节点必须是最后一个成员关闭以避免潜在的回滚。 12#通过进程编号关闭三个节点kill -2 54410 54361 54257 分别启动副本集节点： 123/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf 查看进程情况： 1234[root@bobohost replica_sets]# ps -ef |grep mongodroot 62425 1 5 14:43 ? 00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27017/mongod.confroot 62495 1 7 14:43 ? 00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27018/mongod.confroot 62567 1 11 14:43 ? 00:00:01 /usr/local/mongodb/bin/mongod -f /mongodb/replica_sets/myrs_27019/mongod.conf 3.3.7 在主节点上添加普通账号123456789#先用管理员账号登录#切换到admin库use admin#管理员账号认证db.auth(&quot;myroot&quot;,&quot;123456&quot;)#切换到要认证的库use articledb#添加普通用户db.createUser(&#123;user: &quot;bobo&quot;, pwd: &quot;123456&quot;, roles: [&quot;readWrite&quot;]&#125;) 重新连接，使用普通用户 bobo重新登录，查看数据。 3.3.8 SpringDataMongoDB连接副本集使用用户名和密码连接到 MongoDB 服务器，你必须使用&#39;username:password@hostname/dbname&#39; 格式，’username’为用户名，’password’ 为密码。 目标：使用用户bobo使用密码 123456 连接到MongoDB 服务上。 application.yml： 123456spring: #数据源配置 data: mongodb: #副本集有认证的情况下，字符串连接 uri: mongodb://bobo:123456@180.76.159.126:27017,180.76.159.126:27018,180.76.159.126:27019/articledb?connect=replicaSet&amp;slaveOk=true&amp;replicaSet=myrs 3.4 分片集群环境(扩展)3.4.1 关闭已开启的集群服务分片集群环境下的安全认证和副本集环境下基本上一样。 但分片集群的服务器环境和架构较为复杂，建议在搭建分片集群的时候，直接加入安全认证和服务器间的鉴权，如果之前有数据，可先将之前的数据备份出来，再还原回去。 本文使用之前搭建好的集群服务，因此，先停止之前的集群服务 停止服务的方式有两种：快速关闭和标准关闭，下面依次说明： （1）快速关闭方法（快速，简单，数据可能会出错） 目标：通过系统的kill命令直接杀死进程： 依次杀死 mongos路由、配置副本集服务，分片副本集服务，从次节点开始。直到所有成员都离线。副本集杀的时候，建议先杀仲裁者，再杀副本节点，最后是主节点，以避免潜在的回滚。杀完要检查一下，避免有的没有杀掉。 12#通过进程编号关闭节点kill -2 54410 【补充】 如果一旦是因为数据损坏，则需要进行如下操作（了解）： 1）删除lock文件： 123456789rm -f /mongodb/sharded_cluster/myshardrs01_27018/data/db/*.lock \\/mongodb/sharded_cluster/myshardrs01_27118/data/db/*.lock \\/mongodb/sharded_cluster/myshardrs01_27218/data/db/mongod.lock \\/mongodb/sharded_cluster/myshardrs02_27318/data/db/mongod.lock \\/mongodb/sharded_cluster/myshardrs02_27418/data/db/mongod.lock \\/mongodb/sharded_cluster/myshardrs02_27518/data/db/mongod.lock \\/mongodb/sharded_cluster/myconfigrs_27019/data/db/mongod.lock \\/mongodb/sharded_cluster/myconfigrs_27119/data/db/mongod.lock \\/mongodb/sharded_cluster/myconfigrs_27219/data/db/mongod.lock 2 ）依次修复数据： 12345678910111213141516171819202122&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs01_27018&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs01_27118&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs01_27218&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs02_27318&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs02_27418&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myshardrs02_27518&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myconfigrs_27019&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myconfigrs_27119&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;myconfigrs_27219&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;mymongos_27017&#x2F;data&#x2F;db&#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongod --repair --dbpath&#x3D;&#x2F;mongodb&#x2F;sharded_cluster&#x2F;mymongos_27117&#x2F;data&#x2F;db （2）标准的关闭方法（数据不容易出错，但麻烦）： 目标：通过mongo客户端中的shutdownServer命令来依次关闭各个服务 关闭分片服务器副本集中的服务，建议依次关闭仲裁节点、副本节点、主节点。主要的操作步骤参考如下： 12345678&#x2F;&#x2F;客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。mongo --port 27018&#x2F;&#x2F;告知副本集说本机要下线rs.stepDown()&#x2F;&#x2F;#切换到admin库use admin&#x2F;&#x2F;关闭服务db.shutdownServer() 关闭配置服务器副本集的服务，建议依次关闭副本节点、主节点。主要的操作步骤参考如下： 12345678&#x2F;&#x2F;客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。mongo --port 27019&#x2F;&#x2F;告知副本集说本机要下线rs.stepDown()&#x2F;&#x2F;#切换到admin库use admin&#x2F;&#x2F;关闭服务db.shutdownServer() 关闭路由服务器的服务，建议依次关闭两个路由节点。主要的操作步骤参考如下： 12345678&#x2F;&#x2F;客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。mongo --port 27017&#x2F;&#x2F;告知副本集说本机要下线rs.stepDown()&#x2F;&#x2F;#切换到admin库use admin&#x2F;&#x2F;关闭服务db.shutdownServer() 3.4.2 创建副本集认证的key文件第一步：生成一个key文件到当前文件夹中。 可以使用任何方法生成密钥文件。例如，以下操作使用openssl生成密码文件，然后使用chmod来更改文件权限，仅为文件所有者提供读取权限 1234[root@bobohost ~]# openssl rand -base64 90 -out ./mongo.keyfile[root@bobohost ~]# chmod 400 ./mongo.keyfile[root@bobohost ~]# ll mongo.keyfile-r--------. 1 root root 122 8月 14 14:23 mongo.keyfile 提示： 所有副本集节点都必须要用同一份keyfile，一般是在一台机器上生成，然后拷贝到其他机器上，且必须有读的权限，否则将来会报错： permissions on /mongodb/replica_sets/myrs_27017/mongo.keyfile are too open 一定要保证密钥文件一致，文件位置随便。但是为了方便查找，建议每台机器都放到一个固定的位置，都放到和配置文件一起的目录中。 这里将该文件分别拷贝到多个目录中： 1234567891011echo '/mongodb/sharded_cluster/myshardrs01_27018/mongo.keyfile/mongodb/sharded_cluster/myshardrs01_27118/mongo.keyfile/mongodb/sharded_cluster/myshardrs01_27218/mongo.keyfile/mongodb/sharded_cluster/myshardrs02_27318/mongo.keyfile/mongodb/sharded_cluster/myshardrs02_27418/mongo.keyfile/mongodb/sharded_cluster/myshardrs02_27518/mongo.keyfile/mongodb/sharded_cluster/myconfigrs_27019/mongo.keyfile/mongodb/sharded_cluster/myconfigrs_27119/mongo.keyfile/mongodb/sharded_cluster/myconfigrs_27219/mongo.keyfile/mongodb/sharded_cluster/mymongos_27017/mongo.keyfile/mongodb/sharded_cluster/mymongos_27117/mongo.keyfile' | xargs -n 1 cp -v /root/mongo.keyfile 3.4.3 修改配置文件指定keyfile分别编辑几个服务的mongod.conf文件，添加相关内容： /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myshardrs01_27018/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myshardrs01_27118/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myshardrs01_27218/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myshardrs02_27318/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myshardrs02_27418/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myshardrs02_27518/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myconfigrs_27019/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myconfigrs_27119/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf 12345security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/myconfigrs_27219/mongo.keyfile #开启认证方式运行 authorization: enabled /mongodb/sharded_cluster/mymongos_27017/mongos.conf 123security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/mymongos_27017/mongo.keyfile /mongodb/sharded_cluster/mymongos_27117/mongos.conf 123security: #KeyFile鉴权文件 keyFile: /mongodb/sharded_cluster/mymongos_27117/mongo.keyfile mongos 比mongod少了authorization：enabled的配置。原因是，副本集加分片的安全认证需要配置两方面的，副本集各个节点之间使用内部身份验证，用于内部各个mongo实例的通信，只有相同keyfile才能相互访问。所以都要开启 keyFile: /mongodb/sharded_cluster/mymongos_27117/mongo.keyfile 。 然而对于所有的mongod，才是真正的保存数据的分片。mongos只做路由，不保存数据。所以所有的mongod开启访问数据的授权authorization:enabled。这样用户只有账号密码正确才能访问到数据。 3.4.4 重新启动节点必须依次启动配置节点、分片节点、路由节点： 1234567891011/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27019/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27119/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myconfigrs_27219/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27018/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27118/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs01_27218/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27318/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27418/mongod.conf/usr/local/mongodb/bin/mongod -f /mongodb/sharded_cluster/myshardrs02_27518/mongod.conf/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27017/mongos.conf/usr/local/mongodb/bin/mongos -f /mongodb/sharded_cluster/mymongos_27117/mongos.conf 注意： 这里有个非常特别的情况，就是启动顺序。先启动配置节点，再启动分片节点，最后启动路由节点。 如果先启动分片节点，会卡住，提示： 1about to fork child process, waiting until server is ready for connections 这也许是个 bug。原因未知。 3.3.5 创建帐号和认证客户端mongo，通过localhost登录任意一个mongos路由， 1[root@bobohost db]# /usr/local/mongodb/bin/mongo --port 27017 提示：相当于一个后门，只能在 admin下添加用户。 创建一个管理员帐号： 1234mongos&gt; use adminswitched to db adminmongos&gt; db.createUser(&#123;user:&quot;myroot&quot;,pwd:&quot;123456&quot;,roles:[&quot;root&quot;]&#125;)Successfully added user: &#123; &quot;user&quot; : &quot;myroot&quot;, &quot;roles&quot; : [ &quot;root&quot; ] &#125; 提示：如果在开启认证之前已经创建了管理员账号，这里可以忽略 创建一个普通权限帐号： 123456789101112131415161718mongos&gt; use adminswitched to db adminmongos&gt; db.auth(&quot;myroot&quot;,&quot;123456&quot;)1mongos&gt; use articledbswitched to db articledbmongos&gt; db.createUser(&#123;user: &quot;bobo&quot;, pwd: &quot;123456&quot;, roles: [&#123; role: &quot;readWrite&quot;,db: &quot;articledb&quot; &#125;]&#125;)Successfully added user: &#123; &quot;user&quot; : &quot;bobo&quot;, &quot;roles&quot; : [ &#123; &quot;role&quot; : &quot;readWrite&quot;, &quot;db&quot; : &quot;articledb&quot; &#125; ]&#125;mongos&gt; db.auth(&quot;bobo&quot;,&quot;123456&quot;)1 提示： 通过mongos添加的账号信息，只会保存到配置节点的服务中，具体的数据节点不保存账号信息，因此，分片中的账号信息不涉及到同步问题。 mongo客户端登录mongos路由，用管理员帐号登录可查看分片情况： 12345mongos&gt; use adminswitched to db adminmongos&gt; db.auth(&quot;myroot&quot;,&quot;123456&quot;)1mongos&gt; sh.status() 退出连接，重新连接服务，使用普通权限帐号访问数据： 123456789101112131415[root@bobohost db]# &#x2F;usr&#x2F;local&#x2F;mongodb&#x2F;bin&#x2F;mongo --host 180.76.159.126 --port 27017MongoDB shell version v4.0.10connecting to: mongodb:&#x2F;&#x2F;180.76.159.126:27017&#x2F;?gssapiServiceName&#x3D;mongodbImplicit session: session &#123; &quot;id&quot; : UUID(&quot;6f84fa91-2414-407e-b3ab-c0b7eedde825&quot;)&#125;MongoDB server version: 4.0.10mongos&gt; use articledbswitched to db articledbmongos&gt; db.auth(&quot;bobo&quot;,&quot;123456&quot;)1mongos&gt; show collectionscommentcomment2mongos&gt; db.comment.count()10001 3.3.6 SpringDataMongoDB连接认证使用用户名和密码连接到 MongoDB 服务器，你必须使用&#39;username:password@hostname/dbname&#39; 格式，’username’为用户名，’password’ 为密码。 目标：使用用户bobo使用密码 123456 连接到MongoDB 服务上。 application.yml： 123456spring: #数据源配置 data: mongodb: # 分片集群有认证的情况下，字符串连接 uri: mongodb://bobo:123456@180.76.159.126:27017,180.76.159.126:27117/articledb","tags":[{"name":"服务器中间件","slug":"服务器中间件","permalink":"https://wgy1993.gitee.io/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"NoSQL存储","slug":"NoSQL存储","permalink":"https://wgy1993.gitee.io/tags/NoSQL%E5%AD%98%E5%82%A8/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://wgy1993.gitee.io/tags/MongoDB/"}]},{"title":"Linux(三)","date":"2020-09-14T10:28:08.000Z","path":"archives/c09edc62.html","text":"1. CentOS7扩容根分区1.1 LVM根分区扩容 查看现有分区大小 1df -TH LVM分区，磁盘总大小为20G,根分区总容量为17G 关机增加大小为30G（测试环境使用的Vmware Workstation） 扩展分区到30G 查看扩容后磁盘大小 123df -THlsblk 磁盘总大小为30G,根分区为17G 创建分区 1fdisk /dev/sda 将sda剩余空间全部给sda3 刷新分区并创建物理卷 123partprobe /dev/sdapvcreate /dev/sda3 查看卷组名称，以及卷组使用情况 1vgdisplay VG Name为centos 将物理卷扩展到卷组 1vgextend centos /dev/sda3 使用sda3扩展VG centos 查看当前逻辑卷的空间状态 1lvdisplay 需要扩展LV /dev/centos/root 将卷组中的空闲空间扩展到根分区逻辑卷 1lvextend -l +100%FREE /dev/centos/root 刷新根分区 1xfs_growfs /dev/centos/root 查看磁盘使用情况，扩展之前和之后是不一样的 根分区已经变成27G 1.2 非LVM根分区扩容 查看现有的分区大小 非LVM分区，目前磁盘大小为20G，根分区总容量为17G 关机增加磁盘大小为30G 查看磁盘扩容后状态 123lsblkdf -TH 现在磁盘总大小为30G,根分区为17G 进行分区扩展磁盘，记住根分区起始位置和结束位置 删除根分区，切记不要保存 创建分区，箭头位置为分区起始位置 保存退出并刷新分区 1partpeobe /dev/sda 查看分区状态 刷新根分区并查看状态 1xfs_growfs /dev/sda3 根分区大小已变为27G 2. 配置swap分区 创建分区 设置分区的编号为2，选择分区的类型为linux swap w键将分区信息写入，partprobe进行内核读取分区 开启swap分区，命令为swapon 分区的名称 查看swap的分区信息，命令为swapon -s或者free -m 编辑/etc/fstab文件，设置自动挂载的信息，挂载swap分区 查看硬盘UUID： 123ls -l /dev/disk/by-uuid或blkid /dev/sda1","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wgy1993.gitee.io/tags/Linux/"},{"name":"操作系统","slug":"操作系统","permalink":"https://wgy1993.gitee.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"MongoDB(一)","date":"2020-09-12T14:21:16.000Z","path":"archives/d69f1e.html","text":"1. MongoDB 相关概念1.1 业务应用场景传统的关系型数据库（如MySQL），在数据操作的“三高”需求以及应对Web2.0的网站需求面前，显得力不从心。 解释：“三高”需求： High performance - 对数据库高并发读写的需求。 Huge Storage - 对海量数据的高效率存储和访问的需求。 High Scalability &amp;&amp; High Availability- 对数据库的高可扩展性和高可用性的需求。 而MongoDB可应对“三高”需求。 具体的应用场景如： 社交场景，使用 MongoDB 存储存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能。 游戏场景，使用 MongoDB 存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询、高效率存储和访问。 物流场景，使用 MongoDB 存储订单信息，订单状态在运送过程中会不断更新，以 MongoDB 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来。 物联网场景，使用 MongoDB 存储所有接入的智能设备信息，以及设备汇报的日志信息，并对这些信息进行多维度的分析。 视频直播，使用 MongoDB 存储用户信息、点赞互动信息等。 这些应用场景中，数据操作方面的共同特点是： 数据量大 写入操作频繁（读写都很频繁） 价值较低的数据，对事务性要求不高 对于这样的数据，我们更适合使用MongoDB来实现数据的存储。 什么时候选择MongoDB 在架构选型上，除了上述的三个特点外，如果你还犹豫是否要选择它？可以考虑以下的一些问题： 应用不需要事务及复杂 join 支持 新应用，需求会变，数据模型无法确定，想快速迭代开发 应用需要2000-3000以上的读写QPS（更高也可以） 应用需要TB甚至 PB 级别数据存储 应用发展迅速，需要能快速水平扩展 应用要求存储的数据不丢失 应用需要99.999%高可用 应用需要大量的地理位置查询、文本查询 如果上述有1个符合，可以考虑 MongoDB，2个及以上的符合，选择 MongoDB 绝不会后悔。 1.2 MongoDB 简介MongoDB是一个开源、高性能、无模式的文档型数据库，当初的设计就是用于简化开发和方便扩展，是NoSQL数据库产品中的一种。是最像关系型数据库（MySQL）的非关系型数据库。 它支持的数据结构非常松散，是一种类似于 JSON 的 格式叫BSON，所以它既可以存储比较复杂的数据类型，又相当的灵活。 MongoDB中的记录是一个文档，它是一个由字段和值对（field:value）组成的数据结构。MongoDB文档类似于JSON对象，即一个文档认为就是一个对象。字段的数据类型是字符型，它的值除了使用基本的一些类型外，还可以包括其他文档、普通数组和文档数组。 1.3 体系结构MySQL和MongoDB对比 SQL 术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接,MongoDB不支持 嵌入文档 MongoDB通过嵌入式文档来替代多表连接 primary key primary key 主键,MongoDB自动将_id字段设置为主键 1.4 数据模型MongoDB的最小存储单位就是文档(document)对象。文档(document)对象对应于关系型数据库的行。数据在MongoDB中以BSON（Binary-JSON）文档的格式存储在磁盘上。 BSON（Binary Serialized Document Format）是一种类json的一种二进制形式的存储格式，简称Binary JSON。BSON和JSON一样，支持内嵌的文档对象和数组对象，但是BSON有JSON没有的一些数据类型，如Date和BinData类型。 BSON采用了类似于 C 语言结构体的名称、对表示方法，支持内嵌的文档对象和数组对象，具有轻量性、可遍历性、高效性的三个特点，可以有效描述非结构化数据和结构化数据。这种格式的优点是灵活性高，但它的缺点是空间利用率不是很理想。 Bson中，除了基本的JSON类型：string,integer,boolean,double,null,array和object，mongo还使用了特殊的数据类型。这些类型包括date,object id,binary data,regular expression 和code。每一个驱动都以特定语言的方式实现了这些类型，查看你的驱动的文档来获取详细信息。 BSON数据类型参考列表： 数据类型 描述 举例 字符串 UTF-8字符串都可表示为字符串类型的数据 {“x” : “foobar”} 对象id 对象id是文档的12字节的唯一 ID {“X” :ObjectId() } 布尔值 真或者假：true或者false {“x”:true} 数组 值的集合或者列表可以表示成数组 {“x” ： [“a”, “b”, “c”]} 32位整数 类型不可用。JavaScript仅支持64位浮点数，所以32位整数会被自动转换。 shell是不支持该类型的，shell中默认会转换成64位浮点数 64位整数 不支持这个类型。shell会使用一个特殊的内嵌文档来显示64位整数 shell是不支持该类型的，shell中默认会转换成64位浮点数 64位浮点数 shell中的数字就是这一种类型 {“x”：3.14159，”y”：3} null 表示空值或者未定义的对象 {“x”:null} undefined 文档中也可以使用未定义类型 {“x”:undefined} 符号 shell不支持，shell会将数据库中的符号类型的数据自动转换成字符串 正则表达式 文档中可以包含正则表达式，采用JavaScript的正则表达式语法 {“x” ： /foobar/i} 代码 文档中还可以包含JavaScript代码 {“x” ： function() { /* …… */ }} 二进制数据 二进制数据可以由任意字节的串组成，不过shell中无法使用 最大值/最小值 BSON包括一个特殊类型，表示可能的最大值。shell中没有这个类型。 提示： shell默认使用64位浮点型数值。{“x”：3.14}或{“x”：3}。对于整型值，可以使用NumberInt（4字节符号整数）或NumberLong（8字节符号整数），{“x”:NumberInt(“3”)}{“x”:NumberLong(“3”)} 1.5 MongoDB 的特点MongoDB主要有如下特点： 高性能： MongoDB提供高性能的数据持久性。特别是 对嵌入式数据模型的支持减少了数据库系统上的I/O活动。 索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键。（文本索引解决搜索的需求、TTL索引解决历史数据自动过期的需求、地理位置索引可用于构建各种 O2O 应用） mmapv1、wiredtiger、mongorocks（rocksdb）、in-memory 等多引擎支持满足各种场景需求。 Gridfs解决文件存储的需求。 高可用性： MongoDB的复制工具称为副本集（replica set），它可提供自动故障转移和数据冗余。 高扩展性： MongoDB提供了水平可扩展性作为其核心功能的一部分。 分片将数据分布在一组集群的机器上。（海量数据存储，服务能力水平扩展） 从3.4开始，MongoDB支持基于片键创建数据区域。在一个平衡的集群中，MongoDB将一个区域所覆盖的读写只定向到该区域内的那些片。 丰富的查询支持： MongoDB支持丰富的查询语言，支持读和写操作(CRUD)，比如数据聚合、文本搜索和地理空间查询等。 其他特点：如无模式（动态模式）、灵活的文档模型、 2. 单机部署2.1 Windows 系统中的安装启动2.1.1 下载安装包MongoDB 提供了可用于 32 位和 64 位系统的预编译二进制包，你可以从MongoDB官网下载安装，MongoDB 预编译二进制包下地址：https://www.mongodb.com/download-center#community 根据上图所示下载 zip 包。 提示：版本的选择： MongoDB的版本命名规范如：x.y.z； y为奇数时表示当前版本为开发版，如：1.5.2、4.1.13； y为偶数时表示当前版本为稳定版，如：1.6.3、4.0.10； z是修正版本号，数字越大越好。 详情： http://docs.mongodb.org/manual/release-notes/#release-version-numbers 2.1.2 解压安装启动将压缩包解压到一个目录中。在解压目录中，手动建立一个目录用于存放数据文件，如 data/db 方式1：命令行参数方式启动服务 在 bin 目录中打开命令行提示符，输入如下命令： 1mongod --dbpath=..\\data\\db 我们在启动信息中可以看到， mongoDB的默认端口是27017，如果我们想改变默认的启动端口，可以通过–port来指定端口。 为了方便我们每次启动，可以将安装目录的bin目录设置到环境变量的path中， bin 目录下是一些常用命令，比如 mongod 启动服务用的，mongo 客户端连接服务用的。 方式2：配置文件方式启动服务 在解压目录中新建 config 文件夹，该文件夹中新建配置文件 mongod.conf ，内如参考如下： 123storage: #The directory where the mongod instance stores its data.Default Value is \"\\data\\db\" on Windows. dbPath: D:\\mongodb-win32-x86_64-2008plus-ssl-4.0.1\\data 详细配置项内容可以参考官方文档： https://docs.mongodb.com/manual/reference/configuration-options/ 【注意】 配置文件中如果使用双引号，比如路径地址，自动会将双引号的内容转义。如果不转义，则会报错： 1error-parsing-yaml-config-file-yaml-cpp-error-at-line-3-column-15-unknown-escape-character-d 解决： a. 对 \\ 换成 / 或 \\\\ b. 如果路径中没有空格，则无需加引号。 配置文件中不能以Tab分割字段 解决： 将其转换成空格。 启动方式： 123mongod -f ../config/mongod.conf或mongod --config ../config/mongod.conf 更多参数配置： 123456789101112131415systemLog: destination: file #The path of the log file to which mongod or mongos should send all diagnostic logging information path: \"D:/mongodb-win32-x86_64-2008plus-ssl-4.0.1/log/mongod.log\" logAppend: truestorage: journal: enabled: true #The directory where the mongod instance stores its data.Default Value is \"/data/db\". dbPath: \"D:/mongodb-win32-x86_64-2008plus-ssl-4.0.1/data\"net: #bindIp: 127.0.0.1 port: 27017setParameter: enableLocalhostAuthBypass: false 2.2 Shell 连接(mongo命令)在命令提示符输入以下shell命令即可完成登陆 123mongo或mongo --host=127.0.0.1 --port=27017 2.2.1 查看已经有的数据库1show databases 2.2.2 退出 mongodb1exit 更多参数可以通过帮助查看： 1mongo --help 提示： MongoDB javascript shell是一个基于javascript的解释器，故是支持js程序的。 2.3 Compass- 图形化界面客户端到MongoDB官网下载MongoDB Compass，地址： https://www.mongodb.com/download-center/v2/compass?initial=true 如果是下载安装版，则按照步骤安装；如果是下载加压缩版，直接解压，执行里面的MongoDBCompassCommunity.exe 文件即可。 在打开的界面中，输入主机地址、端口等相关信息，点击连接： 2.4 Linux 系统中的安装启动和连接目标：在Linux中部署一个单机的MongoDB，作为生产环境下使用。 提示：和Windows下操作差不多。 步骤如下： 先到官网下载压缩包 mongod -linux-x86_64-4.0.10.tgz 。 上传压缩包到Linux中，解压到当前目录： 1tar -xvf mongodb-linux-x86_64-4.0.10.tgz 移动解压后的文件夹到指定的目录中： 1mv mongodb-linux-x86_64-4.0.10 /usr/local/mongodb 新建几个目录，分别用来存储数据和日志： 1234#数据存储目录mkdir -p /mongodb/single/data/db#日志存储目录mkdir -p /mongodb/single/log 新建并修改配置文件 1vi /mongodb/single/mongod.conf 配置文件的内容如下： 123456789101112131415161718192021222324systemLog: #MongoDB发送所有日志输出的目标指定为文件 # #The path of the log file to which mongod or mongos should send all diagnostic logging information destination: file #mongod或mongos应向其发送所有诊断日志记录信息的日志文件的路径 path: \"/mongodb/single/log/mongod.log\" #当mongos或mongod实例重新启动时，mongos或mongod会将新条目附加到现有日志文件的末尾。 logAppend: truestorage: #mongod实例存储其数据的目录。storage.dbPath设置仅适用于mongod。 ##The directory where the mongod instance stores its data.Default Value is \"/data/db\". dbPath: \"/mongodb/single/data/db\" journal: #启用或禁用持久性日志以确保数据文件保持有效和可恢复。 enabled: trueprocessManagement: #启用在后台运行mongos或mongod进程的守护进程模式。 fork: truenet: #服务实例绑定的IP，默认是localhost bindIp: localhost,192.168.142.128 #bindIp #绑定的端口，默认是27017 port: 27017 启动MongoDB服务 1/usr/local/mongodb/bin/mongod -f /mongodb/single/mongod.conf 注意： 如果启动后不是 successfully ，则是启动失败了。原因基本上就是配置文件有问题。 通过进程来查看服务是否启动了： 1ps -ef |grep mongod 分别使用mongo命令和compass工具来连接测试。 提示：如果远程连接不上，需要配置防火墙放行，或直接关闭linux防火墙 123456#查看防火墙状态systemctl status firewalld#临时关闭防火墙systemctl stop firewalld#开机禁止启动防火墙systemctl disable firewalld 停止关闭服务 停止服务的方式有两种：快速关闭和标准关闭，下面依次说明： （一）快速关闭方法（快速，简单，数据可能会出错） 目标：通过系统的kill命令直接杀死进程： 杀完要检查一下，避免有的没有杀掉。 12#通过进程编号关闭节点kill -2 54410 【补充】 如果一旦是因为数据损坏，则需要进行如下操作（了解）： 1）删除lock文件： 1rm -f /mongodb/single/data/db/*.lock 2 ）修复数据： 1/usr/local/mongdb/bin/mongod --repair --dbpath=/mongodb/single/data/db （二）标准的关闭方法（数据不容易出错，但麻烦）： 目标：通过mongo客户端中的shutdownServer命令来关闭服务 主要的操作步骤参考如下： 123456//客户端登录服务，注意，这里通过localhost登录，如果需要远程登录，必须先登录认证才行。mongo --port 27017//#切换到admin库use admin//关闭服务db.shutdownServer() 3. 基本常用命令3.1 案例需求存放文章评论的数据存放到MongoDB中，数据结构参考如下： 数据库：articledb 专栏文章评论 comment 字段名称 字段含义 字段类型 备注 _id ID ObjectId或String Mongo的主键的字段 articleid 文章ID String content 评论内容 String userid 评论人ID String nickname 评论人昵称 String createdatetime 评论的日期时间 Date likenum 点赞数 Int32 replynum 回复数 Int32 state 状态 String 0：不可见；1：可见； parentid 上级ID String 如果为0表示文章的顶级评论 3.2 数据库操作3.2.1 选择和创建数据库选择和创建数据库的语法格式： 1use 数据库名称 如果数据库不存在则自动创建，例如，以下语句创建 articledb 数据库： 1use articledb 查看有权限的所有的数据库命令 123show dbs或show databases 注意 : 在 MongoDB 中，集合只有在内容插入后才会创建! 就是说，创建集合(数据表)后要再插入一个文档(记录)，集合才会真正创建。 查看当前正在使用的数据库命令 1db MongoDB 中默认的数据库为 test，如果你没有选择数据库，集合将存放在 test 数据库中。 另外： 数据库名可以是满足以下条件的任意UTF-8字符串。 不能是空字符串（ “”)。 不得含有 ‘ ‘（空格)、.、$、/、\\和\\0 (空字符)。 应全部小写。 最多 64字节。 有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。 admin ： 从权限的角度来看，这是”root”数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。 local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合 config : 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。 3.2.2 数据库的删除MongoDB 删除数据库的语法格式如下： 1db.dropDatabase() 提示：主要用来删除已经持久化的数据库 3.3 集合操作集合，类似关系型数据库中的表。 可以显示的创建，也可以隐式的创建。 3.3.1 集合的显式创建基本语法格式： 1db.createCollection(name) 参数说明： name: 要创建的集合名称 例如：创建一个名为 mycollection 的普通集合。 1db.createCollection(\"mycollection\") 查看当前库中的表： show tables命令 123show collections或show tables 集合的命名规范： 集合名不能是空字符串 “”。 集合名不能含有 \\0字符（空字符)，这个字符表示集合名的结尾。 集合名不能以 “system.”开头，这是为系统集合保留的前缀。 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。 3.3.2 集合的隐式创建当向一个集合中插入一个文档的时候，如果集合不存在，则会自动创建集合。 详见 文档的插入 章节。 提示：通常我们使用隐式创建文档即可。 3.3.3 集合的删除集合删除语法格式如下： 123db.collection.drop()或db.集合.drop() 返回值 如果成功删除选定集合，则 drop() 方法返回 true，否则返回 false。 例如：要删除mycollection集合 1db.mycollection.drop() 3.4 文档基本CRUD文档（document）的数据结构和 JSON 基本一样。 所有存储在集合中的数据都是 BSON 格式。 3.4.1 文档的插入3.4.1.1 单个文档插入使用insert() 或 save() 方法向集合中插入文档，语法如下： 1234567db.collection.insert( &lt;document or array of documents&gt;, &#123; writeConcern: &lt;document&gt;, ordered: &lt;boolean&gt; &#125;) 参数： Parameter Type Description document document or array 要插入到集合中的文档或文档数组。（(json格式） writeConcern document Optional. A document expressing the write concern . Omit to use the default write concern. See Write Concern .Do not explicitly set the write concern for the operation if run in a transaction. To use write concern with transactions, see Transactions and Write Concern ordered boolean 可选。如果为真，则按顺序插入数组中的文档，如果其中一个文档出现错误，MongoDB将返回而不处理数组中的其余文档。如果为假，则执行无序插入，如果其中一个文档出现错误，则继续处理数组中的主文档。在版本2.6+中默认为true 【示例】 要向comment的集合(表)中插入一条测试数据： 1db.comment.insert(&#123;\"articleid\":\"100000\",\"content\":\"今天天气真好，阳光明媚\",\"userid\":\"1001\",\"nickname\":\"Rose\",\"createdatetime\":new Date(),\"likenum\":NumberInt(10),\"state\":null&#125;) 提示： comment集合如果不存在，则会隐式创建 mongo中的数字，默认情况下是double类型，如果要存整型，必须使用函数NumberInt(整型数字)，否则取出来就有问题了。 插入当前日期使用 new Date() 插入的数据没有指定 _id ，会自动生成主键值 如果某字段没值，可以赋值为null，或不写该字段。 执行后，如下，说明插入一个数据成功了。 1WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;) 注意： 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 MongoDB区分类型和大小写。 MongoDB的文档不能有重复的键。 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 文档键命名规范： 键不能含有 \\0 (空字符)。这个字符用来表示键的结尾。 . 和$有特别的意义，只有在特定环境下才能使用。 以下划线 “_”开头的键是保留的(不是严格要求的)。 3.4.1.2 批量插入语法： 1234567db.collection.insertMany( [ &lt;document 1&gt; , &lt;document 2&gt;, ... ], &#123; writeConcern: &lt;document&gt;, ordered: &lt;boolean&gt; &#125;) 【示例】 批量插入多条文章评论： 12345678db.comment.insertMany([ &#123;\"_id\":\"1\",\"articleid\":\"100001\",\"content\":\"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。\",\"userid\":\"1002\",\"nickname\":\"相忘于江湖\",\"createdatetime\":new Date(\"2019-08-05T22:08:15.522Z\"),\"likenum\":NumberInt(1000),\"state\":\"1\"&#125;, &#123;\"_id\":\"2\",\"articleid\":\"100001\",\"content\":\"我夏天空腹喝凉开水，冬天喝温开水\",\"userid\":\"1005\",\"nickname\":\"伊人憔悴\",\"createdatetime\":new Date(\"2019-08-05T23:58:51.485Z\"),\"likenum\":NumberInt(888),\"state\":\"1\"&#125;, &#123;\"_id\":\"3\",\"articleid\":\"100001\",\"content\":\"我一直喝凉开水，冬天夏天都喝。\",\"userid\":\"1004\",\"nickname\":\"杰克船长\",\"createdatetime\":new Date(\"2019-08-06T01:05:06.321Z\"),\"likenum\":NumberInt(666),\"state\":\"1\"&#125;, &#123;\"_id\":\"4\",\"articleid\":\"100001\",\"content\":\"专家说不能空腹吃饭，影响健康。\",\"userid\":\"1003\",\"nickname\":\"凯撒\",\"createdatetime\":new Date(\"2019-08-06T08:18:35.288Z\"),\"likenum\":NumberInt(2000),\"state\":\"1\"&#125;, &#123;\"_id\":\"5\",\"articleid\":\"100001\",\"content\":\"研究表明，刚烧开的水千万不能喝，因为烫嘴。\",\"userid\":\"1003\",\"nickname\":\"凯撒\",\"createdatetime\":new Date(\"2019-08-06T11:01:02.521Z\"),\"likenum\":NumberInt(3000),\"state\":\"1\"&#125; ]); 提示： 插入时指定了 _id ，则主键就是该值。 如果某条数据插入失败，将会终止插入，但已经插入成功的数据不会回滚掉。 因为批量插入由于数据较多容易出现失败，因此，可以使用try catch进行异常捕捉处理，测试的时候可以不处理。如（了解）： 12345678910111213try &#123; db.comment.insertMany([ &#123;\"_id\":\"1\",\"articleid\":\"100001\",\"content\":\"我们不应该把清晨浪费在手机上，健康很重要，一杯温水幸福你我他。\",\"userid\":\"1002\",\"nickname\":\"相忘于江湖\",\"createdatetime\":new Date(\"2019-08-05T22:08:15.522Z\"),\"likenum\":NumberInt(1000),\"state\":\"1\"&#125;, &#123;\"_id\":\"2\",\"articleid\":\"100001\",\"content\":\"我夏天空腹喝凉开水，冬天喝温开水\",\"userid\":\"1005\",\"nickname\":\"伊人憔悴\",\"createdatetime\":new Date(\"2019-08-05T23:58:51.485Z\"),\"likenum\":NumberInt(888),\"state\":\"1\"&#125;, &#123;\"_id\":\"3\",\"articleid\":\"100001\",\"content\":\"我一直喝凉开水，冬天夏天都喝。\",\"userid\":\"1004\",\"nickname\":\"杰克船长\",\"createdatetime\":new Date(\"2019-08-06T01:05:06.321Z\"),\"likenum\":NumberInt(666),\"state\":\"1\"&#125;, &#123;\"_id\":\"4\",\"articleid\":\"100001\",\"content\":\"专家说不能空腹吃饭，影响健康。\",\"userid\":\"1003\",\"nickname\":\"凯撒\",\"createdatetime\":new Date(\"2019-08-06T08:18:35.288Z\"),\"likenum\":NumberInt(2000),\"state\":\"1\"&#125;, &#123;\"_id\":\"5\",\"articleid\":\"100001\",\"content\":\"研究表明，刚烧开的水千万不能喝，因为烫嘴。\",\"userid\":\"1003\",\"nickname\":\"凯撒\",\"createdatetime\":new Date(\"2019-08-06T11:01:02.521Z\"),\"likenum\":NumberInt(3000),\"state\":\"1\"&#125; ]); &#125; catch (e) &#123; print (e);&#125; 3.4.2 文档的基本查询查询数据的语法格式如下： 1db.collection.find(&lt;query&gt;, [projection]) 参数： Parameter Type Description query document 可选。使用查询运算符指定选择筛选器。若要返回集合中的所有文档，请省略此参数或传递空文档( {} )。 projection document 可选。指定要在与查询筛选器匹配的文档中返回的字段（投影）。若要返回匹配文档中的所有字段，请省略此参数。 【示例】 3.4.2.1 查询所有如果我们要查询comment集合的所有文档，我们输入以下命令 123db.comment.find()或db.comment.find(&#123;&#125;) 这里你会发现每条文档会有一个叫 _id的字段，这个相当于我们原来关系数据库中表的主键，当你在插入文档记录时没有指定该字段，MongoDB会自动创建，其类型是ObjectID类型。 如果我们在插入文档记录时指定该字段也可以，其类型可以是ObjectID类型，也可以是MongoDB支持的任意类型。 如果我想按一定条件来查询，比如我想查询userid为1003的记录，怎么办？很简单！只 要在find()中添加参数即可，参数也是json格式，如下： 1db.comment.find(&#123;userid:'1003'&#125;) 如果你只需要返回符合条件的第一条数据，我们可以使用findOne命令来实现，语法和find一样。 如：查询用户编号是1003的记录，但只最多返回符合条件的第一条记录： 1db.comment.findOne(&#123;userid:'1003'&#125;) 3.4.2.2 投影查询（Projection Query）如果要查询结果返回部分字段，则需要使用投影查询（不显示所有字段，只显示指定的字段）。 如：查询结果只显示 _id 、userid、nickname : 1db.comment.find(&#123;userid:\"1003\"&#125;,&#123;userid:1,nickname:1&#125;) 默认 _id 会显示。 如：查询结果只显示 、 userid、nickname ，不显示 _id ： 1db.comment.find(&#123;userid:\"1003\"&#125;,&#123;userid:1,nickname:1,_id:0&#125;) 再例如：查询所有数据，但只显示 _id 、userid、nickname : 1db.comment.find(&#123;&#125;,&#123;userid:1,nickname:1&#125;) 3.4.3 文档的更新更新文档的语法： 1234567891011121314db.collection.update(query, update, options)//或db.collection.update( &lt;query&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt;, collation: &lt;document&gt;, arrayFilters: [ &lt;filterdocument1&gt;, ... ], hint: &lt;document|string&gt; // Available starting in MongoDB 4.2 &#125;) 参数： Parameter Type Description query document 更新的选择条件。可以使用与find（）方法中相同的查询选择器，类似sql update查询内where后面的。。在3.0版中进行了更改：当使用upsert:true执行update（）时，如果查询使用点表示法在_id 字段上指定条件，则MongoDB将拒绝插入新文档。 multi boolean 可选。如果设置为true，则更新符合查询条件的多个文档。如果设置为false，则更新一个文档。默认值为false。 【示例】 3.4.3.1 覆盖的修改如果我们想修改_id为1的记录，点赞量为1001，输入以下语句： 1db.comment.update(&#123;_id:\"1\"&#125;,&#123;likenum:NumberInt(1001)&#125;) 执行后，我们会发现，这条文档除了 likenum字段其它字段都不见了， 3.4.3.2 局部修改为了解决这个问题，我们需要使用修改器$set来实现，命令如下： 我们想修改_id为2的记录，浏览量为889，输入以下语句： 1db.comment.update(&#123;_id:\"2\"&#125;,&#123;$set:&#123;likenum:NumberInt(889)&#125;&#125;) 3.4.3.3 批量的修改更新所有用户为 1003 的用户的昵称为 凯撒大帝 。 1234//默认只修改第一条数据db.comment.update(&#123;userid:\"1003\"&#125;,&#123;$set:&#123;nickname:\"凯撒2\"&#125;&#125;)//修改所有符合条件的数据db.comment.update(&#123;userid:\"1003\"&#125;,&#123;$set:&#123;nickname:\"凯撒大帝\"&#125;&#125;,&#123;multi:true&#125;) 提示：如果不加后面的参数，则只更新符合条件的第一条记录 3.4.3.4 列值增长的修改如果我们想实现对某列值在原有值的基础上进行增加或减少，可以使用 $inc 运算符来实现。 需求：对3号数据的点赞数，每次递增1 1db.comment.update(&#123;_id:\"3\"&#125;,&#123;$inc:&#123;likenum:NumberInt(1)&#125;&#125;) 3.4.4 删除文档删除文档的语法结构： 1db.集合名称.remove(条件) 以下语句可以将数据全部删除，请慎用 1db.comment.remove(&#123;&#125;) 如果删除 _id=1的记录，输入以下语句 1db.comment.remove(&#123;_id:\"1\"&#125;) 3.5 文档的分页查询3.5.1 统计查询统计查询使用count()方法，语法如下： 1db.collection.count(query, options) 参数： Parameter Type Description query document 查询选择条件 options document 可选。用于修改计数的额外选项 【示例】 3.5.1.1 统计所有记录数统计comment集合的所有的记录数： 1db.comment.count() 3.5.1.2 按条件统计记录数例如：统计userid为1003的记录条数 1db.comment.count(&#123;userid:\"1003\"&#125;) 提示： 默认情况下 count() 方法返回符合条件的全部记录条数。 3.5.2 分页列表查询可以使用limit()方法来读取指定数量的数据，使用skip()方法来跳过指定数量的数据。 基本语法如下所示： 1db.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER) 如果你想返回指定条数的记录，可以在find方法后调用limit来返回结果(TopN)，默认值20，例如： 1db.comment.find().limit(3) skip 方法同样接受一个数字参数作为跳过的记录条数。（前N个不要）,默认值是0 1db.comment.find().skip(3) 分页查询：需求：每页2个，第二页开始：跳过前两条数据，接着值显示3和4条数据 123456//第一页db.comment.find().skip(0).limit(2)//第二页db.comment.find().skip(2).limit(2)//第三页db.comment.find().skip(4).limit(2) 3.5.3 排序查询sort() 方法对数据进行排序，sort() 方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1为升序排列，而 -1 是用于降序排列。 语法如下所示： 123db.COLLECTION_NAME.find().sort(&#123;KEY:1&#125;)或db.集合名称.find().sort(排序方式) 例如： 对userid降序排列，并对访问量进行升序排列 1db.comment.find().sort(&#123;userid:-1,likenum:1&#125;) 提示： skip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()，和命令编写顺序无关。 3.6 文档的更多查询3.6.1 正则的复杂条件查询MongoDB的模糊查询是通过正则表达式的方式实现的。格式为： 123db.collection.find(&#123;field:/正则表达式/&#125;)或db.集合.find(&#123;字段:/正则表达式/&#125;) 提示：正则表达式是 js的语法，直接量的写法。 例如，我要查询评论内容包含“开水”的所有文档，代码如下： 1db.comment.find(&#123;content:/开水/&#125;) 如果要查询评论的内容中以 “专家”开头的，代码如下： 1db.comment.find(&#123;content:/^专家/&#125;) 3.6.2 比较查询&lt;, &lt;=, &gt;, &gt;= 这个操作符也是很常用的，格式如下: 12345db.集合名称.find(&#123; \"field\" : &#123; $gt: value &#125;&#125;) // 大于: field &gt; valuedb.集合名称.find(&#123; \"field\" : &#123; $lt: value &#125;&#125;) // 小于: field &lt; valuedb.集合名称.find(&#123; \"field\" : &#123; $gte: value &#125;&#125;) // 大于等于: field &gt;= valuedb.集合名称.find(&#123; \"field\" : &#123; $lte: value &#125;&#125;) // 小于等于: field &lt;= valuedb.集合名称.find(&#123; \"field\" : &#123; $ne: value &#125;&#125;) // 不等于: field != value 示例：查询评论点赞数量大于 700的记录 1db.comment.find(&#123;likenum:&#123;$gt:NumberInt(700)&#125;&#125;) 3.6.3 包含查询包含使用$in操作符。 示例：查询评论的集合中userid字段包含1003或1004的文档 1db.comment.find(&#123;userid:&#123;$in:[\"1003\",\"1004\"]&#125;&#125;) 不包含使用 $nin操作符。 示例：查询评论集合中userid字段不包含1003和1004的文档 1db.comment.find(&#123;userid:&#123;$nin:[\"1003\",\"1004\"]&#125;&#125;) 3.6.4 条件连接查询我们如果需要查询同时满足两个以上条件，需要使用$and操作符将条件进行关联。（相当于SQL的and） 格式为： 1$and:[ &#123; &#125;,&#123; &#125;,&#123; &#125; ] 示例：查询评论集合中 likenum大于等于700 并且小于2000的文档： 1db.comment.find(&#123;$and:[&#123;likenum:&#123;$gte:NumberInt(700)&#125;&#125;,&#123;likenum:&#123;$lt:NumberInt(2000)&#125;&#125;]&#125;) 如果两个以上条件之间是或者的关系，我们使用操作符进行关联，与前面 and的使用方式相同格式为： 1$or:[ &#123; &#125;,&#123; &#125;,&#123; &#125; ] 示例：查询评论集合中 userid为1003，或者点赞数小于1000的文档记录 1db.comment.find(&#123;$or:[ &#123;userid:\"1003\"&#125; ,&#123;likenum:&#123;$lt:1000&#125; &#125;]&#125;) 3.7 常用命令小结123456789101112131415选择切换数据库：use articledb插入数据：db.comment.insert(&#123;bson数据&#125;)查询所有数据：db.comment.find();条件查询数据：db.comment.find(&#123;条件&#125;)查询符合条件的第一条记录：db.comment.findOne(&#123;条件&#125;)查询符合条件的前几条记录：db.comment.find(&#123;条件&#125;).limit(条数)查询符合条件的跳过的记录：db.comment.find(&#123;条件&#125;).skip(条数)修改数据：db.comment.update(&#123;条件&#125;,&#123;修改后的数据&#125;) 或db.comment.update(&#123;条件&#125;,&#123;$set:&#123;要修改部分的字段:数据&#125;)修改数据并自增某字段值：db.comment.update(&#123;条件&#125;,&#123;$inc:&#123;自增的字段:步进值&#125;&#125;)删除数据：db.comment.remove(&#123;条件&#125;)统计查询：db.comment.count(&#123;条件&#125;)模糊查询：db.comment.find(&#123;字段名:&#x2F;正则表达式&#x2F;&#125;)条件比较运算：db.comment.find(&#123;字段名:&#123;$gt:值&#125;&#125;)包含查询：db.comment.find(&#123;字段名:&#123;$in:[值1，值2]&#125;&#125;)或db.comment.find(&#123;字段名:&#123;$nin:[值1，值2]&#125;&#125;)条件连接查询：db.comment.find(&#123;$and:[&#123;条件1&#125;,&#123;条件2&#125;]&#125;)或db.comment.find(&#123;$or:[&#123;条件1&#125;,&#123;条件2&#125;]&#125;) 4. 索引-Index4.1 概述索引支持在MongoDB中高效地执行查询。如果没有索引，MongoDB必须执行全集合扫描，即扫描集合中的每个文档，以选择与查询语句匹配的文档。这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。 如果查询存在适当的索引，MongoDB可以使用该索引限制必须检查的文档数。 索引是特殊的数据结构，它以易于遍历的形式存储集合数据集的一小部分。索引存储特定字段或一组字段的值，按字段值排序。索引项的排序支持有效的相等匹配和基于范围的查询操作。此外，MongoDB还可以使用索引中的排序返回排序结果。 官网文档： https://docs.mongodb.com/manual/indexes/ 了解： MongoDB索引使用B树数据结构（确切的说是B-Tree，MySQL是B+Tree） 4.2 索引的类型4.2.1 单字段索引MongoDB支持在文档的单个字段上创建用户定义的升序/降序索引，称为单字段索引（Single Field Index）。 对于单个字段索引和排序操作，索引键的排序顺序（即升序或降序）并不重要，因为MongoDB可以在任何方向上遍历索引。 4.2.2 复合索引MongoDB还支持多个字段的用户定义索引，即复合索引（Compound Index）。 复合索引中列出的字段顺序具有重要意义。例如，如果复合索引由 { userid: 1, score: -1 } 组成，则索引首先按userid正序排序，然后在每个userid的值内，再在按score倒序排序。 4.2.3 其他索引地理空间索引（Geospatial Index）、文本索引（Text Indexes）、哈希索引（Hashed Indexes）。 地理空间索引（Geospatial Index） 为了支持对地理空间坐标数据的有效查询，MongoDB提供了两种特殊的索引：返回结果时使用平面几何的二维索引和返回结果时使用球面几何的二维球面索引。 文本索引（Text Indexes） MongoDB提供了一种文本索引类型，支持在集合中搜索字符串内容。这些文本索引不存储特定于语言的停止词（例如“the”、“a”、“or”），而将集合中的词作为词干，只存储根词。 哈希索引（Hashed Indexes） 为了支持基于散列的分片，MongoDB提供了散列索引类型，它对字段值的散列进行索引。这些索引在其范围内的值分布更加随机，但只支持相等匹配，不支持基于范围的查询。 4.3 索引的管理操作4.3.1 索引的查看说明： 返回一个集合中的所有索引的数组。 语法： 1db.collection.getIndexes() 提示：该语法命令运行要求是 MongoDB 3.0+ 【示例】 查看comment集合中所有的索引情况 1234567891011&gt; db.comment.getIndexes()[ &#123; \"v\" : 2, \"key\" : &#123; \"_id\" : 1 &#125;, \"name\" : \"_id_\", \"ns\" : \"articledb.comment\" &#125;] 结果中显示的是默认 _id 索引。 默认_id索引： MongoDB在创建集合的过程中，在_id 字段上创建一个唯一的索引，默认名字为_id_，该索引可防止客户端插入两个具有相同值的文档，您不能在_id字段上删除此索引。 注意：该索引是唯一索引，因此值不能重复，即 _id 值不能重复的。在分片集群中，通常使用 _id 作为片键。 4.3.2 索引的创建说明：在集合上创建索引。 语法： 1db.collection.createIndex(keys, options) 参数： Parameter Type Description keys document 包含字段和值对的文档，其中字段是索引键，值描述该字段的索引类型。对于字段上的升序索引，请指定值1；对于降序索引，请指定值-1。比如： { 字段:1或-1} ，其中1 为指定按升序创建索引，如果你想按降序来创建索引指定为 -1 即可。另外，MongoDB支持几种不同的索引类型，包括文本、地理空间和哈希索引。 options document 可选。包含一组控制索引创建的选项的文档。有关详细信息，请参见选项详情列表。 options（更多选项）列表： Parameter Type Description background Boolean 建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加“background” 可选参数。 “background” 默认值为false。 unique Boolean 建立的索引是否唯一。指定为true创建唯一索引。默认值为false. name string 索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。 dropDups Boolean 3.0+版本已废弃。在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为false. sparse Boolean 对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false. expireAfterSeconds integer 指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。 v indexversion 索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。 weights document 索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。 default_language string 对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语 language_override string 对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为language. 提示：注意在 3.0.0 版本前创建索引方法为 db.collection.ensureIndex() ，之后的版本使用了 db.collection.createIndex() 方法，ensureIndex() 还能用，但只是 createIndex() 的别名。 【示例】 4.3.2.1 单字段索引示例：对 userid 字段建立索引： 1db.comment.createIndex(&#123;userid:1&#125;) 参数 1：按升序创建索引 compass查看： 4.3.2.2 复合索引对 userid 和 nickname 同时建立复合（Compound）索引： 1db.comment.createIndex(&#123;userid:1,nickname:-1&#125;) compass 中： 4.3.3 索引的移除说明：可以移除指定的索引，或移除所有索引 4.3.3.1 指定索引的移除语法： 1db.collection.dropIndex(index) 参数： Parameter Type Description index string ordocument 指定要删除的索引。可以通过索引名称或索引规范文档指定索引。若要删除文本索引，请指定索引名称。 【示例】 删除 comment 集合中 userid 字段上的升序索引： 1db.comment.dropIndex(&#123;userid:1&#125;) 4.3.3.2 所有索引的移除语法： 1db.collection.dropIndexes() 【示例】 删除 comment 集合中所有索引。 1db.comment.dropIndexes() 提示： _id 的字段的索引是无法删除的，只能删除非 _id 字段的索引。 4.4 索引的使用4.4.1 执行计划分析查询性能（Analyze Query Performance）通常使用执行计划（解释计划、Explain Plan）来查看查询的情况，如查询耗费的时间、是否基于索引查询等。 那么，通常，我们想知道，建立的索引是否有效，效果如何，都需要通过执行计划查看。 语法： 1db.collection.find(query,options).explain(options) 【示例】 查看根据userid查询数据的情况： 123456789101112131415161718192021222324252627282930&gt; db.comment.find(&#123;userid:\"1003\"&#125;).explain()&#123; \"queryPlanner\" : &#123; \"plannerVersion\" : 1, \"namespace\" : \"articledb.comment\", \"indexFilterSet\" : false, \"parsedQuery\" : &#123; \"userid\" : &#123; \"$eq\" : \"1003\" &#125; &#125;, \"winningPlan\" : &#123; \"stage\" : \"COLLSCAN\", \"filter\" : &#123; \"userid\" : &#123; \"$eq\" : \"1003\" &#125; &#125;, \"direction\" : \"forward\" &#125;, \"rejectedPlans\" : [ ] &#125;, \"serverInfo\" : &#123; \"host\" : \"9ef3740277ad\", \"port\" : 27017, \"version\" : \"4.0.10\", \"gitVersion\" : \"c389e7f69f637f7a1ac3cc9fae843b635f20b766\" &#125;, \"ok\" : 1&#125; 关键点看： “stage” : “COLLSCAN”, 表示全集合扫描 下面对userid建立索引 1db.comment.createIndex(&#123;userid:1&#125;) 再次查看执行计划： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&gt; db.comment.find(&#123;userid:\"1013\"&#125;).explain()&#123; \"queryPlanner\" : &#123; \"plannerVersion\" : 1, \"namespace\" : \"articledb.comment\", \"indexFilterSet\" : false, \"parsedQuery\" : &#123; \"userid\" : &#123; \"$eq\" : \"1013\" &#125; &#125;, \"winningPlan\" : &#123; \"stage\" : \"FETCH\", \"inputStage\" : &#123; \"stage\" : \"IXSCAN\", \"keyPattern\" : &#123; \"userid\" : 1 &#125;, \"indexName\" : \"userid_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : &#123; \"userid\" : [ ] &#125;, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : &#123; \"userid\" : [ \"[\\\"1013\\\", \\\"1013\\\"]\" ] &#125; &#125; &#125;, \"rejectedPlans\" : [ ] &#125;, \"serverInfo\" : &#123; \"host\" : \"9ef3740277ad\", \"port\" : 27017, \"version\" : \"4.0.10\", \"gitVersion\" : \"c389e7f69f637f7a1ac3cc9fae843b635f20b766\" &#125;, \"ok\" : 1&#125; 关键点看： “stage” : “IXSCAN” ,基于索引的扫描 compass查看： 4.4.2 涵盖的查询Covered Queries 当查询条件和查询的投影仅包含索引字段时，MongoDB直接从索引返回结果，而不扫描任何文档或将文档带入内存。 这些覆盖的查询可以非常有效。 更多：https://docs.mongodb.com/manual/core/query-optimization/#read-operations-covered-query 【示例】 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&gt; db.comment.find(&#123;userid:\"1003\"&#125;,&#123;userid:1,_id:0&#125;)&#123; \"userid\" : \"1003\" &#125;&#123; \"userid\" : \"1003\" &#125;&gt; db.comment.find(&#123;userid:\"1003\"&#125;,&#123;userid:1,_id:0&#125;).explain()&#123; \"queryPlanner\" : &#123; \"plannerVersion\" : 1, \"namespace\" : \"articledb.comment\", \"indexFilterSet\" : false, \"parsedQuery\" : &#123; \"userid\" : &#123; \"$eq\" : \"1003\" &#125; &#125;, \"winningPlan\" : &#123; \"stage\" : \"PROJECTION\", \"transformBy\" : &#123; \"userid\" : 1, \"_id\" : 0 &#125;, \"inputStage\" : &#123; \"stage\" : \"IXSCAN\", \"keyPattern\" : &#123; \"userid\" : 1 &#125;, \"indexName\" : \"userid_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : &#123; \"userid\" : [ ] &#125;, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : &#123; \"userid\" : [ \"[\\\"1003\\\", \\\"1003\\\"]\" ] &#125; &#125; &#125;, \"rejectedPlans\" : [ ] &#125;, \"serverInfo\" : &#123; \"host\" : \"bobohost.localdomain\", \"port\" : 27017, \"version\" : \"4.0.10\", \"gitVersion\" : \"c389e7f69f637f7a1ac3cc9fae843b635f20b766\" &#125;, \"ok\" : 1&#125; Compass 中： 5. 文章评论5.1 需求分析某头条的文章评论业务如下： 文章示例参考：早晨空腹喝水，是对还是错？ https://www.toutiao.com/a6721476546088927748/ 需要实现以下功能： 基本增删改查API 根据文章id查询评论 评论点赞 5.2 表结构分析数据库：articledb 专栏文章评论 comment 字段名称 字段含义 字段类型 备注 _id ID ObjectId或String Mongo的主键的字段 articleid 文章ID String content 评论内容 String userid 评论人ID String nickname 评论人昵称 String createdatetime 评论的日期时间 Date likenum 点赞数 Int32 replynum 回复数 Int32 state 状态 String 0：不可见；1：可见； parentid 上级ID String 如果为0表示文章的顶级评论 5.3 技术选型5.3.1 mongodb-drivermongodb-driver是mongo官方推出的java连接mongoDB的驱动包，相当于JDBC驱动。我们通过一个入门的案例来了解mongodb-driver的基本使用。 官方驱动说明和下载： http://mongodb.github.io/mongo-java-driver/ 官方驱动示例文档：http://mongodb.github.io/mongo-java-driver/3.8/driver/getting-started/quick-start/ 5.3.2 SpringDataMongoDBSpringData家族成员之一，用于操作MongoDB的持久层框架，封装了底层的mongodb-driver。 官网主页： https://projects.spring.io/spring-data-mongodb/ 我们十次方项目的吐槽微服务就采用SpringDataMongoDB框架。 5.4 文章微服务模块搭建（1）搭建项目工程article，pom.xml引入依赖： 1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.6.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;article&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; （ 2）创建application.yml 123456789101112131415spring: #数据源配置 data: mongodb: # 主机地址 #host: 192.168.142.128 # 数据库 #database: articledb # 默认端口是27017 #port: 27017 #也可以使用uri连接 uri: mongodb://192.168.142.128:27017/articledbserver: # 端口 port: 80 （ 3）创建启动类 1234567891011121314151617package com.wgy;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;/** * 启动类 * * @author wgy */@SpringBootApplicationpublic class ArticleApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ArticleApplication.class, args); &#125;&#125; （4）启动项目，看是否能正常启动，控制台没有错误。 5.5 文章评论实体类的编写创建实体类 创建包com.wgy.article，包下建包po用于存放实体类，创建实体类 1234567891011121314151617181920212223242526272829303132/** * 评论实体类 * * @author wgy *///把一个java类声明为mongodb的文档，可以通过collection参数指定这个类对应的文档。//@Document(collection=\"mongodb 对应 collection 名\")// 若未加 @Document ，该 bean save 到 mongo 的 comment collection// 若添加 @Document ，则 save 到 comment collection@Document(collection = \"comment\")//可以省略，如果省略，则默认使用类名小写映射集合//复合索引// @CompoundIndex( def = \"&#123;'userid': 1, 'nickname': -1&#125;\")public class Comment implements Serializable &#123; //主键标识，该属性的值会自动对应mongodb的主键字段\"_id\"，如果该属性名就叫“id”,则该注解可以省略，否则必须写 @Id private String id;//主键 //该属性对应mongodb的字段的名字，如果一致，则无需该注解 @Field(\"content\") private String content;//吐槽内容 private Date publishtime;//发布日期 //添加了一个单字段的索引 @Indexed private String userid;//发布人ID private String nickname;//昵称 private LocalDateTime createdatetime;//评论的日期时间 private Integer likenum;//点赞数 private Integer replynum;//回复数 private String state;//状态 private String parentid;//上级ID private String articleid; //set/get/toString...&#125; 说明： 索引可以大大提升查询效率，一般在查询字段上添加索引，索引的添加可以通过Mongo的命令来添加，也可以在Java的实体类中通过注解添加。 1）单字段索引注解@Indexed org.springframework.data.mongodb.core.index.Indexed.class 声明该字段需要索引，建索引可以大大的提高查询效率。 Mongo命令参考： 1db.comment.createIndex(&#123;\"userid\":1&#125;) 2 ）复合索引注解@CompoundIndex org.springframework.data.mongodb.core.index.CompoundIndex.class 复合索引的声明，建复合索引可以有效地提高多字段的查询效率。 Mongo命令参考： 1db.comment.createIndex(&#123;\"userid\":1,\"nickname\":-1&#125;) 5.6 文章评论的基本增删改查（1）创建数据访问接口 com.wgy.article包下创建dao包，包下创建接口 12345678/** * 评论的持久层接口 * * @author wgy */public interface CommentRepository extends MongoRepository&lt;Comment, String&gt; &#123;&#125; （ 2）创建业务逻辑类 com.wgy.article包下创建service包，包下创建类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 评论的业务层 * * @author wgy */@Servicepublic class CommentService &#123; //注入dao @Autowired private CommentRepository commentRepository; /** * 保存一个评论 * * @param comment */ public void saveComment(Comment comment) &#123; //如果需要自定义主键，可以在这里指定主键；如果不指定主键，MongoDB会自动生成主键 //设置一些默认初始值。。。 //调用dao commentRepository.save(comment); &#125; /** * 更新评论 * * @param comment */ public void updateComment(Comment comment) &#123; //调用dao commentRepository.save(comment); &#125; /** * 根据id删除评论 * * @param id */ public void deleteCommentById(String id) &#123; //调用dao commentRepository.deleteById(id); &#125; /** * 查询所有评论 * * @return */ public List&lt;Comment&gt; findCommentList() &#123; //调用dao return commentRepository.findAll(); &#125; /** * 根据id查询评论 * * @param id * @return */ public Comment findCommentById(String id) &#123; //调用dao return commentRepository.findById(id).get(); &#125;&#125; （ 3）新建Junit测试类，测试保存和查询所有： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 评论测试类 * * @author wgy */@RunWith(SpringRunner.class)@SpringBootTestpublic class CommentServiceTest &#123; @Autowired private CommentService commentService; /** * 查询所有数据 */ @Test public void testFindCommentList() &#123; List&lt;Comment&gt; commentList = commentService.findCommentList(); System.out.println(commentList); &#125; /** * 测试根据id查询 */ @Test public void testFindCommentById() &#123; Comment commentById = commentService.findCommentById(\"1\"); System.out.println(commentById); &#125; /** * 保存一个评论 */ @Test public void testSaveComment() &#123; Comment comment = new Comment(); comment.setArticleid(\"100000\"); comment.setContent(\"测试添加的数据\"); comment.setCreatedatetime(LocalDateTime.now()); comment.setUserid(\"1003\"); comment.setNickname(\"凯撒大帝\"); comment.setState(\"1\"); comment.setLikenum(0); comment.setReplynum(0); commentService.saveComment(comment); &#125;&#125; 5.7 根据上级ID查询文章评论的分页列表（1）CommentRepository新增方法定义 12345678/** * 根据父id，查询子评论的分页列表 * * @param parentid * @param pageable * @return */Page&lt;Comment&gt; findByParentid(String parentid, Pageable pageable); （2）CommentService新增方法 1234567891011/** * 根据父id查询分页列表 * * @param parentid * @param page * @param size * @return */public Page&lt;Comment&gt; findCommentListByParentid(String parentid, int page, int size) &#123; return commentRepository.findByParentid(parentid, PageRequest.of(page - 1, size));&#125; （3）junit测试用例： 123456789/** * 测试根据父id查询子评论的分页列表 */@Testpublic void testFindCommentListByParentid() &#123; Page&lt;Comment&gt; pageResponse = commentService.findCommentListByParentid(\"3\", 1, 2); System.out.println(\"----总记录数：\" + pageResponse.getTotalElements()); System.out.println(\"----当前页数据：\" + pageResponse.getContent());&#125; （4）测试 使用compass快速插入一条测试数据，数据的内容是对3号评论内容进行评论。 执行测试，结果： 12----总记录数：1----当前页数据：[Comment&#123;id&#x3D;&#39;33&#39;, content&#x3D;&#39;你年轻，火力大&#39;, publishtime&#x3D;null, userid&#x3D;&#39;1003&#39;, nickname&#x3D;&#39;凯撒大帝&#39;,createdatetime&#x3D;null, likenum&#x3D;null, replynum&#x3D;null, state&#x3D;&#39;null&#39;, parentid&#x3D;&#39;3&#39;, articleid&#x3D;&#39;100001&#39;&#125;] 5.8 MongoTemplate 实现评论点赞我们看一下以下点赞的临时示例代码： CommentService 新增updateThumbup方法 123456789/** * 点赞-效率低 * @param id */public void updateCommentThumbupToIncrementingOld(String id)&#123; Comment comment = CommentRepository.findById(id).get(); comment.setLikenum(comment.getLikenum()+1); CommentRepository.save(comment);&#125; 以上方法虽然实现起来比较简单，但是执行效率并不高，因为我只需要将点赞数加 1就可以了，没必要查询出所有字段修改后再更新所有字段。(蝴蝶效应) 我们可以使用MongoTemplate类来实现对某列的操作。 （1）修改CommentService 12345678910111213141516171819202122//注入MongoTemplate@Autowiredprivate MongoTemplate mongoTemplate;/** * 点赞数+1 * * @param id */public void updateCommentLikenum(String id) &#123; // 查询条件 Query query = Query.query(Criteria.where(\"_id\").is(id)); // 更新条件 Update update = new Update(); //局部更新，相当于$set// update.set(key, value) //递增$inc update.inc(\"likenum\"); //参数3：集合的名字或实体类的类型Comment.class mongoTemplate.updateFirst(query, update, Comment.class);&#125; （ 2）测试用例： 1234567/** * 测试点赞数+1 */@Testpublic void testUpdateCommentLikenum() &#123; commentService.updateCommentLikenum(\"3\");&#125; 执行测试用例后，发现点赞数+1了：","tags":[{"name":"服务器中间件","slug":"服务器中间件","permalink":"https://wgy1993.gitee.io/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"NoSQL存储","slug":"NoSQL存储","permalink":"https://wgy1993.gitee.io/tags/NoSQL%E5%AD%98%E5%82%A8/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://wgy1993.gitee.io/tags/MongoDB/"}]},{"title":"ElasticStack(二)","date":"2020-09-03T11:10:20.000Z","path":"archives/1b8ebebc.html","text":"1. Nginx日志分析系统1.1 项目需求Nginx是一款非常优秀的web服务器，往往nginx服务会作为项目的访问入口，那么，nginx的性能保障就变得非常重要了，如果nginx的运行出现了问题就会对项目有较大的影响，所以，我们需要对nginx的运行有监控措施，实时掌握nginx的运行情况，那就需要收集nginx的运行指标和分析nginx的运行日志了。 1.2 业务流程 说明： 通过Beats采集Nginx的指标数据和日志数据 Beats采集到数据后发送到Elasticsearch中 Kibana读取数据进行分析 用户通过Kibana进行查看分析报表 1.3 部署安装Nginx123456789101112tar -xvf nginx-1.11.6.tar.gzyum -y install pcre-devel zlib-devel./configuremake install#启动cd /usr/local/nginx/sbin/./nginx#通过浏览器访问页面并且查看日志#访问地址：http://192.168.142.128/tail -f /usr/local/nginx/logs/access.log 具体操作参考：https://wgy1993.gitee.io/archives/65b69107.html 2. Beats 简介 官网：https://www.elastic.co/cn/products/beats Beats系列产品： 2.1 Filebeat 2.1.1 架构用于监控、收集服务器日志文件. 2.1.2 部署与运行下载（或使用资料中提供的安装包，版本为：filebeat-6.5.4）：https://www.elastic.co/downloads/beats 123456789101112131415161718192021222324252627282930313233343536373839404142434445mkdir &#x2F;itcast&#x2F;beatstar -xvf filebeat-6.5.4-linux-x86_64.tar.gzcd filebeat-6.5.4-linux-x86_64#创建如下配置文件 itcast.ymlfilebeat.inputs:- type: stdin enabled: truesetup.template.settings: index.number_of_shards: 3output.console: pretty: true enable: true #启动filebeat.&#x2F;filebeat -e -c itcast.yml#输入hello运行结果如下：hello&#123; &quot;@timestamp&quot;: &quot;2019-01-12T12:50:03.585Z&quot;, &quot;@metadata&quot;: &#123; #元数据信息 &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.5.4&quot; &#125;, &quot;source&quot;: &quot;&quot;, &quot;offset&quot;: 0, &quot;message&quot;: &quot;hello&quot;, #输入的内容 &quot;prospector&quot;: &#123; #标准输入勘探器 &quot;type&quot;: &quot;stdin&quot; &#125;, &quot;input&quot;: &#123; #控制台标准输入 &quot;type&quot;: &quot;stdin&quot; &#125;, &quot;beat&quot;: &#123; #beat版本以及主机信息 &quot;name&quot;: &quot;itcast01&quot;, &quot;hostname&quot;: &quot;itcast01&quot;, &quot;version&quot;: &quot;6.5.4&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;itcast01&quot; &#125;&#125; 2.1.3 读取文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#配置读取文件项 itcast-log.ymlfilebeat.inputs:- type: log enabled: true paths: - &#x2F;itcast&#x2F;beats&#x2F;logs&#x2F;*.logsetup.template.settings: index.number_of_shards: 3output.console: pretty: true enable: true #启动filebeat.&#x2F;filebeat -e -c itcast-log.yml#&#x2F;haoke&#x2F;beats&#x2F;logs下创建a.log文件，并输入如下内容helloworld#观察filebeat输出&#123; &quot;@timestamp&quot;: &quot;2019-01-12T14:16:10.192Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.5.4&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;itcast01&quot; &#125;, &quot;source&quot;: &quot;&#x2F;haoke&#x2F;beats&#x2F;logs&#x2F;a.log&quot;, &quot;offset&quot;: 0, &quot;message&quot;: &quot;hello&quot;, &quot;prospector&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;beat&quot;: &#123; &quot;version&quot;: &quot;6.5.4&quot;, &quot;name&quot;: &quot;itcast01&quot;, &quot;hostname&quot;: &quot;itcast01&quot; &#125;&#125;&#123; &quot;@timestamp&quot;: &quot;2019-01-12T14:16:10.192Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.5.4&quot; &#125;, &quot;prospector&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;beat&quot;: &#123; &quot;version&quot;: &quot;6.5.4&quot;, &quot;name&quot;: &quot;itcast01&quot;, &quot;hostname&quot;: &quot;itcast01&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;itcast01&quot; &#125;, &quot;source&quot;: &quot;&#x2F;haoke&#x2F;beats&#x2F;logs&#x2F;a.log&quot;, &quot;offset&quot;: 6, &quot;message&quot;: &quot;world&quot;&#125; 可以看出，已经检测到日志文件有更新，立刻就会读取到更新的内容，并且输出到控制台。 2.1.4 自定义字段1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#配置读取文件项 itcast-log.ymlfilebeat.inputs:- type: log enabled: true paths: - /itcast/beats/logs/*.log tags: [\"web\"] #添加自定义tag，便于后续的处理 fields: #添加自定义字段 from: itcast-im fields_under_root: true #true为添加到根节点，false为添加到子节点中setup.template.settings: index.number_of_shards: 3output.console: pretty: true enable: true#启动filebeat./filebeat -e -c itcast-log.yml#/haoke/beats/logs下创建a.log文件，并输入如下内容123#执行效果&#123; \"@timestamp\": \"2019-01-12T14:37:19.845Z\", \"@metadata\": &#123; \"beat\": \"filebeat\", \"type\": \"doc\", \"version\": \"6.5.4\" &#125;, \"offset\": 0, \"tags\": [ \"web\" ], \"prospector\": &#123; \"type\": \"log\" &#125;, \"beat\": &#123; \"name\": \"itcast01\", \"hostname\": \"itcast01\", \"version\": \"6.5.4\" &#125;, \"host\": &#123; \"name\": \"itcast01\" &#125;, \"source\": \"/itcast/beats/logs/a.log\", \"message\": \"123\", \"input\": &#123; \"type\": \"log\" &#125;, \"from\": \"haoke-im\"&#125; 2.1.5 输出到Elasticsearch1234567891011121314# itcast-log.ymlfilebeat.inputs:- type: log enabled: true paths: - /itcast/beats/logs/*.log tags: [\"haoke-im\"] fields: from: haoke-im fields_under_root: falsesetup.template.settings: index.number_of_shards: 3 #指定索引的分区数output.elasticsearch: #指定ES的配置 hosts: [\"192.168.1.7:9200\",\"192.168.1.7:9201\",\"192.168.1.7:9202\"] 在日志文件中输入新的内容进行测试： 查看数据： 2.1.6 Filebeat工作原理Filebeat由两个主要组件组成：prospector 和 harvester。 harvester： 负责读取单个文件的内容。 如果文件在读取时被删除或重命名，Filebeat将继续读取文件。 prospector prospector 负责管理harvester并找到所有要读取的文件来源。 如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester。 Filebeat目前支持两种prospector类型：log和stdin。 Filebeat如何保持文件的状态 Filebeat 保存每个文件的状态并经常将状态刷新到磁盘上的注册文件中。 该状态用于记住harvester正在读取的最后偏移量，并确保发送所有日志行。 如果输出（例如Elasticsearch或Logstash）无法访问，Filebeat会跟踪最后发送的行，并在输出再次可用时继续读取文件。 在Filebeat运行时，每个prospector内存中也会保存的文件状态信息，当重新启动Filebeat时，将使用注册文件的数据来重建文件状态，Filebeat将每个harvester在从保存的最后偏移量继续读取。 文件状态记录在data/registry文件中。 启动命令： 12345678910111213141516171819202122232425262728293031323334353637383940.&#x2F;filebeat -e -c itcast.yml.&#x2F;filebeat -e -c itcast.yml -d &quot;publish&quot;#参数说明 -e: 输出到标准输出，默认输出到syslog和logs下 -c: 指定配置文件 -d: 输出debug信息#测试： .&#x2F;filebeat -e -c itcast-log.yml -d &quot;publish&quot;DEBUG [publish] pipeline&#x2F;processor.go:308 Publish event: &#123; &quot;@timestamp&quot;: &quot;2019-01-12T15:03:50.820Z&quot;, &quot;@metadata&quot;: &#123; &quot;beat&quot;: &quot;filebeat&quot;, &quot;type&quot;: &quot;doc&quot;, &quot;version&quot;: &quot;6.5.4&quot; &#125;, &quot;offset&quot;: 0, &quot;tags&quot;: [ &quot;haoke-im&quot; ], &quot;input&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;prospector&quot;: &#123; &quot;type&quot;: &quot;log&quot; &#125;, &quot;beat&quot;: &#123; &quot;name&quot;: &quot;itcast01&quot;, &quot;hostname&quot;: &quot;itcast01&quot;, &quot;version&quot;: &quot;6.5.4&quot; &#125;, &quot;source&quot;: &quot;&#x2F;haoke&#x2F;beats&#x2F;logs&#x2F;a.log&quot;, &quot;fields&quot;: &#123; &quot;from&quot;: &quot;haoke-im&quot; &#125;, &quot;host&quot;: &#123; &quot;name&quot;: &quot;itcast01&quot; &#125;, &quot;message&quot;: &quot;456&quot;&#125; 2.1.7 读取Nginx日志文件1234567891011121314# itcast-nginx.ymlfilebeat.inputs:- type: log enabled: true paths: - &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;*.log tags: [&quot;nginx&quot;]setup.template.settings: index.number_of_shards: 3 #指定索引的分区数output.elasticsearch: #指定ES的配置 hosts: [&quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;] #启动.&#x2F;filebeat -e -c itcast-nginx.yml 启动后，可以在Elasticsearch中看到索引以及查看数据： 可以看到，在message中已经获取到了nginx的日志，但是，内容并没有经过处理，只是读取到原数据，那么对于我们后期的操作是不利的，有办法解决吗？ 2.1.8 Module前面要想实现日志数据的读取以及处理都是自己手动配置的，其实，在Filebeat中，有大量的Module，可以简化我们的配置，直接就可以使用，如下： 1234567891011121314151617181920212223.&#x2F;filebeat modules listEnabled:Disabled:apache2auditdelasticsearchhaproxyicingaiiskafkakibanalogstashmongodbmysqlnginxosquerypostgresqlredissuricatasystemtraefik 可以看到，内置了很多的module，但是都没有启用，如果需要启用需要进行enable操作： 123456789101112131415161718192021222324.&#x2F;filebeat modules enable nginx #启动.&#x2F;filebeat modules disable nginx #禁用Enabled:nginxDisabled:apache2auditdelasticsearchhaproxyicingaiiskafkakibanalogstashmongodbmysqlredisosquerypostgresqlsuricatasystemtraefik 可以发现，nginx的module已经被启用。 2.1.8.1 nginx module 配置123456789101112131415- module: nginx # Access logsaccess: enabled: true var.paths: [\"/usr/local/nginx/logs/access.log*\"] # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. #var.paths: # Error logserror: enabled: true var.paths: [\"/usr/local/nginx/logs/error.log*\"] # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. #var.paths: 2.1.8.2 配置filebeat123456789101112131415#vim itcast-nginx.ymlfilebeat.inputs:#- type: log# enabled: true# paths:# - /usr/local/nginx/logs/*.log# tags: [\"nginx\"]setup.template.settings: index.number_of_shards: 3output.elasticsearch: hosts: [\"192.168.40.133:9200\",\"192.168.40.134:9200\",\"192.168.40.135:9200\"]filebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: false 2.1.8.3 测试123456789101112.&#x2F;filebeat -e -c itcast-nginx.yml#启动会出错，如下ERROR fileset&#x2F;factory.go:142 Error loading pipeline: Error loading pipeline for fileset nginx&#x2F;access: This module requires the following Elasticsearch plugins:ingest-user-agent, ingest-geoip. You can install them by running the following commands on all the Elasticsearch nodes: sudo bin&#x2F;elasticsearch-plugin install ingest-user-agent sudo bin&#x2F;elasticsearch-plugin install ingest-geoip #解决：需要在Elasticsearch中安装ingest-user-agent、ingest-geoip插件#在资料中可以找到，ingest-user-agent.tar、ingest-geoip.tar、ingest-geoip-conf.tar 3个文件#其中，ingest-user-agent.tar、ingest-geoip.tar解压到plugins下#ingest-geoip-conf.tar解压到config下 测试发现，数据已经写入到了Elasticsearch中，并且拿到的数据更加明确了： 当然了，其他的Module的用法参加官方文档： https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-modules.html 2.2 Metricbeat 定期收集操作系统或应用服务的指标数据 存储到Elasticsearch中，进行实时分析 2.2.1 Metricbeat组成Metricbeat有2部分组成，一部分是Module，另一部分为Metricset。 Module 收集的对象，如：mysql、redis、nginx、操作系统等； Metricset 收集指标的集合，如：cpu、memory、network等； 以Redis Module为例： 2.2.2 部署与收集系统指标12345678910111213141516171819tar -xvf metricbeat-6.5.4-linux-x86_64.tar.gzcd metricbeat-6.5.4-linux-x86_64vim metricbeat.ymlmetricbeat.config.modules: path: $&#123;path.config&#125;&#x2F;modules.d&#x2F;*.yml reload.enabled: falsesetup.template.settings: index.number_of_shards: 1 index.codec: best_compressionsetup.kibana:output.elasticsearch: hosts: [&quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;]processors: - add_host_metadata: ~ - add_cloud_metadata: ~ #启动.&#x2F;metricbeat -e 在ELasticsearch中可以看到，系统的一些指标数据已经写入进去了： system module配置： 12345678910111213141516171819202122232425262728293031323334353637# Module: system# Docs: https://www.elastic.co/guide/en/beats/metricbeat/6.5/metricbeat-module-system.html- module: system period: 10s metricsets: - cpu - load - memory - network - process - process_summary #- core #- diskio #- socket process.include_top_n: by_cpu: 5 # include top 5 processes by CPU by_memory: 5 # include top 5 processes by memory- module: system period: 1m metricsets: - filesystem - fsstatprocessors:- drop_event.when.regexp: system.filesystem.mount_point: '^/(sys|cgroup|proc|dev|etc|host|lib)($|/)'- module: system period: 15m metricsets: - uptime#- module: system# period: 5m# metricsets:# - raid# raid.mount_point: '/' 2.2.3 Module12345678910111213141516171819202122232425262728293031323334353637383940.&#x2F;metricbeat modules list #查看列表Enabled:system #默认启用Disabled:aerospikeapachecephcouchbasedockerdropwizardelasticsearchenvoyproxyetcdgolanggraphitehaproxyhttpjolokiakafkakibanakuberneteskvmlogstashmemcachedmongodbmuninmysqlnginxphp_fpmpostgresqlprometheusrabbitmqredistraefikuwsgivspherewindowszookeeper 2.2.4 Nginx Module2.2.4.1 开启nginx的状态查询在nginx中，需要开启状态查询，才能查询到指标数据。 12345678910111213141516#重新编译nginx.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --with-http_stub_status_modulemakemake install.&#x2F;nginx -V #查询版本信息nginx version: nginx&#x2F;1.11.6built by gcc 4.4.7 20120313 (Red Hat 4.4.7-23) (GCC)configure arguments: --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx --with-http_stub_status_module#配置nginxvim nginx.conflocation &#x2F;nginx-status &#123; stub_status on; access_log off;&#125; 测试： 结果说明： Active connections：正在处理的活动连接数 server accepts handled requests 第一个 server 表示Nginx启动到现在共处理了9个连接 第二个 accepts 表示Nginx启动到现在共成功创建 9 次握手 第三个 handled requests 表示总共处理了 21 次请求 请求丢失数 = 握手数 - 连接数 ，可以看出目前为止没有丢失请求 Reading: 0 Writing: 1 Waiting: 1 Reading：Nginx 读取到客户端的 Header 信息数 Writing：Nginx 返回给客户端 Header 信息数 Waiting：Nginx 已经处理完正在等候下一次请求指令的驻留链接（开启keep-alive的情况下，这个值等于Active - (Reading+Writing)） 2.2.4.2 配置Nginx Module123456789101112131415161718192021222324#启用redis module.&#x2F;metricbeat modules enable nginx#修改nginx module配置vim modules.d&#x2F;nginx.yml# Module: nginx# Docs: https:&#x2F;&#x2F;www.elastic.co&#x2F;guide&#x2F;en&#x2F;beats&#x2F;metricbeat&#x2F;6.5&#x2F;metricbeat-module-nginx.html- module: nginx #metricsets: # - stubstatus period: 10s # Nginx hosts hosts: [&quot;http:&#x2F;&#x2F;192.168.40.133&quot;] # Path to server status. Default server-status server_status_path: &quot;nginx-status&quot; #username: &quot;user&quot; #password: &quot;secret&quot;#启动.&#x2F;metricbeat -e 测试： 可以看到，nginx的指标数据已经写入到了Elasticsearch。 更多的Module使用参见官方文档： https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-modules.html 3. Kibana Kibana 是一款开源的数据分析和可视化平台，它是 Elastic Stack 成员之一，设计用于和 Elasticsearch 协作。您可以使用 Kibana 对 Elasticsearch 索引中的数据进行搜索、查看、交互操作。您可以很方便的利用图表、表格及地图对数据进行多元化的分析和呈现。 官网：https://www.elastic.co/cn/products/kibana 3.1 配置安装1234567891011121314#解压安装包tar -xvf kibana-6.5.4-linux-x86_64.tar.gz#修改配置文件vim config&#x2F;kibana.ymlserver.host: &quot;192.168.40.133&quot; #对外暴露服务的地址elasticsearch.url: &quot;http:&#x2F;&#x2F;192.168.40.133:9200&quot; #配置Elasticsearch#启动.&#x2F;bin&#x2F;kibana#通过浏览器进行访问http:&#x2F;&#x2F;192.168.40.133:5601&#x2F;app&#x2F;kibana 可以看到kibana页面，并且可以看到提示，导入数据到Kibana。 3.2 功能说明 3.3 数据探索首先先添加索引信息： 即可查看索引数据： 3.4 Metricbeat 仪表盘可以将Metricbeat的数据在Kibana中展示。 123456#修改metricbeat配置setup.kibana: host: &quot;192.168.40.133:5601&quot;#安装仪表盘到Kibana.&#x2F;metricbeat setup --dashboards 即可在Kibana中看到仪表盘数据： 查看系统信息： 3.5 Nginx 指标仪表盘 3.6 Nginx 日志仪表盘12345678910111213141516171819#修改配置文件 vim itcast-nginx.ymlfilebeat.inputs:#- type: log# enabled: true# paths:# - &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;*.log# tags: [&quot;nginx&quot;]setup.template.settings: index.number_of_shards: 3output.elasticsearch: hosts: [&quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;]filebeat.config.modules: path: $&#123;path.config&#125;&#x2F;modules.d&#x2F;*.yml reload.enabled: falsesetup.kibana: host: &quot;192.168.40.133:5601&quot; #安装仪表盘到kibana.&#x2F;filebeat -c itcast-nginx.yml setup 可以看到nginx的FileBeat的仪表盘了： 3.7 自定义图表在Kibana中，也可以进行自定义图表，如制作柱形图： 将图表添加到自定义Dashboard中： 3.8 开发者工具在Kibana中，为开发者的测试提供了便捷的工具使用，如下： 4. Logstash4.1 简介 用途： 4.2 部署安装 12345678#检查jdk环境，要求jdk1.8+java -version#解压安装包tar -xvf logstash-6.5.4.tar.gz#第一个logstash示例bin/logstash -e 'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;' 执行效果如下： 4.3 配置详解Logstash的配置有三部分，如下： 1234567891011input &#123; #输入 stdin &#123; ... &#125; #标准输入&#125;filter &#123; #过滤，对数据进行分割、截取等处理 ...&#125;output &#123; #输出 stdout &#123; ... &#125; #标准输出&#125; 4.3.1 输入 采集各种样式、大小和来源的数据，数据往往以各种各样的形式，或分散或集中地存在于很多系统中。 Logstash 支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件。能够以连续的流式传输方式，轻松地从您的日志、指标、Web 应用、数据存储以及各种 AWS 服务采集数据。 4.3.2 过滤 实时解析和转换数据 数据从源传输到存储库的过程中，Logstash 过滤器能够解析各个事件，识别已命名的字段以构建结构，并将它们转换成通用格式，以便更轻松、更快速地分析和实现商业价值。 4.3.3 输出Logstash 提供众多输出选择，您可以将数据发送到您要指定的地方，并且能够灵活地解锁众多下游用例。 4.4 读取自定义日志前面我们通过Filebeat读取了nginx的日志，如果是自定义结构的日志，就需要读取处理后才能使用，所以，这个时候就需要使用Logstash了，因为Logstash有着强大的处理能力，可以应对各种各样的场景。 4.4.1 日志结构12019-03-15 21:21:21|ERROR|读取数据出错|参数：id&#x3D;1002 可以看到，日志中的内容是使用“|”进行分割的，使用，我们在处理的时候，也需要对数据做分割处理。 4.4.2 编写配置文件1234567891011121314151617#vim itcast-pipeline.confinput &#123; file &#123; path &#x3D;&gt; &quot;&#x2F;itcast&#x2F;logstash&#x2F;logs&#x2F;app.log&quot; start_position &#x3D;&gt; &quot;beginning&quot; &#125;&#125;filter &#123; mutate &#123; split &#x3D;&gt; &#123;&quot;message&quot;&#x3D;&gt;&quot;|&quot;&#125; &#125;&#125;output &#123; stdout &#123; codec &#x3D;&gt; rubydebug &#125;&#125; 4.4.3 启动测试12345678910111213141516171819#启动.&#x2F;bin&#x2F;logstash -f .&#x2F;itcast-pipeline.conf#写日志到文件echo &quot;2019-03-15 21:21:21|ERROR|读取数据出错|参数：id&#x3D;1002&quot; &gt;&gt; app.log#输出的结果&#123; &quot;@timestamp&quot; &#x3D;&gt; 2019-03-15T08:44:04.749Z, &quot;path&quot; &#x3D;&gt; &quot;&#x2F;itcast&#x2F;logstash&#x2F;logs&#x2F;app.log&quot;, &quot;@version&quot; &#x3D;&gt; &quot;1&quot;, &quot;host&quot; &#x3D;&gt; &quot;node01&quot;, &quot;message&quot; &#x3D;&gt; [ [0] &quot;2019-03-15 21:21:21&quot;, [1] &quot;ERROR&quot;, [2] &quot;读取数据出错&quot;, [3] &quot;参数：id&#x3D;1002&quot; ]&#125; 可以看到，数据已经被分割了。 4.4.4 输出到Elasticsearch1234567891011121314151617181920212223input &#123; file &#123; path &#x3D;&gt; &quot;&#x2F;itcast&#x2F;logstash&#x2F;logs&#x2F;app.log&quot; #type &#x3D;&gt; &quot;system&quot; start_position &#x3D;&gt; &quot;beginning&quot; &#125;&#125;filter &#123; mutate &#123; split &#x3D;&gt; &#123;&quot;message&quot;&#x3D;&gt;&quot;|&quot;&#125; &#125;&#125;output &#123; elasticsearch &#123; hosts &#x3D;&gt; [ &quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;] &#125;&#125;#启动.&#x2F;bin&#x2F;logstash -f .&#x2F;itcast-pipeline.conf#写入数据echo &quot;2019-03-15 21:21:21|ERROR|读取数据出错|参数：id&#x3D;1003&quot; &gt;&gt; app.log 测试： 5. 综合练习下面我们将前面所学习到的Elasticsearch + Logstash + Beats + Kibana整合起来做一个综合性的练习，目的就是让学生们能够更加深刻的理解Elastic Stack的使用。 5.1 流程说明 应用APP生产日志，用来记录用户的操作 [INFO] 2019-03-15 22:55:20 [cn.itcast.dashboard.Main] - DAU|5206|使用优惠券|2019-03-15 03:37:20 [INFO] 2019-03-15 22:55:21 [cn.itcast.dashboard.Main] - DAU|3880|浏览页面|2019-03-15 07:25:09 通过Filebeat读取日志文件中的内容，并且将内容发送给Logstash，原因是需要对内容做处理 Logstash接收到内容后，进行处理，如分割操作，然后将内容发送到Elasticsearch中 Kibana会读取Elasticsearch中的数据，并且在Kibana中进行设计Dashboard，最后进行展示 说明：日志格式、图表、Dashboard都是自定义的。 5.2 APP介绍APP在生产环境应该是真实的系统，然而，我们现在仅仅的学习，为了简化操作，所以就做数据的模拟生成即可。 业务代码如下： 12345678910111213141516171819202122232425262728@SpringBootApplicationpublic class Main &#123; public static final String[] VISIT = new String[]&#123;\"浏览页面\", \"评论商品\", \"加入收藏\", \"加入购物车\", \"提交订单\", \"使用优惠券\", \"领取优惠券\", \"搜索\", \"查看订单\"&#125;; private static final Logger LOGGER = LoggerFactory.getLogger(Main.class); public static void main(String[] args) throws Exception &#123; while (true) &#123; Long sleep = RandomUtils.nextLong(200, 1000 * 5); Thread.sleep(sleep); Long maxUserId = 9999L; Long userId = RandomUtils.nextLong(1, maxUserId); String visit = VISIT[RandomUtils.nextInt(0, VISIT.length)]; DateTime now = new DateTime(); int maxHour = now.getHourOfDay(); int maxMillis = now.getMinuteOfHour(); int maxSeconds = now.getSecondOfMinute(); String date = now.plusHours(-(RandomUtils.nextInt(0, maxHour))) .plusMinutes(-(RandomUtils.nextInt(0, maxMillis))) .plusSeconds(-(RandomUtils.nextInt(0, maxSeconds))) .toString(\"yyyy-MM-dd HH:mm:ss\"); String result = \"DAU|\" + userId + \"|\" + visit + \"|\" + date; LOGGER.info(result); &#125; &#125;&#125; 运行结果： 123456789[INFO] 2019-03-15 22:54:42 [cn.itcast.dashboard.Main] - DAU|4645|领取优惠券|2019-03-15 07:40:29[INFO] 2019-03-15 22:54:44 [cn.itcast.dashboard.Main] - DAU|3482|领取优惠券|2019-03-15 18:34:04[INFO] 2019-03-15 22:54:48 [cn.itcast.dashboard.Main] - DAU|5607|加入收藏|2019-03-15 22:44:09[INFO] 2019-03-15 22:54:50 [cn.itcast.dashboard.Main] - DAU|9619|加入收藏|2019-03-15 21:39:47[INFO] 2019-03-15 22:54:53 [cn.itcast.dashboard.Main] - DAU|7666|加入收藏|2019-03-15 17:47:18[INFO] 2019-03-15 22:54:54 [cn.itcast.dashboard.Main] - DAU|4871|提交订单|2019-03-15 02:36:27[INFO] 2019-03-15 22:54:55 [cn.itcast.dashboard.Main] - DAU|7126|加入收藏|2019-03-15 16:11:06[INFO] 2019-03-15 22:55:00 [cn.itcast.dashboard.Main] - DAU|9606|评论商品|2019-03-15 02:12:00[INFO] 2019-03-15 22:55:02 [cn.itcast.dashboard.Main] - DAU|7698|查看订单|2019-03-15 08:17:02 代码在资料中可以找到，itcast-dashboard-generate.zip。 部署： 123#打包成jar包，在linux上运行java -jar itcast-dashboard-generate-1.0-SNAPSHOT.jar#运行之后，就可以将日志写入到&#x2F;itcast&#x2F;logs&#x2F;app.log文件中 5.3 Filebeat1234567891011121314#vim itcast-dashboard.ymlfilebeat.inputs:- type: log enabled: true paths: - &#x2F;itcast&#x2F;logs&#x2F;*.logsetup.template.settings: index.number_of_shards: 3output.logstash: hosts: [&quot;192.168.40.133:5044&quot;]#启动.&#x2F;filebeat -e -c itcast-dashboard.yml 5.4 Logstash1234567891011121314151617181920212223242526272829303132333435363738#vim itcast-dashboard.confinput &#123; beats &#123; port &#x3D;&gt; &quot;5044&quot; &#125;&#125;filter &#123; mutate &#123; split &#x3D;&gt; &#123;&quot;message&quot;&#x3D;&gt;&quot;|&quot;&#125; &#125; mutate &#123; add_field &#x3D;&gt; &#123; &quot;userId&quot; &#x3D;&gt; &quot;%&#123;message[1]&#125;&quot; &quot;visit&quot; &#x3D;&gt; &quot;%&#123;message[2]&#125;&quot; &quot;date&quot; &#x3D;&gt; &quot;%&#123;message[3]&#125;&quot; &#125; &#125; mutate &#123; convert &#x3D;&gt; &#123; &quot;userId&quot; &#x3D;&gt; &quot;integer&quot; &quot;visit&quot; &#x3D;&gt; &quot;string&quot; &quot;date&quot; &#x3D;&gt; &quot;string&quot; &#125; &#125;&#125;#output &#123;# stdout &#123; codec &#x3D;&gt; rubydebug &#125;#&#125;output &#123; elasticsearch &#123; hosts &#x3D;&gt; [ &quot;192.168.40.133:9200&quot;,&quot;192.168.40.134:9200&quot;,&quot;192.168.40.135:9200&quot;] &#125;&#125;#启动.&#x2F;bin&#x2F;logstash -f itcast-dashboard.conf 5.5 Kibana启动Kibana： 12345#启动.&#x2F;bin&#x2F;kibana#通过浏览器进行访问http:&#x2F;&#x2F;192.168.40.133:5601&#x2F;app&#x2F;kibana 添加Logstash索引到Kibana中： 5.5.1 时间间隔的柱形图 说明：x轴是时间，以天为单位，y轴是count数 保存：（my-dashboard-时间间隔的柱形图） 5.5.2 各个操作的饼图分布 统计各个操作的数量，形成饼图。 保存：（my-dashboard-各个操作的饼图） 5.5.3 数据表格 在数据探索中进行保存，并且保存，将各个操作的数据以表格的形式展现出来。 保存：（my-dashboard-表格） 5.6 制作Dashboard","tags":[{"name":"业务解决方案","slug":"业务解决方案","permalink":"https://wgy1993.gitee.io/tags/%E4%B8%9A%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"https://wgy1993.gitee.io/tags/ElasticStack/"}]},{"title":"ElasticStack(一)","date":"2020-09-01T14:24:37.000Z","path":"archives/b4bf25de.html","text":"1. Elastic Stack简介如果你没有听说过Elastic Stack，那你一定听说过ELK，实际上ELK是三款软件的简称，分别是Elasticsearch、Logstash、Kibana组成，在发展的过程中，又有新成员Beats的加入，所以就形成了Elastic Stack。所以说，ELK是旧的称呼，Elastic Stack是新的名字。 全系的Elastic Stack技术栈包括： Elasticsearch Elasticsearch 基于java，是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 Logstash Logstash 基于java，是一个开源的用于收集,分析和存储日志的工具。 Kibana Kibana 基于nodejs，也是一个开源和免费的工具，Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的Web 界面，可以汇总、分析和搜索重要数据日志。 Beats Beats是elastic公司开源的一款采集系统监控数据的代理agent，是在被监控服务器上以客户端形式运行的数据收集器的统称，可以直接把数据发送给Elasticsearch或者通过Logstash发送给Elasticsearch，然后进行后续的数据分析活动。 Beats由如下组成: Packetbeat：是一个网络数据包分析器，用于监控、收集网络流量信息，Packetbeat嗅探服务器之间的流量，解析应用层协议，并关联到消息的处理，其支 持ICMP (v4 and v6)、DNS、HTTP、Mysql、PostgreSQL、Redis、MongoDB、Memcache等协议； Filebeat：用于监控、收集服务器日志文件，其已取代 logstash forwarder； Metricbeat：可定期获取外部系统的监控指标信息，其可以监控、收集 Apache、HAProxy、MongoDB、MySQL、Nginx、PostgreSQL、Redis、System、Zookeeper等服务； Winlogbeat：用于监控、收集Windows系统的日志信息； 2. Elasticsearch2.1 简介 官网：https://www.elastic.co/cn/products/elasticsearch 2.2 安装2.2.1 版本说明Elasticsearch的发展是非常快速的，所以在ES5.0之前，ELK的各个版本都不统一，出现了版本号混乱的状态，所以从5.0开始，所有Elastic Stack中的项目全部统一版本号。目前最新版本是6.5.4，我们将基于这一版本进行学习。 2.2.2 下载地址：https://www.elastic.co/cn/downloads/elasticsearch 或者，使用资料中提供的已下载好的安装包。 2.2.3 单机版安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#创建elsearch用户，Elasticsearch不支持root用户运行useradd elsearch#解压安装包tar -zxvf elasticsearch-6.5.4.tar.gz -C &#x2F;wgy&#x2F;es&#x2F;#修改配置文件vim conf&#x2F;elasticsearch.ymlnetwork.host: 0.0.0.0 #设置ip地址，任意网络均可访问#说明：在Elasticsearch中如果，network.host不是localhost或者127.0.0.1的话，就会认为是生产环境，#会对环境的要求比较高，我们的测试环境不一定能够满足，一般情况下需要修改2处配置，如下：#1：修改jvm启动参数vim conf&#x2F;jvm.options-Xms128m #根据自己机器情况修改-Xmx128m#2：一个进程在VMAs(虚拟内存区域)创建内存映射最大数量vim &#x2F;etc&#x2F;sysctl.confvm.max_map_count&#x3D;655360sysctl -p #配置生效#启动ES服务su elsearchcd bin.&#x2F;elasticsearch 或 .&#x2F;elasticsearch -d #后台启动#通过访问进行测试，看到如下信息，就说明ES启动成功了&#123; &quot;name&quot;: &quot;dSQV6I8&quot;, &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;cluster_uuid&quot;: &quot;v5GPTWAtT5emxFdjigFg-w&quot;, &quot;version&quot;: &#123; &quot;number&quot;: &quot;6.5.4&quot;, &quot;build_flavor&quot;: &quot;default&quot;, &quot;build_type&quot;: &quot;tar&quot;, &quot;build_hash&quot;: &quot;d2ef93d&quot;, &quot;build_date&quot;: &quot;2018-12-17T21:17:40.758843Z&quot;, &quot;build_snapshot&quot;: false, &quot;lucene_version&quot;: &quot;7.5.0&quot;, &quot;minimum_wire_compatibility_version&quot;: &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot;: &quot;5.0.0&quot; &#125;, &quot;tagline&quot;: &quot;You Know, for Search&quot;&#125;#停止服务root@itcast:~# jps68709 Jps68072 Elasticsearchkill 68072 #通过kill结束进程 1234567891011121314151617181920212223#启动出错，环境：Centos6[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]#解决：切换到root用户，编辑limits.conf 添加类似如下内容vi &#x2F;etc&#x2F;security&#x2F;limits.conf添加如下内容:* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096[2]: max number of threads [1024] for user [elsearch] is too low, increase to at least [4096]#解决：切换到root用户，进入limits.d目录下修改配置文件。vi &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;90-nproc.conf#修改如下内容：* soft nproc 1024#修改为* soft nproc 4096[3]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk#解决：Centos6不支持SecComp，而ES5.2.0默认bootstrap.system_call_filter为truevim config&#x2F;elasticsearch.yml添加：bootstrap.system_call_filter: false 2.2.4 elasticsearch-head由于ES官方并没有为ES提供界面管理工具，仅仅是提供了后台的服务。elasticsearch-head是一个为ES开发的一个页面客户端工具，其源码托管于GitHub，地址为：https://github.com/mobz/elasticsearch-head head提供了4种安装方式： 源码安装，通过npm run start启动（不推荐） 通过docker安装（推荐） 通过chrome插件安装（推荐） 通过ES的plugin方式安装（不推荐） 通过docker安装 12345678#拉取镜像docker pull mobz/elasticsearch-head:5#创建容器docker create --name elasticsearch-head -p 9100:9100 mobz/elasticsearch-head:5#启动容器docker start elasticsearch-head 通过浏览器进行访问： 注意： 1234由于前后端分离开发，所以会存在跨域问题，需要在服务端做CORS的配置，如下：vim elasticsearch.ymlhttp.cors.enabled: true http.cors.allow-origin: &quot;*&quot;通过chrome插件的方式安装不存在该问题。 chrome插件的方式安装 打开chrome的应用商店，即可安装https://chrome.google.com/webstore/detail/elasticsearch-head/ffmkiejjmecolpfloofpjologoblkegm 建议：推荐使用chrome插件的方式安装，如果网络环境不允许，就采用其它方式安装。 2.3 基本概念索引 索引（index）是Elasticsearch对逻辑数据的逻辑存储，所以它可以分为更小的部分。 可以把索引看成关系型数据库的表，索引的结构是为快速有效的全文索引准备的，特别是它不存储原始值。 Elasticsearch可以把索引存放在一台机器或者分散在多台服务器上，每个索引有一或多个分片（shard），每个分片可以有多个副本（replica）。 文档 存储在Elasticsearch中的主要实体叫文档（document）。用关系型数据库来类比的话，一个文档相当于数据库表中的一行记录。 Elasticsearch和MongoDB中的文档类似，都可以有不同的结构，但Elasticsearch的文档中，相同字段必须有相同类型。 文档由多个字段组成，每个字段可能多次出现在一个文档里，这样的字段叫多值字段（multivalued）。 每个字段的类型，可以是文本、数值、日期等。字段类型也可以是复杂类型，一个字段包含其他子文档或者数组。 映射 所有文档写进索引之前都会先进行分析，如何将输入的文本分割为词条、哪些词条又会被过滤，这种行为叫做映射（mapping）。一般由用户自己定义规则。 文档类型 在Elasticsearch中，一个索引对象可以存储很多不同用途的对象。例如，一个博客应用程序可以保存文章和评论。 每个文档可以有不同的结构。 不同的文档类型不能为相同的属性设置不同的类型。例如，在同一索引中的所有文档类型中，一个叫title的字段必须具有相同的类型。 2.4 RESTful API在Elasticsearch中，提供了功能丰富的RESTful API的操作，包括基本的CRUD、创建索引、删除索引等操作。 2.4.1 创建非结构化索引在Lucene中，创建索引是需要定义字段名称以及字段的类型的，在Elasticsearch中提供了非结构化的索引，就是不需要创建索引结构，即可写入数据到索引中，实际上在Elasticsearch底层会进行结构化操作，此操作对用户是透明的。 创建空索引： 1234567891011121314PUT &#x2F;haoke#请求体&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;2&quot;, #分片数 &quot;number_of_replicas&quot;: &quot;0&quot; #副本数 &#125; &#125;&#125;#删除索引DELETE &#x2F;haoke 2.4.2 插入数据URL规则：POST /{索引}/{类型}/{id} 12345678910111213141516171819202122232425POST &#x2F;haoke&#x2F;user&#x2F;1001#请求体&#123; &quot;id&quot;:1001, &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:20, &quot;sex&quot;:&quot;男&quot;&#125;#响应&#123; &quot;_index&quot;: &quot;haoke&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 1&#125; 说明：非结构化的索引，不需要事先创建，直接插入数据默认创建索引。 不指定id插入数据： 123456789POST &#x2F;haoke&#x2F;user&#x2F;#请求体&#123; &quot;id&quot;:1002, &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:20, &quot;sex&quot;:&quot;男&quot;&#125; 2.4.3 更新数据在Elasticsearch中，文档数据是不为修改的，但是可以通过覆盖的方式进行更新。 123456789PUT &#x2F;haoke&#x2F;user&#x2F;1001#请求体&#123; &quot;id&quot;:1001, &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:21, &quot;sex&quot;:&quot;女&quot;&#125; 更新结果如下： 可以看到数据已经被覆盖了。 问题来了，可以局部更新吗？ – 可以的。 前面不是说，文档数据不能更新吗？ 其实是这样的： 在内部，依然会查询到这个文档数据，然后进行覆盖操作，步骤如下： 从旧文档中检索JSON 修改它 删除旧文档 索引新文档 示例： 123456789#注意：这里多了_update标识POST &#x2F;haoke&#x2F;user&#x2F;1001&#x2F;_update#请求体&#123; &quot;doc&quot;:&#123; &quot;age&quot;:23 &#125;&#125; 可以看到数据已经被局部更新了。 2.4.4 删除数据在Elasticsearch中，删除文档数据，只需要发起DELETE请求即可。 1DELETE &#x2F;haoke&#x2F;user&#x2F;1001 需要注意的是，result表示已经删除，version也更加了。 如果删除一条不存在的数据，会响应404： 说明： 删除一个文档也不会立即从磁盘上移除，它只是被标记成已删除。Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理。 2.4.5 搜索数据根据id搜索数据 12345678910111213141516GET &#x2F;haoke&#x2F;user&#x2F;BbPe_WcB9cFOnF3uebvr#返回的数据如下&#123; &quot;_index&quot;: &quot;haoke&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;BbPe_WcB9cFOnF3uebvr&quot;, &quot;_version&quot;: 8, &quot;found&quot;: true, &quot;_source&quot;: &#123; #原始数据在这里 &quot;id&quot;: 1002, &quot;name&quot;: &quot;李四&quot;, &quot;age&quot;: 40, &quot;sex&quot;: &quot;男&quot; &#125;&#125; 搜索全部数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667GET &#x2F;haoke&#x2F;user&#x2F;_search #响应：（默认返回10条数据）&#123; &quot;took&quot;: 26, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 2, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 1, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;haoke&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;BbPe_WcB9cFOnF3uebvr&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;id&quot;: 1002, &quot;name&quot;: &quot;李四&quot;, &quot;age&quot;: 40, &quot;sex&quot;: &quot;男&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;haoke&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;1001&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;id&quot;: 1001, &quot;name&quot;: &quot;张三&quot;, &quot;age&quot;: 20, &quot;sex&quot;: &quot;男&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;haoke&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;1003&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;id&quot;: 1003, &quot;name&quot;: &quot;王五&quot;, &quot;age&quot;: 30, &quot;sex&quot;: &quot;男&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;haoke&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;1004&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;id&quot;: 1004, &quot;name&quot;: &quot;赵六&quot;, &quot;age&quot;: 30, &quot;sex&quot;: &quot;男&quot; &#125; &#125; ] &#125;&#125; 关键字搜素数据 12#查询年龄等于20的用户GET &#x2F;haoke&#x2F;user&#x2F;_search?q&#x3D;age:20 结果： 2.4.6 DSL搜索Elasticsearch提供丰富且灵活的查询语言叫做DSL查询(Query DSL),它允许你构建更加复杂、强大的查询。 DSL(Domain Specific Language特定领域语言)以JSON请求体的形式出现。 12345678910POST &#x2F;haoke&#x2F;user&#x2F;_search#请求体&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; #match只是查询的一种 &quot;age&quot; : 20 &#125; &#125;&#125; 响应数据： 实现：查询年龄大于30岁的男性用户。 现有数据： 123456789101112131415161718192021POST &#x2F;haoke&#x2F;user&#x2F;_search#请求体&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gt&quot;: 30 &#125; &#125; &#125;, &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;sex&quot;: &quot;男&quot; &#125; &#125; &#125; &#125;&#125; 查询结果： 全文搜索 12345678910POST &#x2F;haoke&#x2F;user&#x2F;_search#请求体&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;张三 李四&quot; &#125; &#125;&#125; 2.4.7 高亮显示123456789101112131415POST &#x2F;haoke&#x2F;user&#x2F;_search#请求体&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;张三 李四&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;name&quot;: &#123;&#125; &#125; &#125;&#125; 2.4.8 聚合在Elasticsearch中，支持聚合操作，类似SQL中的group by操作。 123456789101112POST &#x2F;haoke&#x2F;user&#x2F;_search#请求体&#123; &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125;&#125; 结果： 从结果可以看出，年龄30的有2条数据，20的有一条，40的一条。 3. 核心详解3.1 文档在Elasticsearch中，文档以JSON格式进行存储，可以是复杂的结构，如： 12345678910111213141516&#123; \"_index\": \"haoke\", \"_type\": \"user\", \"_id\": \"1005\", \"_version\": 1, \"_score\": 1, \"_source\": &#123; \"id\": 1005, \"name\": \"孙七\", \"age\": 37, \"sex\": \"女\", \"card\": &#123; \"card_number\": \"123456789\" &#125; &#125;&#125; 其中，card是一个复杂对象，嵌套的Card对象。 元数据（metadata） 一个文档不只有数据。它还包含了元数据(metadata)——关于文档的信息。三个必须的元数据节点是： 节点 说明 _index 文档存储的地方 _type 文档代表的对象的类 _id 文档的唯一标识 _index 索引(index)类似于关系型数据库里的“数据库”——它是我们存储和索引关联数据的地方。 提示：事实上，我们的数据被存储和索引在分片(shards)中，索引只是一个把一个或多个分片分组在一起的逻辑空间。然而，这只是一些内部细节——我们的程序完全不用关心分片。对于我们的程序而言，文档存储在索引(index)中。剩下的细节由Elasticsearch关心既可。 _type 在应用中，我们使用对象表示一些“事物”，例如一个用户、一篇博客、一个评论，或者一封邮件。每个对象都属于一个类(class)，这个类定义了属性或与对象关联的数据。 user 类的对象可能包含姓名、性别、年龄和Email地址。 在关系型数据库中，我们经常将相同类的对象存储在一个表里，因为它们有着相同的结构。同理，在Elasticsearch中，我们使用相同类型(type)的文档表示相同的“事物”，因为他们的数据结构也是相同的。 每个类型(type)都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。所有类型下的文档被存储在同一个索引下，但是类型的映射(mapping)会告诉Elasticsearch不同的文档如何被索引。 _type 的名字可以是大写或小写，不能包含下划线或逗号。我们将使用 blog 做为类型名。 _id id仅仅是一个字符串，它与 _index 和 _type 组合时，就可以在Elasticsearch中唯一标识一个文档。当创建一个文档，你可以自定义 _id ，也可以让Elasticsearch帮你自动生成（32位长度）。 3.2 查询响应3.2.1 pretty可以在查询url后面添加pretty参数，使得返回的json更易查看。 3.2.2 指定响应字段在响应的数据中，如果我们不需要全部的字段，可以指定某些需要的字段进行返回。 1234567891011121314GET &#x2F;haoke&#x2F;user&#x2F;1005?_source&#x3D;id,name#响应&#123; &quot;_index&quot;: &quot;haoke&quot;, &quot;_type&quot;: &quot;user&quot;, &quot;_id&quot;: &quot;1005&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;孙七&quot;, &quot;id&quot;: 1005 &#125;&#125; 如不需要返回元数据，仅仅返回原始数据，可以这样： 1GET &#x2F;haoke&#x2F;user&#x2F;1005&#x2F;_source 还可以这样： 1GET &#x2F;haoke&#x2F;user&#x2F;1005&#x2F;_source?_source&#x3D;id,name 3.3 判断文档是否存在如果我们只需要判断文档是否存在，而不是查询文档内容，那么可以这样： 1HEAD &#x2F;haoke&#x2F;user&#x2F;1005 1HEAD &#x2F;haoke&#x2F;user&#x2F;1006 当然，这只表示你在查询的那一刻文档不存在，但并不表示几毫秒后依旧不存在。另一个进程在这期间可能创建新文档。 3.4 批量操作有些情况下可以通过批量操作以减少网络请求。如：批量查询、批量插入数据。 3.4.1 批量查询123456POST &#x2F;haoke&#x2F;user&#x2F;_mget#请求体&#123; &quot;ids&quot; : [ &quot;1001&quot;, &quot;1003&quot; ]&#125; 结果： 如果，某一条数据不存在，不影响整体响应，需要通过found的值进行判断是否查询到数据。 123456POST &#x2F;haoke&#x2F;user&#x2F;_mget#请求体&#123; &quot;ids&quot; : [ &quot;1001&quot;, &quot;1006&quot; ]&#125; 结果： 3.4.2 _bulk操作在Elasticsearch中，支持批量的插入、修改、删除操作，都是通过_bulk的api完成的。 请求格式如下：（请求格式不同寻常） 12345&#123; action: &#123; metadata &#125;&#125;\\n&#123; request body &#125;\\n&#123; action: &#123; metadata &#125;&#125;\\n&#123; request body &#125;\\n... 批量插入数据： 123456&#123;\"create\":&#123;\"_index\":\"haoke\",\"_type\":\"user\",\"_id\":2001&#125;&#125;&#123;\"id\":2001,\"name\":\"name1\",\"age\": 20,\"sex\": \"男\"&#125;&#123;\"create\":&#123;\"_index\":\"haoke\",\"_type\":\"user\",\"_id\":2002&#125;&#125;&#123;\"id\":2002,\"name\":\"name2\",\"age\": 20,\"sex\": \"男\"&#125;&#123;\"create\":&#123;\"_index\":\"haoke\",\"_type\":\"user\",\"_id\":2003&#125;&#125;&#123;\"id\":2003,\"name\":\"name3\",\"age\": 20,\"sex\": \"男\"&#125; 注意最后一行的回车。 响应结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; \"took\": 17, \"errors\": false, \"items\": [ &#123; \"create\": &#123; \"_index\": \"haoke\", \"_type\": \"user\", \"_id\": \"2001\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 1, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 24, \"_primary_term\": 1, \"status\": 201 &#125; &#125;, &#123; \"create\": &#123; \"_index\": \"haoke\", \"_type\": \"user\", \"_id\": \"2002\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 1, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 0, \"_primary_term\": 1, \"status\": 201 &#125; &#125;, &#123; \"create\": &#123; \"_index\": \"haoke\", \"_type\": \"user\", \"_id\": \"2003\", \"_version\": 1, \"result\": \"created\", \"_shards\": &#123; \"total\": 1, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 1, \"_primary_term\": 1, \"status\": 201 &#125; &#125; ]&#125; 批量删除： 123&#123;\"delete\":&#123;\"_index\":\"haoke\",\"_type\":\"user\",\"_id\":2001&#125;&#125;&#123;\"delete\":&#123;\"_index\":\"haoke\",\"_type\":\"user\",\"_id\":2002&#125;&#125;&#123;\"delete\":&#123;\"_index\":\"haoke\",\"_type\":\"user\",\"_id\":2003&#125;&#125; 由于delete没有请求体，所以，action的下一行直接就是下一个action。 响应结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&#123; \"took\": 3, \"errors\": false, \"items\": [ &#123; \"delete\": &#123; \"_index\": \"haoke\", \"_type\": \"user\", \"_id\": \"2001\", \"_version\": 2, \"result\": \"deleted\", \"_shards\": &#123; \"total\": 1, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 25, \"_primary_term\": 1, \"status\": 200 &#125; &#125;, &#123; \"delete\": &#123; \"_index\": \"haoke\", \"_type\": \"user\", \"_id\": \"2002\", \"_version\": 2, \"result\": \"deleted\", \"_shards\": &#123; \"total\": 1, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 2, \"_primary_term\": 1, \"status\": 200 &#125; &#125;, &#123; \"delete\": &#123; \"_index\": \"haoke\", \"_type\": \"user\", \"_id\": \"2003\", \"_version\": 2, \"result\": \"deleted\", \"_shards\": &#123; \"total\": 1, \"successful\": 1, \"failed\": 0 &#125;, \"_seq_no\": 3, \"_primary_term\": 1, \"status\": 200 &#125; &#125; ]&#125; 其他操作就类似了。 一次请求多少性能最高？ 整个批量请求需要被加载到接受我们请求节点的内存里，所以请求越大，给其它请求可用的内存就越小。有一个最佳的bulk请求大小。超过这个大小，性能不再提升而且可能降低。 最佳大小，当然并不是一个固定的数字。它完全取决于你的硬件、你文档的大小和复杂度以及索引和搜索的负载。 幸运的是，这个最佳点(sweetspot)还是容易找到的：试着批量索引标准的文档，随着大小的增长，当性能开始降低，说明你每个批次的大小太大了。开始的数量可以在1000~5000个文档之间，如果你的文档非常大，可以使用较小的批次。 通常着眼于你请求批次的物理大小是非常有用的。一千个1kB的文档和一千个1MB的文档大不相同。一个好的批次最好保持在5-15MB大小间。 3.5 分页和SQL使用 LIMIT 关键字返回只有一页的结果一样，Elasticsearch接受 from 和 size 参数： 12size: 结果数，默认10from: 跳过开始的结果数，默认0 如果你想每页显示5个结果，页码从1到3，那请求如下： 123GET &#x2F;_search?size&#x3D;5GET &#x2F;_search?size&#x3D;5&amp;from&#x3D;5GET &#x2F;_search?size&#x3D;5&amp;from&#x3D;10 应该当心分页太深或者一次请求太多的结果。结果在返回前会被排序。但是记住一个搜索请求常常涉及多个分片。每个分片生成自己排好序的结果，它们接着需要集中起来排序以确保整体排序正确。 1GET &#x2F;haoke&#x2F;user&#x2F;_search?size&#x3D;1&amp;from&#x3D;2 在集群系统中深度分页 为了理解为什么深度分页是有问题的，让我们假设在一个有5个主分片的索引中搜索。当我们请求结果的第一页（结果1到10）时，每个分片产生自己最顶端10个结果然后返回它们给请求节点(requesting node)，它再排序这所有的50个结果以选出顶端的10个结果。 现在假设我们请求第1000页——结果10001到10010。工作方式都相同，不同的是每个分片都必须产生顶端的10010个结果。然后请求节点排序这50050个结果并丢弃50040个！ 你可以看到在分布式系统中，排序结果的花费随着分页的深入而成倍增长。这也是为什么网络搜索引擎中任何语句不能返回多于1000个结果的原因。 3.6 映射前面我们创建的索引以及插入数据，都是由Elasticsearch进行自动判断类型，有些时候我们是需要进行明确字段类型的，否则，自动判断的类型和实际需求是不相符的。 自动判断的规则如下： JSON type Field type Boolean: true or false “boolean” Whole number: 123 “long” Floating point: 123.45 “double” String, valid date: “2014-09-15” “date” String: “foo bar” “string” Elasticsearch中支持的类型如下： 类型 表示的数据类型 String string , text , keyword Whole number byte , short , integer , long Floating point float , double Boolean boolean Date date string类型在ElasticSearch 旧版本中使用较多，从ElasticSearch 5.x开始不再支持string，由text和keyword类型替代。 text 类型，当一个字段是要被全文搜索的，比如Email内容、产品描述，应该使用text类型。设置text类型以后，字段内容会被分析，在生成倒排索引以前，字符串会被分析器分成一个一个词项。text类型的字段不用于排序，很少用于聚合。 keyword类型适用于索引结构化的字段，比如email地址、主机名、状态码和标签。如果字段需要进行过滤(比如查找已发布博客中status属性为published的文章)、排序、聚合。keyword类型的字段只能通过精确值搜索到。 创建明确类型的索引： 1234567891011121314151617181920212223242526272829PUT &#x2F;itcast#请求体&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;2&quot;, &quot;number_of_replicas&quot;: &quot;0&quot; &#125; &#125;, &quot;mappings&quot;: &#123; &quot;person&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;mail&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;hobby&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 查看映射： 1GET &#x2F;itcast&#x2F;_mapping 插入数据： 12345678910111213POST &#x2F;itcast&#x2F;_bulk#请求体&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;张三&quot;,&quot;age&quot;: 20,&quot;mail&quot;: &quot;111@qq.com&quot;,&quot;hobby&quot;:&quot;羽毛球、乒乓球、足球&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;李四&quot;,&quot;age&quot;: 21,&quot;mail&quot;: &quot;222@qq.com&quot;,&quot;hobby&quot;:&quot;羽毛球、乒乓球、足球、篮球&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;王五&quot;,&quot;age&quot;: 22,&quot;mail&quot;: &quot;333@qq.com&quot;,&quot;hobby&quot;:&quot;羽毛球、篮球、游泳、听音乐&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;赵六&quot;,&quot;age&quot;: 23,&quot;mail&quot;: &quot;444@qq.com&quot;,&quot;hobby&quot;:&quot;跑步、游泳&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;孙七&quot;,&quot;age&quot;: 24,&quot;mail&quot;: &quot;555@qq.com&quot;,&quot;hobby&quot;:&quot;听音乐、看电影&quot;&#125; 测试搜索： 12345678910POST /itcast/person/_search#请求体&#123; \"query\" : &#123; \"match\" : &#123; \"hobby\" : \"音乐\" &#125; &#125;&#125; 3.7 结构化查询3.7.1 term查询term 主要用于精确匹配哪些值，比如数字，日期，布尔值或 not_analyzed 的字符串(未经分析的文本数据类)： 1234 &#123; &quot;term&quot;: &#123; &quot;age&quot;: 26 &#125;&#125; &#123; &quot;term&quot;: &#123; &quot;date&quot;: &quot;2014-09-01&quot; &#125;&#125; &#123; &quot;term&quot;: &#123; &quot;public&quot;: true &#125;&#125; &#123; &quot;term&quot;: &#123; &quot;tag&quot;: &quot;full_text&quot; &#125;&#125; 示例： 12345678910POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;age&quot; : 20 &#125; &#125;&#125; 3.7.2 terms查询terms 跟 term 有点类似，但 terms 允许指定多个匹配条件。 如果某个字段指定了多个值，那么文档需要一起去做匹配： 12345&#123; &quot;terms&quot;: &#123; &quot;tag&quot;: [ &quot;search&quot;, &quot;full_text&quot;, &quot;nosql&quot; ] &#125;&#125; 示例： 12345678910POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot; : &#123; &quot;terms&quot; : &#123; &quot;age&quot; : [20,21] &#125; &#125;&#125; 3.7.3 range查询range 过滤允许我们按照指定范围查找一批数据： 12345678&#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 20, &quot;lt&quot;: 30 &#125; &#125;&#125; 范围操作符包含： gt 大于gte 大于等于lt 小于lte 小于等于 示例： 12345678910111213POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 20, &quot;lte&quot;: 22 &#125; &#125; &#125;&#125; 3.7.4 exists 查询exists 查询可以用于查找文档中是否包含指定字段或没有某个字段，类似于SQL语句中的 IS_NULL 条件 12345&#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;title&quot; &#125;&#125; 这两个查询只是针对已经查出一批数据来，但是想区分出某个字段是否存在的时候使用。 示例： 12345678910POST &#x2F;haoke&#x2F;user&#x2F;_search#请求体&#123; &quot;query&quot;: &#123; &quot;exists&quot;: &#123; #必须包含 &quot;field&quot;: &quot;card&quot; &#125; &#125;&#125; 3.7.5 match查询match 查询是一个标准查询，不管你需要全文本查询还是精确查询基本上都要用到它。 如果你使用 match 查询一个全文本字段，它会在真正查询之前用分析器先分析 match 一下查询字符： 12345&#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;About Search&quot; &#125;&#125; 如果用 match 下指定了一个确切值，在遇到数字，日期，布尔值或者 not_analyzed 的字符串时，它将为你搜索你给定的值： 1234&#123; &quot;match&quot;: &#123; &quot;age&quot;: 26 &#125;&#125;&#123; &quot;match&quot;: &#123; &quot;date&quot;: &quot;2014-09-01&quot; &#125;&#125;&#123; &quot;match&quot;: &#123; &quot;public&quot;: true &#125;&#125;&#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;full_text&quot; &#125;&#125; 3.7.6 bool查询bool 查询可以用来合并多个条件查询结果的布尔逻辑，它包含一下操作符： must 多个查询条件的完全匹配,相当于 and 。 must_not 多个查询条件的相反匹配，相当于 not 。 should 至少有一个查询条件匹配, 相当于 or 。 这些参数可以分别继承一个查询条件或者一个查询条件的数组： 12345678910&#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;term&quot;: &#123; &quot;folder&quot;: &quot;inbox&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;term&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;starred&quot;: true &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;unread&quot;: true &#125;&#125; ] &#125;&#125; 3.8 过滤查询前面讲过结构化查询，Elasticsearch也支持过滤查询，如term、range、match等。 示例：查询年龄为20岁的用户。 1234567891011121314POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;age&quot;: 20 &#125; &#125; &#125; &#125;&#125; 结果： 查询和过滤的对比 一条过滤语句会询问每个文档的字段值是否包含着特定值。 查询语句会询问每个文档的字段值与特定值的匹配程度如何。 一条查询语句会计算每个文档与查询语句的相关性，会给出一个相关性评分 _score，并且按照相关性对匹配到的文档进行排序。 这种评分方式非常适用于一个没有完全配置结果的全文本搜索。 一个简单的文档列表，快速匹配运算并存入内存是十分方便的， 每个文档仅需要1个字节。这些缓存的过滤结果集与后续请求的结合使用是非常高效的。 查询语句不仅要查找相匹配的文档，还需要计算每个文档的相关性，所以一般来说查询语句要比过滤语句更耗时，并且查询结果也不可缓存。 建议：做精确匹配搜索时，最好用过滤语句，因为过滤语句可以缓存数据。 4. 中文分词4.1 什么是分词分词就是指将一个文本转化成一系列单词的过程，也叫文本分析，在Elasticsearch中称之为Analysis。 举例：我是中国人 –&gt; 我/是/中国人 4.2 分词api指定分词器进行分词 1234567POST &#x2F;_analyze#请求体&#123; &quot;analyzer&quot;:&quot;standard&quot;, &quot;text&quot;:&quot;hello world&quot;&#125; 结果： 在结果中不仅可以看出分词的结果，还返回了该词在文本中的位置。 指定索引分词 12345678POST &#x2F;itcast&#x2F;_analyze#请求体&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;field&quot;: &quot;hobby&quot;, &quot;text&quot;: &quot;听音乐&quot;&#125; 4.3 中文分词中文分词的难点在于，在汉语中没有明显的词汇分界点，如在英语中，空格可以作为分隔符，如果分隔不正确就会造成歧义。 如： 我/爱/炒肉丝 我/爱/炒/肉丝 常用中文分词器，IK、jieba、THULAC等，推荐使用IK分词器。 IK Analyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。从2006年12月推出1.0版开始，IKAnalyzer已经推出了3个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IK Analyzer 3.0则发展为面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。 采用了特有的“正向迭代最细粒度切分算法“，具有80万字/秒的高速处理能力 采用了多子处理器分析模式，支持：英文字母（IP地址、Email、URL）、数字（日期，常用中文数量词，罗马数字，科学计数法），中文词汇（姓名、地名处理）等分词处理。 优化的词典存储，更小的内存占用。 IK分词器 Elasticsearch插件地址：https://github.com/medcl/elasticsearch-analysis-ik 12345678910#安装方法：将下载到的elasticsearch-analysis-ik-6.5.4.zip解压到&#x2F;elasticsearch&#x2F;plugins&#x2F;ik目录下即可。mkdir es&#x2F;plugins&#x2F;ikcp elasticsearch-analysis-ik-6.5.4.zip .&#x2F;es&#x2F;plugins&#x2F;ik#解压unzip elasticsearch-analysis-ik-6.5.4.zip#重启.&#x2F;bin&#x2F;elasticsearch 测试： 1234567POST &#x2F;_analyze#请求体&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;我是中国人&quot;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;我&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 1, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;是&quot;, &quot;start_offset&quot;: 1, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;中国人&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;中国&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;国人&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 4 &#125; ]&#125; 可以看到，已经对中文进行了分词。 5. 全文搜索全文搜索两个最重要的方面是： 相关性（Relevance） 它是评价查询与其结果间的相关程度，并根据这种相关程度对结果排名的一种能力，这种计算方式可以是 TF/IDF 方法、地理位置邻近、模糊相似，或其他的某些算法。 分词（Analysis） 它是将文本块转换为有区别的、规范化的 token 的一个过程，目的是为了创建倒排索引以及查询倒排索引。 5.1 构造数据123456789101112131415161718192021222324252627282930PUT &#x2F;itcast#请求体&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;1&quot;, &quot;number_of_replicas&quot;: &quot;0&quot; &#125; &#125;, &quot;mappings&quot;: &#123; &quot;person&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;mail&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;hobby&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;:&quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125; 批量插入数据： 12345678910111213POST http:&#x2F;&#x2F;172.16.55.185:9200&#x2F;itcast&#x2F;_bulk#请求体&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;张三&quot;,&quot;age&quot;: 20,&quot;mail&quot;: &quot;111@qq.com&quot;,&quot;hobby&quot;:&quot;羽毛球、乒乓球、足球&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;李四&quot;,&quot;age&quot;: 21,&quot;mail&quot;: &quot;222@qq.com&quot;,&quot;hobby&quot;:&quot;羽毛球、乒乓球、足球、篮球&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;王五&quot;,&quot;age&quot;: 22,&quot;mail&quot;: &quot;333@qq.com&quot;,&quot;hobby&quot;:&quot;羽毛球、篮球、游泳、听音乐&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;赵六&quot;,&quot;age&quot;: 23,&quot;mail&quot;: &quot;444@qq.com&quot;,&quot;hobby&quot;:&quot;跑步、游泳、篮球&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;itcast&quot;,&quot;_type&quot;:&quot;person&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;孙七&quot;,&quot;age&quot;: 24,&quot;mail&quot;: &quot;555@qq.com&quot;,&quot;hobby&quot;:&quot;听音乐、看电影、羽毛球&quot;&#125; 结果： 5.2 单词搜索123456789101112131415POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;hobby&quot;:&quot;音乐&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; &quot;took&quot;: 9, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.6841192, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;itcast&quot;, &quot;_type&quot;: &quot;person&quot;, &quot;_id&quot;: &quot;Uv0cDWgBR-bSw8-LpdkZ&quot;, &quot;_score&quot;: 0.6841192, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;王五&quot;, &quot;age&quot;: 22, &quot;mail&quot;: &quot;333@qq.com&quot;, &quot;hobby&quot;: &quot;羽毛球、篮球、游泳、听音乐&quot; &#125;, &quot;highlight&quot;: &#123; &quot;hobby&quot;: [ &quot;羽毛球、篮球、游泳、听&lt;em&gt;音乐&lt;&#x2F;em&gt;&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;itcast&quot;, &quot;_type&quot;: &quot;person&quot;, &quot;_id&quot;: &quot;VP0cDWgBR-bSw8-LpdkZ&quot;, &quot;_score&quot;: 0.6841192, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;孙七&quot;, &quot;age&quot;: 24, &quot;mail&quot;: &quot;555@qq.com&quot;, &quot;hobby&quot;: &quot;听音乐、看电影、羽毛球&quot; &#125;, &quot;highlight&quot;: &#123; &quot;hobby&quot;: [ &quot;听&lt;em&gt;音乐&lt;&#x2F;em&gt;、看电影、羽毛球&quot; ] &#125; &#125; ] &#125;&#125; 过程说明： 检查字段类型 爱好 hobby 字段是一个 text 类型（ 指定了IK分词器），这意味着查询字符串本身也应该被分词。 分析查询字符串 。 将查询的字符串 “音乐” 传入IK分词器中，输出的结果是单个项 音乐。因为只有一个单词项，所以 match 查询执行的是单个底层 term 查询。 查找匹配文档 。 用 term 查询在倒排索引中查找 “音乐” 然后获取一组包含该项的文档，本例的结果是文档：3 、5 。 为每个文档评分 。 用 term 查询计算每个文档相关度评分 _score ，这是种将 词频（term frequency，即词 “音乐” 在相关文档的hobby 字段中出现的频率）和 反向文档频率（inverse document frequency，即词 “音乐” 在所有文档的hobby 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式。 5.3 多词搜索123456789101112131415POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;hobby&quot;:&quot;音乐 篮球&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&#123; &quot;took&quot;: 3, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 4, &quot;max_score&quot;: 1.3192271, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;itcast&quot;, &quot;_type&quot;: &quot;person&quot;, &quot;_id&quot;: &quot;Uv0cDWgBR-bSw8-LpdkZ&quot;, &quot;_score&quot;: 1.3192271, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;王五&quot;, &quot;age&quot;: 22, &quot;mail&quot;: &quot;333@qq.com&quot;, &quot;hobby&quot;: &quot;羽毛球、篮球、游泳、听音乐&quot; &#125;, &quot;highlight&quot;: &#123; &quot;hobby&quot;: [ &quot;羽毛球、&lt;em&gt;篮球&lt;&#x2F;em&gt;、游泳、听&lt;em&gt;音乐&lt;&#x2F;em&gt;&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;itcast&quot;, &quot;_type&quot;: &quot;person&quot;, &quot;_id&quot;: &quot;VP0cDWgBR-bSw8-LpdkZ&quot;, &quot;_score&quot;: 0.81652206, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;孙七&quot;, &quot;age&quot;: 24, &quot;mail&quot;: &quot;555@qq.com&quot;, &quot;hobby&quot;: &quot;听音乐、看电影、羽毛球&quot; &#125;, &quot;highlight&quot;: &#123; &quot;hobby&quot;: [ &quot;听&lt;em&gt;音乐&lt;&#x2F;em&gt;、看电影、羽毛球&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;itcast&quot;, &quot;_type&quot;: &quot;person&quot;, &quot;_id&quot;: &quot;Vf0gDWgBR-bSw8-LOdm_&quot;, &quot;_score&quot;: 0.6987338, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;赵六&quot;, &quot;age&quot;: 23, &quot;mail&quot;: &quot;444@qq.com&quot;, &quot;hobby&quot;: &quot;跑步、游泳、篮球&quot; &#125;, &quot;highlight&quot;: &#123; &quot;hobby&quot;: [ &quot;跑步、游泳、&lt;em&gt;篮球&lt;&#x2F;em&gt;&quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;itcast&quot;, &quot;_type&quot;: &quot;person&quot;, &quot;_id&quot;: &quot;Uf0cDWgBR-bSw8-LpdkZ&quot;, &quot;_score&quot;: 0.50270504, &quot;_source&quot;: &#123; &quot;name&quot;: &quot;李四&quot;, &quot;age&quot;: 21, &quot;mail&quot;: &quot;222@qq.com&quot;, &quot;hobby&quot;: &quot;羽毛球、乒乓球、足球、篮球&quot; &#125;, &quot;highlight&quot;: &#123; &quot;hobby&quot;: [ &quot;羽毛球、乒乓球、足球、&lt;em&gt;篮球&lt;&#x2F;em&gt;&quot; ] &#125; &#125; ] &#125;&#125; 可以看到，包含了“音乐”、“篮球”的数据都已经被搜索到了。 可是，搜索的结果并不符合我们的预期，因为我们想搜索的是既包含“音乐”又包含“篮球”的用户，显然结果返回的“或”的关系。 在Elasticsearch中，可以指定词之间的逻辑关系，如下： 123456789101112131415161718POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;hobby&quot;:&#123; &quot;query&quot;:&quot;音乐 篮球&quot;, &quot;operator&quot;:&quot;and&quot; &#125; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 结果： 可以看到结果符合预期。 前面我们测试了“OR” 和 “AND”搜索，这是两个极端，其实在实际场景中，并不会选取这2个极端，更有可能是选取这种，或者说，只需要符合一定的相似度就可以查询到数据，在Elasticsearch中也支持这样的查询，通过minimum_should_match来指定匹配度，如：70%； 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;hobby&quot;:&#123; &quot;query&quot;:&quot;游泳 羽毛球&quot;, &quot;minimum_should_match&quot;:&quot;80%&quot; &#125; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125;#结果：省略显示&quot;hits&quot;: &#123; &quot;total&quot;: 4, #相似度为80%的情况下，查询到4条数据 &quot;max_score&quot;: 1.621458, &quot;hits&quot;: [ .........&#125;#设置40%进行测试：&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; &quot;hobby&quot;:&#123; &quot;query&quot;:&quot;游泳 羽毛球&quot;, &quot;minimum_should_match&quot;:&quot;40%&quot; &#125; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125;#结果：&quot;hits&quot;: &#123; &quot;total&quot;: 5, #相似度为40%的情况下，查询到5条数据 &quot;max_score&quot;: 1.621458, &quot;hits&quot;: [ ........&#125; 相似度应该多少合适，需要在实际的需求中进行反复测试，才可得到合理的值。 5.4 组合搜索在搜索时，也可以使用过滤器中讲过的bool组合查询，示例： 12345678910111213141516171819202122232425262728293031POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:&#123; &quot;match&quot;:&#123; &quot;hobby&quot;:&quot;篮球&quot; &#125; &#125;, &quot;must_not&quot;:&#123; &quot;match&quot;:&#123; &quot;hobby&quot;:&quot;音乐&quot; &#125; &#125;, &quot;should&quot;:[ &#123; &quot;match&quot;: &#123; &quot;hobby&quot;:&quot;游泳&quot; &#125; &#125; ] &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 上面搜索的意思是：搜索结果中必须包含篮球，不能包含音乐，如果包含了游泳，那么它的相似度更高。 结果： 评分的计算规则 bool 查询会为每个文档计算相关度评分 _score ， 再将所有匹配的 must 和 should 语句的分数 _score 求和，最后除以 must 和 should 语句的总数。 must_not 语句不会影响评分； 它的作用只是将不相关的文档排除。 默认情况下，should中的内容不是必须匹配的，如果查询语句中没有must，那么就会至少匹配其中一个。当然了，也可以通过minimum_should_match参数进行控制，该值可以是数字也可以的百分比。 示例： 1234567891011121314151617181920212223242526272829303132POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;should&quot;:[ &#123; &quot;match&quot;: &#123; &quot;hobby&quot;:&quot;游泳&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;hobby&quot;:&quot;篮球&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;hobby&quot;:&quot;音乐&quot; &#125; &#125; ], &quot;minimum_should_match&quot;:2 &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; minimum_should_match为2，意思是should中的三个词，至少要满足2个。 结果： 5.5 权重有些时候，我们可能需要对某些词增加权重来影响该条数据的得分。如下： 搜索关键字为“游泳篮球”，如果结果中包含了“音乐”权重为10，包含了“跑步”权重为2。 12345678910111213141516171819202122232425262728293031323334353637383940POST &#x2F;itcast&#x2F;person&#x2F;_search#请求体&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;游泳篮球&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;音乐&quot;, &quot;boost&quot;: 10 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &#123; &quot;query&quot;: &quot;跑步&quot;, &quot;boost&quot;: 2 &#125; &#125; &#125; ] &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;hobby&quot;: &#123;&#125; &#125; &#125;&#125; 结果： 如果不设置权重的查询结果是这样： 6. Elasticsearch集群6.1 集群节点ELasticsearch的集群是由多个节点组成的，通过cluster.name设置集群名称，并且用于区分其它的集群，每个节点通过node.name指定节点的名称。 在Elasticsearch中，节点的类型主要有4种： master节点 配置文件中node.master属性为true(默认为true)，就有资格被选为master节点。 master节点用于控制整个集群的操作。比如创建或删除索引，管理其它非master节点等。 data节点 配置文件中node.data属性为true(默认为true)，就有资格被设置成data节点。 data节点主要用于执行数据相关的操作。比如文档的CRUD。 客户端节点 配置文件中node.master属性和node.data属性均为false。 该节点不能作为master节点，也不能作为data节点。 可以作为客户端节点，用于响应用户的请求，把请求转发到其他节点 部落节点 当一个节点配置tribe.*的时候，它是一个特殊的客户端，它可以连接多个集群，在所有连接的集群上执行搜索和其他操作。 6.2 搭建集群1234567891011121314151617181920212223242526272829303132333435363738394041424344#启动3个虚拟机，分别在3台虚拟机上部署安装Elasticsearchmkdir &#x2F;itcast&#x2F;es-cluster#分发到其它机器scp -r es-cluster elsearch@192.168.40.134:&#x2F;itcast#node01的配置：cluster.name: es-itcast-clusternode.name: node01node.master: truenode.data: truenetwork.host: 0.0.0.0http.port: 9200discovery.zen.ping.unicast.hosts: [&quot;192.168.40.133&quot;,&quot;192.168.40.134&quot;,&quot;192.168.40.135&quot;]discovery.zen.minimum_master_nodes: 2http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;#node02的配置：cluster.name: es-itcast-clusternode.name: node02node.master: truenode.data: truenetwork.host: 0.0.0.0http.port: 9200discovery.zen.ping.unicast.hosts: [&quot;192.168.40.133&quot;,&quot;192.168.40.134&quot;,&quot;192.168.40.135&quot;]discovery.zen.minimum_master_nodes: 2http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;#node03的配置：cluster.name: es-itcast-clusternode.name: node03node.master: truenode.data: truenetwork.host: 0.0.0.0http.port: 9200discovery.zen.ping.unicast.hosts: [&quot;192.168.40.133&quot;,&quot;192.168.40.134&quot;,&quot;192.168.40.135&quot;]discovery.zen.minimum_master_nodes: 2http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;#分别启动3个节点.&#x2F;elasticsearch 创建索引： 查询集群状态：/_cluster/health 1234567891011121314151617&#123; cluster_name: &quot;es-itcast-cluster&quot; status: &quot;green&quot; timed_out: false number_of_nodes: 3 number_of_data_nodes: 3 active_primary_shards: 5 active_shards: 10 relocating_shards: 0 initializing_shards: 0 unassigned_shards: 0 delayed_unassigned_shards: 0 number_of_pending_tasks: 0 number_of_in_flight_fetch: 0 task_max_waiting_in_queue_millis: 0 active_shards_percent_as_number: 100&#125; 集群状态的三种颜色： 颜色 意义 green 所有主要分片和复制分片都可用 yellow 所有主要分片可用，但不是所有复制分片都可用 red 不是所有的主要分片都可用 6.3 分片和副本为了将数据添加到Elasticsearch，我们需要索引(index)——一个存储关联数据的地方。实际上，索引只是一个用来指向一个或多个分片(shards)的“逻辑命名空间(logical namespace)”. 一个分片(shard)是一个最小级别“工作单元(worker unit)”,它只是保存了索引中所有数据的一部分。 我们需要知道是分片就是一个Lucene实例，并且它本身就是一个完整的搜索引擎。应用程序不会和它直接通信。 分片可以是主分片(primary shard)或者是复制分片(replica shard)。 索引中的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。 复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的shard取回文档。 当索引创建完成的时候，主分片的数量就固定了，但是复制分片的数量可以随时调整。 6.4 故障转移6.4.1 将data节点停止这里选择将node02停止： 说明： 当前集群状态为黄色，表示主节点可用，副本节点不完全可用 过一段时间观察，发现节点列表中看不到node02，副本节点分配到了node01和node03，集群状态恢复到绿色。 将node02恢复： 1.&#x2F;node02&#x2F;bin&#x2F;elasticsearch 可以看到，node02恢复后，重新加入了集群，并且重新分配了节点信息。 6.4.2 将master节点停止接下来，测试将node01停止，也就是将主节点停止。 从结果中可以看出，集群对master进行了重新选举，选择node03为master。并且集群状态变成黄色。 等待一段时间后，集群状态从黄色变为了绿色： 恢复node01节点： 1.&#x2F;node01&#x2F;bin&#x2F;elasticsearch 重启之后，发现node01可以正常加入到集群中，集群状态依然为绿色： 特别说明： 如果在配置文件中discovery.zen.minimum_master_nodes设置的不是N/2+1时，会出现脑裂问题，之前宕机的主节点恢复后不会加入到集群。 6.5 分布式文档6.5.1 路由首先，来看个问题： 如图所示：当我们想一个集群保存文档时，文档该存储到哪个节点呢？ 是随机吗？ 是轮询吗？ 实际上，在ELasticsearch中，会采用计算的方式来确定存储到哪个节点，计算公式如下： 1shard &#x3D; hash(routing) % number_of_primary_shards outing值是一个任意字符串，它默认是_id但也可以自定义。 这个routing字符串通过哈希函数生成一个数字，然后除以主切片的数量得到一个余数(remainder)，余数的范围永远是0到number_of_primary_shards - 1，这个数字就是特定文档所在的分片。 这就是为什么创建了主分片后，不能修改的原因。 6.5.2 文档的写操作新建索引和删除请求都是写(write)操作，它们必须在主分片上成功完成才能复制到相关的复制分片上。 下面我们罗列在主分片和复制分片上成功新建索引或删除一个文档必要的顺序步骤： 客户端给 Node 1 发送新建索引或删除请求。 节点使用文档的 _id 确定文档属于分片 0 。它转发请求到 Node 3 ，分片 0 位于这个节点上。 Node 3 在主分片上执行请求，如果成功，它转发请求到相应的位于 Node 1 和 Node 2 的复制节点上。当所有的复制节点报告成功， Node 3 报告成功到请求的节点，请求的节点再报告给客户端。 客户端接收到成功响应的时候，文档的修改已经被应用于主分片和所有的复制分片。你的修改生效了。 6.5.3 搜索文档（单个文档）文档能够从主分片或任意一个复制分片被检索。 下面我们罗列在主分片或复制分片上检索一个文档必要的顺序步骤： 客户端给 Node 1 发送get请求。 节点使用文档的 _id 确定文档属于分片 0 。分片 0 对应的复制分片在三个节点上都有。此时，它转发请求到Node 2 。 Node 2 返回文档(document)给 Node 1 然后返回给客户端。 对于读请求，为了平衡负载，请求节点会为每个请求选择不同的分片——它会循环所有分片副本。 可能的情况是，一个被索引的文档已经存在于主分片上却还没来得及同步到复制分片上。这时复制分片会报告文档未找到，主分片会成功返回文档。一旦索引请求成功返回给用户，文档则在主分片和复制分片都是可用的。 6.5.4 全文搜索对于全文搜索而言，文档可能分散在各个节点上，那么在分布式的情况下，如何搜索文档呢？ 搜索，分为2个阶段，搜索（query）+取回（fetch）。 搜索（query） 查询阶段包含以下三步： 客户端发送一个 search（搜索） 请求给 Node 3 , Node 3 创建了一个长度为 from+size 的空优先级队 Node 3 转发这个搜索请求到索引中每个分片的原本或副本。每个分片在本地执行这个查询并且结果将结果到一个大小为 from+size 的有序本地优先队列里去。 每个分片返回document的ID和它优先队列里的所有document的排序值给协调节点 Node 3 。 Node 3 把这些值合并到自己的优先队列里产生全局排序结果。 取回（fetch） 分发阶段由以下步骤构成： 协调节点辨别出哪个document需要取回，并且向相关分片发出 GET 请求。 每个分片加载document并且根据需要丰富（enrich）它们，然后再将document返回协调节点。 一旦所有的document都被取回，协调节点会将结果返回给客户端。 7. Java客户端在Elasticsearch中，为java提供了2种客户端，一种是REST风格的客户端，另一种是Java API的客户端。 https://www.elastic.co/guide/en/elasticsearch/client/index.html 7.1 REST客户端Elasticsearch提供了2种REST客户端，一种是低级客户端，一种是高级客户端。 Java Low Level REST Client：官方提供的低级客户端。该客户端通过http来连接Elasticsearch集群。用户在使用该客户端时需要将请求数据手动拼接成Elasticsearch所需JSON格式进行发送，收到响应时同样也需要将返回的JSON数据手动封装成对象。虽然麻烦，不过该客户端兼容所有的Elasticsearch版本。 Java High Level REST Client：官方提供的高级客户端。该客户端基于低级客户端实现，它提供了很多便捷的API来解决低级客户端需要手动转换数据格式的问题。 7.2 构造数据123456789101112131415POST &#x2F;haoke&#x2F;house&#x2F;_bulk#请求体&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;haoke&quot;,&quot;_type&quot;:&quot;house&quot;&#125;&#125;&#123;&quot;id&quot;:&quot;1001&quot;,&quot;title&quot;:&quot;整租 · 南丹大楼 1居室 7500&quot;,&quot;price&quot;:&quot;7500&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;haoke&quot;,&quot;_type&quot;:&quot;house&quot;&#125;&#125;&#123;&quot;id&quot;:&quot;1002&quot;,&quot;title&quot;:&quot;陆家嘴板块，精装设计一室一厅，可拎包入住诚意租。&quot;,&quot;price&quot;:&quot;8500&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;haoke&quot;,&quot;_type&quot;:&quot;house&quot;&#125;&#125;&#123;&quot;id&quot;:&quot;1003&quot;,&quot;title&quot;:&quot;整租 · 健安坊 1居室 4050&quot;,&quot;price&quot;:&quot;7500&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;haoke&quot;,&quot;_type&quot;:&quot;house&quot;&#125;&#125;&#123;&quot;id&quot;:&quot;1004&quot;,&quot;title&quot;:&quot;整租 · 中凯城市之光+视野开阔+景色秀丽+拎包入住&quot;,&quot;price&quot;:&quot;6500&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;haoke&quot;,&quot;_type&quot;:&quot;house&quot;&#125;&#125;&#123;&quot;id&quot;:&quot;1005&quot;,&quot;title&quot;:&quot;整租 · 南京西路品质小区 21213三轨交汇 配套齐* 拎包入住&quot;,&quot;price&quot;:&quot;6000&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;haoke&quot;,&quot;_type&quot;:&quot;house&quot;&#125;&#125;&#123;&quot;id&quot;:&quot;1006&quot;,&quot;title&quot;:&quot;祥康里 简约风格 *南户型 拎包入住 看房随时&quot;,&quot;price&quot;:&quot;7000&quot;&#125; 7.3 REST低级客户端7.3.1 创建工程创建工程itcast-elasticsearch： 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.itcast.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;itcast-elasticsearch&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;6.5.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- java编译插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 7.3.2 编写测试用例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class TestESREST &#123; private static final ObjectMapper MAPPER = new ObjectMapper(); private RestClient restClient; @Before public void init() &#123; RestClientBuilder restClientBuilder = RestClient.builder( new HttpHost(\"172.16.55.185\", 9200, \"http\"), new HttpHost(\"172.16.55.185\", 9201, \"http\"), new HttpHost(\"172.16.55.185\", 9202, \"http\")); restClientBuilder.setFailureListener(new RestClient.FailureListener() &#123; @Override public void onFailure(Node node) &#123; System.out.println(\"出错了 -&gt; \" + node); &#125; &#125;); this.restClient = restClientBuilder.build(); &#125; @After public void after() throws IOException &#123; restClient.close(); &#125; // 查询集群状态 @Test public void testGetInfo() throws IOException &#123; Request request = new Request(\"GET\", \"/_cluster/state\"); request.addParameter(\"pretty\",\"true\"); Response response = this.restClient.performRequest(request); System.out.println(response.getStatusLine()); System.out.println(EntityUtils.toString(response.getEntity())); &#125; // 新增数据 @Test public void testCreateData() throws IOException &#123; Request request = new Request(\"POST\", \"/haoke/house\"); Map&lt;String, Object&gt; data = new HashMap&lt;&gt;(); data.put(\"id\",\"2001\"); data.put(\"title\",\"张江高科\"); data.put(\"price\",\"3500\"); request.setJsonEntity(MAPPER.writeValueAsString(data)); Response response = this.restClient.performRequest(request); System.out.println(response.getStatusLine()); System.out.println(EntityUtils.toString(response.getEntity())); &#125; // 根据id查询数据 @Test public void testQueryData() throws IOException &#123; Request request = new Request(\"GET\", \"/haoke/house/G0pfE2gBCKv8opxuRz1y\"); Response response = this.restClient.performRequest(request); System.out.println(response.getStatusLine()); System.out.println(EntityUtils.toString(response.getEntity())); &#125; // 搜索数据 @Test public void testSearchData() throws IOException &#123; Request request = new Request(\"POST\", \"/haoke/house/_search\"); String searchJson = \"&#123;\\\"query\\\": &#123;\\\"match\\\": &#123;\\\"title\\\": \\\"拎包入住\\\"&#125;&#125;&#125;\"; request.setJsonEntity(searchJson); request.addParameter(\"pretty\",\"true\"); Response response = this.restClient.performRequest(request); System.out.println(response.getStatusLine()); System.out.println(EntityUtils.toString(response.getEntity())); &#125;&#125; 从使用中，可以看出，基本和我们使用RESTful api使用几乎是一致的。 7.4 REST高级客户端7.4.1 引入依赖12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;6.5.4&lt;/version&gt;&lt;/dependency&gt; 7.4.2 编写测试用例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154public class TestRestHighLevel &#123; private RestHighLevelClient client; @Before public void init() &#123; RestClientBuilder restClientBuilder = RestClient.builder( new HttpHost(\"172.16.55.185\", 9200, \"http\"), new HttpHost(\"172.16.55.185\", 9201, \"http\"), new HttpHost(\"172.16.55.185\", 9202, \"http\")); this.client = new RestHighLevelClient(restClientBuilder); &#125; @After public void after() throws Exception &#123; this.client.close(); &#125; /** * 新增文档，同步操作 * * @throws Exception */ @Test public void testCreate() throws Exception &#123; Map&lt;String, Object&gt; data = new HashMap&lt;&gt;(); data.put(\"id\", \"2002\"); data.put(\"title\", \"南京西路 拎包入住 一室一厅\"); data.put(\"price\", \"4500\"); IndexRequest indexRequest = new IndexRequest(\"haoke\", \"house\").source(data); IndexResponse indexResponse = this.client.index(indexRequest, RequestOptions.DEFAULT); System.out.println(\"id-&gt;\" + indexResponse.getId()); System.out.println(\"index-&gt;\" + indexResponse.getIndex()); System.out.println(\"type-&gt;\" + indexResponse.getType()); System.out.println(\"version-&gt;\" + indexResponse.getVersion()); System.out.println(\"result-&gt;\" + indexResponse.getResult()); System.out.println(\"shardInfo-&gt;\" + indexResponse.getShardInfo()); &#125; /** * 新增文档，异步操作 * * @throws Exception */ @Test public void testCreateAsync() throws Exception &#123; Map&lt;String, Object&gt; data = new HashMap&lt;&gt;(); data.put(\"id\", \"2003\"); data.put(\"title\", \"南京东路 最新房源 二室一厅\"); data.put(\"price\", \"5500\"); IndexRequest indexRequest = new IndexRequest(\"haoke\", \"house\").source(data); this.client.indexAsync(indexRequest, RequestOptions.DEFAULT, new ActionListener&lt;IndexResponse&gt;() &#123; @Override public void onResponse(IndexResponse indexResponse) &#123; System.out.println(\"id-&gt;\" + indexResponse.getId()); System.out.println(\"index-&gt;\" + indexResponse.getIndex()); System.out.println(\"type-&gt;\" + indexResponse.getType()); System.out.println(\"version-&gt;\" + indexResponse.getVersion()); System.out.println(\"result-&gt;\" + indexResponse.getResult()); System.out.println(\"shardInfo-&gt;\" + indexResponse.getShardInfo()); &#125; @Override public void onFailure(Exception e) &#123; System.out.println(e); &#125; &#125;); System.out.println(\"ok\"); Thread.sleep(20000); &#125; @Test public void testQuery() throws Exception &#123; GetRequest getRequest = new GetRequest(\"haoke\", \"house\",\"GkpdE2gBCKv8opxuOj12\"); // 指定返回的字段 String[] includes = new String[]&#123;\"title\", \"id\"&#125;; String[] excludes = Strings.EMPTY_ARRAY; FetchSourceContext fetchSourceContext = new FetchSourceContext(true, includes, excludes); getRequest.fetchSourceContext(fetchSourceContext); GetResponse response = this.client.get(getRequest, RequestOptions.DEFAULT); System.out.println(\"数据 -&gt; \" + response.getSource()); &#125; /** * 判断是否存在 * * @throws Exception * */ @Test public void testExists() throws Exception &#123; GetRequest getRequest = new GetRequest(\"haoke\", \"house\",\"GkpdE2gBCKv8opxuOj12\"); // 不返回的字段 getRequest.fetchSourceContext(new FetchSourceContext(false)); boolean exists = this.client.exists(getRequest, RequestOptions.DEFAULT); System.out.println(\"exists -&gt; \" + exists); &#125; /** * 删除数据 * * @throws Exception * */ @Test public void testDelete() throws Exception &#123; DeleteRequest deleteRequest = new DeleteRequest(\"haoke\", \"house\", \"GkpdE2gBCKv8opxuOj12\"); DeleteResponse response = this.client.delete(deleteRequest, RequestOptions.DEFAULT); System.out.println(response.status());// OK or NOT_FOUND &#125; /** * 更新数据 * * @throws Exception * */ @Test public void testUpdate() throws Exception &#123; UpdateRequest updateRequest = new UpdateRequest(\"haoke\", \"house\", \"G0pfE2gBCKv8opxuRz1y\"); Map&lt;String, Object&gt; data = new HashMap&lt;&gt;(); data.put(\"title\", \"张江高科2\"); data.put(\"price\", \"5000\"); updateRequest.doc(data); UpdateResponse response = this.client.update(updateRequest, RequestOptions.DEFAULT); System.out.println(\"version -&gt; \" + response.getVersion()); &#125; /** * 测试搜索 * * @throws Exception * */ @Test public void testSearch() throws Exception &#123; SearchRequest searchRequest = new SearchRequest(\"haoke\"); searchRequest.types(\"house\"); SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.matchQuery(\"title\", \"拎包入住\")); sourceBuilder.from(0); sourceBuilder.size(5); sourceBuilder.timeout(new TimeValue(60, TimeUnit.SECONDS)); searchRequest.source(sourceBuilder); SearchResponse search = this.client.search(searchRequest, RequestOptions.DEFAULT); System.out.println(\"搜索到 \" + search.getHits().totalHits + \" 条数据.\"); SearchHits hits = search.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125; &#125;&#125;","tags":[{"name":"业务解决方案","slug":"业务解决方案","permalink":"https://wgy1993.gitee.io/tags/%E4%B8%9A%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"name":"ElasticStack","slug":"ElasticStack","permalink":"https://wgy1993.gitee.io/tags/ElasticStack/"}]},{"title":"Lucene(二)","date":"2020-08-31T04:02:36.000Z","path":"archives/6c0148b0.html","text":"1. Lucene高级搜索1.1 文本搜索QueryParser支持默认搜索域, 第一个参数为默认搜索域。 如果在执行parse方法的时候，查询语法中包含域名则从指定的这个域名中搜索， 如果只有查询的关键字，则从默认搜索域中搜索结果。 需求描述 : 查询名称中包含华为手机关键字的结果。 测试代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Testpublic void testIndexSearch() throws Exception &#123; //1. 创建Query搜索对象 //1.1 创建分词器(对搜索的关键词进行分词使用) //注意: 分词器要和创建索引的时候使用的分词器一模一样 Analyzer analyzer = new StandardAnalyzer(); //1.2 创建搜索解析器 //第一个参数: 默认查询域, 如果查询的关键字中带搜索的域名, 则从指定域中查询, 如果不带域名则从, 默认搜索域中查询 //queryParser.parse(\"brandName:华为手机\"); //第二个参数: 使用的分词器 QueryParser queryParser = new QueryParser(\"name\", analyzer); //1.3 创建搜索对象 //华 OR 为 手 机 Query query = queryParser.parse(\"华为手机\"); //2. 创建Directory流对象,声明索引库位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //3. 创建索引读取对象IndexReader IndexReader indexReader = DirectoryReader.open(dir); //4. 创建索引搜索对象 IndexSearcher indexSearcher = new IndexSearcher(indexReader); //5. 使用索引搜索对象，执行搜索，返回结果集TopDocs // 第一个参数：搜索对象，第二个参数：返回的数据条数，指定查询结果最顶部的n条数据返回 TopDocs topDocs = indexSearcher.search(query, 10); //获取查询到的结果集的总数, 打印 System.out.println(\"=======count=======\" + topDocs.totalHits); //5.1 获取结果集 ScoreDoc[] scoreDocs = topDocs.scoreDocs; //6. 解析结果集 if (scoreDocs != null) &#123; for (ScoreDoc scoreDoc : scoreDocs) &#123; //获取查询到的文档唯一标识, 文档id, 这个id是lucene在创建文档的时候自动分配的 int docID = scoreDoc.doc; //通过文档id, 读取文档 Document doc = indexSearcher.doc(docID); System.out.println(\"==================================================\"); //通过域名, 从文档中获取域值 System.out.println(\"===id==\" + doc.get(\"id\")); System.out.println(\"===name==\" + doc.get(\"name\")); System.out.println(\"===price==\" + doc.get(\"price\")); System.out.println(\"===image==\" + doc.get(\"image\")); System.out.println(\"===brandName==\" + doc.get(\"brandName\")); System.out.println(\"===categoryName==\" + doc.get(\"categoryName\")); &#125; &#125; //7. 关闭流 indexReader.close();&#125; 1.2 数值范围搜索需求描述 : 查询价格大于等于100, 小于等于1000的商品测试代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 数值范围查询 * * @throws Exception */@Testpublic void testRangeQuery() throws Exception &#123; //1. 创建分词器(对搜索的关键词进行分词使用) //注意: 分词器要和创建索引的时候使用的分词器一模一样 Analyzer analyzer = new StandardAnalyzer(); //2. 创建查询对象, Query query = IntPoint.newRangeQuery(\"price\", 100, 1000); //4. 创建Directory目录对象, 指定索引库的位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建输入流对象 IndexReader indexReader = DirectoryReader.open(dir); //6. 创建搜索对象 IndexSearcher indexSearcher = new IndexSearcher(indexReader); //7. 搜索, 并返回结果 //第二个参数: 是返回多少条数据用于展示, 分页使用 TopDocs topDocs = indexSearcher.search(query, 10); //获取查询到的结果集的总数, 打印 System.out.println(\"=======count=======\" + topDocs.totalHits); //8. 获取结果集 ScoreDoc[] scoreDocs = topDocs.scoreDocs; //9. 遍历结果集 if (scoreDocs != null) &#123; for (ScoreDoc scoreDoc : scoreDocs) &#123; //获取查询到的文档唯一标识, 文档id, 这个id是lucene在创建文档的时候自动分配的 int docID = scoreDoc.doc; //通过文档id, 读取文档 Document doc = indexSearcher.doc(docID); System.out.println(\"==================================================\"); //通过域名, 从文档中获取域值 System.out.println(\"===id==\" + doc.get(\"id\")); System.out.println(\"===name==\" + doc.get(\"name\")); System.out.println(\"===price==\" + doc.get(\"price\")); System.out.println(\"===image==\" + doc.get(\"image\")); System.out.println(\"===brandName==\" + doc.get(\"brandName\")); System.out.println(\"===categoryName==\" + doc.get(\"categoryName\")); &#125; &#125; //10. 关闭流 indexReader.close();&#125; 1.3 组合搜索需求描述 : 查询价格大于等于100, 小于等于1000, 并且名称中不包含华为手机关键字的商品 BooleanClause.Occur.MUST 必须 相当于and, 并且 BooleanClause.Occur.MUST_NOT 不必须 相当于not, 非 BooleanClause.Occur.SHOULD 应该 相当于or, 或者 注意 : 如果逻辑条件中, 只有MUST_NOT, 或者多个逻辑条件都是MUST_NOT, 无效, 查询不出任何数据. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 组合查询 * * @throws Exception */@Testpublic void testBooleanQuery() throws Exception &#123; //1. 创建分词器(对搜索的关键词进行分词使用) //注意: 分词器要和创建索引的时候使用的分词器一模一样 Analyzer analyzer = new StandardAnalyzer(); //2. 创建查询对象, Query query1 = IntPoint.newRangeQuery(\"price\", 100, 1000); QueryParser queryParser = new QueryParser(\"name\", analyzer); //3. 设置搜索关键词 //华 OR 为 手 机 Query query2 = queryParser.parse(\"华为手机\"); //创建布尔查询对象(组合查询对象) /** * BooleanClause.Occur.MUST 必须相当于and, 也就是并且的关系 * BooleanClause.Occur.SHOULD 应该相当于or, 也就是或者的关系 * BooleanClause.Occur.MUST_NOT 不必须, 相当于not, 非 * 注意: 如果查询条件都是MUST_NOT, 或者只有一个查询条件, 然后这一个查询条件是MUST_NOT则 * 查询不出任何数据. */ BooleanQuery.Builder query = new BooleanQuery.Builder(); query.add(query1, BooleanClause.Occur.MUST); query.add(query2, BooleanClause.Occur.MUST); //4. 创建Directory目录对象, 指定索引库的位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建输入流对象 IndexReader indexReader = DirectoryReader.open(dir); //6. 创建搜索对象 IndexSearcher indexSearcher = new IndexSearcher(indexReader); //7. 搜索, 并返回结果 //第二个参数: 是返回多少条数据用于展示, 分页使用 TopDocs topDocs = indexSearcher.search(query.build(), 10); //获取查询到的结果集的总数, 打印 System.out.println(\"=======count=======\" + topDocs.totalHits); //8. 获取结果集 ScoreDoc[] scoreDocs = topDocs.scoreDocs; //9. 遍历结果集 if (scoreDocs != null) &#123; for (ScoreDoc scoreDoc : scoreDocs) &#123; //获取查询到的文档唯一标识, 文档id, 这个id是lucene在创建文档的时候自动分配的 int docID = scoreDoc.doc; //通过文档id, 读取文档 Document doc = indexSearcher.doc(docID); System.out.println(\"==================================================\"); //通过域名, 从文档中获取域值 System.out.println(\"===id==\" + doc.get(\"id\")); System.out.println(\"===name==\" + doc.get(\"name\")); System.out.println(\"===price==\" + doc.get(\"price\")); System.out.println(\"===image==\" + doc.get(\"image\")); System.out.println(\"===brandName==\" + doc.get(\"brandName\")); System.out.println(\"===categoryName==\" + doc.get(\"categoryName\")); &#125; &#125; //10. 关闭流 indexReader.close();&#125; 2. 搜索案例成品效果: 2.1 引入依赖在项目的pom.xml中引入依赖: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;skipTests&gt;true&lt;/skipTests&gt;&lt;/properties&gt;&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;7.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;7.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;7.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- mysql数据库驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.48&lt;/version&gt; &lt;/dependency&gt; &lt;!-- IK中文分词器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.wltea.ik-analyzer&lt;/groupId&gt; &lt;artifactId&gt;ik-analyzer&lt;/artifactId&gt; &lt;version&gt;8.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--web起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入thymeleaf --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Json转换工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.51&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.2 项目加入页面和资源将Lucene课程资料\\资源\\页面和静态资源, 下的页面和静态资源拷贝到项目的resources目录下 2.3 创建包和启动类创建目录, 并加入启动类: 启动类代码： 123456789101112/** * 启动类 * * @author wgy */@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 2.4 配置文件项目的resources目录下创建application.yml内容如下: 12345spring: thymeleaf: cache: falseserver: port: 8080 2.5 业务代码2.5.1 封装pojopojo包下加入ResultModel实体类 123456789101112131415161718/** * 自定义分页实体类 * * @author wgy */public class ResultModel &#123; // 商品列表 private List&lt;Sku&gt; skuList; // 商品总数 private Long recordCount; // 总页数 private Long pageCount; // 当前页 private long curPage; .....get和set方法.......略&#125; 2.5.2 controller代码1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 控制类 * * @author wgy */@Controller@RequestMapping(\"/list\")public class SearchController &#123; @Autowired private SearchService searchService; /** * 搜索 * * @param queryString 查询的关键字 * @param price 查询价格范围 * @param page 当前页 * @return * @throws Exception */ @RequestMapping public String query(String queryString, String price, Integer page, Model model) throws Exception &#123; //处理当前页 if (StringUtils.isEmpty(page)) &#123; page = 1; &#125; if (page &lt;= 0) &#123; page = 1; &#125; //调用service查询 ResultModel resultModel = searchService.query(queryString, price, page); model.addAttribute(\"result\", resultModel); //查询条件回显到页面 model.addAttribute(\"queryString\", queryString); model.addAttribute(\"price\", price); model.addAttribute(\"page\", page); return \"search\"; &#125;&#125; 2.5.3 service代码service接口: 123456789/** * 业务接口 * * @author wgy */public interface SearchService &#123; public ResultModel query(String queryString, String price, Integer page) throws Exception;&#125; service实现类: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** * 业务实现类 * * @author wgy */@Servicepublic class SearchServiceImpl implements SearchService &#123; //每页查询20条数据 public final static Integer PAGE_SIZE = 20; @Override public ResultModel query(String queryString, String price, Integer page) throws Exception &#123; long startTime = System.currentTimeMillis(); //1. 需要使用的对象封装 ResultModel resultModel = new ResultModel(); //从第几条开始查询 int start = (page - 1) * PAGE_SIZE; //查询到多少条为止 Integer end = page * PAGE_SIZE; //创建分词器 Analyzer analyzer = new IKAnalyzer(); //创建组合查询对象 BooleanQuery.Builder builder = new BooleanQuery.Builder(); //2. 根据查询关键字封装查询对象 QueryParser queryParser = new QueryParser(\"name\", analyzer); Query query1 = null; //判断传入的查询关键字是否为空, 如果为空查询所有, 如果不为空, 则根据关键字查询 if (StringUtils.isEmpty(queryString)) &#123; query1 = queryParser.parse(\"*:*\"); &#125; else &#123; query1 = queryParser.parse(queryString); &#125; //将关键字查询对象, 封装到组合查询对象中 builder.add(query1, BooleanClause.Occur.MUST); //3. 根据价格范围封装查询对象 if (!StringUtils.isEmpty(price)) &#123; String[] split = price.split(\"-\"); Query query2 = IntPoint.newRangeQuery(\"price\", Integer.parseInt(split[0]), Integer.parseInt(split[1])); //将价格查询对象, 封装到组合查询对象中 builder.add(query2, BooleanClause.Occur.MUST); &#125; //4. 创建Directory目录对象, 指定索引库的位置 /** * 使用MMapDirectory消耗的查询时间 * ====消耗时间为=========324ms * ====消耗时间为=========18ms */ Directory directory = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建输入流对象 IndexReader reader = DirectoryReader.open(directory); //6. 创建搜索对象 IndexSearcher indexSearcher = new IndexSearcher(reader); //7. 搜索并获取搜索结果 TopDocs topDocs = indexSearcher.search(builder.build(), end); //8. 获取查询到的总条数 resultModel.setRecordCount(topDocs.totalHits); //9. 获取查询到的结果集 ScoreDoc[] scoreDocs = topDocs.scoreDocs; long endTime = System.currentTimeMillis(); System.out.println(\"====消耗时间为=========\" + (endTime - startTime) + \"ms\"); //10. 遍历结果集封装返回的数据 List&lt;Sku&gt; skuList = new ArrayList&lt;&gt;(); if (scoreDocs != null) &#123; for (int i = start; i &lt; end; i++) &#123; //通过查询到的文档编号, 找到对应的文档对象 Document document = reader.document(scoreDocs[i].doc); //封装Sku对象 Sku sku = new Sku(); sku.setId(document.get(\"id\")); sku.setPrice(Integer.parseInt(document.get(\"price\"))); sku.setImage(document.get(\"image\")); sku.setName(document.get(\"name\")); sku.setBrandName(document.get(\"brandName\")); sku.setCategoryName(document.get(\"categoryName\")); skuList.add(sku); &#125; &#125; //封装查询到的结果集 resultModel.setSkuList(skuList); //封装当前页 resultModel.setCurPage(page); //总页数 Long pageCount = topDocs.totalHits % PAGE_SIZE &gt; 0 ? (topDocs.totalHits / PAGE_SIZE) + 1 : topDocs.totalHits / PAGE_SIZE; resultModel.setPageCount(pageCount); return resultModel; &#125;&#125; 3. Lucene 底层储存结构(高级)3.1 详细理解lucene存储结构存储结构 : 索引 (Index) ： 一个目录一个索引，在 Lucene中一个索引是放在一个文件夹中的。 段(Segment) : 一个索引 (逻辑索引)由多个段组成, 多个段可以合并, 以减少读取内容时候的磁盘IO. Lucene 中的数据写入会先写内存的一个Buffer，当Buffer内数据到一定量后会被flush成一个Segment，每个Segment有自己独立的索引，可独立被查询，但数据永远不能被更改。这种模式避免了随机写，数据写入都是批量追加，能达到很高的吞吐量。Segment中写入的文档不可被修改，但可被删除，删除的方式也不是在文件内部原地更改，而是会由另外一个文件保存需要被删除的文档的DocID，保证数据文件不可被修改。Index的查询需要对多个Segment进行查询并对结果进行合并，还需要处理被删除的文档，为了对查询进行优化，Lucene会有策略对多个Segment进行合并。 文档(Document) ： 文档是我们建索引的基本单位，不同的文档是保存在不同的段中的，一个段可以包含多篇文档。 新添加的文档是单独保存在一个新生成的段中，随着段的合并，不同的文档合并到同一个段中。 域(Field) ： 一篇文档包含不同类型的信息，可以分开索引，比如标题，时间，正文，描述等，都可以保存在不同的域里。 不同域的索引方式可以不同。 词(Term) ： 词是索引的最小单位，是经过词法分析和语言处理后的字符串。 3.2 索引库物理文件 3.3 索引库文件扩展名对照表 名称 文件扩展名 简短描述 Segments File segments_N 保存了一个提交点（a commit point）的信息 Lock File write.lock 防止多个IndexWriter同时写到一份索引文件中 Segment Info .si 保存了索引段的元数据信息 Compound File .cfs，.cfe 一个可选的虚拟文件，把所有索引信息都存储到复合索引文件中 Fields .fnm 保存fields的相关信息 Field Index .fdx 保存指向field data的指针 Field Data .fdt 文档存储的字段的值 Term Dictionary .tim term词典，存储term信息 Term Index .tip 到Term Dictionary的索引 Frequencies .doc 由包含每个term以及频率的docs列表组成 Positions .pos 存储出现在索引中的term的位置信息 Payloads .pay 存储额外的per-position元数据信息，例如字符偏移和用户payloads Norms .nvd，.nvm .nvm文件保存索引字段加权因子的元数据，.nvd文件保存索引字段加权数据 Per-Document Values .dvd，.dvm .dvm文件保存索引文档评分因子的元数据，.dvd文件保存索引文档评分数据 Term Vector Index .tvx 将偏移存储到文档数据文件中 Term Vector Documents .tvd 包含有term vectors的每个文档信息 Term Vector Fields .tvf 字段级别有关term vectors的信息 Live Documents .liv 哪些是有效文件的信息 Point values .dii，.dim 保留索引点，如果有的话 3.4 词典的构建为何Lucene大数据量搜索快, 要分两部分来看 : 一点是因为底层的倒排索引存储结构 . 另一点就是查询关键字的时候速度快 , 因为词典的索引结构. 3.4.1 词典数据结构对比倒排索引中的词典位于内存，其结构尤为重要，有很多种词典结构，各有各的优缺点，最简单如排序数组，通过二分查找来检索数据，更快的有哈希表，磁盘查找有B树、B+树，但一个能支持TB级数据的倒排索引结构需要在时间和空间上有个平衡，下图列了一些常见词典的优缺点： 数据结构 优缺点 跳跃表 占用内存小，且可调，但是对模糊查询支持不好 排序列表Array/List 使用二分法查找，不平衡 字典树 查询效率跟字符串长度有关，但只适合英文词典 哈希表 性能高，内存消耗大，几乎是原始数据的三倍 双数组字典树 适合做中文词典，内存占用小，很多分词工具均采用此种算法 Finite State Transducers (FST) 一种有限状态转移机，Lucene 4有开源实现，并大量使用 B树 磁盘索引，更新方便，但检索速度慢，多用于数据库 Lucene3.0之前使用的也是跳跃表结构，后换成了FST，但跳跃表在Lucene其他地方还有应用如倒排表合并和文档号索引。 3.4.2 跳跃表原理Lucene3.0版本之前使用的跳跃表结构后换成了FST结构 优点 ：结构简单、跳跃间隔、级数可控，Lucene3.0之前使用的也是跳跃表结构，，但跳跃表在Lucene其他地方还有应用如倒排表合并和文档号索引。 缺点 ：模糊查询支持不好. 单链表 : 单链表中查询一个元素即使是有序的，我们也不能通过二分查找法的方式缩减查询时间。 通俗的讲也就是按照链表顺序一个一个找. 举例: 查找85这个节点, 需要查找7次. 跳跃表 : 举例: 查询85这个节点, 一共需要查询6次. 在level3层, 查询3次, 查询到1结尾, 退回到37节点 在level2层, 从37节点开始查询, 查询2次, 查询到1结尾, 退回到71节点 在level1层, 从71节点开始查询, 查询1次, 查询到85节点. 3.4.3 FST原理简析Lucene现在采用的数据结构为FST，它的特点就是： 优点：内存占用率低，压缩率一般在 3 倍~20倍之间、模糊查询支持好、查询快 缺点：结构复杂、输入要求有序、更新不易 已知FST要求输入有序，所以Lucene会将解析出来的文档单词预先排序，然后构建FST，我们假设输入为abd,abe,acf,acg，那么整个构建过程如下： 输入数据： 12String inputValues[] &#x3D; &#123;&quot;hei&quot;,&quot;ma&quot;,&quot;cheng&quot;,&quot;xu&quot;,&quot;yuan&quot;,&quot;good&quot;&#125;;long outputValues[] &#x3D; &#123;0,1,2,3,4,5&#125;; 输入的数据如下 : hei/0 ma/1 cheng/2 xu/3 yuan/4 good/5 存储结果如下: 4. Lucene 优化(高级)4.1 解决大量磁盘IO config.setMaxBufferedDocs(100000); 控制写入一个新的segment前内存中保存的document的数目，设置较大的数目可以加快建索引速度。 数值越大索引速度越快, 但是会消耗更多的内存 indexWriter.forceMerge( 文档数量); 设置N个文档合并为一个段 数值越大索引速度越快, 搜索速度越慢; 值越小索引速度越慢, 搜索速度越快 更高的值意味着索引期间更低的段合并开销，但同时也意味着更慢的搜索速度，因为此时的索引通常会包含更多的段。如果该值设置的过高，能获得更高的索引性能。但若在最后进行索引优化，那么较低的值会带来更快的搜索速度，因为在索引操作期间程序会利用并发机制完成段合并操作。故建议对程序分别进行高低多种值的测试，利用计算机的实际性能来告诉你最优值。 创建索引代码优化测试: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 测试创建索引速度优化 * * @throws Exception */@Testpublic void createIndexTest2() throws Exception &#123; //1. 采集数据 SkuDao skuDao = new SkuDaoImpl(); List&lt;Sku&gt; skuList = skuDao.querySkuList(); //文档集合 List&lt;Document&gt; docList = new ArrayList&lt;&gt;(); for (Sku sku : skuList) &#123; //2. 创建文档对象 Document document = new Document(); document.add(new StringField(\"id\", sku.getId(), Field.Store.YES)); document.add(new TextField(\"name\", sku.getName(), Field.Store.YES)); document.add(new IntPoint(\"price\", sku.getPrice())); document.add(new StoredField(\"price\", sku.getPrice())); document.add(new StoredField(\"image\", sku.getImage())); document.add(new StringField(\"categoryName\", sku.getCategoryName(), Field.Store.YES)); document.add(new StringField(\"brandName\", sku.getBrandName(), Field.Store.YES)); //将文档对象放入到文档集合中 docList.add(document); &#125; long start = System.currentTimeMillis(); //3. 创建分词器, StandardAnalyzer标准分词器, 对英文分词效果好, 对中文是单字分词, 也就是一个字就认为是一个词. Analyzer analyzer = new StandardAnalyzer(); //4. 创建Directory目录对象, 目录对象表示索引库的位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建IndexWriterConfig对象, 这个对象中指定切分词使用的分词器 /** * 没有优化 小100万条数据, 创建索引需要7725ms * */ IndexWriterConfig config = new IndexWriterConfig(analyzer); //设置在内存中多少个文档向磁盘中批量写入一次数据 //如果设置的数字过大, 会过多消耗内存, 但是会提升写入磁盘的速度 //config.setMaxBufferedDocs(500000); //6. 创建IndexWriter输出流对象, 指定输出的位置和使用的config初始化对象 IndexWriter indexWriter = new IndexWriter(dir, config); //设置多少给文档合并成一个段文件,数值越大索引速度越快, 搜索速度越慢; 值越小索引速度越慢, 搜索速度越快 //indexWriter.forceMerge(1000000); //7. 写入文档到索引库 for (Document doc : docList) &#123; indexWriter.addDocument(doc); &#125; //8. 释放资源 indexWriter.close(); long end = System.currentTimeMillis(); System.out.println(\"=====消耗的时间为:==========\" + (end - start) + \"ms\");&#125; 4.2 选择合适的分词器不同的分词器分词效果不同, 所用时间也不同 虽然StandardAnalyzer切分词速度快过IKAnalyzer, 但是由于StandardAnalyzer对中文支持不好, 所以为了追求好的分词效果, 为了追求查询时的准确率, 也只能用IKAnalyzer分词器, IKAnalyzer支持停用词典和扩展词典, 可以通过调整两个词典中的内容, 来提升查询匹配的精度 4.3 选择合适的位置存放索引库 类 写操作 读操作 特点 SimpleFSDirectory java.io.RandomAccessFile java.io.RandomAccessFile 简单实现，并发能力差 NIOFSDirectory java.nio.FileChannel FSDirectory.FSIndexOutput 并发能力强,windows平台下有重大bug MMapDirectory 内存映射 FSDirectory.FSIndexOutput 读取操作基于内存 测试代码修改: 1Directory directory = MMapDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); 4.4 搜索api的选择1、尽量使用TermQuery代替QueryParser 2、尽量避免大范围的日期查询 5. Lucene 相关度排序(高级)5.1 什么是相关度排序Lucene对查询关键字和索引文档的相关度进行打分，得分高的就排在前边。 5.1.1 如何打分Lucene是在用户进行检索时实时根据搜索的关键字计算出来的，分两步： 1、计算出词（Term）的权重 2、根据词的权重值，计算文档相关度得分。 5.1.2 什么是词的权重明确索引的最小单位是一个Term(索引词典中的一个词)，搜索也是要从Term中搜索，再根据Term找到文档，Term对文档的重要性称为权重，影响Term权重有两个因素： Term Frequency (tf) ： 指此Term在此文档中出现了多少次。tf 越大说明越重要。 词(Term)在文档中出现的次数越多，说明此词(Term)对该文档越重要，如“Lucene”这个词，在文档中出现的次数很多，说明该文档主要就是讲Lucene技术的。 Document Frequency (df) ： 指有多少文档包含次Term。df 越大说明越不重要。 比如，在一篇英语文档中，this出现的次数更多，就说明越重要吗？不是的，有越多的文档包含此词(Term), 说明此词(Term)太普通，不足以区分这些文档，因而重要性越低。 5.1.3 怎样影响相关度排序boost是一个加权值（默认加权值为1.0f），它可以影响权重的计算。 在索引时对某个文档中的 field设置加权值高，在搜索时匹配到这个文档就可能排在前边。 在搜索时对某个域进行加权，在进行组合域查询时，匹配到加权值高的域最后计算的相关度得分就高。 设置boost是给域（field）或者Document设置的 5.2 人为影响相关度排序查询的时候, 通过设置查询域的权重, 可以人为影响查询结果. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 测试相关度排序 * * @throws Exception */@Testpublic void testIndexSearch2() throws Exception &#123; //1. 创建分词器(对搜索的关键词进行分词使用) //注意: 分词器要和创建索引的时候使用的分词器一模一样 Analyzer analyzer = new IKAnalyzer(); //需求: 不管是名称域还是品牌域或者是分类域有关于手机关键字的查询出来 //查询的多个域名 String[] fields = &#123;\"name\", \"categoryName\", \"brandName\"&#125;; //设置影响排序的权重, 这里设置域的权重 Map&lt;String, Float&gt; boots = new HashMap&lt;&gt;(); boots.put(\"categoryName\", 10000000000f); //从多个域查询对象 MultiFieldQueryParser multiFieldQueryParser = new MultiFieldQueryParser(fields, analyzer, boots); //设置查询的关键词 Query query = multiFieldQueryParser.parse(\"手机\"); //4. 创建Directory目录对象, 指定索引库的位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建输入流对象 IndexReader indexReader = DirectoryReader.open(dir); //6. 创建搜索对象 IndexSearcher indexSearcher = new IndexSearcher(indexReader); //7. 搜索, 并返回结果 //第二个参数: 是返回多少条数据用于展示, 分页使用 TopDocs topDocs = indexSearcher.search(query, 10); //获取查询到的结果集的总数, 打印 System.out.println(\"=======count=======\" + topDocs.totalHits); //8. 获取结果集 ScoreDoc[] scoreDocs = topDocs.scoreDocs; //9. 遍历结果集 if (scoreDocs != null) &#123; for (ScoreDoc scoreDoc : scoreDocs) &#123; //获取查询到的文档唯一标识, 文档id, 这个id是lucene在创建文档的时候自动分配的 int docID = scoreDoc.doc; //通过文档id, 读取文档 Document doc = indexSearcher.doc(docID); System.out.println(\"==================================================\"); //通过域名, 从文档中获取域值 System.out.println(\"===id==\" + doc.get(\"id\")); System.out.println(\"===name==\" + doc.get(\"name\")); System.out.println(\"===price==\" + doc.get(\"price\")); System.out.println(\"===image==\" + doc.get(\"image\")); System.out.println(\"===brandName==\" + doc.get(\"brandName\")); System.out.println(\"===categoryName==\" + doc.get(\"categoryName\")); &#125; &#125; //10. 关闭流 indexReader.close();&#125; 6. Lucene 使用注意事项(高级) 关键词区分大小写 OR AND TO等关键词是区分大小写的，lucene只认大写的，小写的当做普通单词。 读写互斥性 同一时刻只能有一个对索引的写操作，在写的同时可以进行搜索 文件锁 在写索引的过程中强行退出将在tmp目录留下一个lock文件，使以后的写操作无法进行，可以将其手工删除 时间格式 lucene只支持一种时间格式yyMMddHHmmss，所以你传一个yy-MM-dd HH:mm:ss的时间给lucene它是不会当作时间来处理的 设置 boost 有些时候在搜索时某个字段的权重需要大一些，例如你可能认为标题中出现关键词的文章比正文中出现关键词的文章更有价值，你可以把标题的boost设置的更大，那么搜索结果会优先显示标题中出现关键词的文章.","tags":[{"name":"业务解决方案","slug":"业务解决方案","permalink":"https://wgy1993.gitee.io/tags/%E4%B8%9A%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"name":"Lucene","slug":"Lucene","permalink":"https://wgy1993.gitee.io/tags/Lucene/"}]},{"title":"Lucene(一)","date":"2020-08-15T07:24:06.000Z","path":"archives/c330d3d2.html","text":"1. 搜索技术理论基础1.1 为什么要学习Lucene原来的方式实现搜索功能，我们的搜索流程如下图： 上图就是原始搜索引擎技术，如果用户比较少而且数据库的数据量比较小，那么这种方式实现搜索功能在企业中是比较常见的。 但是数据量过多时，数据库的压力就会变得很大，查询速度会变得非常慢。我们需要使用更好的解决方案来分担数据库的压力。 现在的方案（使用Lucene），如下图 为了解决数据库压力和速度的问题，我们的数据库就变成了索引库，我们使用Lucene的API的来操作服务器上的索引库。这样完全和数据库进行了隔离。 1.2 数据查询方法1.2.1 顺序扫描法算法描述： 所谓顺序扫描，例如要找内容包含一个字符串的文件，就是一个文档一个文档的看，对于每一个文档，从头看到尾，如果此文档包含此字符串，则此文档为我们要找的文件，接着看下一个文件，直到扫描完所有的文件。 优点： 查询准确率高 缺点： 查询速度会随着查询数据量的增大， 越来越慢 使用场景： 数据库中的like关键字模糊查询 文本编辑器的Ctrl + F 查询功能 1.2.2 倒排索引先举一个栗子： 例如我们使用新华字典查询汉字，新华字典有偏旁部首的目录（索引），我们查字首先查这个目录，找到这个目录中对应的偏旁部首，就可以通过这个目录中的偏旁部首找到这个字所在的位置（文档）。 Lucene会对文档建立倒排索引 1 、 提取资源中关键信息， 建立索引 （目录） 2 、 搜索时，根据关键字（目录），找到资源的位置 算法描述： 查询前会先将查询的内容提取出来组成文档(正文), 对文档进行切分词组成索引(目录), 索引和文档有关联关系, 查询的时候先查询索引, 通过索引找文档的这个过程叫做全文检索。 切分词 : 就是将一句一句话切分成一个一个的词, 去掉停用词(的, 地, 得, a, an, the等)。去掉空格, 去掉标点符号, 大写字母转成小写字母, 去掉重复的词。 为什么倒排索引比顺序扫描快? 理解 : 因为索引可以去掉重复的词, 汉语常用的字和词大概等于, 字典加词典, 常用的英文在牛津词典也有收录.如果用计算机的速度查询, 字典+词典+牛津词典这些内容是非常快的. 但是用这些字典, 词典组成的文章却是千千万万不计其数. 索引的大小最多也就是字典+词典. 所以通过查询索引, 再通过索引和文档的关联关系找到文档速度比较快. 顺序扫描法则是直接去逐个查询那些不计其数的文章就算是计算的速度也会很慢. 优点： 查询准确率高 查询速度快， 并且不会因为查询内容量的增加， 而使查询速度逐渐变慢 缺点： 索引文件会占用额外的磁盘空间， 也就是占用磁盘量会增大。 使用场景： 海量数据查询 1.3 全文检索技术应用场景应用场景 ： 1 、 站内搜索 （baidu贴吧、论坛、 京东、 taobao） 2 、 垂直领域的搜索 （ 818 工作网） 3 、 专业搜索引擎公司 （google、baidu） 2. Lucene介绍2.1 什么是全文检索计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式 2.2 什么是Lucene 他是Lucene、Nutch 、Hadoop等项目的发起人Doug Cutting Lucene是apache软件基金会4 jakarta项目组的一个子项目，是一个开放源代码的全文检索引擎工具包，但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎（英文与德文两种西方语言）。 Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。 目前已经有很多应用程序的搜索功能是基于 Lucene 的，比如 Eclipse 的帮助系统的搜索功能。Lucene能够为文本类型的数据建立索引，所以你只要能把你要索引的数据格式转化的文本的，Lucene 就能对你的文档进行索引和搜索。比如你要对一些 HTML 文档，PDF 文档进行索引的话你就首先需要把HTML 文档和 PDF 文档转化成文本格式的，然后将转化后的内容交给 Lucene 进行索引，然后把创建好的索引文件保存到磁盘或者内存中，最后根据用户输入的查询条件在索引文件上进行查询。不指定要索引的文档的格式也使 Lucene 能够几乎适用于所有的搜索应用程序。 Lucene是一套用于全文检索和搜寻的开源程式库，由Apache软件基金会支持和提供 Lucene提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻， 在Java开发环境里Lucene是一个成熟的免费开放源代码工具 Lucene并不是现成的搜索引擎产品，但可以用来制作搜索引擎产品 2.3 Lucene官网官网： http://lucene.apache.org/ 3. Lucene全文检索的流程3.1 索引和搜索流程图 1 、绿色表示索引过程，对要搜索的原始内容进行索引构建一个索引库，索引过程包括： 确定原始内容即要搜索的内容 获得文档 创建文档 分析文档 索引文档 2 、红色表示搜索过程，从索引库中搜索内容，搜索过程包括： 用户通过搜索界面 创建查询 执行搜索，从索引库搜索 渲染搜索结果 3.2 索引流程对文档索引的过程，将用户要搜索的文档内容进行索引，索引存储在索引库（index）中。 3.2.1 原始内容原始内容是指要索引和搜索的内容。 原始内容包括互联网上的网页、数据库中的数据、磁盘上的文件等。 3.2.2 获得文档（采集数据）从互联网上、数据库、文件系统中等获取需要搜索的原始信息，这个过程就是信息采集，采集数据的目的是为了对原始内容进行索引。 采集数据分类： 1 、对于互联网上网页，可以使用工具将网页抓取到本地生成html文件。 2 、数据库中的数据，可以直接连接数据库读取表中的数据。 3 、文件系统中的某个文件，可以通过I/O操作读取文件的内容。 在Internet上采集信息的软件通常称为爬虫或蜘蛛，也称为网络机器人，爬虫访问互联网上的每一个网页，将获取到的网页内容存储起来。 3.2.3 创建文档获取原始内容的目的是为了索引，在索引前需要将原始内容创建成文档（Document），文档中包括一个一个的域（Field），域中存储内容。 这里我们可以将磁盘上的一个文件当成一个document，Document中包括一些Field，如下图： 注意：每个Document可以有多个Field，不同的Document可以有不同的Field，同一个Document可以有相同的Field（域名和域值都相同） 3.2.4 分析文档将原始内容创建为包含域（Field）的文档（document），需要再对域中的内容进行分析，分析成为一个一个的单词。 比如下边的文档经过分析如下： 原文档内容： vivo X23 8GB+128GB 幻夜蓝 全网通4G手机 华为 HUAWEI 麦芒7 6G+64G 亮黑色 全网通4G手机 分析后得到的词： vivo, x23, 8GB, 128GB, 幻夜, 幻夜蓝, 全网, 全网通, 网通, 4G, 手机, 华为, HUAWEI, 麦芒 7 。。。。 3.2.5 索引文档对所有文档分析得出的语汇单元进行索引，索引的目的是为了搜索，最终要实现只搜索被索引的语汇单元从而找到Document（文档）。 创建索引是对语汇单元索引，通过词语找文档，这种索引的结构叫倒排索引结构。 倒排索引结构是根据内容（词汇）找文档，如下图： 倒排索引结构也叫反向索引结构，包括索引和文档两部分，索引即词汇表，它的规模较小，而文档集合较大。 3.2.6 Lucene底层存储结构 3.3 搜索流程搜索就是用户输入关键字，从索引中进行搜索的过程。根据关键字搜索索引，根据索引找到对应的文档，从而找到要搜索的内容。 3.3.1 用户就是使用搜索的角色，用户可以是自然人，也可以是远程调用的程序。 3.3.2 用户搜索界面全文检索系统提供用户搜索的界面供用户提交搜索的关键字，搜索完成展示搜索结果。如下图： Lucene不提供制作用户搜索界面的功能，需要根据自己的需求开发搜索界面。 3.3.3 创建查询用户输入查询关键字执行搜索之前需要先构建一个查询对象，查询对象中可以指定查询要查询关键字、要搜索的Field文档域等，查询对象会生成具体的查询语法，比如： name:手机 : 表示要搜索name这个Field域中，内容为“手机”的文档。 name:华为 AND 手机 : 表示要搜索即包括关键字“华为” 并且也包括“手机”的文档。 3.3.4 执行搜索搜索索引过程： 1.根据查询语法在倒排索引词典表中分别找出对应搜索词的索引，从而找到索引所链接的文档链表。 例如搜索语法为 “name:华为 AND 手机 ” 表示搜索出的文档中既要包括”华为”也要包括”手机”。 2 、由于是AND，所以要对包含 华为 和 手机 词语的链表进行交集，得到文档链表应该包括每一个搜索词语 3 、获取文档中的Field域数据。 3.3.5 渲染结果以一个友好的界面将查询结果展示给用户，用户根据搜索结果找自己想要的信息，为了帮助用户很快找到自己的结果，提供了很多展示的效果，比如搜索结果中将关键字高亮显示，百度提供的快照等。 4. Lucene入门4.1 Lucene准备Lucene可以在官网上下载。课程已经准备好了Lucene的文件，我们使用的是7.7.2版本，文件位置如下图： 解压后的效果： 使用这三个文件的jar包，就可以实现lucene功能 4.2 开发环境JDK： 1.8 （Lucene7以上，必须使用JDK1.8及以上版本） 数据库： MySQL 数据库脚本位置如下图： 导入到MySQL效果如下图： 4.3 创建Java工程创建maven工程不依赖骨架, 测试即可，效果如下： pom.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;luceneDemo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;skipTests&gt;true&lt;/skipTests&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;7.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;7.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;7.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 测试 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- mysql数据库驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.48&lt;/version&gt; &lt;/dependency&gt; &lt;!-- IK中文分词器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.wltea.ik-analyzer&lt;/groupId&gt; &lt;artifactId&gt;ik-analyzer&lt;/artifactId&gt; &lt;version&gt;8.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;!--web起步依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 引入thymeleaf --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Json转换工具 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.51&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 4.4 索引流程4.4.1 数据采集在电商网站中，全文检索的数据源在数据库中，需要通过jdbc访问数据库中 sku 表的内容。 4.4.1.1 创建pojo1234567891011121314151617181920212223242526272829/** * 商品实体类 * * @author wgy */public class Sku &#123; //商品主键id private String id; //商品名称 private String name; //价格 private Integer price; //库存数量 private Integer num; //图片 private String image; //分类名称 private String categoryName; //品牌名称 private String brandName; //规格 private String spec; //销量 private Integer saleNum; get/set/toString... &#125; 4.4.1.2 创建DAO接口1234567891011121314/** * 持久层接口 * * @author wgy */public interface SkuDao &#123; /** * 查询所有的Sku数据 * * @return **/ public List&lt;Sku&gt; querySkuList();&#125; 4.4.1.3 创建DAO接口实现类使用jdbc实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 持久层实现类 * * @author wgy */public class SkuDaoImpl implements SkuDao &#123; @Override public List&lt;Sku&gt; querySkuList() &#123; // 数据库链接 Connection connection = null; // 预编译statement PreparedStatement preparedStatement = null; // 结果集 ResultSet resultSet = null; // 商品列表 List&lt;Sku&gt; list = new ArrayList&lt;Sku&gt;(); try &#123; // 加载数据库驱动 Class.forName(\"com.mysql.jdbc.Driver\"); // 连接数据库 connection = (Connection) DriverManager.getConnection(\"jdbc:mysql://localhost:3306/lucene\", \"root\", \"root\"); // SQL语句 String sql = \"SELECT * FROM tb_sku\"; // 创建preparedStatement preparedStatement = (PreparedStatement) connection.prepareStatement(sql); // 获取结果集 resultSet = preparedStatement.executeQuery(); // 结果集解析 while (resultSet.next()) &#123; Sku sku = new Sku(); sku.setId(resultSet.getString(\"id\")); sku.setName(resultSet.getString(\"name\")); sku.setSpec(resultSet.getString(\"spec\")); sku.setBrandName(resultSet.getString(\"brand_name\")); sku.setCategoryName(resultSet.getString(\"category_name\")); sku.setImage(resultSet.getString(\"image\")); sku.setNum(resultSet.getInt(\"num\")); sku.setPrice(resultSet.getInt(\"price\")); sku.setSaleNum(resultSet.getInt(\"sale_num\")); list.add(sku); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return list; &#125;&#125; 4.4.2 实现索引流程 采集数据 创建Document文档对象 创建分析器（分词器） 创建Directory对象，声明索引库存储位置 创建IndexWriterConfig配置信息类 创建IndexWriter写入对象 把Document写入到索引库中 释放资源 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 索引库维护 * * @author wgy */public class TestIndexManager &#123; /** * 创建索引库 */ @Test public void createIndexTest() throws Exception &#123; // 1. 采集数据 SkuDao skuDao = new SkuDaoImpl(); List&lt;Sku&gt; skuList = skuDao.querySkuList(); // 2. 创建Document文档对象 List&lt;Document&gt; documents = new ArrayList&lt;Document&gt;(); for (Sku sku : skuList) &#123; Document document = new Document(); // Document文档中添加Field域 // 商品Id // Store.YES:表示存储到文档域中 document.add(new TextField(\"id\", sku.getId(), Field.Store.YES)); // 商品名称 document.add(new TextField(\"name\", sku.getName(), Field.Store.YES)); // 商品价格 document.add(new TextField(\"price\", String.valueOf(sku.getPrice()), Field.Store.YES)); // 品牌名称 document.add(new TextField(\"brandName\", sku.getBrandName(), Field.Store.YES)); // 分类名称 document.add(new TextField(\"categoryName\", sku.getCategoryName(), Field.Store.YES)); // 图片地址 document.add(new TextField(\"image\", sku.getImage(), Field.Store.YES)); // 把Document放到list中 documents.add(document); &#125; //3. 创建分词器, StandardAnalyzer标准分词器, 对英文分词效果好, 对中文是单字分词, 也就是一个字就认为是一个词. Analyzer analyzer = new StandardAnalyzer(); //4. 创建Directory目录对象，目录对象表示索引库的位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建IndexWriterConfig对象，这个对象中指定切分词使用的分词器 IndexWriterConfig config = new IndexWriterConfig(analyzer); //6. 创建IndexWriter输出流对象，指定输出的位置和使用的config初始化对象 IndexWriter indexWriter = new IndexWriter(dir, config); //7. 写入文档到索引库 for (Document document : docList) &#123; indexWriter.addDocument(document); &#125; //8. 释放资源 indexWriter.close(); &#125;&#125; 执行效果： 在文件夹中出现了以下文件，表示创建索引成功 4.5 使用Luke查看索引Luke作为Lucene工具包中的一个工具（http://www.getopt.org/luke/），可以通过界面来进行索引文件的查询、修改 luke所在位置如下图： 将luke-swing-8.0.0里面的内容, 放到一个硬盘根目录的文件夹下, 不能有空格和中文名称. 运行luke.bat 打开后，使用如下图： 下图是索引域的展示效果： 下图是文档域展示效果 4.6 搜索流程4.6.1 输入查询语句Lucene可以通过query对象输入查询语句。同数据库的sql一样，lucene也有固定的查询语法： 最基本的有比如：AND, OR, NOT 等（必须大写） 举个栗子: 用户想找一个 name 域中包括 手 或 机 关键字的文档。 它对应的查询语句：name:手 OR name:机 如下图是使用luke搜索的例子： 4.6.1.1 搜索分词和索引过程的分词一样，这里要对用户输入的关键字进行分词，一般情况索引和搜索使用的分词器一致。 比如：输入搜索关键字“java学习”，分词后为java和学习两个词，与java和学习有关的内容都搜索出来了，如下： 4.6.2 代码实现 创建Query搜索对象 创建Directory流对象,声明索引库位置 创建索引读取对象IndexReader 创建索引搜索对象IndexSearcher 使用索引搜索对象，执行搜索，返回结果集TopDocs 解析结果集 释放资源 IndexSearcher搜索方法如下： 方法 说明 indexSearcher.search(query, n) 根据Query搜索，返回评分最高的n条记录 indexSearcher.search(query, filter, n) 根据Query搜索，添加过滤策略，返回评分最高的n条记录 indexSearcher.search(query, n, sort) 根据Query搜索，添加排序策略，返回评分最高的n条记录 indexSearcher.search(query,filter, n, sort) 根据Query搜索，添加过滤策略，添加排序策略，返回评分最高的n条记录 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 测试搜索过程 * * @author wgy */public class TestSearch &#123; @Test public void testIndexSearch() throws Exception &#123; //1. 创建Query搜索对象 //1.1 创建分词器(对搜索的关键词进行分词使用) //注意: 分词器要和创建索引的时候使用的分词器一模一样 Analyzer analyzer = new StandardAnalyzer(); //1.2 创建搜索解析器 //第一个参数: 默认查询域, 如果查询的关键字中带搜索的域名, 则从指定域中查询, 如果不带域名则从, 默认搜索域中查询 //queryParser.parse(\"brandName:华为手机\"); //第二个参数: 使用的分词器 QueryParser queryParser = new QueryParser(\"name\", analyzer); //1.3 创建搜索对象 //华 OR 为 手 机 Query query = queryParser.parse(\"华为手机\"); //2. 创建Directory流对象,声明索引库位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //3. 创建索引读取对象IndexReader IndexReader indexReader = DirectoryReader.open(dir); //4. 创建索引搜索对象 IndexSearcher indexSearcher = new IndexSearcher(indexReader); //5. 使用索引搜索对象，执行搜索，返回结果集TopDocs // 第一个参数：搜索对象，第二个参数：返回的数据条数，指定查询结果最顶部的n条数据返回 TopDocs topDocs = indexSearcher.search(query, 10); //获取查询到的结果集的总数, 打印 System.out.println(\"=======count=======\" + topDocs.totalHits); //5.1 获取结果集 ScoreDoc[] scoreDocs = topDocs.scoreDocs; //6. 解析结果集 if (scoreDocs != null) &#123; for (ScoreDoc scoreDoc : scoreDocs) &#123; //获取查询到的文档唯一标识, 文档id, 这个id是lucene在创建文档的时候自动分配的 int docID = scoreDoc.doc; //通过文档id, 读取文档 Document doc = indexSearcher.doc(docID); System.out.println(\"==================================================\"); //通过域名, 从文档中获取域值 System.out.println(\"===id==\" + doc.get(\"id\")); System.out.println(\"===name==\" + doc.get(\"name\")); System.out.println(\"===price==\" + doc.get(\"price\")); System.out.println(\"===image==\" + doc.get(\"image\")); System.out.println(\"===brandName==\" + doc.get(\"brandName\")); System.out.println(\"===categoryName==\" + doc.get(\"categoryName\")); &#125; &#125; //7. 关闭流 indexReader.close(); &#125;&#125; 5. Field域类型5.1 Field属性Field是文档中的域，包括Field名和Field值两部分，一个文档可以包括多个Field，Document只是Field的一个承载体，Field值即为要索引的内容，也是要搜索的内容。 是否分词(tokenized) 是：作分词处理，即将Field值进行分词，分词的目的是为了索引。 比如：商品名称、商品描述等，这些内容用户要输入关键字搜索，由于搜索的内容格式大、内容多需要分词后将语汇单元建立索引 否：不作分词处理 比如：商品id、订单号、身份证号等 是否索引(indexed) 是：进行索引。将Field分词后的词或整个Field值进行索引，存储到索引域，索引的目的是为了搜索。 比如：商品名称、商品描述分析后进行索引，订单号、身份证号不用分词但也要索引，这些将来都要作为查询条件。 否：不索引。 比如：图片路径、文件路径等，不用作为查询条件的不用索引。 是否存储(stored) 是：将Field值存储在文档域中，存储在文档域中的Field才可以从Document中获取。 比如：商品名称、订单号，凡是将来要从Document中获取的Field都要存储。 否：不存储Field值 比如：商品描述，内容较大不用存储。如果要向用户展示商品描述可以从系统的关系数据库中获取。 5.2 Field常用类型下边列出了开发中常用 的Filed类型，注意Field的属性，根据需求选择： Field类 数据类型 Analyzed是否分词 Indexed是否索引 Stored是否存储 说明 StringField(FieldName,FieldValue,Store.YES)) 字符串 N Y Y或N 这个Field用来构建一个字符串Field，但是不会进行分词，会将整个串存储在索引中，比如(订单号,身份证号等)是否存储在文档中用Store.YES或Store.NO决定 FloatPoint(FieldName, FieldValue) Float型 Y Y N 这个Field用来构建一个Float数字型Field，进行分词和索引，不存储, 比如(价格) 存储在文档中 DoublePoint(FieldName,FieldValue) Double型 Y Y N 这个Field用来构建一个Double数字型Field，进行分词和索引，不存储 LongPoint(FieldName, FieldValue) Long型 Y Y N 这个Field用来构建一个Long数字型Field，进行分词和索引，不存储 IntPoint(FieldName, FieldValue) Integer型 Y Y N 这个Field用来构建一个Integer数字型Field，进行分词和索引，不存储 StoredField(FieldName, FieldValue) 重载方法，支持多种类型 N N Y 这个Field用来构建不同类型Field不分析，不索引，但要Field存储在文档中 TextField(FieldName, FieldValue,Store.NO) 或 TextField(FieldName,reader) 字符串或流 Y Y Y或N 如果是一个Reader, lucene猜测内容比较多,会采用Unstored的策略. NumericDocValuesField(FieldName,FieldValue) 数值 _ _ _ 配合其他域排序使用 5.3 Field修改对之前编写的testCreateIndex()方法进行修改。 代码片段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//创建域对象并且放入文档对象中/* * 是否分词: 否, 因为主键分词后无意义 * 是否索引: 是, 如果根据id主键查询, 就必须索引 * 是否存储: 是, 因为主键id比较特殊, 可以确定唯一的一条数据, 在业务上一般有重要所用, 所以存储存储后, 才可以获取到id具体的内容 *///document.add(new TextField(\"id\", sku.getId(), Field.Store.YES));document.add(new StringField(\"id\", sku.getId(), Field.Store.YES));/* * 是否分词: 是, 因为名称字段需要查询, 并且分词后有意义所以需要分词 * 是否索引: 是, 因为需要根据名称字段查询 * 是否存储: 是, 因为页面需要展示商品名称, 所以需要存储 */document.add(new TextField(\"name\", sku.getName(), Field.Store.YES));/* * 是否分词: 是(因为lucene底层算法规定, 如果根据价格范围查询, 必须分词) * 是否索引: 是, 需要根据价格进行范围查询, 所以必须索引 * 是否存储: 是, 因为页面需要展示价格 *///document.add(new TextField(\"price\", String.valueOf(sku.getPrice()), Field.Store.YES));document.add(new IntPoint(\"price\", sku.getPrice()));document.add(new StoredField(\"price\", sku.getPrice()));/* * 是否分词: 否, 因为不查询, 所以不索引, 因为不索引所以不分词 * 是否索引: 否, 因为不需要根据图片地址路径查询 * 是否存储: 是, 因为页面需要展示商品图片 *///document.add(new TextField(\"image\", sku.getImage(), Field.Store.YES));document.add(new StoredField(\"image\", sku.getImage()));/* * 是否分词: 否, 因为分类是专有名词, 是一个整体, 所以不分词 * 是否索引: 是, 因为需要根据分类查询 * 是否存储: 是, 因为页面需要展示分类 *///document.add(new TextField(\"categoryName\", sku.getCategoryName(), Field.Store.YES));document.add(new StringField(\"categoryName\", sku.getCategoryName(), Field.Store.YES));/* * 是否分词: 否, 因为品牌是专有名词, 是一个整体, 所以不分词 * 是否索引: 是, 因为需要根据品牌进行查询 * 是否存储: 是, 因为页面需要展示品牌 *///document.add(new TextField(\"brandName\", sku.getBrandName(), Field.Store.YES));document.add(new StringField(\"brandName\", sku.getBrandName(), Field.Store.YES)); 6. 索引维护6.1 需求管理人员通过电商系统更改图书信息，这时更新的是关系数据库，如果使用lucene搜索图书信息，需要在数据库表book信息变化时及时更新lucene索引库。 6.2 添加索引调用 indexWriter.addDocument（doc）添加索引。 参考入门程序的创建索引。 6.3 修改索引更新索引是先删除再添加，建议对更新需求采用此方法并且要保证对已存在的索引执行更新，可以先查询出来，确定更新记录存在执行更新操作。 如果更新索引的目标文档对象不存在，则执行添加。 代码 123456789101112131415161718192021222324252627282930313233/** * 索引库修改操作 * * @throws Exception */@Testpublic void updateIndexTest() throws Exception &#123; //需要变更成的内容 Document document = new Document(); document.add(new StringField(\"id\", \"100000003145\", Field.Store.YES)); document.add(new TextField(\"name\", \"xxxx\", Field.Store.YES)); document.add(new IntPoint(\"price\", 123)); document.add(new StoredField(\"price\", 123)); document.add(new StoredField(\"image\", \"xxxx.jpg\")); document.add(new StringField(\"categoryName\", \"手机\", Field.Store.YES)); document.add(new StringField(\"brandName\", \"华为\", Field.Store.YES)); //3. 创建分词器, StandardAnalyzer标准分词器, 对英文分词效果好, 对中文是单字分词, 也就是一个字就认为是一个词. Analyzer analyzer = new StandardAnalyzer(); //4. 创建Directory目录对象, 目录对象表示索引库的位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建IndexWriterConfig对象, 这个对象中指定切分词使用的分词器 IndexWriterConfig config = new IndexWriterConfig(analyzer); //6. 创建IndexWriter输出流对象, 指定输出的位置和使用的config初始化对象 IndexWriter indexWriter = new IndexWriter(dir, config); //7. 修改, 第一个参数: 修改条件, 第二个参数: 修改成的内容 indexWriter.updateDocument(new Term(\"id\", \"100000003145\"), document); //8. 释放资源 indexWriter.close();&#125; 6.4 删除索引6.4.1 删除指定索引根据Term项删除索引，满足条件的将全部删除。 12345678910111213141516171819202122/** * 测试根据条件删除 * * @throws Exception */@Testpublic void deleteIndexTest() throws Exception &#123; //3. 创建分词器, StandardAnalyzer标准分词器, 对英文分词效果好, 对中文是单字分词, 也就是一个字就认为是一个词. Analyzer analyzer = new StandardAnalyzer(); //4. 创建Directory目录对象, 目录对象表示索引库的位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建IndexWriterConfig对象, 这个对象中指定切分词使用的分词器 IndexWriterConfig config = new IndexWriterConfig(analyzer); //6. 创建IndexWriter输出流对象, 指定输出的位置和使用的config初始化对象 IndexWriter indexWriter = new IndexWriter(dir, config); //7. 测试根据条件删除 indexWriter.deleteDocuments(new Term(\"id\", \"998188\")); //8. 释放资源 indexWriter.close();&#125; 效果如下图：索引域没有变化 文档域数据被删除掉 6.4.2 删除全部索引（慎用）将索引目录的索引信息全部删除，直接彻底删除，无法恢复。 建议参照关系数据库基于主键删除方式，所以在创建索引时需要创建一个主键Field，删除时根据此主键Field删除。 索引删除后将放在Lucene的回收站中，Lucene3.X版本可以恢复删除的文档，3.X之后无法恢复。 代码： 12345678910111213141516171819202122/** * 测试根据条件删除 * * @throws Exception */@Testpublic void deleteIndexTest() throws Exception &#123; //3. 创建分词器, StandardAnalyzer标准分词器, 对英文分词效果好, 对中文是单字分词, 也就是一个字就认为是一个词. Analyzer analyzer = new StandardAnalyzer(); //4. 创建Directory目录对象, 目录对象表示索引库的位置 Directory dir = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); //5. 创建IndexWriterConfig对象, 这个对象中指定切分词使用的分词器 IndexWriterConfig config = new IndexWriterConfig(analyzer); //6. 创建IndexWriter输出流对象, 指定输出的位置和使用的config初始化对象 IndexWriter indexWriter = new IndexWriter(dir, config); //测试删除所有内容 indexWriter.deleteAll(); //8. 释放资源 indexWriter.close();&#125; 索引域数据清空 文档域数据也清空 7. 分词器7.1 分词理解在对Document中的内容进行索引之前，需要使用分词器进行分词 ，分词的目的是为了搜索。分词的主要过程就是先分词后过滤。 分词：采集到的数据会存储到document对象的Field域中，分词就是将Document中Field的value值切分成一个一个的词。 过滤：包括去除标点符号过滤、去除停用词过滤（的、是、a、an、the等）、大写转小写、词的形还原（复数形式转成单数形参、过去式转成现在式。。。）等。 什么是停用词？停用词是为节省存储空间和提高搜索效率，搜索引擎在索引页面或处理搜索请求时会自动忽略某些字或词，这些字或词即被称为Stop Words(停用词)。比如语气助词、副词、介词、连接词等，通常自身并无明确的意义，只有将其放入一个完整的句子中才有一定作用，如常见的“的”、“在”、“是”、“啊”等。 对于分词来说，不同的语言，分词规则不同。Lucene作为一个工具包提供不同国家的分词器 7.2 Analyzer使用时机7.2.1 索引时使用Analyzer输入关键字进行搜索，当需要让该关键字与文档域内容所包含的词进行匹配时需要对文档域内容进行分析，需要经过Analyzer分析器处理生成语汇单元（Token）。分析器分析的对象是文档中的Field域。当Field的属性tokenized（是否分词）为true时会对Field值进行分析，如下图： 对于一些Field可以不用分析： 1 、不作为查询条件的内容，比如文件路径 2 、不是匹配内容中的词而匹配Field的整体内容，比如订单号、身份证号等。 7.2.2 搜索时使用Analyzer对搜索关键字进行分析和索引分析一样，使用Analyzer对搜索关键字进行分析、分词处理，使用分析后每个词语进行搜索。比如：搜索关键字：spring web ，经过分析器进行分词，得出：spring web拿词去索引词典表查找 ，找到索引链接到Document，解析Document内容。 对于匹配整体Field域的查询可以在搜索时不分析，比如根据订单号、身份证号查询等。 注意：搜索使用的分析器要和索引使用的分析器一致。 7.3 Lucene原生分词器以下是Lucene中自带的分词器 7.3.1 StandardAnalyzer特点 : Lucene提供的标准分词器, 可以对用英文进行分词, 对中文是单字分词, 也就是一个字就认为是一个词. 如下是org.apache.lucene.analysis.standard.standardAnalyzer的部分源码： 123456789101112protected TokenStreamComponents createComponents(String fieldName) &#123; final StandardTokenizer src = new StandardTokenizer(); src.setMaxTokenLength(this.maxTokenLength); TokenStream tok = new LowerCaseFilter(src); TokenStream tok = new StopFilter(tok, this.stopwords); return new TokenStreamComponents(src, tok) &#123; protected void setReader(Reader reader) &#123; src.setMaxTokenLength(StandardAnalyzer.this.maxTokenLength); super.setReader(reader); &#125; &#125;;&#125; Tokenizer就是分词器，负责将reader转换为语汇单元即进行分词处理，Lucene提供了很多的分词器，也可以使用第三方的分词，比如IKAnalyzer一个中文分词器。 TokenFilter是分词过滤器，负责对语汇单元进行过滤，TokenFilter可以是一个过滤器链儿，Lucene提供了很多的分词器过滤器，比如大小写转换、去除停用词等。 如下图是语汇单元的生成过程： 从一个Reader字符流开始，创建一个基于Reader的Tokenizer分词器，经过三个TokenFilter生成语汇单元Token。 比如下边的文档经过分析器分析如下： 原文档内容： 分析后得到的多个语汇单元： 7.3.2 WhitespaceAnalyzer特点 : 仅仅是去掉了空格，没有其他任何操作，不支持中文。 测试代码: 123456789101112131415161718192021222324252627/** * 去掉空格分词器, 不支持中文 * * @throws Exception */@Testpublic void TestWhitespaceAnalyzer() throws Exception &#123; // 1. 创建分词器,分析文档，对文档进行分词 Analyzer analyzer = new WhitespaceAnalyzer(); // 2. 创建Directory对象,声明索引库的位置 Directory directory = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); // 3. 创建IndexWriteConfig对象，写入索引需要的配置 IndexWriterConfig config = new IndexWriterConfig(analyzer); // 4.创建IndexWriter写入对象 IndexWriter indexWriter = new IndexWriter(directory, config); // 5.写入到索引库，通过IndexWriter添加文档对象document Document doc = new Document(); doc.add(new TextField(\"name\", \"vivo X23 8GB+128GB 幻夜蓝\", Field.Store.YES)); indexWriter.addDocument(doc); // 6.释放资源 indexWriter.close();&#125; 结果: 7.3.3 SimpleAnalyzer特点 : 将除了字母以外的符号全部去除，并且将所有字母变为小写，需要注意的是这个分词器同样把数字也去除了，同样不支持中文。 测试: 123456789101112131415161718192021222324252627/** * 简单分词器: 不支持中文, 将除了字母之外的所有符号全部取出, 所有大写字母转换成小写字母, 对于数字也会去除 * * @throws Exception */@Testpublic void TestSimpleAnalyzer() throws Exception &#123; // 1. 创建分词器,分析文档，对文档进行分词 Analyzer analyzer = new SimpleAnalyzer(); // 2. 创建Directory对象,声明索引库的位置 Directory directory = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); // 3. 创建IndexWriteConfig对象，写入索引需要的配置 IndexWriterConfig config = new IndexWriterConfig(analyzer); // 4.创建IndexWriter写入对象 IndexWriter indexWriter = new IndexWriter(directory, config); // 5.写入到索引库，通过IndexWriter添加文档对象document Document doc = new Document(); doc.add(new TextField(\"name\", \"vivo，X23。 8GB+128GB； 幻夜蓝\", Field.Store.YES)); indexWriter.addDocument(doc); // 6.释放资源 indexWriter.close();&#125; 结果: 7.3.4 CJKAnalyzer特点 : 这个支持中日韩文字，前三个字母也就是这三个国家的缩写。对中文是二分法分词, 去掉空格, 去掉标点符号。个人感觉对中文支持依旧很烂。 代码: 123456789101112131415161718192021222324252627/** * 中日韩分词器: 使用二分法分词, 去掉空格, 去掉标点符号, 所有大写字母转换成小写字母 * * @throws Exception */@Testpublic void TestCJKAnalyzer() throws Exception &#123; // 1. 创建分词器,分析文档，对文档进行分词 Analyzer analyzer = new CJKAnalyzer(); // 2. 创建Directory对象,声明索引库的位置 Directory directory = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); // 3. 创建IndexWriteConfig对象，写入索引需要的配置 IndexWriterConfig config = new IndexWriterConfig(analyzer); // 4.创建IndexWriter写入对象 IndexWriter indexWriter = new IndexWriter(directory, config); // 5.写入到索引库，通过IndexWriter添加文档对象document Document doc = new Document(); doc.add(new TextField(\"name\", \"vivo，X23。 8GB+128GB； 幻夜蓝\", Field.Store.YES)); indexWriter.addDocument(doc); // 6.释放资源 indexWriter.close();&#125; 结果: 7.3.5 SmartChineseAnalyzer特点 : 对中文支持也不是很好，扩展性差，扩展词库，禁用词库和同义词库等不好处理。 代码: 12345678910111213141516171819202122@Testpublic void TestSmartChineseAnalyzer() throws Exception &#123; // 1. 创建分词器,分析文档，对文档进行分词 Analyzer analyzer = new SmartChineseAnalyzer(); // 2. 创建Directory对象,声明索引库的位置 Directory directory = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); // 3. 创建IndexWriteConfig对象，写入索引需要的配置 IndexWriterConfig config = new IndexWriterConfig(analyzer); // 4.创建IndexWriter写入对象 IndexWriter indexWriter = new IndexWriter(directory, config); // 5.写入到索引库，通过IndexWriter添加文档对象document Document doc = new Document(); doc.add(new TextField(\"name\", \"vivo，X23。 8GB+128GB； 幻夜蓝\", Field.Store.YES)); indexWriter.addDocument(doc); // 6.释放资源 indexWriter.close();&#125; 结果: 7.4 第三方中文分词器7.4.1 什么是中文分词器学过英文的都知道，英文是以单词为单位的，单词与单词之间以空格或者逗号句号隔开。所以对于英文，我们可以简单以空格判断某个字符串是否为一个单词，比如I love China，love 和 China很容易被程序区分开来。 而中文则以字为单位，字又组成词，字和词再组成句子。中文“我爱中国”就不一样了，电脑不知道“中国”是一个词语还是“爱中”是一个词语。 把中文的句子切分成有意义的词，就是中文分词，也称切词。我爱中国，分词的结果是：我、爱、中国。 7.4.2 第三方中文分词器简介 paoding： 庖丁解牛最新版在 https://code.google.com/p/paoding/ 中最多支持Lucene 3.0，且最新提交的代码在 2008-06-03，在svn中最新也是 2010 年提交，已经过时，不予考虑。 mmseg4j：最新版已从 https://code.google.com/p/mmseg4j/ 移至 https://github.com/chenlb/mmseg4j-solr，支持Lucene 4.10，且在github中最新提交代码是 2014 年 6 月，从 09 年～ 14 年一共有： 18 个版本，也就是一年几乎有 3 个大小版本，有较大的活跃度，用了mmseg算法。 IK-analyzer： 最新版在https://code.google.com/p/ik-analyzer/上，支持Lucene 4.10从 2006 年12 月推出1.0版开始， IKAnalyzer已经推出了 4 个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。从3.0版本开 始，IK发展为面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。在 2012 版本中，IK实现了简单的分词 歧义排除算法，标志着IK分词器从单纯的词典分词向模拟语义分词衍化。 但是也就是2012 年 12 月后没有在更新。 ansj_seg：最新版本在 https://github.com/NLPchina/ansj_seg tags仅有1.1版本，从 2012 年到2014 年更新了大小 6 次，但是作者本人在 2014 年 10 月 10 日说明：“可能我以后没有精力来维护ansj_seg了”，现在由”nlp_china”管理。 2014 年 11 月有更新。并未说明是否支持Lucene，是一个由CRF（条件随机场）算法所做的分词算法。 imdict-chinese-analyzer：最新版在 https://code.google.com/p/imdict-chinese-analyzer/ ， 最新更新也在 2009 年 5 月，下载源码，不支持Lucene 4.10 。是利用HMM（隐马尔科夫链）算法。 Jcseg：最新版本在git.oschina.net/lionsoul/jcseg，支持Lucene 4.10，作者有较高的活跃度。利用mmseg算法。 7.4.3 使用中文分词器IKAnalyzerIKAnalyzer继承Lucene的Analyzer抽象类，使用IKAnalyzer和Lucene自带的分析器方法一样，将Analyzer测试代码改为IKAnalyzer测试中文分词效果。 如果使用中文分词器ik-analyzer，就需要在索引和搜索程序中使用一致的分词器：IK-analyzer。 1、添加依赖, pom.xml中加入依赖 123456&lt;!-- IK中文分词器 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.wltea.ik-analyzer&lt;/groupId&gt; &lt;artifactId&gt;ik-analyzer&lt;/artifactId&gt; &lt;version&gt;8.1.0&lt;/version&gt;&lt;/dependency&gt; 2、加入配置文件: 3、测试代码 12345678910111213141516171819202122232425262728/** * 使用第三方分词器(IK分词) * 特点: 支持中文语义分析, 提供停用词典, 提供扩展词典, 供程序员扩展使用 * * @throws Exception */@Testpublic void TestIKAnalyzer() throws Exception &#123; // 1. 创建分词器,分析文档，对文档进行分词 Analyzer analyzer = new IKAnalyzer(); // 2. 创建Directory对象,声明索引库的位置 Directory directory = FSDirectory.open(Paths.get(\"E:\\\\LuceneDir\")); // 3. 创建IndexWriteConfig对象，写入索引需要的配置 IndexWriterConfig config = new IndexWriterConfig(analyzer); // 4.创建IndexWriter写入对象 IndexWriter indexWriter = new IndexWriter(directory, config); // 5.写入到索引库，通过IndexWriter添加文档对象document Document doc = new Document(); doc.add(new TextField(\"name\", \"vivo X23 8GB+128GB 幻夜蓝,水滴屏全面屏,游戏手机.移动联通电信全网通4G手机\", Field.Store.YES)); indexWriter.addDocument(doc); // 6.释放资源 indexWriter.close();&#125; 4、测试结果 7.4.4 扩展中文词库如果想配置扩展词和停用词，就创建扩展词的文件和停用词的文件。 从ikanalyzer包中拷贝配置文件 拷贝到资源文件夹中 IKAnalyzer.cfg.xml配置文件 1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE properties SYSTEM \"http://java.sun.com/dtd/properties.dtd\"&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=\"ext_dict\"&gt;ext.dic;&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=\"ext_stopwords\"&gt;stopword.dic;&lt;/entry&gt;&lt;/properties&gt; 停用词典stopword.dic作用 : 停用词典中的词例如: a, an, the, 的, 地, 得等词汇, 凡是出现在停用词典中的字或者词, 在切分词的时候会被过滤掉. 扩展词典ext.dic作用 : 扩展词典中的词例如: 程序员, 贵州茅台等专有名词, 在汉语中一些公司名称, 行业名称, 分类, 品牌等不是汉语中的词汇, 是专有名词. 这些分词器默认不识别, 所以需要放入扩展词典中, 效果是被强制分成一个词.","tags":[{"name":"业务解决方案","slug":"业务解决方案","permalink":"https://wgy1993.gitee.io/tags/%E4%B8%9A%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"name":"Lucene","slug":"Lucene","permalink":"https://wgy1993.gitee.io/tags/Lucene/"}]},{"title":"SpringBoot","date":"2020-08-12T06:17:29.000Z","path":"archives/6f2612a2.html","text":"1. SpringBoot简介Spring Boot 是由 Pivotal 团队提供的全新框架，其设计目的是用来简化老的Spring 应用开发。该框架使用了自动方式来对开发工程进行配置，减少开发人员定义配置复杂度。 1.1 设计初衷 为Spring开发者提供一种，更快速、体验更好的Spring应用开发方式。 开箱即用，同时也可快速扩展，嵌入式的Tomcat。 绝对没有冗余代码，无需XML配置。 1.2 核心功能 核心能力：Spring容器、日志、自动配置AutoCongfiguration、Starters web应用的能力：MVC、嵌入式容器 数据访问(持久化)：关系型数据库、非关系型数据库 强大的整合其他技术的能力 测试：强悍的应用测试 1.2.1 SpringBoot在开发中的地位1、农业时代Java开发方式： 基于Java底层原生API，纯手动去实现，典型技术Html、JavaScript、CSS，JDBC，DBUtils，Socket….. 框架是拯救者，解放了农业时代的程序猿们，框架为我们做的更多 2、工业时代Java开发方式： 各种框架一顿搞：典型代表Spring，SpringMVC，Mybatis，Hibernate，Struts，Freemaker，JBPM… 微服务是拯救者，解放了工业时代的程序猿们，微服务让我们过上了小康生活 3、现代化Java开发方式： SpringBoot整合并简化一切Spring应用开发中的技术 各种SpringCloud微服务：服务注册与发现，负载均衡与熔断，网关和集群 想要学习SpringCloud的整套微服务架构系统，必先学习SpringBoot，它是SpringCloud的基础。SpringCloud项目都是SpringBoot开发出来的。 4、人工智能化的Java开发方式： 在未来。智能Ai可以替我们写代码，到时候我们都就做机器人的指挥者，不用干活。闲余时间天天玩游戏… 1.3 开发环境要求 Spring Boot 的2.1.7.RELEASES正式发行版 使用Java8或 Java 11 Spring版本是5.1.9及以上 构建工具版本：Maven ，版本要求是3.3及以上。 Servlet容器版本：Spring Boot应用程序最低支持到Servlet 3.1的容器 Name Servlet Version Tomcat 9.0 4.0 Jetty 9.4 3.1 Undertow 2.0 4.0 1.4 Spring怎么做Web开发？我们怎么开发一个web项目： web.xml配置：SpringMVC核心控制器(DispatchServlet)，Spring容器监听器，编码过滤器…. Spring 配置：包扫描(service、dao)，配置数据源，配置事务…. SpringMVC配置：包扫描(controller)，视图解析器，注解驱动，拦截器，静态资源…. 日志配置 少量业务代码 … 部署 Tomcat 调试，每次测试都需要部署 … 但是如果用 Spring Boot 呢？ 超简单！无需配置！！无感Tomcat！超迅速搭建功能强大的整套 Web！到底多简单？入门案例揭晓。 2. SpringBoot快速入门2.1 Maven搭建SpringBoot工程Maven搭建SpringBoot工程，实现web的请求响应。浏览器访问在页面中输出 helloworld 。 2.1.1 实现步骤 创建Maven工程 pom.xml文件中配置起步依赖 编写SpringBoot启动引导类 编写Controller 访问http://localhost:8080/hello 测试 2.1.2 实现过程2.1.2.1 创建Maven工程2.1.2.2 配置起步依赖在pom.xml文件中配置父坐标和web的起步依赖 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!--继承SpringBoot父POM文件--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.7.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;springboot_maven&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;!--web 开发的相关依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.1.2.3 编写SpringBoot引导类1234567891011121314/** * 编写SpringBoot引导类 * * @author wgy */@Configuration//配置类@EnableAutoConfiguration//开启自动配置@ComponentScan()//包扫描public class DemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125;&#125; 2.1.2.4 编写Controller12345678910111213/** * 编写Controller * * @author wgy */@RestControllerpublic class HelloController &#123; @RequestMapping(\"/hello\") public String hello() &#123; return \"hello world!!!\"; &#125;&#125; 2.1.2.5 访问测试 2.2 使用IDEA快速创建SpringBoot项目使用Spring Initializr 方式创建SpringBoot工程。然后实现入门案例的代码。 2.2.1 实现步骤 创建SpringBoot项目：使用Spring Initializr 配置项目元信息 勾选起步依赖 再次编写controller 访问接口测试：http://localhost:8080/hello 2.2.2 实现过程2.2.2.1 使用Spring Initializr创建工程 2.2.2.2 配置项目信息 2.2.2.3 勾选起步依赖 创建完成后工程目录结构 123456789101112/** * 启动引导类 * * @author wgy */@SpringBootApplication//组合注解，一个顶三个public class SpringBootFasterApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootFasterApplication.class, args); &#125;&#125; pom文件介绍 2.2.2.4 编写Controller同上案例 2.2.2.5 访问测试http://localhost:8080/hello 2.3 SpringBoot工程热部署(LiveReload)每次重启服务很麻烦，可以使用热部署方式 2.3.1 实现步骤2.3.1.1 添加热部署支持的依赖坐标12345&lt;!--spring-boot开发工具jar包，支持热部署--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;&lt;/dependency&gt; 2.3.1.2 开启自动编译默认情况IDEA不会自动编译 如何开启自动编译：需要设置 Build Project Automatically 如何开启运行时自动编译：快捷键Shift + Ctrl + Alt + / 打开Maintenance维护，选择Registry(注册表)开启运行时自动编译 注意：配置完成之后，IDEA进行热部署，偶尔也会失败。 3. SpringBoot原理分析3.1 依赖管理的原理继承了SpringBoot的父pom文件，继承了很多东西，其中最重要的要数&lt;dependency management&gt;。 继承 spring-boot-starter-parent 的&lt;dependency management&gt; spring-boot-denpendencies通过Maven的&lt;dependency management&gt;标签特性实现jar版本管理 通过spring-boot-denpendencies的pom管理所有公共Starter依赖的版本 Starter是随用随去，避免一下子继承父类所有的starter依赖 3.2 starters的原理starters是依赖关系的整理和封装。是一套依赖坐标的整合，可以让导入应用开发的依赖坐标更方便。 有了这些Starters，你获得Spring和其整合的所有技术的一站式服务。无需配置(自动配置)、无需复制粘贴依赖坐标，一个坐标即可完成所有入门级别操作。举例：JPA or Web开发，只需要导入 spring-boot-starter-data-jpa 或 spring-boot-starter-web 。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 每个Starter包含了当前功能下的许多必备依赖坐标，这些依赖坐标是项目开发，上线和运行必须的。同时这些依赖也支持依赖传递。举例： spring-boot-starter-web 包含了所有web开发必须的依赖坐标 starter的命名规范：官方的starter写法 spring-boot-starter-* ，非官方的starter写法thirdpartyproject-spring-boot-starter 常用的starters有哪些？ 非常多，一下只列举部分： 3.3 自动配置(AutoConfiguration)原理 每个Starter基本都会有自动配置AutoConfiguration AutoConfiguration的jar包定义了约定的默认配置信息。 SpringBoot采用约定大于配置设计思想。 自动配置的值在哪里？ 自动配置的值怎么才能生效？ 查看启动类注解@SpringBootApplication 追踪步骤： @EnableAutoConfiguration @Import({AutoConfigurationImportSelector.class}) spring.factories org.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration @EnableConfigurationProperties({ServerProperties.class}) private final ServerProperties.Tomcat tomcat = new ServerProperties.Tomcat(); 有了自动配置，那么基本全部采用默认配置。当然也可以更改默认配置。 4. SpringBoot的配置文件我们知道SpringBoot是约定大于配置的，所以很多配置都有默认值。如果想修改默认配置，可以使用application.properties或application.yml(application.yaml)自定义配置。SpringBoot默认从Resource目录加载自定义配置文件。application.properties是键值对类型。application.yml是SpringBoot中一种新的配置文件方式。 例如： application.properties文件 12server.port=8888server.servlet.context-path=demo application.yml文件 12345678910server: # 端口 port: 8888 # Path路径 servlet: context-path: /demo # 简写server.port: 8888server.servlet.context-path: /demo 4.1 application.yml配置文件YML文件格式是YAML(YAML Aint Markup Language)编写的文件格式。可以直观被电脑识别的格式。容易阅读，容易与脚本语言交互。可以支持各种编程语言(C/C++、Ruby、Python、Java、Perl、C#、PHP)。以数据为核心，比XML更简洁。扩展名为.yml或.yaml 4.1.1 配置普通数据语法key: value 12# yamlusername: haohao 注意：Value之前有一个空格 4.1.2 配置对象数据1234567person: name: haohao age: 31 addr: beijing # 行内配置person: &#123;name: haohao,age: 31,addr: beijing&#125; 注意：yml语法中，相同缩进代表同一个级别 4.1.3 配置集合、数组数据语法123456789101112131415161718192021222324# 数组citys: - beijing - tianjin - shanghai - chongqing # 或者行内注入citys: [beijing,tianjin,shanghai,chongqing]#集合中的元素是对象形式students: - name: zhangsan age: 18 score: 100 - name: lisi age: 28 score: 88 - name: wangwu age: 38 score: 90 # 或者使用行内注入student: [&#123;name: zhangsan,age: 18,score: 100&#125;,&#123;name: lisi,age: 28,score: 88&#125;,&#123;name: wangwu,age: 38,score: 90&#125;] 注意：value与-之间存在一个空格 4.2 SpringBoot配置信息的查询修改配置时，配置项目查询方式 第一种：自动配置jar包中的META-INF文件夹下，spring-configuration-metadata.json文件中 第二种：官方配置文件地址 官方查询地址：https://docs.spring.io/spring-boot/docs/2.0.1.RELEASE/reference/htmlsingle/#common-application-properties 4.3 配置文件属性注入Bean4.3.1 使用注解@Value映射@value注解将配置文件的值映射到Spring管理的Bean属性值 12345678910111213141516/** * 编写controller * * @author wgy */@RestControllerpublic class HelloController &#123; @Value(\"$&#123;persion.name&#125;\") private String name; @RequestMapping(\"/hello\") public String hello() &#123; return \"hello world!!!\"; &#125;&#125; 4.3.2 使用注解@ConfigurationProperties映射通过注解@ConfigurationProperties(prefix=’’配置文件中的key的前缀”)可以将配置文件中的配置自动与实体进行映射。 使用@ConfigurationProperties方式必须提供Setter方法，使用@Value注解不需要Setter方法。 application.yml 12345678910111213person: age: 18 username: zhangsan citys: - beijing - shanghai - guangzhou birthday: 2019/08/13 animals: - name: dog age: 3 - name: cat age: 2 123456789101112@ConfigurationProperties(prefix = \"person\")@Componentpublic class Person &#123; private String username; private Integer age; private Date birthday; private String[] citys; private List&lt;animal&gt; animals; get/setter方法,toString&#125; 5. SpringBoot集成一切5.1 集成 Spring Data JPASpring Data是一个用于简化数据访问，并支持云服务的开源框架。其主要目标是使得对数据的访问变得方便快捷。Spring Data JPA 是其中之一。 Spring Data JPA 是Spring 基于 ORM 框架、JPA 规范的基础上封装的一套JPA应用框架，可使开发者用简的代码即可实现对数据库的访问和操作。它提供了包括增删改查等在内的常用功能，且易于扩展！学习并使用 Spring Data JPA 可以极大提高开发效率！ 目标：使用SpringBoot整合SpringDataJPA，完成数据的增删改查基本功能。 5.1.1 实现步骤 创建SpringBoot工程 勾选依赖坐标 配置：数据库连接、jpa相关 创建User表、创建实体User配置实体 编写UserRepository 编写Controller、Service 访问测试 5.1.2 实现过程5.1.2.1 创建SpringBoot工程 5.1.2.2 勾选依赖坐标 5.1.2.3 配置：数据库连接、jpa相关123456789101112131415server.port=8090# DB 配置(可以不写，有嵌入式数据库，如果不写必须显示导入嵌入式数据库starter，h2database)spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.password=rootspring.datasource.username=rootspring.datasource.url=jdbc:mysql://127.0.0.1/springboot?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTC#jpa 相关配置(可以不写，有默认值)# 数据库类型spring.jpa.database=mysql# 是否显示sqlspring.jpa.show-sql=true# hibernate初始化数据库表策略spring.jpa.hibernate.ddl-auto=update# 是否生成数据库定义表语句spring.jpa.generate-ddl=true 5.1.2.4 创建表，创建实体配置实体12345678910111213141516-- ------------------------------ Table structure for &#96;user&#96;-- ----------------------------DROP TABLE IF EXISTS &#96;user&#96;;CREATE TABLE &#96;user&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;username&#96; varchar(50) DEFAULT NULL, &#96;password&#96; varchar(50) DEFAULT NULL, &#96;name&#96; varchar(50) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;3 DEFAULT CHARSET&#x3D;utf8;-- ------------------------------ Records of user-- ----------------------------INSERT INTO &#96;user&#96; VALUES (&#39;1&#39;, &#39;zhangsan&#39;, &#39;123&#39;, &#39;张三&#39;);INSERT INTO &#96;user&#96; VALUES (&#39;2&#39;, &#39;lisi&#39;, &#39;123&#39;, &#39;李四&#39;); 1234567891011121314@Entity//实体类注解@Table(name = \"user\")//关联数据库表public class User &#123; //注解设置当前id为注解 @Id //注解值生成策略 @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; private String username;//用户名 private String password;//密码 private String name;//姓名 //getter setter //toString&#125; 5.1.2.5 编写UserRepository泛型需要实体类，和实体类的ID 12345678/** * 用户持久层 * * @author wgy */@Repositorypublic interface UserDao extends JpaRepository&lt;User, Integer&gt; &#123;&#125; 5.1.2.6 编写Controller、Service123456789101112131415161718192021222324252627282930313233343536373839404142/** * 用户Controller * * @author wgy */@RestController@RequestMapping(\"/user\")public class UserController &#123; @Autowired UserService userService; //查询所有 @RequestMapping(\"/findAll\") public List&lt;User&gt; findAll() throws JsonProcessingException &#123; return userService.findAll(); &#125; //根据id查询 @RequestMapping(\"/findById\") public User findById(Integer id) &#123; return userService.findById(id); &#125; //新增 @RequestMapping(\"/save\") public void save(@RequestBody User user) &#123; userService.save(user); &#125; //修改 @RequestMapping(\"/update\") public void update(User user) &#123; userService.update(user); &#125; //删除 @RequestMapping(\"/delete\") public void delete(Integer id) &#123; userService.delete(id); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738/** * 用户实现类 * * @author wgy */@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired private UserDao userDao; @Override public List&lt;User&gt; findAll() &#123; return userDao.findAll(); &#125; @Override public User findById(Integer id) &#123; return userDao.findById(id).get(); &#125; @Override public void save(User user) &#123; userDao.save(user); &#125; @Override public void update(User user) &#123; userDao.save(user); &#125; @Override public void delete(Integer id) &#123; User user = new User(); user.setId(id); userDao.delete(user); &#125;&#125; 5.2 集成MyBatis使用SpringBoot整合MyBatis，完成查询所有功能 5.2.1 实现步骤 创建SpringBoot工程 勾选依赖坐标 数据库连接信息 创建User表、创建实体User 编写三层架构：Mapper、Service、controller，编写查询所有的方法 配置Mapper映射文件 在application.properties中添加MyBatis配置，扫描mapper.xml和mapper 访问测试地址http://localhost:8080/queryUsers 5.2.2 实现过程5.2.2.1 创建SpringBoot工程 5.2.2.2 勾选依赖坐标 5.2.2.3 数据库连接信息123456server.port=8090# DB 配置spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.password=rootspring.datasource.username=rootspring.datasource.url=jdbc:mysql://127.0.0.1/springboot?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTC 数据库连接地址后加 ?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTC ，不然会报错 5.2.2.4 创建User表、实体User12345678910111213141516-- ------------------------------ Table structure for &#96;user&#96;-- ----------------------------DROP TABLE IF EXISTS &#96;user&#96;;CREATE TABLE &#96;user&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;username&#96; varchar(50) DEFAULT NULL, &#96;password&#96; varchar(50) DEFAULT NULL, &#96;name&#96; varchar(50) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;3 DEFAULT CHARSET&#x3D;utf8;-- ------------------------------ Records of user-- ----------------------------INSERT INTO &#96;user&#96; VALUES (&#39;1&#39;, &#39;zhangsan&#39;, &#39;123&#39;, &#39;张三&#39;);INSERT INTO &#96;user&#96; VALUES (&#39;2&#39;, &#39;lisi&#39;, &#39;123&#39;, &#39;李四&#39;); 12345678public class User &#123; private Integer id; private String username;//用户名 private String password;//密码 private String name;//姓名 //getter setter... //toString&#125; 5.2.2.5 编写Mapper使用@Mapper标记该类是一个Mapper接口，可以被SpringBoot自动扫描 1234567891011121314@Repository@Mapper//表明当前接口是一个Mapper，被Mybatis框架扫描public interface UserMapper &#123; List&lt;User&gt; findAll(); User findById(Integer id); void save(User user); void update(User user); void delete(Integer id);&#125; 5.2.2.6 配置Mapper映射文件在src/main/resources/mapper路径下加入UserMapper.xml配置文件 12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"utf-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.wgy.mapper.UserMapper\"&gt; &lt;select id=\"findAll\" resultType=\"user\"&gt; select * from user &lt;/select&gt; &lt;select id=\"findById\" parameterType=\"Integer\" resultType=\"user\"&gt; select * from user where id = #&#123;id&#125; &lt;/select&gt; &lt;insert id=\"save\" parameterType=\"user\"&gt; INSERT into user (username, password, name) VALUES (#&#123;username&#125;, #&#123;password&#125;, #&#123;name&#125;) &lt;/insert&gt; &lt;update id=\"update\" parameterType=\"user\"&gt; update user set username=#&#123;username&#125;, password=#&#123;password&#125;, name=#&#123;name&#125; where id = #&#123;id&#125; &lt;/update&gt; &lt;delete id=\"delete\" parameterType=\"Integer\"&gt; delete from user where id = #&#123;id&#125; &lt;/delete&gt;&lt;/mapper&gt; 5.2.2.7 添加MyBatis信息1234# 扫描实体mybatis.type-aliases-package=com.wgy.domain# mapper.xml配置文件路径mybatis.mapper-locations=classpath:mapper/*Mapper.xml 5.2.2.8 编写Controller12345678910111213141516171819202122232425262728293031@RestController@RequestMapping(\"/user\")public class UserController &#123; @Autowired UserService userService; //查询所有 @RequestMapping(\"/findAll\") public List&lt;User&gt; findAll() throws JsonProcessingException &#123; return userService.findAll(); &#125; //根据id查询 @RequestMapping(\"/findById\") public User findById(Integer id) &#123; return userService.findById(id); &#125; //新增 @RequestMapping(\"/save\") public void save(User user) &#123; userService.save(user); &#125; //修改 @RequestMapping(\"/update\") public void update(User user) &#123; userService.update(user); &#125; //删除 @RequestMapping(\"/delete\") public void delete(Integer id) &#123; userService.delete(id); &#125;&#125; 5.3 集成Spring Data RedisSpringBoot整合了Redis之后，做用户数据查询缓存 5.3.1 实现步骤 添加Redis的Starter 在application.properties中配置redis端口、地址 注入RedisTemplate操作Redis缓存查询所有用户数据 测试缓存 5.3.2 实现过程5.3.2.1 添加Redis起步依赖12345&lt;!--spring data redis 依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 5.3.2.2 配置Redis连接信息123# Redis 配置(不填也是可以的)spring.redis.host=192.168.142.128spring.redis.port=6379 5.3.2.3 注入RedisTemplate测试Redis操作123456789101112131415161718192021222324252627@RestController@RequestMapping(\"/user\")public class UserController &#123; @Autowired RedisTemplate redisTemplate; @Autowired UserService userService; //查询所有 @RequestMapping(\"/findAll\") public String findAll() throws JsonProcessingException &#123; Object findAllusers = redisTemplate.boundValueOps(\"findAllusers\").get(); //从缓存取值 //如果没有，从数据库中查询,放到缓存然后返回数据 if (StringUtils.isEmpty(findAllusers)) &#123; List&lt;User&gt; all = userService.findAll(); ObjectMapper objectMapper = new ObjectMapper(); findAllusers = objectMapper.writeValueAsString(all);//把对象数据格式转化为json字符串 redisTemplate.boundValueOps(\"findAllusers\").set(findAllusers);//数据存储到缓存中 return findAllusers + \"从数据库查询\"; &#125; //如果有直接返回 return findAllusers + \"从redis缓存中查询\"; &#125;&#125; 5.4 集成定时器使用SpringBoot开发定时器，每隔5秒输出一个当前时间。 实现步骤： 5.4.1 开启定时器注解123456789101112/** * @author wgy */@SpringBootApplication@EnableScheduling//开启定时器public class SpringbootMybatisApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootMybatisApplication.class, args); &#125;&#125; 5.4.2 配置定时器方法123456789101112131415161718/** * @author wgy */@Componentpublic class Timer &#123;// @Scheduled(cron = \"0/5 * * * * ?\") /** * initialDelay = 项目启动后，多久执行,fixedRate = 固定的频率执行 * initialDelay = 项目启动后, 多久执行，fixedDelay = 上一个任务执行完成，多久之后执行下一个任务 */ @Scheduled(initialDelay = 1000, fixedRate = 5000)// @Scheduled(initialDelay = 1000,fixedDelay = 5000) public void mytask() &#123; System.out.println(\"当前系统时间：\" + new Date()); &#125;&#125; 5.5 扩展了解 集成 MongoDB 集成 ElasticSearch 集成 Memcached 集成邮件服务：普通邮件、模板邮件、验证码、带Html的邮件 集成RabbitMQ消息中间件 集成Freemarker或者Thymeleaf ……………. 6. SpringBoot如何代码测试SpringBoot集成JUnit测试功能，进行查询用户接口测试。 实现步骤： 6.1 添加Junit起步依赖(默认就有)123456&lt;!--spring boot测试依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 6.2 编写测试类SpringRunner继承SpringJUnit4ClassRunner，使用哪一个Spring提供的测试引擎都可以。指定运行测试的引擎 @SpringBootTest的属性值指的是引导类的字节码对象 12345678910111213@RunWith(SpringRunner.class)@SpringBootTestclass SpringbootMybatisApplicationTests &#123; @Autowired UserService userService; @Test public void testFindAll() &#123; List&lt;User&gt; users = userService.findAll(); System.out.println(users); &#125;&#125; 7. Spring Boot 如何打包部署启动方式有两种，一种是打成jar直接执行，另一种是打包成war包放到Tomcat服务下，启动Tomcat。 7.1 打成Jar包部署执行maven打包命令或者使用IDEA的Maven工具打包 1234## 移动至项目根目录，与pom.xml同级mvn clean package## 或者执行下面的命令 排除测试代码后进行打包mvn clean package -Dmaven.test.skip=true 需要注意项目pom.xml文件中的打包类型 1&lt;packaging&gt;jar&lt;/packaging&gt; 启动命令：启动之前先检查自己的pom.xml文件中是否有springboot的maven插件 1java -jar target/springboot_demo.jar 启动命令的时候配置jvm参数也是可以的。然后查看一下Java的参数配置结果 1java -Xmx80m -Xms20m --server.port=8090 -jar target/springboot_demo.jar 7.2 打成war包部署执行maven打包命令或者使用IDEA的Maven工具打包，需要修改pom.xml文件中的打包类型。 1&lt;packaging&gt;war&lt;&#x2F;packaging&gt; 注册启动类： 创建 ServletInitializer.java，继承 SpringBootServletInitializer ，覆盖 configure()，把启动类Application 注册进去。外部 Web 应用服务器构建 Web Application Context 的时候，会把启动类添加进去。 1234567//web.xmlpublic class ServletInitializer extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(DemoApplication.class); &#125;&#125; 然后执行打包操作。同6.1 小节打包是一样的 拷贝到Tomcat的webapp下，启动Tomcat访问即可 因为访问地址不再是根目录了，所有路径中需要加入项目名称：http://localhost:8080/springboot_demo/hello","tags":[{"name":"分布式架构方案","slug":"分布式架构方案","permalink":"https://wgy1993.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://wgy1993.gitee.io/tags/SpringBoot/"}]},{"title":"zookeeper(二)","date":"2020-08-05T08:55:24.000Z","path":"archives/4ebc2ef7.html","text":"1. zookeeper源码解析1.1 下载zookeeper源码，导入IDEA中下载地址：https://github.com/apache/zookeeper 1.2 启动根据bin目录下的启动脚本zkServer.sh中加载启动类：QuorumPeerMain类 QuorumPeerMain 中main方法执行initializeAndRun方法 跟进 initializeAndRun方法 123456789101112131415161718192021222324protected void initializeAndRun(String[] args) throws ConfigException, IOException, AdminServerException &#123; //1.加载配置文件 QuorumPeerConfig config = new QuorumPeerConfig(); if (args.length == 1) &#123; config.parse(args[0]); &#125; // Start and schedule the the purge task //2.启动清除任务 主要清除旧的快照和日志文件 DatadirCleanupManager purgeMgr = new DatadirCleanupManager(config.getDataDir(), config.getDataLogDir(), config.getSnapRetainCount(), config.getPurgeInterval()); purgeMgr.start(); // 3. 启动zk zookeeper启动方式分为两种：单机启动和集群启动 if (args.length == 1 &amp;&amp; config.isDistributed()) &#123; //启动集群 runFromConfig(config); &#125; else &#123; LOG.warn(\"Either no config or no quorum defined in config, running \" + \" in standalone mode\"); // there is only server in the quorum -- run as standalone //启动单机 ZooKeeperServerMain.main(args); &#125;&#125; 在 initializeAndRun方法中主要做了三件事 加载解析配置文件 123456789101112131415File configFile = (new VerifyingFileFactory.Builder(LOG).warnForRelativePath().failForNonExistingPath().build()).create(path);Properties cfg = new Properties();FileInputStream in = new FileInputStream(configFile);try &#123; cfg.load(in); configFileStr = path;&#125; finally &#123; in.close();&#125;/* Read entire config file as initial configuration */initialConfig = new String(Files.readAllBytes(configFile.toPath()));parseProperties(cfg); 将配置文件加载到 Properties cfg对象中，解析cfg对象。zookeeper所有配置信息封装到一个QuorumPeerConfig对象中 启动定时清除任务 PurgeTask继承TimeTask，定时执行run方法中的purge方法 12345678910111213141516171819202122static class PurgeTask extends TimerTask &#123; private File logsDir; private File snapsDir; private int snapRetainCount; public PurgeTask(File dataDir, File snapDir, int count) &#123; logsDir = dataDir; snapsDir = snapDir; snapRetainCount = count; &#125; @Override public void run() &#123; LOG.info(\"Purge task started.\"); try &#123; PurgeTxnLog.purge(logsDir, snapsDir, snapRetainCount); &#125; catch (Exception e) &#123; LOG.error(\"Error occurred while purging.\", e); &#125; LOG.info(\"Purge task completed.\"); &#125;&#125; purge 方法主要清除旧的快照和日志文件 启动 zk zookeeper启动方式分为两种：单机启动和集群启动 12345678910if (args.length == 1 &amp;&amp; config.isDistributed()) &#123; //启动集群 runFromConfig(config);&#125; else &#123; LOG.warn(\"Either no config or no quorum defined in config, running \" + \" in standalone mode\"); // there is only server in the quorum -- run as standalone //启动单机 ZooKeeperServerMain.main(args);&#125; 首先我们看看单机启动的源码 main方法调用initializeAndRun方法，initializeAndRun首先加载配置文件，然后执行runFromConfig(config)方法，我们看看runFromConfig具体执行了什么操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public void runFromConfig(ServerConfig config) throws IOException, AdminServerException &#123; LOG.info(\"Starting server\"); FileTxnSnapLog txnLog = null; try &#123; try &#123; //1.首先开启一下metrics监控 metricsProvider = MetricsProviderBootstrap.startMetricsProvider(config.getMetricsProviderClassName(), config.getMetricsProviderConfiguration()); &#125; catch (MetricsProviderLifeCycleException error) &#123; throw new IOException(\"Cannot boot MetricsProvider \" + config.getMetricsProviderClassName(), error); &#125; ServerMetrics.metricsProviderInitialized(metricsProvider); // Note that this thread isn't going to be doing anything else, // so rather than spawning another thread, we will just call // run() in this thread. // create a file logger url from the command line args //2. 创建了FileTxnLog实例和FIleSnap实例，并保存刚启动时候日志数据 txnLog = new FileTxnSnapLog(config.dataLogDir, config.dataDir); JvmPauseMonitor jvmPauseMonitor = null; if (config.jvmPauseMonitorToRun) &#123; jvmPauseMonitor = new JvmPauseMonitor(config); &#125; final ZooKeeperServer zkServer = new ZooKeeperServer(jvmPauseMonitor, txnLog, config.tickTime, config.minSessionTimeout, config.maxSessionTimeout, config.listenBacklog, null, config.initialConfig); txnLog.setServerStats(zkServer.serverStats()); // Registers shutdown handler which will be used to know the // server error or shutdown state changes. final CountDownLatch shutdownLatch = new CountDownLatch(1); zkServer.registerServerShutdownHandler(new ZooKeeperServerShutdownHandler(shutdownLatch)); // Start Admin server //3. 启动adminServer adminServer = AdminServerFactory.createAdminServer(); adminServer.setZooKeeperServer(zkServer); adminServer.start(); //4. 启动NIOServerCnxnFactory boolean needStartZKServer = true; //4.1从解析出的配置中配置NIOServerCnxnFactory if (config.getClientPortAddress() != null) &#123; cnxnFactory = ServerCnxnFactory.createFactory(); cnxnFactory.configure(config.getClientPortAddress(), config.getMaxClientCnxns(), config.getClientPortListenBacklog(), false); //4.2启动ZookeeperServer， cnxnFactory.startup(zkServer); // zkServer has been started. So we don't need to start it again in secureCnxnFactory. needStartZKServer = false; &#125; if (config.getSecureClientPortAddress() != null) &#123; secureCnxnFactory = ServerCnxnFactory.createFactory(); secureCnxnFactory.configure(config.getSecureClientPortAddress(), config.getMaxClientCnxns(), config.getClientPortListenBacklog(), true); secureCnxnFactory.startup(zkServer, needStartZKServer); &#125; containerManager = new ContainerManager(zkServer.getZKDatabase(), zkServer.firstProcessor, Integer.getInteger(\"znode.container.checkIntervalMs\", (int) TimeUnit.MINUTES.toMillis(1)), Integer.getInteger(\"znode.container.maxPerMinute\", 10000) ); containerManager.start(); // Watch status of ZooKeeper server. It will do a graceful shutdown // if the server is not running or hits an internal error. shutdownLatch.await(); shutdown(); if (cnxnFactory != null) &#123; cnxnFactory.join(); &#125; if (secureCnxnFactory != null) &#123; secureCnxnFactory.join(); &#125; if (zkServer.canShutdown()) &#123; zkServer.shutdown(true); &#125; &#125; catch (InterruptedException e) &#123; // warn, but generally this is ok LOG.warn(\"Server interrupted\", e); &#125; finally &#123; if (txnLog != null) &#123; txnLog.close(); &#125; if (metricsProvider != null) &#123; try &#123; metricsProvider.stop(); &#125; catch (Throwable error) &#123; LOG.warn(\"Error while stopping metrics\", error); &#125; &#125; &#125;&#125; 启动过程首先开启一下 metrics监控,然后启动admin server,然后启动zk server，我们来看看启动过程 ServerCnxnFactory中startup方法调用NettyServerCnxnFactory实现类启动方法 12345678public void startup(ZooKeeperServer zks, boolean startServer) throws IOException, InterruptedException &#123; start(); setZooKeeperServer(zks); if (startServer) &#123; zks.startdata(); zks.startup(); &#125;&#125; 启动方法执行操作 12345678910111213141516171819public synchronized void startup() &#123; if (sessionTracker == null) &#123; createSessionTracker(); &#125; startSessionTracker(); setupRequestProcessors(); startRequestThrottler(); registerJMX(); startJvmPauseMonitor(); registerMetrics(); setState(State.RUNNING); requestPathMetricsCollector.start(); notifyAll();&#125; 接下来我们看看集群启动过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public void runFromConfig(QuorumPeerConfig config) throws IOException, AdminServerException &#123; try &#123; ManagedUtil.registerLog4jMBeans(); &#125; catch (JMException e) &#123; LOG.warn(\"Unable to register log4j JMX control\", e); &#125; LOG.info(\"Starting quorum peer\"); MetricsProvider metricsProvider; try &#123; metricsProvider = MetricsProviderBootstrap.startMetricsProvider(config.getMetricsProviderClassName(), config.getMetricsProviderConfiguration()); &#125; catch (MetricsProviderLifeCycleException error) &#123; throw new IOException(\"Cannot boot MetricsProvider \" + config.getMetricsProviderClassName(), error); &#125; try &#123; ServerMetrics.metricsProviderInitialized(metricsProvider); ServerCnxnFactory cnxnFactory = null; ServerCnxnFactory secureCnxnFactory = null; if (config.getClientPortAddress() != null) &#123; cnxnFactory = ServerCnxnFactory.createFactory(); cnxnFactory.configure(config.getClientPortAddress(), config.getMaxClientCnxns(), config.getClientPortListenBacklog(), false); &#125; if (config.getSecureClientPortAddress() != null) &#123; secureCnxnFactory = ServerCnxnFactory.createFactory(); secureCnxnFactory.configure(config.getSecureClientPortAddress(), config.getMaxClientCnxns(), config.getClientPortListenBacklog(), true); &#125; quorumPeer = getQuorumPeer(); quorumPeer.setTxnFactory(new FileTxnSnapLog(config.getDataLogDir(), config.getDataDir())); quorumPeer.enableLocalSessions(config.areLocalSessionsEnabled()); quorumPeer.enableLocalSessionsUpgrading(config.isLocalSessionsUpgradingEnabled()); //quorumPeer.setQuorumPeers(config.getAllMembers()); quorumPeer.setElectionType(config.getElectionAlg()); quorumPeer.setMyid(config.getServerId()); quorumPeer.setTickTime(config.getTickTime()); quorumPeer.setMinSessionTimeout(config.getMinSessionTimeout()); quorumPeer.setMaxSessionTimeout(config.getMaxSessionTimeout()); quorumPeer.setInitLimit(config.getInitLimit()); quorumPeer.setSyncLimit(config.getSyncLimit()); quorumPeer.setConnectToLearnerMasterLimit(config.getConnectToLearnerMasterLimit()); quorumPeer.setObserverMasterPort(config.getObserverMasterPort()); quorumPeer.setConfigFileName(config.getConfigFilename()); quorumPeer.setClientPortListenBacklog(config.getClientPortListenBacklog()); quorumPeer.setZKDatabase(new ZKDatabase(quorumPeer.getTxnFactory())); quorumPeer.setQuorumVerifier(config.getQuorumVerifier(), false); if (config.getLastSeenQuorumVerifier() != null) &#123; quorumPeer.setLastSeenQuorumVerifier(config.getLastSeenQuorumVerifier(), false); &#125; quorumPeer.initConfigInZKDatabase(); quorumPeer.setCnxnFactory(cnxnFactory); quorumPeer.setSecureCnxnFactory(secureCnxnFactory); quorumPeer.setSslQuorum(config.isSslQuorum()); quorumPeer.setUsePortUnification(config.shouldUsePortUnification()); quorumPeer.setLearnerType(config.getPeerType()); quorumPeer.setSyncEnabled(config.getSyncEnabled()); quorumPeer.setQuorumListenOnAllIPs(config.getQuorumListenOnAllIPs()); if (config.sslQuorumReloadCertFiles) &#123; quorumPeer.getX509Util().enableCertFileReloading(); &#125; // sets quorum sasl authentication configurations quorumPeer.setQuorumSaslEnabled(config.quorumEnableSasl); if (quorumPeer.isQuorumSaslAuthEnabled()) &#123; quorumPeer.setQuorumServerSaslRequired(config.quorumServerRequireSasl); quorumPeer.setQuorumLearnerSaslRequired(config.quorumLearnerRequireSasl); quorumPeer.setQuorumServicePrincipal(config.quorumServicePrincipal); quorumPeer.setQuorumServerLoginContext(config.quorumServerLoginContext); quorumPeer.setQuorumLearnerLoginContext(config.quorumLearnerLoginContext); &#125; quorumPeer.setQuorumCnxnThreadsSize(config.quorumCnxnThreadsSize); quorumPeer.initialize(); if (config.jvmPauseMonitorToRun) &#123; quorumPeer.setJvmPauseMonitor(new JvmPauseMonitor(config)); &#125; quorumPeer.start(); quorumPeer.join(); &#125; catch (InterruptedException e) &#123; // warn, but generally this is ok LOG.warn(\"Quorum Peer interrupted\", e); &#125; finally &#123; if (metricsProvider != null) &#123; try &#123; metricsProvider.stop(); &#125; catch (Throwable error) &#123; LOG.warn(\"Error while stopping metrics\", error); &#125; &#125; &#125;&#125; 在 runFromConfig执行过程中主要是QuorumPeer对象属性的赋值并执行start方法，通过查看QuorumPeer类的源码，发现QuorumPeer继承了ZooKeeperThread，而ZooKeeperThread继承了Thread,通过start方法启动了QuorumPeer线程，线程运行执行线程的run方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117/* * Main loop 主循环：集群启动的核心代码 */while (running) &#123; switch (getPeerState()) &#123; case LOOKING: LOG.info(\"LOOKING\"); ServerMetrics.getMetrics().LOOKING_COUNT.add(1); if (Boolean.getBoolean(\"readonlymode.enabled\")) &#123; LOG.info(\"Attempting to start ReadOnlyZooKeeperServer\"); // Create read-only server but don't start it immediately final ReadOnlyZooKeeperServer roZk = new ReadOnlyZooKeeperServer(logFactory, this, this.zkDb); // Instead of starting roZk immediately, wait some grace // period before we decide we're partitioned. // // Thread is used here because otherwise it would require // changes in each of election strategy classes which is // unnecessary code coupling. Thread roZkMgr = new Thread() &#123; public void run() &#123; try &#123; // lower-bound grace period to 2 secs sleep(Math.max(2000, tickTime)); if (ServerState.LOOKING.equals(getPeerState())) &#123; roZk.startup(); &#125; &#125; catch (InterruptedException e) &#123; LOG.info(\"Interrupted while attempting to start ReadOnlyZooKeeperServer, not started\"); &#125; catch (Exception e) &#123; LOG.error(\"FAILED to start ReadOnlyZooKeeperServer\", e); &#125; &#125; &#125;; try &#123; roZkMgr.start(); reconfigFlagClear(); if (shuttingDownLE) &#123; shuttingDownLE = false; startLeaderElection(); &#125; setCurrentVote(makeLEStrategy().lookForLeader()); &#125; catch (Exception e) &#123; LOG.warn(\"Unexpected exception\", e); setPeerState(ServerState.LOOKING); &#125; finally &#123; // If the thread is in the the grace period, interrupt // to come out of waiting. roZkMgr.interrupt(); roZk.shutdown(); &#125; &#125; else &#123; try &#123; reconfigFlagClear(); if (shuttingDownLE) &#123; shuttingDownLE = false; startLeaderElection(); &#125; setCurrentVote(makeLEStrategy().lookForLeader()); &#125; catch (Exception e) &#123; LOG.warn(\"Unexpected exception\", e); setPeerState(ServerState.LOOKING); &#125; &#125; break; case OBSERVING: try &#123; LOG.info(\"OBSERVING\"); setObserver(makeObserver(logFactory)); observer.observeLeader(); &#125; catch (Exception e) &#123; LOG.warn(\"Unexpected exception\", e); &#125; finally &#123; observer.shutdown(); setObserver(null); updateServerState(); // Add delay jitter before we switch to LOOKING // state to reduce the load of ObserverMaster if (isRunning()) &#123; Observer.waitForObserverElectionDelay(); &#125; &#125; break; case FOLLOWING: try &#123; LOG.info(\"FOLLOWING\"); setFollower(makeFollower(logFactory)); follower.followLeader(); &#125; catch (Exception e) &#123; LOG.warn(\"Unexpected exception\", e); &#125; finally &#123; follower.shutdown(); setFollower(null); updateServerState(); &#125; break; case LEADING: LOG.info(\"LEADING\"); try &#123; setLeader(makeLeader(logFactory)); leader.lead(); setLeader(null); &#125; catch (Exception e) &#123; LOG.warn(\"Unexpected exception\", e); &#125; finally &#123; if (leader != null) &#123; leader.shutdown(\"Forcing shutdown\"); setLeader(null); &#125; updateServerState(); &#125; break; &#125;&#125; 核心逻辑在 while循环中，判断节点的状态，分为 LOOKING 、 OBSERVING 、 FOLLOWING 、LEADING ，当某个QuorumPeerq刚启动时，状态为 LOOKING ，启动线程将zk节点启动，然后进行leader选举，这是zookeeper的选举算法的核心，leader的选举在org.apache.zookeeper.server.quorum.FastLeaderElection的lookForLeader方法中 1.3 leader选举123456789101112131415161718192021// 记录当前server接受其他server的本轮投票信息Map&lt;Long, Vote&gt; recvset = new HashMap&lt;Long, Vote&gt;();// 选举结束后法定server的投票信息Map&lt;Long, Vote&gt; outofelection = new HashMap&lt;Long, Vote&gt;();// 选举超时时间int notTimeout = minNotificationInterval;synchronized (this) &#123; // 逻辑时钟 +1 logicalclock.incrementAndGet(); //初始化选票 给自己投票 updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());&#125;LOG.info(\"New election. My id = \" + self.getId() + \", proposed zxid=0x\" + Long.toHexString(proposedZxid));sendNotifications(); 此处两个变量，一个 recvset，用来保存当前server的接受其他server的本轮投票信息，key为当前server的id，也即是我们在配置文件中配置的myid，而另外一个变量outofelection保存选举结束以后法定的server的投票信息，这里的法定指的是FOLLOWING和LEADING状态的server，不包活OBSERVING状态的server。 更新逻辑时钟，此处逻辑时钟是为了在选举leader时比较其他选票中的server中的epoch和本地谁最新，然后将自己的选票proposal发送给其他所有server。 12345678910111213141516171819private void sendNotifications() &#123; for (long sid : self.getCurrentAndNextConfigVoters()) &#123; QuorumVerifier qv = self.getQuorumVerifier(); ToSend notmsg = new ToSend(ToSend.mType.notification, proposedLeader, proposedZxid, logicalclock.get(), QuorumPeer.ServerState.LOOKING, sid, proposedEpoch, qv.toString().getBytes()); if (LOG.isDebugEnabled()) &#123; LOG.debug(\"Sending Notification: \" + proposedLeader + \" (n.leader), 0x\" + Long.toHexString(proposedZxid) + \" (n.zxid), 0x\" + Long.toHexString(logicalclock.get()) + \" (n.round), \" + sid + \" (recipient), \" + self.getId() + \" (myid), 0x\" + Long.toHexString(proposedEpoch) + \" (n.peerEpoch)\"); &#125; sendqueue.offer(notmsg); &#125;&#125; 此方法遍历所有投票参与者集合，将选票信息构造成一个 ToSend对象，分别发送消息放置到队列sendqueue中。同理集群中每一个server节点都会将自己的选票发送给其他server，那么既然有发送选票，肯定存在接受选票信息，并选出leader，接下来我们就来看看每一个server如何接受选票并处理的。 首先我们应该从队列出取出选票信息 12345678910111213141516171819202122232425//从队列中取出选票信息Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS);/* * Sends more notifications if haven't received enough. * Otherwise processes new notification. *///判断选票信息是否为空 如果为空if (n == null) &#123; // 判断是否投递过选票信息 if (manager.haveDelivered()) &#123; // 重新发送选票信息 sendNotifications(); &#125; else &#123; //重连所有server manager.connectAll(); &#125; /* * Exponential backoff */ int tmpTimeOut = notTimeout * 2; notTimeout = (tmpTimeOut &lt; maxNotificationInterval ? tmpTimeOut : maxNotificationInterval); LOG.info(\"Notification time out: \" + notTimeout);&#125; 选出的选票信息封装在一个 Notification 对象中，如果取出的选票为null，我们通过QuorumCnxManager检查发送队列中是否投递过选票，如果投递过说明连接并没有断开，则重新发送选票到其他sever，否则，说明连接断开，重连所有server即可。那么连接没有断开，为什么会收不到选票信息呢，有可能是选票超时时限导致没有收到选票，所有将选票时限延长了一倍。 123456789101112131415161718//校验选票中选举server和选举的leader sever是否合法 else if(validVoter(n.sid) &amp;&amp; validVoter(n.leader)) &#123; /* * Only proceed if the vote comes from a replica in the current or next * voting view for a replica in the current or next voting view. */ switch (n.state) &#123; case LOOKING: ...... break; case OBSERVING: LOG.debug(\"Notification from observer: \" + n.sid); break; case FOLLOWING: case LEADING: ...... &#125;&#125; 如果选出的选票 Notification不为null，校验投票server和选举leader是否合法，然后根据选票状态执行不同分支，选举过程走LOOKING分支，接下来比较选票epoch和当前逻辑时钟，如果选票 epoch&gt;逻辑时钟，说明选票是最新的，自己的选票这一轮已经过时，应该更新当前自己server的逻辑时钟，并清空当前收到的其他server的选票，然后比较自己和选票中谁更适合做leader，发送新的投票给其他所有server。 1234567891011121314151617181920if (getInitLastLoggedZxid() == -1) &#123; LOG.debug(\"Ignoring notification as our zxid is -1\"); break;&#125;if (n.zxid == -1) &#123; LOG.debug(\"Ignoring notification from member with -1 zxid &#123;&#125;\", n.sid); break;&#125;// If notification &gt; current, replace and send messages outif (n.electionEpoch &gt; logicalclock.get()) &#123; logicalclock.set(n.electionEpoch); recvset.clear(); //比较选票和自己谁更适合做leader，比较规则epoch&gt;zxid&gt;sid if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) &#123; updateProposal(n.leader, n.zxid, n.peerEpoch); &#125; else &#123; updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch()); &#125; sendNotifications();&#125; 如果选票 epoch&lt;逻辑时钟,zk放弃此次选票，不做任何处理。 12345678else if (n.electionEpoch &lt; logicalclock.get()) &#123; if (LOG.isDebugEnabled()) &#123; LOG.debug(\"Notification election epoch is smaller than logicalclock. n.electionEpoch = 0x\" + Long.toHexString(n.electionEpoch) + \", logicalclock=0x\" + Long.toHexString(logicalclock.get())); &#125; break;&#125; 如果选票 epoch=逻辑时钟,仍然是比较选票和当前自己server谁更适合当leader，并重新更新选票，发送给其他所有的server 1234else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123; updateProposal(n.leader, n.zxid, n.peerEpoch); sendNotifications();&#125; 接下来将收到的选票放入 recvset的map中保存。 1recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); 接下来是判断本轮选举是否结束，如果超过半数的，则 leader预选举结束，注意此时还要比较其他少半选票中有没有谁更适合做leader？如果在选票找不到任何一个server比当前server更适合做leader，则更新更新server状态，清空recvqueue队列，确定最终选票并返回，否则将更适合做leader的Notification放回队列开始新一轮的选举。 12345678910111213141516171819202122232425262728voteSet = getVoteTracker(recvset, new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch));if (voteSet.hasAllQuorums()) &#123; // Verify if there is any change in the proposed leader //比较剩下少数的server是否更适合做leader while ((n = recvqueue.poll(finalizeWait, TimeUnit.MILLISECONDS)) != null) &#123; if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch, proposedLeader, proposedZxid, proposedEpoch)) &#123; recvqueue.put(n); break; &#125; &#125; /* * This predicate is true once we don't read any new * relevant message from the reception queue */ //如果全部比较都没有当前谁比当前server更适合做leader，则更新server状态 if (n == null) &#123; //更新状态 setPeerState(proposedLeader, voteSet); //构建最终选票，便于其他server同步 Vote endVote = new Vote(proposedLeader, proposedZxid, logicalclock.get(), proposedEpoch); leaveInstance(endVote); //清空队列 return endVote; &#125;&#125; 更新状态后，若选票中的服务器状态为 FOLLOWING或者LEADING时，其大致步骤会再次判断选举epoch是否等于逻辑时钟.如果相等，再次盘检查选中的leader过半 123456789101112131415161718192021222324252627if (n.electionEpoch == logicalclock.get()) &#123; recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch)); voteSet = getVoteTracker(recvset, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state)); if (voteSet.hasAllQuorums() &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; setPeerState(n.leader, voteSet); Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); leaveInstance(endVote); return endVote; &#125;&#125;/* * Before joining an established ensemble, verify that * a majority are following the same leader. */outofelection.put(n.sid, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state));voteSet = getVoteTracker(outofelection, new Vote(n.version, n.leader, n.zxid, n.electionEpoch, n.peerEpoch, n.state));if (voteSet.hasAllQuorums() &amp;&amp; checkLeader(outofelection, n.leader, n.electionEpoch)) &#123; synchronized (this) &#123; logicalclock.set(n.electionEpoch); setPeerState(n.leader, voteSet); &#125; Vote endVote = new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch); leaveInstance(endVote); return endVote;&#125; 2. zookeeper应用场景2.1 配置中心在平常的业务开发过程中，我们通常需要将系统的一些通用的全局配置，例如机器列表配置，运行时开关配置，数据库配置信息等统一集中存储，让集群所有机器共享配置信息，系统在启动会首先从配置中心读取配置信息，进行初始化。传统的实现方式将配置存储在本地文件和内存中，一旦机器规模更大，配置变更频繁情况下，本地文件和内存方式的配置维护成本较高，使用zookeeper作为分布式的配置中心就可以解决这个问题。 我们将配置信息存在zk中的一个节点中，同时给该节点注册一个数据节点变更的watcher监听，一旦节点数据发生变更，所有的订阅该节点的客户端都可以获取数据变更通知。 2.2 负载均衡建立server节点，并建立监听器监视servers子节点的状态（用于在服务器增添时及时同步当前集群中服务器列表）。在每个服务器启动时，在servers节点下建立具体服务器地址的子节点,并在对应的字节点下存入服务器的相关信息。这样，我们在zookeeper服务器上可以获取当前集群中的服务器列表及相关信息，可以自定义一个负载均衡算法，在每个请求过来时从zookeeper服务器中获取当前集群服务器列表，根据算法选出其中一个服务器来处理请求。 2.3 命名服务命名服务是分布式系统中的基本功能之一。被命名的实体通常可以是集群中的机器、提供的服务地址或者远程对象，这些都可以称作为名字。常见的就是一些分布式服务框架（RPC、RMI）中的服务地址列表，通过使用名称服务客户端可以获取资源的实体、服务地址和提供者信息。命名服务就是通过一个资源引用的方式来实现对资源的定位和使用。在分布式环境中，上层应用仅仅需要一个全局唯一名称，就像数据库中的主键。 在单库单表系统中可以通过自增ID来标识每一条记录，但是随着规模变大分库分表很常见，那么自增ID有仅能针对单一表生成ID，所以在这种情况下无法依靠这个来标识唯一ID。UUID就是一种全局唯一标识符。但是长度过长不易识别。 在 Zookeeper中通过创建顺序节点就可以实现，所有客户端都会根据自己的任务类型来创建一个顺序节点，例如 job-00000001 节点创建完毕后， create()接口会返回一个完整的节点名，例如：job-00000002 拼接 type类型和完整节点名作为全局唯一的ID 2.4 DNS服务2.4.1 域名配置在分布式系统应用中，每一个应用都需要分配一个域名，日常开发中，往往使用本地HOST绑定域名解析，开发阶段可以随时修改域名和IP的映射，大大提高开发的调试效率。如果应用的机器规模达到一定程度后，需要频繁更新域名时，需要在规模的集群中变更，无法保证实时性。所有我们在zk上创建一个节点来进行域名配置 2.4.2 域名解析应用解析时，首先从zk域名节点中获取域名映射的IP和端口。 2.4.3 域名变更每个应用都会在在对应的域名节点注册一个数据变更的watcher监听，一旦监听的域名节点数据变更，zk会向所有订阅的客户端发送域名变更通知。 2.5 集群管理随着分布式系统规模日益扩大，集群中机器的数量越来越多。有效的集群管理越来越重要了，zookeeper集群管理主要利用了watcher机制和创建临时节点来实现。以机器上下线和机器监控为例： 2.5.1 机器上下线新增机器的时候，将Agent部署到新增的机器上，当Agent部署启动时，会向zookeeper指定的节点下创建一个临时子节点，当Agent在zk上创建完这个临时节点后，当关注的节点zookeeper/machines下的子节点新加入新的节点时或删除都会发送通知，这样就对机器的上下线进行监控。 2.5.2 机器监控在机器运行过程中，Agent会定时将主机的的运行状态信息写入到/machines/hostn主机节点，监控中心通过订阅这些节点的数据变化来获取主机的运行信息。 2.6 分布式锁2.6.1 数据库实现分布式锁 首先我们创建一张锁表，锁表中字段设置唯一约束 123456CREATE TABLE &#96;lock_record&#96; ( &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;主键&#39;, &#96;lock_name&#96; varchar(50) DEFAULT NULL COMMENT &#39;锁名称&#39;, PRIMARY KEY (&#96;id&#96;), UNIQUE KEY &#96;lock_name&#96; (&#96;lock_name&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;38 DEFAULT CHARSET&#x3D;utf8 定义锁，实现 Lock接口，tryLock()尝试获取锁，从锁表中查询指定的锁记 录，如果查询到记录，说明已经上锁，不能再上锁 12345678910111213//尝试获取锁@Overridepublic boolean tryLock() &#123; //查询lockRecord的记录 Example example = new Example(LockRecord.class); Example.Criteria criteria = example.createCriteria(); criteria.andEqualTo(\"lockName\",LOCK_NAME); LockRecord lockRecord = lockRecordMapper.selectOneByExample(example); if(lockRecord==null)&#123; return true; &#125; return false;&#125; 在 lock方法获取锁之前先调用tryLock()方法尝试获取锁，如果未加锁则向锁表中插入一条锁记录来获取锁，这里我们通过循环，如果上锁我们一致等待锁的释放 123456789101112131415//上锁@Overridepublic void lock() &#123; while(true)&#123; if(tryLock())&#123; //向锁表中插入一条记录 LockRecord lockRecord = new LockRecord(); lockRecord.setLockName(LOCK_NAME); lockRecordMapper.insertSelective(lockRecord); return; &#125;else&#123; System.out.println(\"等待锁.......\"); &#125; &#125;&#125; 释放锁，即是将数据库中对应的锁表记录删除 12345678//释放锁的操作@Overridepublic void unlock() &#123; Example example = new Example(LockRecord.class); Example.Criteria criteria = example.createCriteria(); criteria.andEqualTo(\"lockName\",LOCK_NAME); lockRecordMapper.deleteByExample(example);&#125; 注意在尝试获取锁的方法 tryLock中，存在多个线程同时获取锁的情况，可以简单通过synchronized解决 2.6.2 redis实现分布式锁redis分布式锁的实现基于setnx（set if not exists），设置成功，返回1；设置失败，返回0，释放锁的操作通过del指令来完成 如果设置锁后在执行中间过程时，程序抛出异常，导致del指令没有调用，锁永远无法释放，这样就会陷入死锁。所以我们拿到锁之后会给锁加上一个过期时间，这样即使中间出现异常，过期时间到后会自动释放锁。 同时在setnx 和 expire 如果进程挂掉，expire不能执行也会死锁。所以要保证setnx和expire是一个原子性操作即可。redis 2.8之后推出了setnx和expire的组合指令 1set key value ex 5 nx lock获取锁方法 1234567891011121314@Overridepublic void lock() &#123; while(true)&#123; //上锁 setnx // Boolean isLock = redisTemplate.opsForValue().setIfAbsent(\"lockName\", LOCK_NAME); Boolean isLock = redisTemplate.opsForValue().setIfAbsent(\"lockName\",LOCK_NAME,10,TimeUnit.SECONDS); if(isLock)&#123; return; &#125;else&#123; System.out.println(\"等待锁........\"); &#125; &#125;&#125; 释放锁 12345@Overridepublic void unlock() &#123; // 删除指定的锁的key redisTemplate.delete(\"lockName\");&#125; redis 实现分布式锁存在的问题，为了解决redis单点问题，我们会部署redis集群，在 Sentinel 集群中，主节点突然挂掉了。同时主节点中有把锁还没有来得及同步到从节点。这样就会导致系统中同样一把锁被两个客户端同时持有，不安全性由此产生。redis官方为了解决这个问题，推出了Redlock 算法解决这个问题。但是带来的网络消耗较大。 分布式锁的redisson实现： 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt;&lt;/dependency&gt; 获取锁释放锁 1234567Config config = new Config();config.useSingleServer().setAddress(\"redis://127.0.0.1:6379\").setDatabase(0);Redisson redisson = (Redisson) Redisson.create(config);RLock mylock = redisson.getLock(\"redisson_lock\");//获取锁mylock.lock();//释放锁mylock.unlock(); 2.6.3 zookeeper实现分布式锁原理：zookeeper通过创建临时序列节点来实现分布式锁，适用于顺序执行的程序，大体思路就是创建临时序列节点，找出最小的序列节点，获取分布式锁，程序执行完成之后此序列节点消失，通过watch来监控节点的变化，从剩下的节点的找到最小的序列节点，获取分布式锁，执行相应处理，依次类推…… 2.6.3.1 原生实现首先在ZkLock的构造方法中，连接zk,创建lock根节点 12345678910111213141516171819202122232425262728293031323334353637383940//zk客户端private ZooKeeper zk;//zk是一个目录结构，locksprivate String root = \"/locks\";//锁的名称private String lockName;//当前线程创建的序列nodeprivate ThreadLocal&lt;String&gt; nodeId = new ThreadLocal&lt;&gt;();//用来同步等待zkclient链接到了服务端private CountDownLatch connectedSignal = new CountDownLatch(1);private final static int sessionTimeout = 3000;private final static byte[] data= new byte[0];public ZkLock(String config, String lockName) &#123; this.lockName = lockName; try &#123; zk = new ZooKeeper(config, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 建立连接 if (event.getState() == Event.KeeperState.SyncConnected) &#123; connectedSignal.countDown(); &#125; &#125; &#125;); connectedSignal.await(); Stat stat = zk.exists(root, false); if (null == stat) &#123; // 创建根节点 zk.create(root, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125;&#125; 添加 watch监听临时顺序节点的删除 1234567891011121314class LockWatcher implements Watcher &#123; private CountDownLatch latch = null; public LockWatcher(CountDownLatch latch) &#123; this.latch = latch; &#125; @Override public void process(WatchedEvent event) &#123; if (event.getType() == Event.EventType.NodeDeleted) latch.countDown(); &#125;&#125; 获取锁操作 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic void lock() &#123; try &#123; // 创建临时子节点 String myNode = zk.create(root + \"/\" + lockName , data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(Thread.currentThread().getName()+myNode+ \"created\"); // 取出所有子节点 List&lt;String&gt; subNodes = zk.getChildren(root, false); TreeSet&lt;String&gt; sortedNodes = new TreeSet&lt;&gt;(); for(String node :subNodes) &#123; sortedNodes.add(root +\"/\" +node); &#125; String smallNode = sortedNodes.first(); if (myNode.equals( smallNode)) &#123; // 如果是最小的节点,则表示取得锁 System.out.println(Thread.currentThread().getName()+ myNode+\"get lock\"); this.nodeId.set(myNode); return; &#125; String preNode = sortedNodes.lower(myNode); CountDownLatch latch = new CountDownLatch(1); Stat stat = zk.exists(preNode, new LockWatcher(latch));// 同时注册监听。 // 判断比自己小一个数的节点是否存在,如果不存在则无需等待锁,同时注册监听 if (stat != null) &#123; System.out.println(Thread.currentThread().getName()+ myNode + \" waiting for \" + root + \"/\" + preNode + \" released lock\"); latch.await();// 等待，这里应该一直等待其他线程释放锁 nodeId.set(myNode); latch = null; &#125; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125;&#125; 释放锁 1234567891011121314@Overridepublic void unlock() &#123; try &#123; System.out.println(Thread.currentThread().getName()+ \"unlock \"); if (null != nodeId) &#123; zk.delete(nodeId.get(), -1); &#125; nodeId.remove(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125;&#125; 2.6.3.2 基于 curator实现分布式锁maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt;&lt;/dependency&gt; 锁操作 12345678910111213//创建zookeeper的客户端RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);//集群通过，分割CuratorFramework client =CuratorFrameworkFactory.newClient(\"127.0.0.1:2181\", retryPolicy);client.start();//创建分布式锁, 锁空间的根节点路径为/curator/lock InterProcessMutex mutex = new InterProcessMutex(client, \"/curator/lock\"); mutex.acquire();//获得了锁, 进行业务流程//完成业务流程, 释放锁mutex.release();//关闭客户端client.close(); 2.7 分布式队列队列特性：FIFO（先入先出），zookeeper实现分布式队列的步骤 在队列节点下创建临时顺序节点 例如/queue_info/192.168.1.1-0000001 调用 getChildren()接口来获取/queue_info节点下所有子节点，获取队列中所有元素 比较自己节点是否是序号最小的节点，如果不是，则等待其他节点出队列，在序号最小的节点注册watcher 获取 watcher通知后，重复步骤 3. zookeeper 开源客户端curator3.1 curator简介curator是Netflix公司开源的一个zookeeper客户端，后捐献给apache，curator框架在zookeeper原生API接口上进行了包装，解决了很多zooKeeper客户端非常底层的细节开发。提供zooKeeper各种应用场景(比如：分布式锁服务、集群领导选举、共享计数器、缓存机制、分布式队列等)的抽象封装，实现了Fluent风格的API接口，是最好用，最流行的zookeeper的客户端。 原生zookeeperAPI的不足： 连接对象异步创建，需要开发人员自行编码等待 连接没有自动重连超时机制 watcher一次注册生效一次 不支持递归创建树形节点 curator特点： 解决session会话超时重连 watcher反复注册 简化开发api 遵循Fluent风格的API 提供了分布式锁服务、共享计数器、缓存机制等机制 maven依赖: 1234567891011121314151617181920212223242526272829303132&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.7&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.2 连接到ZooKeeper案例： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 连接到ZooKeeper * * @author wgy */public class CuratorConnection &#123; public static void main(String[] args) &#123; // session重连策略 /* 3秒后重连一次，只重连1次 RetryPolicy retryPolicy = new RetryOneTime(3000); */ /* 每3秒重连一次，重连3次 RetryPolicy retryPolicy = new RetryNTimes(3,3000); */ /* 每3秒重连一次，总等待时间超过10秒后停止重连 RetryPolicy retryPolicy=new RetryUntilElapsed(10000,3000); */ // baseSleepTimeMs * Math.max(1, random.nextInt(1 &lt;&lt; (retryCount + 1))) RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); // 创建连接对象 CuratorFramework client = CuratorFrameworkFactory.builder() // IP地址端口号 .connectString(\"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\") // 会话超时时间 .sessionTimeoutMs(5000) // 重连机制 .retryPolicy(retryPolicy) // 命名空间 .namespace(\"create\") // 构建连接对象 .build(); // 打开连接 client.start(); System.out.println(client.isStarted()); // 关闭连接 client.close(); &#125;&#125; 3.3 新增节点案例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * 新增节点 * * @author wgy */public class CuratorCreate &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory.builder() .connectString(IP) .sessionTimeoutMs(5000) .retryPolicy(retryPolicy) .namespace(\"create\") .build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void create1() throws Exception &#123; // 新增节点 client.create() // 节点的类型 .withMode(CreateMode.PERSISTENT) // 节点的权限列表 world:anyone:cdrwa .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) // arg1:节点的路径 // arg2:节点的数据 .forPath(\"/node1\", \"node1\".getBytes()); System.out.println(\"结束\"); &#125; @Test public void create2() throws Exception &#123; // 自定义权限列表 // 权限列表 List&lt;ACL&gt; list = new ArrayList&lt;ACL&gt;(); // 授权模式和授权对象 Id id = new Id(\"ip\", \"192.168.142.128\"); list.add(new ACL(ZooDefs.Perms.ALL, id)); client.create().withMode(CreateMode.PERSISTENT).withACL(list).forPath(\"/node2\", \"node2\".getBytes()); System.out.println(\"结束\"); &#125; @Test public void create3() throws Exception &#123; // 递归创建节点树 client.create() // 递归节点的创建 .creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT) .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) .forPath(\"/node3/node31\", \"node31\".getBytes()); System.out.println(\"结束\"); &#125; @Test public void create4() throws Exception &#123; // 异步方式创建节点 client.create() .creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT) .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) // 异步回调接口 .inBackground(new BackgroundCallback() &#123; public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; // 节点的路径 System.out.println(curatorEvent.getPath()); // 时间类型 System.out.println(curatorEvent.getType()); &#125; &#125;) .forPath(\"/node4\", \"node4\".getBytes()); Thread.sleep(5000); System.out.println(\"结束\"); &#125;&#125; 3.4 更新节点案例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 更新节点 * * @author wgy */public class CuratorSet &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory.builder() .connectString(IP) .sessionTimeoutMs(5000) .retryPolicy(retryPolicy) .namespace(\"set\").build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void set1() throws Exception &#123; // 更新节点 client.setData() // arg1:节点的路径 // arg2:节点的数据 .forPath(\"/node1\", \"node11\".getBytes()); System.out.println(\"结束\"); &#125; @Test public void set2() throws Exception &#123; client.setData() // 指定版本号 .withVersion(2) .forPath(\"/node1\", \"node1111\".getBytes()); System.out.println(\"结束\"); &#125; @Test public void set3() throws Exception &#123; // 异步方式修改节点数据 client.setData() .withVersion(-1).inBackground(new BackgroundCallback() &#123; public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; // 节点的路径 System.out.println(curatorEvent.getPath()); // 事件的类型 System.out.println(curatorEvent.getType()); &#125; &#125;).forPath(\"/node1\", \"node1\".getBytes()); Thread.sleep(5000); System.out.println(\"结束\"); &#125;&#125; 3.5 删除节点案例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 删除节点 * * @author wgy */public class CuratorDelete &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory.builder() .connectString(IP) .sessionTimeoutMs(10000) .retryPolicy(retryPolicy) .namespace(\"delete\").build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void delete1() throws Exception &#123; // 删除节点 client.delete() // 节点的路径 .forPath(\"/node1\"); System.out.println(\"结束\"); &#125; @Test public void delete2() throws Exception &#123; client.delete() // 版本号 .withVersion(0) .forPath(\"/node1\"); System.out.println(\"结束\"); &#125; @Test public void delete3() throws Exception &#123; //删除包含子节点的节点 client.delete() .deletingChildrenIfNeeded() .withVersion(-1) .forPath(\"/node1\"); System.out.println(\"结束\"); &#125; @Test public void delete4() throws Exception &#123; // 异步方式删除节点 client.delete() .deletingChildrenIfNeeded() .withVersion(-1) .inBackground(new BackgroundCallback() &#123; public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; // 节点路径 System.out.println(curatorEvent.getPath()); // 事件类型 System.out.println(curatorEvent.getType()); &#125; &#125;) .forPath(\"/node1\"); Thread.sleep(5000); System.out.println(\"结束\"); &#125;&#125; 3.6 查看节点案例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 查看节点 * * @author wgy */public class CuratorGet &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory.builder() .connectString(IP) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .namespace(\"get\").build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void get1() throws Exception &#123; // 读取节点数据 byte[] bys = client.getData() // 节点的路径 .forPath(\"/node1\"); System.out.println(new String(bys)); &#125; @Test public void get2() throws Exception &#123; // 读取数据时读取节点的属性 Stat stat = new Stat(); byte[] bys = client.getData() // 读取属性 .storingStatIn(stat) .forPath(\"/node1\"); System.out.println(new String(bys)); System.out.println(stat.getVersion()); &#125; @Test public void get3() throws Exception &#123; // 异步方式读取节点的数据 client.getData() .inBackground(new BackgroundCallback() &#123; public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; // 节点的路径 System.out.println(curatorEvent.getPath()); // 事件类型 System.out.println(curatorEvent.getType()); // 数据 System.out.println(new String(curatorEvent.getData())); &#125; &#125;) .forPath(\"/node1\"); Thread.sleep(5000); System.out.println(\"结束\"); &#125;&#125; 3.7 查看子节点案例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * 查看子节点 * * @author wgy */public class CuratorGetChild &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory.builder() .connectString(IP) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void getChild1() throws Exception &#123; // 读取子节点数据 List&lt;String&gt; list = client.getChildren() // 节点路径 .forPath(\"/get\"); for (String str : list) &#123; System.out.println(str); &#125; &#125; @Test public void getChild2() throws Exception &#123; // 异步方式读取子节点数据 client.getChildren() .inBackground(new BackgroundCallback() &#123; public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; // 节点路径 System.out.println(curatorEvent.getPath()); // 事件类型 System.out.println(curatorEvent.getType()); // 读取子节点数据 List&lt;String&gt; list = curatorEvent.getChildren(); for (String str : list) &#123; System.out.println(str); &#125; &#125; &#125;) .forPath(\"/get\"); Thread.sleep(5000); System.out.println(\"结束\"); &#125;&#125; 3.8 检查节点是否存在案例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 检查节点是否存在 * * @author wgy */public class CuratorExists &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory.builder() .connectString(IP) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .namespace(\"get\").build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void exists1() throws Exception &#123; // 判断节点是否存在 Stat stat = client.checkExists() // 节点路径 .forPath(\"/node2\"); System.out.println(stat.getVersion()); &#125; @Test public void exists2() throws Exception &#123; // 异步方式判断节点是否存在 client.checkExists() .inBackground(new BackgroundCallback() &#123; public void processResult(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; // 节点路径 System.out.println(curatorEvent.getPath()); // 事件类型 System.out.println(curatorEvent.getType()); System.out.println(curatorEvent.getStat().getVersion()); &#125; &#125;) .forPath(\"/node2\"); Thread.sleep(5000); System.out.println(\"结束\"); &#125;&#125; 3.9 watcherAPIcurator提供了两种Watcher(Cache)来监听结点的变化 Node Cache : 只是监听某一个特定的节点，监听节点的新增和修改 PathChildren Cache : 监控一个ZNode的子节点. 当一个子节点增加， 更新，删除时， Path Cache会改变它的状态， 会包含最新的子节点， 子节点的数据和状态 案例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * watcherAPI * * @author wgy */public class CuratorWatcher &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory .builder() .connectString(IP) .sessionTimeoutMs(10000) .retryPolicy(retryPolicy) .build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void watcher1() throws Exception &#123; // 监视某个节点的数据变化 // arg1:连接对象 // arg2:监视的节点路径 final NodeCache nodeCache = new NodeCache(client, \"/watcher1\"); // 启动监视器对象 nodeCache.start(); nodeCache.getListenable().addListener(new NodeCacheListener() &#123; // 节点变化时回调的方法 public void nodeChanged() throws Exception &#123; System.out.println(nodeCache.getCurrentData().getPath()); System.out.println(new String(nodeCache.getCurrentData().getData())); &#125; &#125;); Thread.sleep(100000); System.out.println(\"结束\"); //关闭监视器对象 nodeCache.close(); &#125; @Test public void watcher2() throws Exception &#123; // 监视子节点的变化 // arg1:连接对象 // arg2:监视的节点路径 // arg3:事件中是否可以获取节点的数据 PathChildrenCache pathChildrenCache = new PathChildrenCache(client, \"/watcher1\", true); // 启动监听 pathChildrenCache.start(); pathChildrenCache.getListenable().addListener(new PathChildrenCacheListener() &#123; // 当子节点方法变化时回调的方法 public void childEvent(CuratorFramework curatorFramework, PathChildrenCacheEvent pathChildrenCacheEvent) throws Exception &#123; // 节点的事件类型 System.out.println(pathChildrenCacheEvent.getType()); // 节点的路径 System.out.println(pathChildrenCacheEvent.getData().getPath()); // 节点数据 System.out.println(new String(pathChildrenCacheEvent.getData().getData())); &#125; &#125;); Thread.sleep(100000); System.out.println(\"结束\"); // 关闭监听 pathChildrenCache.close(); &#125;&#125; 3.10 事务案例： 12345678910111213141516171819202122232425262728293031323334353637/** * 事务 * * @author wgy */public class CuratorTransaction &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory.builder() .connectString(IP) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .namespace(\"create\").build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void tra1() throws Exception &#123; // 开启事务 client.inTransaction() .create().forPath(\"/node1\", \"node1\".getBytes()) .and() .create().forPath(\"/node2\", \"node2\".getBytes()) .and() //事务提交 .commit(); &#125;&#125; 3.11 分布式锁InterProcessMutex：分布式可重入排它锁 InterProcessReadWriteLock：分布式读写锁 案例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283/** * 分布式锁 * * @author wgy */public class CuratorLock &#123; String IP = \"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\"; CuratorFramework client; @Before public void before() &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); client = CuratorFrameworkFactory .builder() .connectString(IP) .sessionTimeoutMs(10000) .retryPolicy(retryPolicy) .build(); client.start(); &#125; @After public void after() &#123; client.close(); &#125; @Test public void lock1() throws Exception &#123; // 排他锁 // arg1:连接对象 // arg2:节点路径 InterProcessLock interProcessLock = new InterProcessMutex(client, \"/lock1\"); System.out.println(\"等待获取锁对象!\"); // 获取锁 只能一个进程进行 interProcessLock.acquire(); for (int i = 1; i &lt;= 10; i++) &#123; Thread.sleep(3000); System.out.println(i); &#125; // 释放锁 interProcessLock.release(); System.out.println(\"等待释放锁!\"); &#125; @Test public void lock2() throws Exception &#123; // 读写锁 InterProcessReadWriteLock interProcessReadWriteLock = new InterProcessReadWriteLock(client, \"/lock1\"); // 获取读锁对象 InterProcessLock interProcessLock = interProcessReadWriteLock.readLock(); System.out.println(\"等待获取锁对象!\"); // 获取锁 两个进程可同时读 interProcessLock.acquire(); for (int i = 1; i &lt;= 10; i++) &#123; Thread.sleep(3000); System.out.println(i); &#125; // 释放锁 interProcessLock.release(); System.out.println(\"等待释放锁!\"); &#125; @Test public void lock3() throws Exception &#123; // 读写锁 InterProcessReadWriteLock interProcessReadWriteLock = new InterProcessReadWriteLock(client, \"/lock1\"); // 获取写锁对象 InterProcessLock interProcessLock = interProcessReadWriteLock.writeLock(); System.out.println(\"等待获取锁对象!\"); // 获取锁 相当排他锁 interProcessLock.acquire(); for (int i = 1; i &lt;= 10; i++) &#123; Thread.sleep(3000); System.out.println(i); &#125; // 释放锁 interProcessLock.release(); System.out.println(\"等待释放锁!\"); &#125;&#125; 4. zookeeper四字监控命令zooKeeper支持某些特定的四字命令与其的交互。它们大多是查询命令，用来获取 zooKeeper服务的当前状态及相关信息。用户在客户端可以通过 telnet 或 nc 向zooKeeper提交相应的命令。 zooKeeper常用四字命令见下表 所示： 命令 描述 conf 输出相关服务配置的详细信息。比如端口、zk数据及日志配置路径、最大连接数，session超时时间、serverId等 cons 列出所有连接到这台服务器的客户端连接/会话的详细信息。包括“接受/发送”的包数量、session id 、操作延迟、最后的操作执行等信息 crst 重置当前这台服务器所有连接/会话的统计信息 dump 列出未经处理的会话和临时节点 envi 输出关于服务器的环境详细信息 ruok 测试服务是否处于正确运行状态。如果正常返回”imok”，否则返回空 stat 输出服务器的详细信息：接收/发送包数量、连接数、模式（leader/follower）、节点总数、延迟。 所有客户端的列表 srst 重置server状态 wchs 列出服务器watches的简洁信息：连接总数、watching节点总数和watches总数 wchc 通过session分组，列出watch的所有节点，它的输出是一个与 watch 相关的会话的节点列表 mntr 列出集群的健康状态。包括“接受/发送”的包数量、操作延迟、当前服务模式（leader/follower）、节点总数、watch总数、临时节点总数 nc命令工具安装: 12345#root用户安装#下载安装包wget http://vault.centos.org/6.6/os/x86_64/Packages/nc-1.84-22.el6.x86_64.rpm#rpm安装rpm -iUv nc-1.84-22.el6.x86_64.rpm 使用方式，在shell终端输入：echo mntr | nc localhost 2181 5. zookeeper图形化的客户端工具ZooInspector下载地址：https://issues.apache.org/jira/secure/attachment/12436620/ZooInspector.zip 解压后进入目录ZooInspector\\build，运行zookeeper-dev-ZooInspector.jar 12#执行命令如下java -jar zookeeper-dev-ZooInspector.jar 点击左上角连接按钮，输入zk服务地址：ip或者主机名:2181 点击OK，即可查看ZK节点信息","tags":[{"name":"分布式架构方案","slug":"分布式架构方案","permalink":"https://wgy1993.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://wgy1993.gitee.io/tags/zookeeper/"}]},{"title":"zookeeper(一)","date":"2020-08-01T10:35:22.000Z","path":"archives/e18db595.html","text":"1. zookeeper简介1.1 分布式概述早期我们使用单体架构，即所有服务部署在一台服务器的一个进程中，随着互联网的发展，逐步演进为分布式架构，多个服务分别部署在不同机器的不同进程中。 1.2 zookeeper概述zookeeper是一个开源的分布式协调服务，提供分布式数据一致性解决方案，分布式应用程序可以实现数据发布订阅、负载均衡、命名服务、集群管理分布式锁、分布式队列等功能。 zookeeper提供了分布式数据一致性解决方案，那什么是分布式数据一致性？首先我们谈谈什么叫一致性？ 如图在上图中有用户user在DB1中修改balance=900,如果user下一次read请求到DB2数据，此时DB1的数据还没及时更新到DB2中，就会造成整个数据库集群数据不一致。 数据一致性分为强一致性和最终一致性，强一致性指的如果数据不一致，就不对外提供数据服务，保证用户读取的数据始终是一致的。数据强一致性只需要通过锁机制即可解决，在案例中通过在DB2没有从DB1同步数据之前上锁，不对外提供读操作。只有当同步完成以后才对外提供服务。而最终一致性要求数据最终同步即可，没有实时性要求。 1.3 CAP原则CAP在分布式系统中主要指的是一致性（(Consistency）、可用性（Availability）和分区容错性（Partition tolerance） 一致性 一致性指的是强一致性 可用性 系统提供的服务一直处于可用状态，用户的操作请求在指定的响应时间内响应请求，超出时间范围，认为系统不可用 分区容错性 分布式系统在遇到任何网络分区故障的时候，仍需要能够保证对外提供一致性和可用性服务，除非是整个网络都发生故障。 在一个分布式系统中不可能同时满足一致性、可用性、分区容错性，最多满足两个，对于分布式互联网应用而言，必须保证P，所以要么满足AP模型、要么满足CP模型 1.4 一致性协议事务需要跨多个分布式节点时，为了保证事务的ACID特性，需要选举出一个协调者来协调分布式各个节点的调度，基于这个思想衍生了多种一致性协议: 1.4.1 2PC 二阶段提交顾名思义，二阶段提交叫事务的提交过程分为两个阶段： 阶段一 提交事务请求 1 、协调者向所有的参与者节点发送事务内容，询问是否可以执行事务操作，并等待其他参与者节点的反馈 2 、各参与者节点执行事务操作 3 、各参与者节点反馈给协调者，事务是否可以执行 阶段二 事务提交 根据一阶段各个参与者节点反馈的ack,如果所有参与者节点反馈ack，则执行事务提交，否则中断事务 事务提交： 1 、协调者向各个参与者节点发送commit请求 2 、参与者节点接受到commit请求后，执行事务的提交操作 3 、各参与者节点完成事务提交后，向协调者返送提交commit成功确认消息 4 、协调者接受各个参与者节点的ack后，完成事务commit 中断事务： 1 、发送回滚请求 2 、各个参与者节点回滚事务 3 、反馈给协调者事务回滚结果 4 、协调者接受各参与者节点ack后回滚事务 二阶段提交存在的问题： 同步阻塞 二阶段提交过程中，所有参与事务操作的节点处于同步阻塞状态，无法进行其他的操作 单点问题 一旦协调者出现单点故障，无法保证事务的一致性操作 脑裂导致数据不一致 如果分布式节点出现网络分区，某些参与者未收到commit提交命令。则出现部分参与者完成数据提交。未收到commit的命令的参与者则无法进行事务提交，整个分布式系统便出现了数据不一致性现象。 1.4.2 3PC 三阶段提交3PC是2PC的改进版，实质是将2PC中提交事务请求拆分为两步，形成了CanCommit、PreCommit、doCommit三个阶段的事务一致性协议 阶段一 : CanCommit 1 、事务询问 2 、各参与者节点向协调者反馈事务询问的响应 阶段二 : PreCommit 根据阶段一的反馈结果分为两种情况 执行事务预提交 1 、发送预提交请求 协调者向所有参与者节点发送preCommit请求，进入prepared阶段 2 、事务预提交 各参与者节点接受到preCommit请求后，执行事务操作 3 、各参与者节点向协调者反馈事务执行 中断事务 任意一个参与者节点反馈给协调者响应No时，或者在等待超时后，协调者还未收到参与者的反馈，就中断事务，中断事务分为两步： 1 、协调者向各个参与者节点发送abort请求 2 、参与者收到abort请求，或者等待超时时间后，中断事务 阶段三 : doCommit 执行提交 1 、发送提交请求 协调者向所有参与者节点发送doCommit请求 2 、事务提交 各参与者节点接受到doCommit请求后，执行事务提交操作 3 、反馈事务提交结果 各参与者节点完成事务提交以后，向协调者发送ack 4 、事务完成 协调者接受各个参与者反馈的ack后，完成事务 中断事务 1 、参与者接受到abort请求后，执行事务回滚 2 、参与者完成事务回滚以后，向协调者发送ack 3 、协调者接受回滚ack后，回滚事务 3PC相较于2PC而言，解决了协调者挂点后参与者无限阻塞和单点问题，但是仍然无法解决网络分区问题 1.4.3 Paxos算法Paxos算法是Leslie Lamport 1990年提出的一种一致性算法，该算法是一种提高分布式系统容错性的一致性算法，解决了3PC中网络分区的问题，paxos算法可以在节点失效、网络分区、网络延迟等各种异常情况下保证所有节点都处于同一状态，同时paxos算法引入了“过半”理念，即少数服从多数原则。 paxos有三个版本： Basic Paxos Multi Paxos Fast Paxos 在paxos算法中，有四种种角色，分别具有三种不同的行为，但多数情况，一个进程可能同时充当多种角色。 client：系统外部角色，请求发起者，不参与决策 proposer：提案提议者 acceptor：提案的表决者，即是否accept该提案，只有超过半数以上的acceptor接受了提案，该提案才被认为被“选定” learners：提案的学习者，当提案被选定后，其同步执行提案，不参与决策 Paxos算法分为两个阶段：prepare阶段、accept阶段 prepare阶段 1 、proposer提出一个提案，编号为N,发送给所有的acceptor。 2 、每个表决者都保存自己的accept的最大提案编号maxN，当表决者收到prepare(N)请求时，会比较N与maxN的值，若N小于maxN,则提案已过时，拒绝prepare(N)请求。若N大于等于maxN，则接受提案，并将该表决者曾经接受过的编号最大的提案Proposal(myid,maxN,value)反馈给提议者：其中myid表示表决者acceptor的标识id，maxN表示接受过的最大提案编号maxN，value表示提案内容。若当前表决者未曾accept任何提议，会将proposal(myid,null,null)反馈给提议者。 accept阶段 1 、提议者proposal发出prepare(N),若收到超过半数表决者acceptor的反馈，proposal将真正的提案内容proposal(N,value)发送给所有表决者。 2 、表决者acceptor接受提议者发送的proposal(N,value)提案后，会将自己曾经accept过的最大提案编号maxN和反馈过的prepare的最大编号，若N大于这两个编号，则当前表决者accept该提案，并反馈给提议者。否则拒绝该提议。 3 、若提议者没有收到半数以上的表决者accept反馈，则重新进入prepare阶段，递增提案编号，重新提出prepare请求。若收到半数以上的accept，则其他未向提议者反馈的表决者称为learner，主动同步提议者的提案。 正常流程 单点故障，部分节点失败 proposer失败 Basic Paxos算法存在活锁问题（liveness）或dueling，而且较难实现 Multi Paxos: 唯一的proposor，即leader 简化角色 1.4.4 ZAB协议(Fast Paxos)由于paxos算法实现起来较难，存在活锁和全序问题（无法保证两次最终提交的顺序），所以zookeeper并没有使用paxos作为一致性协议，而是使用了ZAB协议。 ZAB（zookeeper atomic broadcast）:是一种支持崩溃恢复的原子广播协议，基于Fast paxos实现 ZooKeeper使用单一主进程Leader用于处理客户端所有事务请求,，即写请求。当服务器数据发生变更好，集群采用ZAB原子广播协议，以事务提交proposal的形式广播到所有的副本进程，每一个事务分配一个全局的递增的事务编号xid。 若客户端提交的请求为读请求时，则接受请求的节点直接根据自己保存的数据响应。若是写请求，且当前节点不是leader，那么该节点就会将请求转发给leader，leader会以提案的方式广播此写请求，如果超过半数的节点同意写请求，则该写请求就会提交。leader会通知所有的订阅者同步数据。 zookeeper的三种角色： 为了避免zk的单点问题，zk采用集群方式保证zk高可用 leader leader负责处理集群的写请求，并发起投票，只有超过半数的节点同意后才会提交该写请求 follower 处理读请求，响应结果。转发写请求到leader，在选举leader过程中参与投票 observer observer可以理解为没有投票权的follower，主要职责是协助follower处理读请求。那么当整个zk集群读请求负载很高时，为什么不增加follower节点呢？原因是增加follower节点会让leader在提出写请求提案时，需要半数以上的follower投票节点同意，这样会增加leader和follower的通信通信压力，降低写操作效率。 zookeeper两种模式： 恢复模式 当服务启动或领导崩溃后，zk进入恢复状态，选举leader，leader选出后，将完成leader和其他机器的数据同步，当大多数server完成和leader的同步后，恢复模式结束 广播模式 一旦Leader已经和多数的Follower进行了状态同步后，进入广播模式。进入广播模式后，如果有新加入的服务器，会自动从leader中同步数据。leader在接收客户端请求后，会生成事务提案广播给其他机器，有超过半数以上的follower同意该提议后，再提交事务。 注意在ZAB的事务的二阶段提交中，移除了事务中断的逻辑，follower要么ack，要么放弃，leader无需等待所有的follower的ack。 zxid zxid是 64 位长度的Long类型，其中高 32 位表示纪元epoch，低 32 位表示事务标识xid。即zxid由两部分构成：epoch和xid 每个leader都会具有不同的epoch值，表示一个纪元，每一个新的选举开启时都会生成一个新的epoch，新的leader产生，会更新所有的zkServer的zxid的epoch，xid是一个依次递增的事务编号。 leader选举算法： 启动过程 每一个server发出一个投票给集群中其他节点 收到各个服务器的投票后，判断该投票有效性，比如是否是本轮投票，是否是 looking状态 处理投票，pk别人的投票和自己的投票 比较规则xid&gt;myid “取大原则” 统计是否超过半数的接受相同的选票 确认leader，改变服务器状态 添加新server，leader已经选举出来，只能以follower身份加入集群中 崩溃恢复过程 leader挂掉后，集群中其他follower会将状态从FOLLOWING变为LOOKING,重新进入leader选举 同上启动过程 消息广播算法： 一旦进入广播模式，集群中非leader节点接受到事务请求，首先会将事务请求转发给服务器，leader服务器为其生成对应的事务提案proposal,并发送给集群中其他节点，如果过半则事务提交； leader接受到消息后，消息通过全局唯一的 64 位自增事务id，zxid标识 leader发送给follower的提案是有序的，leader会创建一个FIFO队列，将提案顺序写入队列中发送给follower follower接受到提案后，会比较提案zxid和本地事务日志最大的zxid，若提案zxid比本地事务id大，将提案记录到本地日志中，反馈ack给leader，否则拒绝 leader接收到过半ack后，leader向所有的follower发送commit，通知每个follower执行本地事务 2. zookeeper环境搭建zookeeper安装以linux环境为例，windows省略 2.1 单机环境2.1.1 安装jdk略…… 2.1.2 zookeeper压缩包上传到linuxAlt+P 进入SFTP ，输入put d:\\zookeeper-3.4.6.tar.gz 上传，d:\\zookeeper-3.4.6.tar.gz是本地存放zookeeper的路径或者rz上传 2.1.3 解压缩压缩包1tar -zxvf zookeeper-3.4.6.tar.gz 2.1.4 创建 data 文件夹进入 zookeeper-3.4.13 目录,创建data文件夹并修改conf文件夹下的zoo_sample.cfg为zoo.cfg 1234mkdir datacd confmv zoo_sample.cfg zoo.cfg 2.1.5 修改zoo.cfg中的data属性1dataDir=/root/zookeeper-3.4.6/data 2.1.6 zookeeper服务启动进入bin目录，启动服务输入命令 1./zkServer.sh start 输出以下内容表示启动成功 关闭服务输入命令 1./zkServer.sh stop 输出以下提示信息 查看状态 1./zkServer.sh status 如果启动状态，提示 如果未启动状态，提示 2.2 集群环境真实的集群是需要部署在不同的服务器上的，但是在我们测试时启动多个虚拟机的内存消耗太大，所以我们通常会搭建伪集群 ，也就是把所有的服务都搭建在一台虚拟机上，用端口进行区分。 2.2.1 准备工作（ 1 ）安装JDK 【此步骤省略】。 （ 2 ）Zookeeper压缩包上传到服务器 （ 3 ）将Zookeeper解压到 /usr/local/zookeeper-cluster，复制三份zookeeper并改名为 zookeeper-1、zookeeper-2、zookeeper-3 （ 4 ）在解压后的Zookeeper目录下创建data目录 12345678cd /usr/local/zookeeper-cluster/zookeeper-1mkdir datacd /usr/local/zookeeper-cluster/zookeeper-2mkdir datacd /usr/local/zookeeper-cluster/zookeeper-3mkdir data （ 5 ） 配置每一个Zookeeper 的dataDir（zoo.cfg） clientPort 分别为 2181 2182 2183 修改 /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfg 12clientPort&#x3D;2181dataDir&#x3D;&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-1&#x2F;data 修改/usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfg 12clientPort&#x3D;2182dataDir&#x3D;&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-2&#x2F;data 修改/usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg 12clientPort&#x3D;2183dataDir&#x3D;&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-3&#x2F;data 2.2.2 配置集群（ 1 ）在每个zookeeper的 data 目录下创建一个 myid 文件，内容分别是 1 、 2 、 3 。这个文件就是记录每个服务器的ID 1touch myid （ 2 ）在每一个zookeeper 的 zoo.cfg配置客户端访问端口（clientPort）和集群服务器IP列表。 1234server.1&#x3D;192.168.25.129:2881:3881server.2&#x3D;192.168.25.129:2882:3882server.3&#x3D;192.168.25.129:2883:3883server.服务器ID&#x3D;服务器IP地址：服务器之间通信端口：服务器之间投票选举端口 2.2.3 启动集群依次启动三个zk实例，其中有一个leader和两个follower 2.2.4 observer角色及其配置observer角色特点： 不参与集群的leader选举 不参与集群中写数据时的ack反馈 为了使用observer角色，在任何想变成observer角色的配置文件中加入如下配置： 1peerType&#x3D;observer 并在所有server的配置文件中，配置成observer模式的server的那行配置追加:observer，例如： 1server.3&#x3D;192.168.25.129:2883:3883:observer 2.2.5 zookeeperAPI连接集群123456789101112131415161718192021222324252627282930313233/** * zookeeper连接 * * @author wgy */public class ZookeeperConnection &#123; public static void main(String[] args) &#123; try &#123; // 计数器对象 CountDownLatch countDownLatch = new CountDownLatch(1); // arg1:服务器的ip和端口 // arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的 // arg3:监视器对象 ZooKeeper zooKeeper = new ZooKeeper(\"192.168.142.128:2181,192.168.142.128:2182,192.168.142.128:2183\", 5000, new Watcher() &#123; @Override public void process(WatchedEvent watchedEvent) &#123; if (watchedEvent.getState() == Event.KeeperState.SyncConnected) &#123; System.out.println(\"连接创建成功!\"); countDownLatch.countDown(); &#125; &#125; &#125;); // 主线程阻塞等待连接对象的创建成功 countDownLatch.await(); // 会话编号 System.out.println(zooKeeper.getSessionId()); zooKeeper.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 3. zookeeper基本使用3.1 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode，每个ZNode都可以通过其路径唯一标识 Znode节点类型 持久化目录节点（PERSISTENT） 客户端与zookeeper断开连接后，该节点依旧存在 持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点依旧存在，Zookeeper会给该节点按照顺序编号 临时目录节点（EPHEMERAL） 客户端与zookeeper断开连接后，该节点被删除 临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点被删除，Zookeeper会给该节点按照顺序编号 节点的状态stat： 用来描述当前节点的创建、修改记录，包括cZxid、ctime等 12345678910111213141516171819202122232425262728#在zookeeper shell中使用stat命令查看指定路径节点的stat信息：stat &#x2F;ns-1&#x2F;tenantcZxid &#x3D; 0x6a0000000actime &#x3D; Wed Mar 27 09:56:44 CST 2019mZxid &#x3D; 0x6a0000000amtime &#x3D; Wed Mar 27 09:56:44 CST 2019pZxid &#x3D; 0x6a0000000ecversion &#x3D; 2dataVersion &#x3D; 0aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x0dataLength &#x3D; 0numChildren &#x3D; 2#属性说明：cZxid：数据节点创建时的事务 IDctime：数据节点创建时的时间mZxid：数据节点最后一次更新时的事务 IDmtime：数据节点最后一次更新时的时间pZxid：数据节点的子节点最后一次被修改时的事务 IDcversion：子节点的更改次数dataVersion：节点数据的更改次数aclVersion：节点的 ACL 的更改次数ephemeralOwner：如果节点是临时节点，则表示创建该节点的会话的SessionID；如果节点是持久节点，则该属性值为 0dataLength：数据内容的长度numChildren：数据节点当前的子节点个数 3.2 命令行使用通过zkClient进入zookeeper客户端命令行，输入help查看zookeeper客户端的指令 123./zkcli.shhelp 如上图列出了zookeeper所有的客户端命令行，下面主要讲解常见的几个命令行 使用 ls 命令来查看当前znode中所包含的内容 123ls pathls / 查看当前节点数据并能看到更新次数等数据 123ls2 pathls2 / 创建节点 -s 含有序列 -e 临时 12345678910111213141516171819202122232425create [-s] [-e] path data #其中-s 为有序节点，-e 临时节点#创建持久化节点并写入数据：create /test \"testvalue\"#创建持久化有序节点，此时创建的节点名为指定节点名 + 自增序号create -s /a \"aaa\"Created /a0000000000create -s /b \"bbb\"Created /b0000000001create -s /c \"ccc\"Created /c0000000002#创建临时节点，临时节点会在会话过期后被删除create -e /tmp \"tmp\"Created /tmp#创建临时有序节点，临时节点会在会话过期后被删除create -s -e /aa \"aaa\"Created /aa0000000004create -s -e /bb \"bbb\"Created /bb0000000005 获得节点的值 123get pathget /test 设置节点的值 1234567set path data [version]set /test \"testvalueupdate\"#也可以基于版本号进行更改，此时类似于乐观锁机制，当你传入的数据版本号(dataVersion) 和当前节点的数据版本号不符合时，zookeeper 会拒绝本次修改set /test \"3456\" 1version No is not valid : /test 查看节点状态 123stat pathstat /test 删除节点 1234567delete path [version]delete /test#和更新节点数据一样，也可以传入版本号，当你传入的数据版本号 (dataVersion)和当前节点的数据版本号不符合时，zookeeper 不会执行删除操作delete /test 0version No is not valid : /test #无效的版本号 递归删除节点 12345create /test \"testvalue\"create /test/test001 \"testvalue001\"rmr /test 监听器get path [watch] 12345#使用 get path [watch] 注册的监听器能够在节点内容发生改变的时候，向客户端发出通知。需要注意的是 zookeeper 的触发器是一次性的 (One-time trigger)，即触发一次后就会立即失效get /test watchset /test 45678WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/test #节点值改变 监听器stat path [watch] 1234#使用 stat path [watch] 注册的监听器能够在节点状态发生改变的时候，向客户端发出通知stat /test watchset /test 112233WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/hadoop #节点值改变 监听器ls\\ls2 path [watch] 1234#使用 ls path [watch] 或 ls2 path [watch] 注册的监听器能够监听该节点下所有子节点的增加和删除操作ls /test watchcreate /test/yarn \"aaa\"WatchedEvent state:SyncConnected type:NodeCreated path:/test/yarn 3.3 api使用maven依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt;&lt;/dependency&gt; API 说明 ZooKeeper zk = new ZooKeeper(String connectString, int sessionTimeout,Watcher watcher) 创建zookeeper连接，connectString表示连接的zookeeper服务器的地址，sessionTimeOut指定会话的的超时时间，Watcher配置监听 String create(String path, byte[] data, List acl,CreateMode createMode) 创建一个给定的目录节点 path, 并给它设置数据，CreateMode 标识有四种形式的目录节点，分别是 PERSISTENT：持久化目录节点，这个目录节点存储的数据不会丢失；PERSISTENT_SEQUENTIAL：顺序自动编号的目录节点，这种目录节点会根据当前已近存在的节点数自动加 1 ，然后返回给客户端已经成功创建的目录节点名；EPHEMERAL：临时目录节点，一旦创建这个节点的客户端与服务器端口也就是 session 超时，这种节点会被自动删除；EPHEMERAL_SEQUENTIAL：临时自动编号节点 Stat exists(String path, boolean watch) 判断某个 path 是否存在，并设置是否监控这个目录节点，这里的watcher 是在创建 ZooKeeper 实例时指定的 watcher，exists方法还有一个重载方法，可以指定特定的watcher Stat exists(String path,Watcher watcher) 重载方法，这里给某个目录节点设置特定的 watcher，Watcher 在ZooKeeper 是一个核心功能，Watcher 可以监控目录节点的数据变化以及子目录的变化，一旦这些状态发生变化，服务器就会通知所有设置在这个目录节点上的 Watcher，从而每个客户端都很快知道它所关注的目录节点的状态发生变化，而做出相应的反应 void delete(String path, int version 删除 path 对应的目录节点，version 为 -1 可以匹配任何版本，也就删除了这个目录节点所有数据 List getChildren(String path, boolean watch) 获取指定 path 下的所有子目录节点，同样 getChildren方法也有一个重载方法可以设置特定的 watcher 监控子节点的状态 Stat setData(String path, byte[] data, int version) 给 path 设置数据，可以指定这个数据的版本号，如果 version 为 -1 怎可以匹配任何版本 byte[] getData(String path, boolean watch,Stat stat) 获取这个 path 对应的目录节点存储的数据，数据的版本等信息可以通过 stat 来指定，同时还可以设置是否监控这个目录节点数据的状态 代码： 123456789101112131415161718192021222324252627282930313233343536373839/** * zookeeper API测试 * * @author wgy */public class ZkApiTest &#123; @Test public void test() throws Exception &#123; //1. 创建zookeeper连接 ZooKeeper zooKeeper = new ZooKeeper(\"192.168.142.128:2181\", 30000, new Watcher() &#123; public void process(WatchedEvent watchedEvent) &#123; System.out.println(\"触发了\" + watchedEvent.getType() + \"的事件\"); &#125; &#125;); //2. 创建父节点 String path = zooKeeper.create(\"/test\", \"testValue\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(path); //3. 创建子节点 String childrenPath = zooKeeper.create(\"/test/children\", \"childrenValue\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(childrenPath); //4. 获取节点的值（父节点和子节点） byte[] data = zooKeeper.getData(\"/test\", false, null); System.out.println(new String(data)); List&lt;String&gt; children = zooKeeper.getChildren(\"/test\", false); for (String child : children) &#123; System.out.println(child); &#125;; //5. 修改节点的值 Stat stat = zooKeeper.setData(\"/test\", \"testUpdate\".getBytes(), -1); System.out.println(stat); //6. 判断某个节点是否存在 Stat exists = zooKeeper.exists(\"/test/children\", false); System.out.println(exists); //7. 删除节点 zooKeeper.delete(\"/test/children\", -1); &#125;&#125; 4. zookeeper的acl权限控制4.1 概述zookeeper 类似文件系统，client 可以创建节点、更新节点、删除节点，那么如何做到节点的权限的控制呢？zookeeper的access control list 访问控制列表可以做到这一点。 acl 权限控制，使用scheme：id：permission: 来标识，主要涵盖 3 个方面： 权限模式（scheme）：授权的策略 授权对象（id）：授权的对象 权限（permission）：授予的权限 其特性如下： zooKeeper的权限控制是基于每个znode节点的，需要对每个节点设置权限 每个znode支持设置多种权限控制方案和多个权限 子节点不会继承父节点的权限，客户端无权访问某节点，但可能可以访问它的子节点 12// 将节点权限设置为Ip:192.168.60.130的客户端可以对节点进行增、删、改、查、管理权限setAcl /test2 ip:192.168.60.130:crwda 4.2 权限模式采用何种方式授权 方案 描述 world 只有一个用户：anyone，代表登录zokeeper所有人（默认） ip 对客户端使用IP地址认证 auth 使用已添加认证的用户认证 digest 使用“用户名:密码”方式认证 4.3 授权的对象给谁授予权限 授权对象ID是指，权限赋予的实体，例如：IP 地址或用户。 4.4 授予的权限授予什么权限 create、delete、read、writer、admin也就是 增、删、改、查、管理权限，这5种权限简写为cdrwa，注意:这5种权限中，delete是指对子节点的删除权限，其它4种权限指对自身节点的操作权限 权限 ACL简写 描述 create c 可以创建子节点 delete d 可以删除子节点（仅下一级节点） read r 可以读取节点数据及显示子节点列表 write w 可以设置节点数据 admin a 可以设置节点访问控制列表权限 4.5 授权的相关命令 命令 使用方式 描述 getAcl getAcl 读取ACL权限 setAcl setAcl 设置ACL权限 addauth addauth 添加认证用户 4.6 案例4.6.1 world授权模式1234567891011#命令setAcl &lt;path&gt; world:anyone:&lt;acl&gt;#案例create /node1 \"node1\"getAcl /node1'world,'anyone #world方式对所有用户进行授权: cdrwa #增、删、改、查、管理setAcl /node1 world:anyone:cdrwa 4.6.2 IP授权模式1234567891011121314151617#命令setAcl &lt;path&gt; ip:&lt;ip&gt;:&lt;acl&gt;#案例#注意：远程登录zookeeper命令:./zkCli.sh -server ipcreate /node2 \"node2\"setAcl /node2 ip:192.168.60.129:cdrwagetAcl /node2'ip,'192.168.60.129: cdrwa#使用IP非 192.168.60.129 的机器get /node2Authentication is not valid : /node2 #没有权限 4.6.3 Auth授权模式12345678910111213#命令addauth digest &lt;user&gt;:&lt;password&gt; #添加认证用户setAcl &lt;path&gt; auth:&lt;user&gt;:&lt;acl&gt;#案例create /node3 \"node3\"#添加认证用户addauth digest wgy:123456setAcl /node3 auth:wgy:cdrwagetAcl /node3'digest,'wgy:860WKAFm6CvqEZcdivw5c3Q3i6Y=: cdrwa 4.6.4 Digest授权模式123456789101112131415161718#命令setAcl &lt;path&gt; digest:&lt;user&gt;:&lt;password&gt;:&lt;acl&gt;#这里的密码是经过SHA1及BASE64处理的密文，在SHELL中可以通过以下命令计算：echo -n &lt;user&gt;:&lt;password&gt; | openssl dgst -binary -sha1 | openssl base64#案例create /node4 \"node4\"setAcl /node4 digest:wgy:860WKAFm6CvqEZcdivw5c3Q3i6Y=:cdrwagetAcl /node4'digest,'wgy:860WKAFm6CvqEZcdivw5c3Q3i6Y=: cdrwaget /node4Authentication is not valid : /node4 #没有权限#添加认证用户addauth digest wgy:123456 4.6.5 多种模式授权12345678#同一个节点可以同时使用多种模式授权create /node5 \"node5\"#添加认证用户addauth digest wgy:123456 setAcl /node5 ip:192.168.60.129:cdra,auth:wgy:cdrwa,digest:haha:3RTWRMbFGscZqJBONcrAlMYMrv0=:cdrwa 4.7 acl 超级管理员zookeeper的权限管理模式有一种叫做super，该模式提供一个超管可以方便的访问任何权限的节点 假设这个超管是：super:admin，需要先为超管生成密码的密文 1echo -n super:admin | openssl dgst -binary -sha1 | openssl base64 那么打开zookeeper目录下的/bin/zkServer.sh服务器脚本文件，找到如下一行 1nohup $JAVA \"-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;\" \"-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;\" 这就是脚本中启动zookeeper的命令，默认只有以上两个配置项，我们需要加一个超管的配置项 1&quot;-Dzookeeper.DigestAuthenticationProvider.superDigest&#x3D;super:xQJmxLMiHGwaqBvst5y6rkB6HQs&#x3D;&quot; 那么修改以后这条完整命令变成了 12nohup $JAVA \"-Dzookeeper.log.dir=$&#123;ZOO_LOG_DIR&#125;\" \"-Dzookeeper.root.logger=$&#123;ZOO_LOG4J_PROP&#125;\" \"-Dzookeeper.DigestAuthenticationProvider.superDigest=super:xQJmxLMiHGwaqBvst5y6rkB6HQs=\" \\-cp \"$CLASSPATH\" $JVMFLAGS $ZOOMAIN \"$ZOOCFG\" &gt; \"$_ZOO_DAEMON_OUT\" 2&gt;&amp;1 &lt; /dev/null &amp; 之后启动zookeeper,输入如下命令添加权限 12#添加认证用户addauth digest super:admin 5. zookeeper javaAPIznode是zooKeeper集合的核心组件，zookeeper API提供了一小组方法使用zookeeper集合来操纵znode的所有细节。 客户端应该遵循以步骤，与zookeeper服务器进行清晰和干净的交互。 连接到zookeeper服务器。zookeeper服务器为客户端分配会话ID。 定期向服务器发送心跳。否则，zookeeper服务器将过期会话ID，客户端需要重新连接。 只要会话ID处于活动状态，就可以获取/设置znode。 所有任务完成后，断开与zookeeper服务器的连接。如果客户端长时间不活动，则zookeeper服务器将自动断开客户端。 5.1 连接到ZooKeeper12345ZooKeeper(String connectionString, int sessionTimeout, Watcher watcher)connectionString - zookeeper主机sessionTimeout - 会话超时（以毫秒为单位)watcher - 实现“监视器”对象。zookeeper集合通过监视器对象返回连接状态。 案例: 123456789101112131415161718192021222324252627282930313233/** * zookeeper连接 * * @author wgy */public class ZookeeperConnection &#123; public static void main(String[] args) &#123; try &#123; // 计数器对象 CountDownLatch countDownLatch = new CountDownLatch(1); // arg1:服务器的ip和端口 // arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的 // arg3:监视器对象 ZooKeeper zooKeeper = new ZooKeeper(\"192.168.142.128:2181\", 5000, new Watcher() &#123; @Override public void process(WatchedEvent watchedEvent) &#123; if (watchedEvent.getState() == Event.KeeperState.SyncConnected) &#123; System.out.println(\"连接创建成功!\"); countDownLatch.countDown(); &#125; &#125; &#125;); // 主线程阻塞等待连接对象的创建成功 countDownLatch.await(); // 会话编号 System.out.println(zooKeeper.getSessionId()); zooKeeper.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 5.2 新增节点1234567891011&#x2F;&#x2F; 同步方式create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode)&#x2F;&#x2F; 异步方式create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode，AsyncCallback.StringCallback callBack,Object ctx)path - znode路径。例如，&#x2F;node1 &#x2F;node1&#x2F;node11data - 要存储在指定znode路径中的数据acl - 要创建的节点的访问控制列表。zookeeper API提供了一个静态接口ZooDefs.Ids 来获取一些基本的acl列表。例如，ZooDefs.Ids.OPEN_ACL_UNSAFE返回打开znode的acl列表。createMode - 节点的类型,这是一个枚举。callBack-异步回调接口ctx-传递上下文参数 案例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198/** * zookeeper新增节点 * * @author wgy */public class ZKCreate &#123; String IP = \"192.168.142.128:2181\"; ZooKeeper zooKeeper; @Before public void before() throws Exception &#123; // 计数器对象 CountDownLatch countDownLatch = new CountDownLatch(1); // arg1:服务器的ip和端口 // arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的 // arg3:监视器对象 zooKeeper = new ZooKeeper(IP, 5000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; if (event.getState() == Event.KeeperState.SyncConnected) &#123; System.out.println(\"连接创建成功!\"); countDownLatch.countDown(); &#125; &#125; &#125;); // 主线程阻塞等待连接对象的创建成功 countDownLatch.await(); &#125; @After public void after() throws Exception &#123; zooKeeper.close(); &#125; /** * 持久化节点 * * @throws Exception */ @Test public void create1() throws Exception &#123; // arg1:节点的路径 // arg2:节点的数据 // arg3:权限列表 world:anyone:cdrwa // arg4:节点类型 持久化节点 zooKeeper.create(\"/create/node1\", \"node1\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; /** * 指定权限节点 * * @throws Exception */ @Test public void create2() throws Exception &#123; // Ids.READ_ACL_UNSAFE world:anyone:r zooKeeper.create(\"/create/node2\", \"node2\".getBytes(), ZooDefs.Ids.READ_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; /** * world授权模式 * * @throws Exception */ @Test public void create3() throws Exception &#123; // world授权模式 // 权限列表 List&lt;ACL&gt; acls = new ArrayList&lt;ACL&gt;(); // 授权模式和授权对象 Id id = new Id(\"world\", \"anyone\"); // 权限设置 acls.add(new ACL(ZooDefs.Perms.READ, id)); acls.add(new ACL(ZooDefs.Perms.WRITE, id)); zooKeeper.create(\"/create/node3\", \"node3\".getBytes(), acls, CreateMode.PERSISTENT); &#125; /** * ip授权模式 * * @throws Exception */ @Test public void create4() throws Exception &#123; // ip授权模式 // 权限列表 List&lt;ACL&gt; acls = new ArrayList&lt;ACL&gt;(); // 授权模式和授权对象 Id id = new Id(\"ip\", \"192.168.60.130\"); // 权限设置 acls.add(new ACL(ZooDefs.Perms.ALL, id)); zooKeeper.create(\"/create/node4\", \"node4\".getBytes(), acls, CreateMode.PERSISTENT); &#125; /** * auth授权模式 * * @throws Exception */ @Test public void create5() throws Exception &#123; // auth授权模式 // 添加授权用户 zooKeeper.addAuthInfo(\"digest\", \"wgy:123456\".getBytes()); zooKeeper.create(\"/create/node5\", \"node5\".getBytes(), ZooDefs.Ids.CREATOR_ALL_ACL, CreateMode.PERSISTENT); &#125; @Test public void create6() throws Exception &#123; // auth授权模式 // 添加授权用户 zooKeeper.addAuthInfo(\"digest\", \"wgy:123456\".getBytes()); // 权限列表 List&lt;ACL&gt; acls = new ArrayList&lt;ACL&gt;(); // 授权模式和授权对象 Id id = new Id(\"auth\", \"wgy\"); // 权限设置 acls.add(new ACL(ZooDefs.Perms.READ, id)); zooKeeper.create(\"/create/node6\", \"node6\".getBytes(), acls, CreateMode.PERSISTENT); &#125; /** * digest授权模式 * * @throws Exception */ @Test public void create7() throws Exception &#123; // digest授权模式 // 权限列表 List&lt;ACL&gt; acls = new ArrayList&lt;ACL&gt;(); // 授权模式和授权对象 Id id = new Id(\"digest\", \"haha:3RTWRMbFGscZqJBONcrAlMYMrv0=\"); // 权限设置 acls.add(new ACL(ZooDefs.Perms.ALL, id)); zooKeeper.create(\"/create/node7\", \"node7\".getBytes(), acls, CreateMode.PERSISTENT); &#125; /** * 持久化顺序节点 * * @throws Exception */ @Test public void create8() throws Exception &#123; // 持久化顺序节点 // Ids.OPEN_ACL_UNSAFE world:anyone:cdrwa String result = zooKeeper.create(\"/create/node8\", \"node8\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL); System.out.println(result); &#125; /** * 临时节点 * * @throws Exception */ @Test public void create9() throws Exception &#123; // 临时节点 // Ids.OPEN_ACL_UNSAFE world:anyone:cdrwa String result = zooKeeper.create(\"/create/node9\", \"node9\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(result); &#125; /** * 临时顺序节点 * * @throws Exception */ @Test public void create10() throws Exception &#123; // 临时顺序节点 // Ids.OPEN_ACL_UNSAFE world:anyone:cdrwa String result = zooKeeper.create(\"/create/node10\", \"node10\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(result); &#125; @Test public void create11() throws Exception &#123; // 异步方式创建节点 zooKeeper.create(\"/create/node11\", \"node11\".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT, new AsyncCallback.StringCallback() &#123; @Override public void processResult(int rc, String path, Object ctx, String name) &#123; // 0 代表创建成功 System.out.println(rc); // 节点的路径 System.out.println(path); // 节点的路径 System.out.println(name); // 上下文参数 I am context System.out.println(ctx); &#125; &#125;, \"I am context\"); Thread.sleep(10000); System.out.println(\"结束\"); &#125;&#125; 5.3 更新节点12345678910&#x2F;&#x2F; 同步方式setData(String path, byte[] data, int version)&#x2F;&#x2F; 异步方式setData(String path, byte[] data, int version，AsyncCallback.StatCallback callBack， Object ctx)path- znode路径data - 要存储在指定znode路径中的数据。version- znode的当前版本。每当数据更改时，ZooKeeper会更新znode的版本号。callBack-异步回调接口ctx-传递上下文参数 案例: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 更新节点 * * @author wgy */public class ZKSet &#123; String IP = \"192.168.142.128:2181\"; ZooKeeper zooKeeper; @Before public void before() throws Exception &#123; // 计数器对象 CountDownLatch countDownLatch = new CountDownLatch(1); // arg1:服务器的ip和端口 // arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的 // arg3:监视器对象 zooKeeper = new ZooKeeper(IP, 5000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; if (event.getState() == Event.KeeperState.SyncConnected) &#123; System.out.println(\"连接创建成功!\"); countDownLatch.countDown(); &#125; &#125; &#125;); // 主线程阻塞等待连接对象的创建成功 countDownLatch.await(); &#125; @After public void after() throws Exception &#123; zooKeeper.close(); &#125; @Test public void set1() throws Exception &#123; // arg1:节点的路径 // arg2:修改的数据 // arg3:数据版本号 -1代表版本号不参与更新 Stat stat = zooKeeper.setData(\"/set/node1\", \"node11\".getBytes(), 2); // 当前节点的版本号 System.out.println(stat.getVersion()); &#125; @Test public void set2() throws Exception &#123; zooKeeper.setData(\"/set/node1\", \"node14\".getBytes(), -1, new AsyncCallback.StatCallback() &#123; @Override public void processResult(int rc, String path, Object ctx, Stat stat) &#123; // 0代表修改成功 System.out.println(rc); // 节点的路径 System.out.println(path); // 上下文参数对象 I am Context System.out.println(ctx); // 属性描述对象 System.out.println(stat.getVersion()); &#125; &#125;, \"I am Context\"); Thread.sleep(10000); System.out.println(\"结束\"); &#125;&#125; 5.4 删除节点123456789&#x2F;&#x2F; 同步方式delete(String path, int version)&#x2F;&#x2F; 异步方式delete(String path, int version, AsyncCallback.VoidCallback callBack,Object ctx)path - znode路径。version - znode的当前版本callBack-异步回调接口ctx-传递上下文参数 案例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 删除节点 * * @author wgy */public class ZKDelete &#123; String IP = \"192.168.142.128:2181\"; ZooKeeper zooKeeper; @Before public void before() throws Exception &#123; // 计数器对象 CountDownLatch countDownLatch = new CountDownLatch(1); // arg1:服务器的ip和端口 // arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的 // arg3:监视器对象 zooKeeper = new ZooKeeper(IP, 5000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; if (event.getState() == Event.KeeperState.SyncConnected) &#123; System.out.println(\"连接创建成功!\"); countDownLatch.countDown(); &#125; &#125; &#125;); // 主线程阻塞等待连接对象的创建成功 countDownLatch.await(); &#125; @After public void after() throws Exception &#123; zooKeeper.close(); &#125; @Test public void delete1() throws Exception &#123; // arg1:删除节点的节点路径 // arg2:数据版本信息 -1代表删除节点时不考虑版本信息 zooKeeper.delete(\"/delete/node1\", -1); &#125; @Test public void delete2() throws Exception &#123; // 异步使用方式 zooKeeper.delete(\"/delete/node2\", -1, new AsyncCallback.VoidCallback() &#123; @Override public void processResult(int rc, String path, Object ctx) &#123; // 0代表删除成功 System.out.println(rc); // 节点的路径 System.out.println(path); // 上下文参数对象 I am Context System.out.println(ctx); &#125; &#125;, \"I am Context\"); Thread.sleep(10000); System.out.println(\"结束\"); &#125;&#125; 5.5 查看节点12345678910&#x2F;&#x2F; 同步方式getData(String path, boolean b, Stat stat)&#x2F;&#x2F; 异步方式getData(String path, boolean b，AsyncCallback.DataCallback callBack，Object ctx)path - znode路径。b- 是否使用连接对象中注册的监视器。stat - 返回znode的元数据。callBack-异步回调接口ctx-传递上下文参数 案例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * 查看节点 * * @author wgy */public class ZKGet &#123; String IP = \"192.168.142.128:2181\"; ZooKeeper zooKeeper; @Before public void before() throws Exception &#123; // 计数器对象 CountDownLatch countDownLatch = new CountDownLatch(1); // arg1:服务器的ip和端口 // arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的 // arg3:监视器对象 zooKeeper = new ZooKeeper(IP, 5000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; if (event.getState() == Event.KeeperState.SyncConnected) &#123; System.out.println(\"连接创建成功!\"); countDownLatch.countDown(); &#125; &#125; &#125;); // 主线程阻塞等待连接对象的创建成功 countDownLatch.await(); &#125; @After public void after() throws Exception &#123; zooKeeper.close(); &#125; @Test public void get1() throws Exception &#123; // arg1:节点的路径 // arg3:读取节点属性的对象 Stat stat = new Stat(); byte[] bys = zooKeeper.getData(\"/get/node1\", false, stat); // 打印数据 System.out.println(new String(bys)); // 版本信息 System.out.println(stat.getVersion()); &#125; @Test public void get2() throws Exception &#123; //异步方式 zooKeeper.getData(\"/get/node1\", false, new AsyncCallback.DataCallback() &#123; @Override public void processResult(int rc, String path, Object ctx, byte[] data, Stat stat) &#123; // 0代表读取成功 System.out.println(rc); // 节点的路径 System.out.println(path); // 上下文参数对象 I am Context System.out.println(ctx); // 数据 System.out.println(new String(data)); // 属性对象 System.out.println(stat.getVersion()); &#125; &#125;, \"I am Context\"); Thread.sleep(10000); System.out.println(\"结束\"); &#125;&#125; 5.6 查看子节点123456789&#x2F;&#x2F; 同步方式getChildren(String path, boolean b)&#x2F;&#x2F; 异步方式getChildren(String path, boolean b,AsyncCallback.ChildrenCallback callBack,Object ctx)path - Znode路径。b- 是否使用连接对象中注册的监视器。callBack - 异步回调接口。ctx-传递上下文参数 案例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 查看子节点 * * @author wgy */public class ZKGetChid &#123; String IP = \"192.168.142.128:2181\"; ZooKeeper zooKeeper; @Before public void before() throws Exception &#123; // 计数器对象 CountDownLatch countDownLatch = new CountDownLatch(1); // arg1:服务器的ip和端口 // arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的 // arg3:监视器对象 zooKeeper = new ZooKeeper(IP, 5000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; if (event.getState() == Event.KeeperState.SyncConnected) &#123; System.out.println(\"连接创建成功!\"); countDownLatch.countDown(); &#125; &#125; &#125;); // 主线程阻塞等待连接对象的创建成功 countDownLatch.await(); &#125; @After public void after() throws Exception &#123; zooKeeper.close(); &#125; @Test public void get1() throws Exception &#123; // arg1:节点的路径 List&lt;String&gt; list = zooKeeper.getChildren(\"/get\", false); for (String str : list) &#123; System.out.println(str); &#125; &#125; @Test public void get2() throws Exception &#123; // 异步用法 zooKeeper.getChildren(\"/get\", false, new AsyncCallback.ChildrenCallback() &#123; @Override public void processResult(int rc, String path, Object ctx, List&lt;String&gt; children) &#123; // 0代表读取成功 System.out.println(rc); // 节点的路径 System.out.println(path); // 上下文参数对象 I am Context System.out.println(ctx); // 子节点信息 for (String str : children) &#123; System.out.println(str); &#125; &#125; &#125;, \"I am Context\"); Thread.sleep(10000); System.out.println(\"结束\"); &#125;&#125; 5.7 检查节点是否存在123456789&#x2F;&#x2F; 同步方法exists(String path, boolean b)&#x2F;&#x2F; 异步方法exists(String path, boolean b，AsyncCallback.StatCallback callBack,Object ctx)path- znode路径。b- 是否使用连接对象中注册的监视器。callBack - 异步回调接口。ctx-传递上下文参数 案例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * 检查节点是否存在 * * @author wgy */public class ZKExists &#123; String IP = \"192.168.142.128:2181\"; ZooKeeper zooKeeper; @Before public void before() throws Exception &#123; // 计数器对象 CountDownLatch countDownLatch = new CountDownLatch(1); // arg1:服务器的ip和端口 // arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的 // arg3:监视器对象 zooKeeper = new ZooKeeper(IP, 5000, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; if (event.getState() == Event.KeeperState.SyncConnected) &#123; System.out.println(\"连接创建成功!\"); countDownLatch.countDown(); &#125; &#125; &#125;); // 主线程阻塞等待连接对象的创建成功 countDownLatch.await(); &#125; @After public void after() throws Exception &#123; zooKeeper.close(); &#125; @Test public void exists1() throws Exception &#123; // arg1:节点的路径 Stat stat = zooKeeper.exists(\"/exists1\", false); // 节点的版本信息 System.out.println(stat.getVersion()); &#125; @Test public void exists2() throws Exception &#123; // 异步方式 zooKeeper.exists(\"/exists1\", false, new AsyncCallback.StatCallback() &#123; @Override public void processResult(int rc, String path, Object ctx, Stat stat) &#123; // 0 代表方式执行成功 System.out.println(rc); // 节点的路径 System.out.println(path); // 上下文参数 I am Context System.out.println(ctx); // 节点的版本信息 System.out.println(stat.getVersion()); &#125; &#125;, \"I am Context\"); Thread.sleep(10000); System.out.println(\"结束\"); &#125;&#125;","tags":[{"name":"分布式架构方案","slug":"分布式架构方案","permalink":"https://wgy1993.gitee.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E6%96%B9%E6%A1%88/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://wgy1993.gitee.io/tags/zookeeper/"}]},{"title":"SpringMVC(三)","date":"2020-06-26T07:55:56.000Z","path":"archives/94a25162.html","text":"1. SSM 整合1.1 搭建整合环境1.1.1 整合说明SSM整合可以使用多种方式，选择XML + 注解的方式 1.1.2 整合的思路 先搭建整合的环境 先把Spring的配置搭建完成 再使用Spring整合SpringMVC框架 最后使用Spring整合MyBatis框架 1.1.3 创建数据库和表结构1234567create database ssm;use ssm;create table account( id int primary key auto_increment, name varchar(20), money double); 1.1.4 创建maven的工程1234&lt;groupId&gt;com.wgy&lt;/groupId&gt;&lt;artifactId&gt;ssm&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;war&lt;/packaging&gt; 1.1.5 导入依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;mysql.version&gt;5.1.6&lt;/mysql.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log start --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log end --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.1.6 编写实体类123456789101112/** * 帐户 * * @author wgy */public class Account implements Serializable &#123; private Integer id; private String name; private Double money; ...&#125; 1.1.7 编写dao接口123456789101112131415161718192021/** * 帐户dao接口 * * @author wgy */public interface AccountDao &#123; /** * 查询所有账户 * * @return */ public List&lt;Account&gt; findAll(); /** * 保存帐户信息 * * @param account */ public void saveAccount(Account account);&#125; 1.1.8 编写service接口和实现类接口: 12345678910111213141516171819202122/** * 账户业务接口 * * @author wgy */public interface AccountService &#123; /** * 查询所有账户 * * @return */ public List&lt;Account&gt; findAll(); /** * 保存帐户信息 * * @param account */ public void saveAccount(Account account);&#125; 实现类: 12345678910111213141516171819/** * 账户业务实现类 * * @author wgy */@Service(\"accountService\")public class AccountServiceImpl implements AccountService &#123; @Override public List&lt;Account&gt; findAll() &#123; System.out.println(\"业务层：查询所有账户...\"); return null; &#125; @Override public void saveAccount(Account account) &#123; System.out.println(\"业务层：保存帐户...\"); &#125;&#125; 1.2 整合步骤1.2.1 保证 Spring 框架在 web 工程中独立运行1.2.1.1 编写 spring 配置文件并导入约束123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!--开启注解的扫描，希望处理service和dao，controller不需要Spring框架去处理--&gt; &lt;context:component-scan base-package=\"com.wgy\"&gt; &lt;!--配置哪些注解不扫描--&gt; &lt;context:exclude-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/&gt; &lt;/context:component-scan&gt;&lt;/beans&gt; 1.2.1.2 测试 spring1234567891011121314151617/** * 测试spring * * @author wgy */public class SpringTest &#123; @Test public void run1() &#123; // 加载配置文件 ApplicationContext ac = new ClassPathXmlApplicationContext(\"classpath:applicationContext.xml\"); // 获取对象 AccountService as = (AccountService) ac.getBean(\"accountService\"); // 调用方法 as.findAll(); &#125;&#125; 1.2.2 保证 SpringMVC 在 web 工程中独立运行1.2.2.1 在 web.xml 中配置核心控制器123456789101112131415161718192021222324252627282930&lt;!--配置前端控制器--&gt;&lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--加载springmvc.xml配置文件--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!--启动服务器，创建该servlet--&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!--解决中文乱码的过滤器--&gt;&lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 1.2.2.2 编写 SpringMVC 的配置文件12345678910111213141516171819202122232425262728293031&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!--开启注解扫描，只扫描Controller注解--&gt; &lt;context:component-scan base-package=\"com.wgy\"&gt; &lt;context:include-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/&gt; &lt;/context:component-scan&gt; &lt;!--配置的视图解析器对象--&gt; &lt;bean id=\"internalResourceViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/pages/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt; &lt;/bean&gt; &lt;!--过滤静态资源--&gt; &lt;mvc:resources location=\"/css/\" mapping=\"/css/**\"/&gt; &lt;mvc:resources location=\"/images/\" mapping=\"/images/**\"/&gt; &lt;mvc:resources location=\"/js/\" mapping=\"/js/**\"/&gt; &lt;!--开启SpringMVC注解的支持--&gt; &lt;mvc:annotation-driven/&gt;&lt;/beans&gt; 1.2.2.3 编写 Controller 和 和 jsp 页面JSP: 1&lt;a href=\"account/findAll\"&gt;查询所有&lt;/a&gt; 12345678910111213141516&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" isELIgnored=\"false\" %&gt;&lt;%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h3&gt;查询所有的帐户&lt;/h3&gt;&lt;c:forEach items=\"$&#123;list&#125;\" var=\"account\"&gt; $&#123;account.name&#125;&lt;/c:forEach&gt;&lt;/body&gt;&lt;/html&gt; controller: 123456789101112131415/** * 帐户控制器 * * @author wgy */@Controller@RequestMapping(\"/account\")public class AccountController &#123; @RequestMapping(\"/findAll\") public String findAll(Model model) &#123; System.out.println(\"表现层：查询所有账户...\"); return \"list\"; &#125;&#125; 1.2.3 整合 Spring 和 SpringMVC目的：在controller中能成功的调用service对象中的方法。 1.2.3.1 配置监听器实现启动服务创建容器web.xml 123456789&lt;!--配置Spring的监听器，默认只加载WEB-INF目录下的applicationContext.xml配置文件--&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt;&lt;!--设置配置文件的路径--&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt; 1.2.3.2 在controller中注入service对象123456789101112131415161718192021/** * 帐户控制器 * * @author wgy */@Controller@RequestMapping(\"/account\")public class AccountController &#123; @Autowired private AccountService accountService; @RequestMapping(\"/findAll\") public String findAll(Model model) &#123; System.out.println(\"表现层：查询所有账户...\"); // 调用service的方法 List&lt;Account&gt; list = accountService.findAll(); model.addAttribute(\"list\", list); return \"list\"; &#125;&#125; 1.2.4 保证 MyBatis 框架在 web 工程中独立运行1.2.4.1 编写SqlMapConfig.xml的配置文件1234567891011121314151617181920212223&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;environments default=\"dev\"&gt; &lt;environment id=\"dev\"&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql:///ssm\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 使用的是注解 --&gt; &lt;mappers&gt; &lt;!-- 该包下所有的dao接口都可以使用 --&gt; &lt;package name=\"com.wgy.dao\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 1.2.4.2 AccountDao使用注解编写SQL语句123456789101112131415/** * 查询所有账户 * * @return */@Select(\"select * from account\")public List&lt;Account&gt; findAll();/** * 保存帐户信息 * * @param account */@Insert(\"insert into account (name,money) values (#&#123;name&#125;,#&#123;money&#125;)\")public void saveAccount(Account account); 1.2.4.3 测试MyBatis1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * 测试mybatis * * @author wgy */public class MyBatisTest &#123; /** * 测试查询 * * @throws Exception */ @Test public void run1() throws Exception &#123; // 加载配置文件 InputStream in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); // 创建SqlSessionFactory对象 SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); // 创建SqlSession对象 SqlSession session = factory.openSession(); // 获取到代理对象 AccountDao dao = session.getMapper(AccountDao.class); // 查询所有数据 List&lt;Account&gt; list = dao.findAll(); for (Account account : list) &#123; System.out.println(account); &#125; // 关闭资源 session.close(); in.close(); &#125; /** * 测试保存 * * @throws Exception */ @Test public void run2() throws Exception &#123; Account account = new Account(); account.setName(\"熊大\"); account.setMoney(400d); // 加载配置文件 InputStream in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); // 创建SqlSessionFactory对象 SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); // 创建SqlSession对象 SqlSession session = factory.openSession(); // 获取到代理对象 AccountDao dao = session.getMapper(AccountDao.class); // 保存 dao.saveAccount(account); // 提交事务 session.commit(); // 关闭资源 session.close(); in.close(); &#125;&#125; 1.2.5 整合 Spring 和 MyBatis目的：把SqlMapConfig.xml配置文件中的内容配置到applicationContext.xml配置文件中，SqlMapConfig.xml文件删除。 1.2.5.1 Spring 接管 MyBatis 的 Session 工厂12345678910111213&lt;!--Spring整合MyBatis框架--&gt;&lt;!--配置连接池--&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"jdbcUrl\" value=\"jdbc:mysql:///ssm\"/&gt; &lt;property name=\"user\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt;&lt;/bean&gt;&lt;!--配置SqlSessionFactory工厂--&gt;&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt; 1.2.5.2 配置自动扫描所有 Mapper 接口1234&lt;!--配置AccountDao接口所在包--&gt;&lt;bean id=\"mapperScanner\" class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.wgy.dao\"/&gt;&lt;/bean&gt; 1.2.5.3 配置 spring 的事务123456789101112131415161718&lt;!--配置Spring框架声明式事务管理--&gt;&lt;!--配置事务管理器--&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt;&lt;!--配置事务通知--&gt;&lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"find*\" read-only=\"true\" propagation=\"SUPPORTS\"/&gt; &lt;tx:method name=\"*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!--配置AOP增强--&gt;&lt;aop:config&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut=\"execution(* com.wgy.service.impl.*.*(..))\"/&gt;&lt;/aop:config&gt; 1.2.5.4 在service中注入dao对象1234567891011121314151617181920212223/** * 账户业务实现类 * * @author wgy */@Service(\"accountService\")public class AccountServiceImpl implements AccountService &#123; @Autowired private AccountDao accountDao; @Override public List&lt;Account&gt; findAll() &#123; System.out.println(\"业务层：查询所有账户...\"); return accountDao.findAll(); &#125; @Override public void saveAccount(Account account) &#123; System.out.println(\"业务层：保存帐户...\"); accountDao.saveAccount(account); &#125;&#125; 1.2.6 测试 SSM 整合结果1.2.6.1 编写测试 jsp12345678910111213141516171819&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;a href=\"account/findAll\"&gt;测试查询&lt;/a&gt;&lt;h3&gt;测试包&lt;/h3&gt;&lt;form action=\"account/save\" method=\"post\"&gt; 姓名：&lt;input type=\"text\" name=\"name\"/&gt;&lt;br/&gt; 金额：&lt;input type=\"text\" name=\"money\"/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"保存\"/&gt;&lt;br/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 1.2.6.2 修改控制器中的方法123456789101112131415161718192021222324252627282930313233/** * 帐户控制器 * * @author wgy */@Controller@RequestMapping(\"/account\")public class AccountController &#123; @Autowired private AccountService accountService; @RequestMapping(\"/findAll\") public String findAll(Model model) &#123; System.out.println(\"表现层：查询所有账户...\"); // 调用service的方法 List&lt;Account&gt; list = accountService.findAll(); model.addAttribute(\"list\", list); return \"list\"; &#125; /** * 保存 * * @return */ @RequestMapping(\"/save\") public void save(Account account, HttpServletRequest request, HttpServletResponse response) throws IOException &#123; accountService.saveAccount(account); response.sendRedirect(request.getContextPath() + \"/account/findAll\"); return; &#125;&#125;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://wgy1993.gitee.io/tags/SpringMVC/"}]},{"title":"SpringMVC(二)","date":"2020-06-25T15:07:54.000Z","path":"archives/ea517149.html","text":"1. 响应数据和结果视图1.1 返回值分类1.1.1 void如果控制器的方法返回值编写成void，执行程序报404的异常，默认查找JSP页面没有找到。 默认会跳转到@RequestMapping(value=”/testVoid”) testVoid的页面。 可以使用请求转发或者重定向跳转到指定的页面 123456789101112131415161718192021/** * 是void * 请求转发一次请求，不用编写项目的名称 */@RequestMapping(\"/testVoid\")public void testVoid(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; System.out.println(\"testVoid方法执行了...\"); // 编写请求转发的程序 // request.getRequestDispatcher(\"/WEB-INF/pages/success.jsp\").forward(request,response); // 重定向 // response.sendRedirect(request.getContextPath()+\"/index.jsp\"); // 设置中文乱码 response.setCharacterEncoding(\"UTF-8\"); response.setContentType(\"text/html;charset=UTF-8\"); // 直接会进行响应 response.getWriter().print(\"你好\"); return;&#125; 1.1.2 字符串controller 方法返回字符串可以指定逻辑视图名，通过视图解析器解析为物理视图地址。 123456789101112131415161718/** * 返回String * 指定逻辑视图名，经过视图解析器解析为 jsp 物理路径：/WEB-INF/pages/success.jsp * @param model * @return */@RequestMapping(\"/testString\")public String testString(Model model) &#123; System.out.println(\"testString方法执行了...\"); // 模拟从数据库中查询出User对象 User user = new User(); user.setUsername(\"美美\"); user.setPassword(\"123\"); user.setAge(30); // model对象 model.addAttribute(\"user\", user); return \"success\";&#125; 1.1.3 ModelAndViewModelAndView 是 SpringMVC 为我们提供的一个对象，该对象也可以用作控制器方法的返回值。 该对象中有两个方法： 示例代码: 123456789101112131415161718192021/** * 返回ModelAndView * * @return */@RequestMapping(\"/testModelAndView\")public ModelAndView testModelAndView() &#123; // 创建ModelAndView对象 ModelAndView mv = new ModelAndView(); System.out.println(\"testModelAndView方法执行了...\"); // 模拟从数据库中查询出User对象 User user = new User(); user.setUsername(\"小凤\"); user.setPassword(\"456\"); user.setAge(30); // 把user对象存储到mv对象中，也会把user对象存入到request对象 mv.addObject(\"user\", user); // 跳转到哪个页面 mv.setViewName(\"success\"); return mv;&#125; JSP: 1234567891011&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" isELIgnored=\"false\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h3&gt;执行成功&lt;/h3&gt; $&#123;user.username&#125; $&#123;user.password&#125;&lt;/body&gt;&lt;/html&gt; 1.2 转发和重定向1.2.1 forward 转发controller 方法在提供了 String 类型的返回值之后，默认就是请求转发。我们也可以写成： 1234567891011/** * 使用关键字的方式进行转发 * * @return */@RequestMapping(\"/testForward\")public String testForward() &#123; System.out.println(\"testForward方法执行了...\"); // 请求的转发 return \"forward:/WEB-INF/pages/success.jsp\";&#125; 需要注意的是，如果用了 formward ：则路径必须写成实际视图 url，不能写逻辑视图。 它相当于“request.getRequestDispatcher(“url”).forward(request,response)”。使用请求转发，既可以转发到 jsp，也可以转发到其他的控制器方法。 1.2.2 Redirect 重定向contrller 方法提供了一个 String 类型返回值之后，它需要在返回值里使用redirect: 1234567891011/** * 使用关键字的方式进行转发或者重定向 * * @return */@RequestMapping(\"/testRedirect\")public String testRedirect() &#123; System.out.println(\"testRedirect方法执行了...\"); // 重定向 return \"redirect:/index.jsp\";&#125; 它相当于“response.sendRedirect(url)”。需要注意的是，如果是重定向到 jsp 页面，则 jsp 页面不能写在 WEB-INF 目录中，否则无法找到。 1.3 @ResponseBody 响应 json 数据1.3.1 &lt;mvc:resources/&gt;DispatcherServlet会拦截到所有的资源，导致一个问题就是静态资源（img、css、js）也会被拦截到，从而不能被使用。解决问题就是需要配置静态资源不进行拦截，在springmvc.xml配置文件添加如下配置 12345678&lt;!--前端控制器，哪些静态资源不拦截 mvc:resources标签配置不过滤 location元素表示webapp目录下的包下的所有文件 mapping元素表示以/static开头的所有请求路径，如/static/a 或者/static/a/b--&gt;&lt;mvc:resources location=\"/css/\" mapping=\"/css/**\"/&gt; &lt;!-- 样式 --&gt;&lt;mvc:resources location=\"/images/\" mapping=\"/images/**\"/&gt; &lt;!-- 图片 --&gt;&lt;mvc:resources location=\"/js/\" mapping=\"/js/**\"/&gt; &lt;!-- javascript --&gt; 1.3.2 @RequestBody获取请求体数据JSP: 123456789101112131415161718192021222324252627282930&lt;head&gt; &lt;script src=\"js/jquery.min.js\"&gt;&lt;/script&gt; &lt;script&gt; // 页面加载，绑定单击事件 $(function () &#123; $(\"#btn\").click(function () &#123; // alert(\"hello btn\"); // 发送ajax请求 $.ajax(&#123; // 编写json格式，设置属性和值 url: \"user/testAjax\", contentType: \"application/json;charset=UTF-8\", data: '&#123;\"username\":\"hehe\",\"password\":\"123\",\"age\":30&#125;', dataType: \"json\", type: \"post\", success: function (data) &#123; // data服务器端响应的json的数据，进行解析 alert(data); alert(data.username); alert(data.password); alert(data.age); &#125; &#125;); &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;button id=\"btn\"&gt;发送ajax的请求&lt;/button&gt;&lt;/body&gt; 控制器: 12345678910/** * 获取请求体的数据 */@RequestMapping(\"/testAjax\")public void testAjax(@RequestBody String body)) &#123; System.out.println(\"testAjax方法执行了...\"); // 客户端发送ajax的请求，传的是json字符串 System.out.println(body); return;&#125; 1.3.3 json 字符串转换成JavaBean 对象12345678910/** * 模拟异步请求响应 */@RequestMapping(\"/testAjax\")public void testAjax(@RequestBody User user) &#123; System.out.println(\"testAjax方法执行了...\"); // 客户端发送ajax的请求，传的是json字符串，后端把json字符串封装到user对象中 System.out.println(user); return;&#125; 1.3.4 @ResponseBody响应JSON数据1234567891011121314/** * 模拟异步请求响应 */@RequestMapping(\"/testAjax\")public @ResponseBody User testAjax(@RequestBody User user) &#123; System.out.println(\"testAjax方法执行了...\"); // 客户端发送ajax的请求，传的是json字符串，后端把json字符串封装到user对象中 System.out.println(user); // 做响应，模拟查询数据库 user.setUsername(\"haha\"); user.setAge(40); // 做响应 return user;&#125; json字符串和JavaBean对象互相转换的过程中，需要使用jackson的jar包： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 2. SpringMVC实现文件上传2.1 文件上传的回顾2.1.1 文件上传的必要前提1234567form 表单的 enctype 取值必须是：multipart&#x2F;form-data (默认值是:application&#x2F;x-www-form-urlencoded) enctype:是表单请求正文的类型 method 属性取值必须是 Post提供一个文件选择域&lt;input type&#x3D;”file” &#x2F;&gt; 2.1.2 文件上传的原理分析12345678910111213141516当 form 表单的 enctype 取值不是默认值后，request.getParameter()将失效。enctype&#x3D;”application&#x2F;x-www-form-urlencoded”时，form 表单的正文内容是： key&#x3D;value&amp;key&#x3D;value&amp;key&#x3D;value当 form 表单的 enctype 取值为 Mutilpart&#x2F;form-data 时，请求正文内容就变成：每一部分都是 MIME 类型描述的正文-----------------------------7de1a433602ac 分界符Content-Disposition: form-data; name&#x3D;&quot;userName&quot; 协议头aaa 协议的正文-----------------------------7de1a433602acContent-Disposition: form-data; name&#x3D;&quot;file&quot;;filename&#x3D;&quot;C:\\Users\\zhy\\Desktop\\fileupload_demofile\\b.txt&quot;Content-Type: text&#x2F;plain 协议的类型（MIME 类型）bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb-----------------------------7de1a433602ac-- 2.1.3 文件上传2.1.3.1 导jar包12345678910&lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt;&lt;/dependency&gt; 2.1.3.2 编写文件上传的JSP页面123456&lt;h3&gt;传统文件上传&lt;/h3&gt;&lt;form action=\"/user/fileupload1\" method=\"post\" enctype=\"multipart/form-data\"&gt; 选择文件：&lt;input type=\"file\" name=\"upload\"/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"上传\"/&gt;&lt;/form&gt; 2.1.3.3 Controller控制器123456789101112131415161718192021222324252627282930313233343536373839404142/** * 文件上传 * * @return */@RequestMapping(\"/fileupload1\")public String fileuoload1(HttpServletRequest request) throws Exception &#123; System.out.println(\"文件上传...\"); // 使用fileupload组件完成文件上传 // 上传的位置 String path = request.getSession().getServletContext().getRealPath(\"/uploads/\"); // 判断，该路径是否存在 File file = new File(path); if (!file.exists()) &#123; // 创建该文件夹 file.mkdirs(); &#125; // 解析request对象，获取上传文件项 DiskFileItemFactory factory = new DiskFileItemFactory(); ServletFileUpload upload = new ServletFileUpload(factory); // 解析request List&lt;FileItem&gt; items = upload.parseRequest(request); // 遍历 for (FileItem item : items) &#123; // 进行判断，当前item对象是否是上传文件项 if (item.isFormField()) &#123; // 说明普通表单向 &#125; else &#123; // 说明上传文件项 // 获取上传文件的名称 String filename = item.getName(); // 把文件的名称设置唯一值，uuid String uuid = UUID.randomUUID().toString().replace(\"-\", \"\"); filename = uuid + \"_\" + filename; // 完成文件上传 item.write(new File(path, filename)); // 删除临时文件 item.delete(); &#125; &#125; return \"success\";&#125; 2.2 SpringMVC文件上传SpringMVC框架提供了MultipartFile对象，该对象表示上传的文件，要求变量名称必须和表单file标签的 name属性名称相同。 2.2.1 导jar包12345678910&lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt;&lt;/dependency&gt; 2.2.2 编写文件上传的JSP页面123456&lt;h3&gt;Springmvc文件上传&lt;/h3&gt;&lt;form action=\"/user/fileupload2\" method=\"post\" enctype=\"multipart/form-data\"&gt; 选择文件：&lt;input type=\"file\" name=\"upload\"/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"上传\"/&gt;&lt;/form&gt; 2.2.3 Controller控制器12345678910111213141516171819202122232425262728/** * SpringMVC文件上传 * * @return */@RequestMapping(\"/fileupload2\")public String fileuoload2(HttpServletRequest request, MultipartFile upload) throws Exception &#123; System.out.println(\"springmvc文件上传...\"); // 使用fileupload组件完成文件上传 // 上传的位置 String path = request.getSession().getServletContext().getRealPath(\"/uploads/\"); // 判断，该路径是否存在 File file = new File(path); if (!file.exists()) &#123; // 创建该文件夹 file.mkdirs(); &#125; // 说明上传文件项 // 获取上传文件的名称 String filename = upload.getOriginalFilename(); // 把文件的名称设置唯一值，uuid String uuid = UUID.randomUUID().toString().replace(\"-\", \"\"); filename = uuid + \"_\" + filename; // 完成文件上传 upload.transferTo(new File(path, filename)); return \"success\";&#125; 2.2.4 配置文件解析器对象123456&lt;!--配置文件解析器对象--&gt;&lt;!--id 的值是固定的--&gt;&lt;bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\"&gt; &lt;!-- 设置上传文件的最大尺寸为 10MB --&gt; &lt;property name=\"maxUploadSize\" value=\"10485760\"/&gt;&lt;/bean&gt; 2.3 SpringMVC跨服务器方式文件上传2.3.1 分服务器的目的12345在实际开发中，我们会有很多处理不同功能的服务器。例如： 应用服务器：负责部署我们的应用 数据库服务器：运行我们的数据库 缓存和消息服务器：负责处理大并发访问的缓存和消息 文件服务器：负责存储用户上传文件的服务器。 (注意：此处说的不是服务器集群） 分服务器处理的目的是让服务器各司其职，从而提高我们项目 2.3.2 搭建图片服务器配置端口不同的Tomcat项目 2.3.3 实现SpringMVC跨服务器方式文件上传2.3.3.1 导jar包1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-core&lt;/artifactId&gt; &lt;version&gt;1.18.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-client&lt;/artifactId&gt; &lt;version&gt;1.18.1&lt;/version&gt;&lt;/dependency&gt; 2.3.3.2 编写文件上传的JSP页面123456&lt;h3&gt;跨服务器文件上传&lt;/h3&gt;&lt;form action=\"/user/fileupload3\" method=\"post\" enctype=\"multipart/form-data\"&gt; 选择文件：&lt;input type=\"file\" name=\"upload\"/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"上传\"/&gt;&lt;/form&gt; 2.3.3.3 Controller控制器123456789101112131415161718192021222324/** * 跨服务器文件上传 * * @return */@RequestMapping(\"/fileupload3\")public String fileuoload3(MultipartFile upload) throws Exception &#123; System.out.println(\"跨服务器文件上传...\"); // 定义上传文件服务器路径 String path = \"http://localhost:9090/uploads/\"; // 说明上传文件项 // 获取上传文件的名称 String filename = upload.getOriginalFilename(); // 把文件的名称设置唯一值，uuid String uuid = UUID.randomUUID().toString().replace(\"-\", \"\"); filename = uuid + \"_\" + filename; // 创建 sun 公司提供的 jersey 包中的 Client 对象 Client client = Client.create(); // 和图片服务器进行连接 WebResource webResource = client.resource(path + filename); // 上传文件 webResource.put(upload.getBytes()); return \"success\";&#125; 2.3.3.4 配置文件解析器对象123456&lt;!--配置文件解析器对象--&gt;&lt;!--id 的值是固定的--&gt;&lt;bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\"&gt; &lt;!-- 设置上传文件的最大尺寸为 10MB --&gt; &lt;property name=\"maxUploadSize\" value=\"10485760\"/&gt;&lt;/bean&gt; 3. SpringMVC的异常处理3.1 异常处理思路系统中异常包括两类：预期异常和运行时异常 RuntimeException，前者通过捕获异常从而获取异常信息，后者主要通过规范代码开发、测试通过手段减少运行时异常的发生。 系统的 dao、service、controller 出现都通过 throws Exception 向上抛出，最后由 springmvc 前端控制器交由异常处理器进行异常处理，如下图： 3.2 SpringMVC的异常处理3.2.1 自定义异常类1234567891011121314151617181920212223/** * 自定义异常类 * * @author wgy */public class SysException extends Exception &#123; // 存储提示信息的 private String message; public SysException(String message) &#123; this.message = message; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125;&#125; 3.2.2 控制器123456789101112131415161718192021222324/** * 控制器 * * @author wgy */@Controller@RequestMapping(\"/user\")public class UserController &#123; @RequestMapping(\"/testException\") public String testException() throws SysException &#123; System.out.println(\"testException执行了...\"); try &#123; // 模拟异常 int a = 10 / 0; &#125; catch (Exception e) &#123; // 打印异常信息 e.printStackTrace(); // 抛出自定义异常信息 throw new SysException(\"查询所有用户出现错误了...\"); &#125; return \"success\"; &#125;&#125; 3.2.3 自定义异常处理器1234567891011121314151617181920212223242526272829303132/** * 异常处理器 * * @author wgy */public class SysExceptionResolver implements HandlerExceptionResolver &#123; /** * 处理异常业务逻辑 * * @param request * @param response * @param handler * @param ex * @return */ @Override public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; // 获取到异常对象 SysException e = null; if (ex instanceof SysException) &#123; e = (SysException) ex; &#125; else &#123; e = new SysException(\"系统正在维护....\"); &#125; // 创建ModelAndView对象 ModelAndView mv = new ModelAndView(); mv.addObject(\"errorMsg\", e.getMessage()); mv.setViewName(\"error\"); return mv; &#125;&#125; 3.2.4 配置异常处理器12&lt;!--配置异常处理器--&gt;&lt;bean id=\"sysExceptionResolver\" class=\"com.wgy.exception.SysExceptionResolver\"/&gt; 4. SpringMVC框架中的拦截器4.1 拦截器的概述SpringMVC框架中的拦截器用于对处理器进行预处理和后处理的技术。 可以定义拦截器链，连接器链就是将拦截器按着一定的顺序结成一条链，在访问被拦截的方法时，拦截器链中的拦截器会按着定义的顺序执行。 拦截器和过滤器的功能比较类似，有区别 过滤器是 servlet 规范中的一部分，任何 java web 工程都可以使用。 拦截器是 SpringMVC 框架自己的，只有使用了 SpringMVC 框架的工程才能用。 过滤器在 url-pattern 中配置了/*之后，可以对所有要访问的资源拦截。 拦截器它是只会拦截访问的控制器方法，如果访问的是 jsp，html,css,image 或者 js 是不会进行拦截的。 拦截器也是AOP思想的一种实现方式。 想要自定义拦截器，需要实现HandlerInterceptor接口。 4.2 自定义拦截器步骤4.2.1 实现HandlerInterceptor接口123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 自定义拦截器 * * @author wgy */public class MyInterceptor1 implements HandlerInterceptor &#123; /** * 预处理，controller方法执行前 * return true 放行，执行下一个拦截器，如果没有，执行controller中的方法 * return false不放行 * * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(\"preHandle 拦截器拦截了\"); // request.getRequestDispatcher(\"/WEB-INF/pages/error.jsp\").forward(request,response); return true; &#125; /** * 后处理方法，controller方法执行后，success.jsp执行之前 * * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println(\"postHandle 方法执行了\"); // request.getRequestDispatcher(\"/WEB-INF/pages/error.jsp\").forward(request,response); &#125; /** * success.jsp页面执行后，该方法会执行 * * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; System.out.println(\"afterCompletion 方法执行了\"); &#125;&#125; 4.2.2 配置拦截器123456789101112&lt;!--配置拦截器--&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;!--要拦截的具体的方法--&gt; &lt;mvc:mapping path=\"/user/*\"/&gt; &lt;!--不要拦截的方法 &lt;mvc:exclude-mapping path=\"\"/&gt; --&gt; &lt;!--配置拦截器对象--&gt; &lt;bean class=\"com.wgy.interceptor.MyInterceptor1\"/&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 4.2.3 运行结果 4.3 拦截器的细节4.3.1 HandlerInterceptor接口中的方法preHandle方法是controller方法执行前拦截的方法 可以使用request或者response跳转到指定的页面 return true放行，执行下一个拦截器，如果没有拦截器，执行controller中的方法。 return false不放行，不会执行controller中的方法。 postHandle是controller方法执行后执行的方法，在JSP视图执行前。 可以使用request或者response跳转到指定的页面 如果指定了跳转的页面，那么controller方法跳转的页面将不会显示。 postHandle方法是在JSP执行后执行 request或者response不能再跳转页面了 4.3.2 多个拦截器的执行顺序多个拦截器是按照配置的顺序决定的。先进后出 12345678910111213141516171819202122&lt;!--配置拦截器--&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;!--要拦截的具体的方法--&gt; &lt;mvc:mapping path=\"/user/*\"/&gt; &lt;!--不要拦截的方法 &lt;mvc:exclude-mapping path=\"\"/&gt; --&gt; &lt;!--配置拦截器对象--&gt; &lt;bean class=\"com.wgy.interceptor.HandlerInterceptorDemo1\"/&gt; &lt;/mvc:interceptor&gt; &lt;!--配置第二个拦截器--&gt; &lt;mvc:interceptor&gt; &lt;!--要拦截的具体的方法--&gt; &lt;mvc:mapping path=\"/**\"/&gt; &lt;!--不要拦截的方法 &lt;mvc:exclude-mapping path=\"\"/&gt; --&gt; &lt;!--配置拦截器对象--&gt; &lt;bean class=\"com.wgy.interceptor.HandlerInterceptorDemo2\"/&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://wgy1993.gitee.io/tags/SpringMVC/"}]},{"title":"SpringMVC(一)","date":"2020-06-24T11:53:20.000Z","path":"archives/4560ea2b.html","text":"1. SpringMVC 的基本概念1.1 三层架构和MVC1.1.1 三层架构开发服务器端程序，一般都基于两种形式，一种C/S(客户端/服务器)架构程序，一种B/S(浏览器/服务器)架构程序。 使用Java语言基本上都是开发B/S架构的程序，B/S架构又分成了三层架构 ： 表现层：WEB层，用来和客户端进行数据交互的。表现层一般会采用MVC的设计模型 业务层：处理公司具体的业务逻辑的 持久层：用来操作数据库的 1.1.2 MVC模型MVC 全名是 Model View Controller，是模型(model)－视图(view)－控制器(controller)的缩写，是一种用于设计创建 Web 应用程序表现层的模式。MVC 中每个部分各司其职： Model：数据模型，JavaBean的类，用来进行数据封装。 View：指JSP、HTML用来展示数据给用户 Controller：用来接收用户的请求，整个流程的控制器。用来进行数据校验等。 1.2 SpringMVC 概述1.2.1 SpringMVC 是什么 是一种基于Java实现的MVC设计模型的请求驱动类型的轻量级WEB框架。 Spring MVC属于SpringFrameWork的后续产品，已经融合在Spring Web Flow里面。Spring 框架提供了构建Web 应用程序的全功能 MVC 模块。 使用 Spring 可插入的 MVC 架构，从而在使用Spring进行WEB开发时，可以选择使用Spring的 SpringMVC框架或集成其他MVC开发框架，如Struts1(现在一般不用)，Struts2等。 1.2.2 SpringMVC 在三层架构的位置 1.2.3 SpringMVC 和 Struts2 的优略分析共同点： 它们都是表现层框架，都是基于 MVC 模型编写的。 它们的底层都离不开原始 ServletAPI。 它们处理请求的机制都是一个核心控制器。 区别： Spring MVC 的入口是 Servlet, 而 Struts2 是 Filter Spring MVC 是基于方法设计的，而 Struts2 是基于类，Struts2 每次执行都会创建一个动作类。所以 Spring MVC 会稍微比 Struts2 快些。 Spring MVC 使用更加简洁,同时还支持 JSR303, 处理 ajax 的请求更方便(JSR303 是一套 JavaBean 参数校验的标准，它定义了很多常用的校验注解，我们可以直接将这些注解加在我们 JavaBean 的属性上面，就可以在需要校验的时候进行校验了。) Struts2 的 OGNL 表达式使页面的开发效率相比 Spring MVC 更高些，但执行效率并没有比 JSTL 提升，尤其是 struts2 的表单标签，远没有 html 执行效率高。 2. SpringMVC 的入门2.1 SpringMVC 的入门案例2.1.1 创建WEB工程，引入jar包1234567891011121314151617181920212223242526272829303132333435363738&lt;groupId&gt;com.wgy&lt;/groupId&gt;&lt;artifactId&gt;SpringMVC01_start&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;war&lt;/packaging&gt;&lt;!-- 版本锁定 --&gt;&lt;properties&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.1.2 配置核心的控制器12345678910111213141516&lt;!-- 配置 spring mvc 的核心控制器 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 配置初始化参数，用于读取 SpringMVC 的配置文件 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- 配置 servlet 的对象的创建时间点：应用加载时创建。取值只能是非 0 正整数，表示启动顺序 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 2.1.3 编写springmvc.xml的配置文件123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=\"com.wgy\"/&gt; &lt;!-- 视图解析器对象 --&gt; &lt;bean id=\"internalResourceViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/pages/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt; &lt;/bean&gt; &lt;!-- 开启SpringMVC框架注解的支持 --&gt; &lt;mvc:annotation-driven/&gt;&lt;/beans&gt; 2.1.4 编写index.jsp1234&lt;body&gt;&lt;h3&gt;入门案例&lt;/h3&gt; &lt;a href=\"$&#123; pageContext.request.contextPath &#125;/hello\"&gt;入门案例&lt;/a&gt;&lt;/body&gt; 2.1.5 编写HelloController控制器类12345678910111213141516171819/** * 控制器类 * * @author wgy */@Controllerpublic class HelloController &#123; /** * 入门案例 * * @return */ @RequestMapping(path = \"/hello\") public String sayHello() &#123; System.out.println(\"Hello StringMVC\"); return \"success\"; &#125;&#125; 2.1.6 编写success.jsp123&lt;body&gt; &lt;h3&gt;入门成功！！&lt;/h3&gt;&lt;/body&gt; 2.1.7 测试启动Tomcat服务器，进行测试 2.2 入门案例的执行过程及原理分析2.2.1 案例的执行过程 服务器启动，应用被加载。读取到 web.xml 中的配置创建 spring 容器并且初始化容器中的对象。 浏览器发送请求，被 DispatherServlet 捕获，该 Servlet 并不处理请求，而是把请求转发出去。转发的路径是根据请求 URL，匹配@RequestMapping 中的内容。 匹配到了后，执行对应方法。该方法有一个返回值。 根据方法的返回值，借助 InternalResourceViewResolver 找到对应的结果视图。 渲染结果视图，响应浏览器。 2.2.2 SpringMVC 的请求响应流程 2.3 入门案例中涉及的组件2.3.1 前端控制器（DispatcherServlet）用户请求到达前端控制器，它就相当于 mvc 模式中的 c，dispatcherServlet 是整个流程控制的中心，由它调用其它组件处理用户的请求，dispatcherServlet 的存在降低了组件之间的耦合性。 2.3.2 处理器映射器（HandlerMapping）HandlerMapping 负责根据用户请求找到 Handler 即处理器，SpringMVC 提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 2.3.3 处理器（Handler）它就是我们开发中要编写的具体业务控制器。由 DispatcherServlet 把用户请求转发到 Handler。由Handler 对具体的用户请求进行处理。 2.3.4 处理器适配器（HandlAdapter）通过 HandlerAdapter 对处理器进行执行 2.3.5 视图解析器（View Resolver）View Resolver 负责将处理结果生成 View 视图，View Resolver 首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成 View 视图对象，最后对 View 进行渲染将处理结果通过页面展示给用户。 2.3.6 视图（View）SpringMVC 框架提供了很多的 View 视图类型的支持，包括：jstlView、freemarkerView、pdfView等。我们最常用的视图就是 jsp。 2.3.7 &lt;mvc:annotation-driven&gt; 说明在 SpringMVC 的各个组件中，处理器映射器、处理器适配器、视图解析器称为 SpringMVC 的三大组件。 使 用 &lt;mvc:annotation-driven&gt; 自 动加载 RequestMappingHandlerMapping （处理映射器） 和RequestMappingHandlerAdapter （ 处 理 适 配 器 ） ， 可 用 在 SpringMVC.xml 配 置 文 件 中 使 用&lt;mvc:annotation-driven&gt;替代注解处理器和适配器的配置。 2.4 RequestMapping 注解2.4.1 使用说明RequestMapping注解的作用是建立请求URL和处理方法之间的对应关系 RequestMapping注解可以作用在方法和类上 作用在类上：第一级的访问目录 作用在方法上：第二级的访问目录 RequestMapping的属性 path：指定请求路径的url value：value属性和path属性是一样的 mthod：指定该方法的请求方式 params：指定限制请求参数的条件。例如： params = {“accountName”}，表示请求参数必须有 accountName params = {“moeny!100”}，表示请求参数中 money 不能是 100。 headers：发送的请求中必须包含的请求头 2.4.2 使用示例2.4.2.1 出现位置的示例控制器： 1234567891011121314151617181920/** * 控制器类 * * @author wgy */@Controller(\"accountController\")@RequestMapping(path = \"/account\")public class AccountController &#123; /** * RequestMapping注解 * * @return */ @RequestMapping(value = \"/findAccount\") public String findAccount() &#123; System.out.println(\"查询了账户。。。。\"); return \"success\"; &#125;&#125; JSP: 12345678&lt;body&gt; &lt;!-- 第一种访问方式 --&gt; &lt;a href=\"$&#123;pageContext.request.contextPath&#125;/account/findAccount\"&gt;查询账户&lt;/a&gt; &lt;br/&gt; &lt;!-- 第二种访问方式 --&gt; &lt;a href=\"account/findAccount\"&gt;查询账户&lt;/a&gt;&lt;/body&gt; 在 jsp 中第二种写法时，不要在访问 URL 前面加/ ，否则无法找到资源。 2.4.2.2 method 属性的示例控制器： 12345678910/** * 保存账户 * * @return */@RequestMapping(value=\"/saveAccount\",method=RequestMethod.POST)public String saveAccount() &#123; System.out.println(\"保存了账户\"); return \"success\";&#125; JSP: 1234567&lt;!-- 请求方式的示例 --&gt;&lt;a href=\"account/saveAccount\"&gt;保存账户，get 请求&lt;/a&gt;&lt;br/&gt;&lt;form action=\"account/saveAccount\" method=\"post\"&gt; &lt;input type=\"submit\" value=\" 保存账户， post 请求 \"&gt;&lt;/form&gt; 当使用 get 请求时，提示错误信息是 405，信息是方法不支持 get 方式请求 2.4.2.3 params 属性的示例控制器： 123456789/** * 删除账户 * @return */@RequestMapping(value=\"/removeAccount\",params= &#123;\"accountName\",\"money&gt;100\"&#125;)public String removeAccount() &#123; System.out.println(\"删除了账户\"); return \"success\";&#125; JSP: 12345&lt;!-- 请求参数的示例 --&gt;&lt;a href=\"account/removeAccount?accountName=aaa&amp;money&gt;100\"&gt;删除账户，金额 100&lt;/a&gt;&lt;br/&gt;&lt;a href=\"account/removeAccount?accountName=aaa&amp;money&gt;150\"&gt;删除账户，金额 150&lt;/a&gt; 当我们点击第一个超链接时,可以访问成功。当我们点击第二个超链接时，无法访问。如下图： 3. 请求参数的绑定3.1 绑定说明3.1.1 绑定的机制表单中请求参数都是基于 key=value 的。SpringMVC 绑定请求参数的过程是通过把表单提交请求参数，作为控制器中方法参数进行绑定的。 JSP: 123&lt;a href=\"account/findAccount?accountId=10\"&gt;查询账户&lt;/a&gt;请求参数是：accountId=10 控制器： 12345678910/** * 查询账户 * * @return */@RequestMapping(\"/findAccount\")public String findAccount(Integer accountId) &#123; System.out.println(\"查询了账户。。。。\"+accountId); return \"success\";&#125; 3.1.2 支持的数据类型12345678基本类型参数 ： 包括基本类型和 String 类型POJO 类型参数 ： 包括实体类，以及关联的实体类数组和集合类型参数 ： 包括 List 结构和 Map 结构的集合（包括数组）SpringMVC 绑定请求参数是自动实现的，但是要想使用，必须遵循使用要求。 3.1.3 使用要求12345678910111213如果是基本类型或者 String 类型： 要求我们的参数名称必须和控制器中方法的形参名称保持一致。(严格区分大小写)如果是 POJO 类型，或者它的关联对象 ： 要求表单中参数名称和 POJO 类的属性名称保持一致。并且控制器方法的参数类型是 POJO 类型。如果是集合类型, 有两种方式： 第一种： 要求集合类型的请求参数必须在 POJO 中。在表单中请求参数名称要和 POJO 中集合属性名称相同。 给 List 集合中的元素赋值，使用下标。 给 Map 集合中的元素赋值，使用键值对。 第二种： 接收的请求参数是 json 格式数据。需要借助一个注解实现。它可以实现一些数据类型自动转换,如遇特殊类型转换要求，需要我们自己编写自定义类型转换器。 3.1.4 使用示例3.1.4.1 基本类型和 String 类型作为参数JSP: 12&lt;!-- 基本类型示例 --&gt;&lt;a href=\"account/findAccount?accountId=10&amp;accountName=zhangsan\"&gt;查询账户&lt;/a&gt; 控制器: 12345678910/** * 查询账户 * * @return */@RequestMapping(\"/findAccount\")public String findAccount(Integer accountId,String accountName) &#123; System.out.println(\"查询了账户。。。。\"+accountId+\",\"+accountName); return \"success\";&#125; 运行结果： 3.1.4.2 POJO 类型作为参数实体类： 123456789101112/** * 账户信息 * * @author wgy */public class Account implements Serializable &#123; private Integer id; private String name; private Float money; private Address address; ...&#125; 12345678910/** * 地址的实体类 * * @author wgy */public class Address implements Serializable &#123; private String provinceName; private String cityName; ...&#125; JSP: 12345678&lt;!-- pojo 类型演示 --&gt;&lt;form action=\"account/saveAccount\" method=\"post\"&gt; 账户名称：&lt;input type=\"text\" name=\"name\" &gt;&lt;br/&gt; 账户金额：&lt;input type=\"text\" name=\"money\" &gt;&lt;br/&gt; 账户省份：&lt;input type=\"text\" name=\"address.provinceName\" &gt;&lt;br/&gt; 账户城市：&lt;input type=\"text\" name=\"address.cityName\" &gt;&lt;br/&gt; &lt;input type=\"submit\" value=\" 保存 \"&gt;&lt;/form&gt; 控制器: 1234567891011/** * 保存账户 * * @param account * @return */@RequestMapping(\"/saveAccount\")public String saveAccount(Account account) &#123; System.out.println(\"保存了账户。。。。\"+account); return \"success\";&#125; 运行结果: 3.1.4.3 POJO 类中包含集合类型参数实体类: 12345678910111213/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private String username; private String password; private Integer age; private List&lt;Account&gt; accounts; private Map&lt;String,Account&gt; accountMap; ...&#125; JSP: 123456789101112131415&lt;!-- POJO 类包含集合类型演示 --&gt;&lt;form action=\"account/updateAccount\" method=\"post\"&gt; 用户名称：&lt;input type=\"text\" name=\"username\" &gt;&lt;br/&gt; 用户密码：&lt;input type=\"password\" name=\"password\" &gt;&lt;br/&gt; 用户年龄：&lt;input type=\"text\" name=\"age\" &gt;&lt;br/&gt; 账户 1 名称：&lt;input type=\"text\" name=\"accounts[0].name\" &gt;&lt;br/&gt; 账户 1 金额：&lt;input type=\"text\" name=\"accounts[0].money\" &gt;&lt;br/&gt; 账户 2 名称：&lt;input type=\"text\" name=\"accounts[1].name\" &gt;&lt;br/&gt; 账户 2 金额：&lt;input type=\"text\" name=\"accounts[1].money\" &gt;&lt;br/&gt; 账户 3 名称：&lt;input type=\"text\" name=\"accountMap['one'].name\" &gt;&lt;br/&gt; 账户 3 金额：&lt;input type=\"text\" name=\"accountMap['one'].money\" &gt;&lt;br/&gt; 账户 4 名称：&lt;input type=\"text\" name=\"accountMap['two'].name\" &gt;&lt;br/&gt; 账户 4 金额：&lt;input type=\"text\" name=\"accountMap['two'].money\" &gt;&lt;br/&gt; &lt;input type=\"submit\" value=\" 保存 \"&gt;&lt;/form&gt; 控制器: 12345678910/** * 更新账户 * * @return */@RequestMapping(\"/updateAccount\")public String updateAccount(User user) &#123; System.out.println(\"更新了账户。。。。\"+user); return \"success\";&#125; 运行结果: 3.1.4.4 请求参数乱码问题post 请求方式： 12345678910111213141516在 web.xml 中配置一个过滤器&lt;!-- 配置 springMVC 编码过滤器 --&gt;&lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;!-- 设置过滤器中的属性值 --&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;!-- 过滤所有请求 --&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; get 请求方式： 1234567tomacat对GET和POST请求处理方式是不同的，GET请求的编码问题，要改tomcat的server.xml配置文件，如下：&lt;Connector connectionTimeout=\"20000\" port=\"8080\" protocol=\"HTTP/1.1\" edirectPort=\"8443\"/&gt;改为：&lt;Connector connectionTimeout=\"20000\" port=\"8080\" protocol=\"HTTP/1.1\" redirectPort=\"8443\" useBodyEncodingForURI=\"true\"/&gt;如果遇到 ajax 请求仍然乱码，请把：useBodyEncodingForURI=\"true\"改为 URIEncoding=\"UTF-8\"即可。 3.2 特殊情况3.2.1 自定义类型转换器表单提交的任何数据类型全部都是字符串类型，但是后台定义Integer类型，数据也可以封装上，说明 Spring框架内部会默认进行数据类型转换。 如果想自定义数据类型转换，可以实现Converter的接口 3.2.1.1 自定义类型转换器1234567891011121314151617181920212223242526272829/** * 把字符串转换日期 * * @author wgy */public class StringToDateConverter implements Converter&lt;String, Date&gt; &#123; /** * String source 传入进来字符串 * * @param source * @return */ @Override public Date convert(String source) &#123; // 判断 if (source == null) &#123; throw new RuntimeException(\"请您传入数据\"); &#125; DateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\"); try &#123; // 把字符串转换日期 return df.parse(source); &#125; catch (Exception e) &#123; throw new RuntimeException(\"数据类型转换出现错误\"); &#125; &#125;&#125; 3.2.1.2 注册自定义类型转换器1234567891011121314&lt;!-- 配置类型转换器工厂 --&gt;&lt;bean id=\"conversionService\" class=\"org.springframework.context.support.ConversionServiceFactoryBean\"&gt; &lt;!-- 给工厂注入一个新的类型转换器 --&gt; &lt;property name=\"converters\"&gt; &lt;set&gt; &lt;!-- 配置自定义类型转换器 --&gt; &lt;bean class=\"com.wgy.utils.StringToDateConverter\"/&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 开启SpringMVC框架注解的支持 --&gt;&lt;!-- 引用自定义类型转换器 --&gt;&lt;mvc:annotation-driven conversion-service=\"conversionService\"/&gt; 3.2.2 使用 ServletAPI 对象作为方法参数SpringMVC 还支持使用原始 ServletAPI 对象作为控制器方法的参数。 12345678910111213141516171819/** * 原生的API * * @return */@RequestMapping(\"/testServlet\")public String testServlet(HttpServletRequest request, HttpServletResponse response) &#123; System.out.println(\"执行了...\"); System.out.println(request); HttpSession session = request.getSession(); System.out.println(session); ServletContext servletContext = session.getServletContext(); System.out.println(servletContext); System.out.println(response); return \"success\";&#125; 4. 常用注解4.1 RequestParam4.1.1 使用说明12345作用： 把请求中指定名称的参数给控制器中的形参赋值。属性： value：请求参数中的名称。 required：请求参数中是否必须提供此参数。默认值：true。表示必须提供，如果不提供将报错。 4.1.2 使用示例JSP: 12&lt;!-- requestParams 注解的使用 --&gt;&lt;a href=\"springmvc/useRequestParam?name=test\"&gt;requestParam 注解&lt;/a&gt; 控制器: 1234567891011/** * requestParam 注解的使用 * * @param username * @return */@RequestMapping(\"/useRequestParam\")public String useRequestParam(@RequestParam(\"name\")String username,@RequestParam(value=\"age\",required=false)Integer age)&#123; System.out.println(username+\",\"+age); return \"success\";&#125; 运行结果: 4.2 RequestBody4.2.1 使用说明12345作用： 用于获取请求体内容。直接使用得到是 key&#x3D;value&amp;key&#x3D;value...结构的数据。 get 请求方式不适用。属性： required：是否必须有请求体。默认值是:true。当取值为 true 时,get 请求方式会报错。如果取值为 false，get 请求得到是 null。 4.2.2 使用示例JSP: 123456789&lt;!-- request body 注解 --&gt;&lt;form action=\"springmvc/useRequestBody\" method=\"post\"&gt; 用户名称：&lt;input type=\"text\" name=\"username\" &gt;&lt;br/&gt; 用户密码：&lt;input type=\"password\" name=\"password\" &gt;&lt;br/&gt; 用户年龄：&lt;input type=\"text\" name=\"age\" &gt;&lt;br/&gt; &lt;input type=\"submit\" value=\" 保存 \"&gt;&lt;/form&gt;&lt;a href=\"springmvc/useRequestBody?body=test\"&gt;requestBody 注解 get 请求&lt;/a&gt; 控制器: 1234567891011/** * RequestBody 注解 * * @param user * @return */@RequestMapping(\"/useRequestBody\")public String useRequestBody(@RequestBody(required=false) String body)&#123; System.out.println(body); return \"success\";&#125; post 请求运行结果: get 请求运行结果: 4.3 PathVaribale4.3.1 使用说明123456作用： 用于绑定 url 中的占位符。例如：请求 url 中 &#x2F;delete&#x2F;&#123;id&#125;，这个&#123;id&#125;就是 url 占位符。 url 支持占位符是 spring3.0 之后加入的。是 springmvc 支持 rest 风格 URL 的一个重要标志。属性： value：用于指定 url 中占位符名称。 required：是否必须提供占位符。 4.3.2 使用示例JSP: 12&lt;!-- PathVariable 注解 --&gt;&lt;a href=\"springmvc/usePathVariable/100\"&gt;pathVariable 注解&lt;/a&gt; 控制器: 1234567891011/** * PathVariable 注解 * * @param user * @return */@RequestMapping(\"/usePathVariable/&#123;id&#125;\")public String usePathVariable(@PathVariable(\"id\") Integer id)&#123; System.out.println(id); return \"success\";&#125; 运行结果: 4.3.3 REST 风格 URL4.3.3.1 什么是 restREST（英文：Representational State Transfer，简称 REST）描述了一个架构样式的网络系统，比如 web 应用程序。它首次出现在 2000 年 Roy Fielding 的博士论文中，他是 HTTP 规范的主要编写者之一。在目前主流的三种 Web 服务交互方案中，REST 相比于 SOAP（Simple Object Access protocol，简单对象访问协议）以及 XML-RPC 更加简单明了，无论是对 URL 的处理还是对 Payload 的编码，REST 都倾向于用更加简单轻量的方法设计和实现。值得注意的是 REST 并没有一个明确的标准，而更像是一种设计的风格。 它本身并没有什么实用性，其核心价值在于如何设计出符合 REST 风格的网络接口。 4.3.3.2 restful 的优点它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用。 请求路径一样，可以根据不同的请求方式去执行后台的不同方法 4.3.3.3 restful 的特性 资源（ Resources）：网络上的一个实体，或者说是网络上的一个具体信息。 它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的存在。可以用一个 URI （统一资源定位符）指向它，每种资源对应一个特定的 URI 。要获取这个资源，访问它的 URI 就可以，因此 URI 即为每一个资源的独一无二的识别符。 表现层（ Representation）：把资源具体呈现出来的形式，叫做它的表现层 （ Representation）。 比如，文本可以用 txt 格式表现，也可以用 HTML 格式、XML 格式、JSON 格式表现，甚至可以采用二进制格式。 状态转化（ State Transfer）：每发出一个请求，就代表了客户端和服务器的一次交互过程。 HTTP 协议，是一个无状态协议，即所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生 “状态转化 ”（ State Tran sfer）。而这种转化是建立在表现层之上的，所以就是 “表现层状态转化 ”。具体说，就是 HTTP 协议里面，四个表示操作方式的动词： GET、 POST、 PUT、DELETE。它们分别对应四种基本操作： GET 用来获取资源， POST 用来新建资源， PUT 用来更新资源， DELETE 用来删除资源。 4.3.3.4 restful 的示例1234&#x2F;account&#x2F;1 HTTP GET ： 得到 id &#x3D; 1 的 account&#x2F;account&#x2F;1 HTTP DELETE： 删除 id &#x3D; 1 的 account&#x2F;account&#x2F;1 HTTP PUT： 更新 id &#x3D; 1 的 account&#x2F;account HTTP POST： 新增 account 4.4 RequestHeader4.4.1 使用说明1234567作用： 用于获取请求消息头。属性： value：提供消息头名称 required：是否必须有此消息头注： 在实际开发中一般不怎么用。 4.4.2 使用示例JSP: 12&lt;!-- RequestHeader 注解 --&gt;&lt;a href=\"springmvc/useRequestHeader\"&gt;获取请求消息头&lt;/a&gt; 控制器: 1234567891011/** * RequestHeader 注解 * * @param user * @return */@RequestMapping(\"/useRequestHeader\")public String useRequestHeader(@RequestHeader(value=\"Accept-Language\",required=false)String requestHeader)&#123; System.out.println(requestHeader); return \"success\";&#125; 运行结果: 4.5 CookieValue4.5.1 使用说明12345作用： 用于把指定 cookie 名称的值传入控制器方法参数。属性： value：指定 cookie 的名称。 required：是否必须有此 cookie。 4.5.2 使用示例JSP: 12&lt;!-- CookieValue 注解 --&gt;&lt;a href=\"springmvc/useCookieValue\"&gt;绑定 cookie 的值&lt;/a&gt; 控制器: 1234567891011/** * Cookie 注解注解 * * @param user * @return */@RequestMapping(\"/useCookieValue\")public String useCookieValue(@CookieValue(value=\"JSESSIONID\",required=false) String cookieValue)&#123; System.out.println(cookieValue); return \"success\";&#125; 运行结果: 4.6 ModelAttribute4.6.1 使用说明12345678910作用： 该注解是 SpringMVC4.3 版本以后新加入的。它可以用于修饰方法和参数。 出现在方法上，表示当前方法会在控制器的方法执行之前，先执行。它可以修饰没有返回值的方法，也可以修饰有具体返回值的方法。 出现在参数上，获取指定的数据给参数赋值。属性： value：用于获取数据的 key。key 可以是 POJO 的属性名称，也可以是 map 结构的 key。应用场景： 当表单提交数据不是完整的实体类数据时，保证没有提交数据的字段使用数据库对象原来的数据。 例如： 我们在编辑一个用户时，用户有一个创建信息字段，该字段的值是不允许被修改的。在提交表单数据是肯定没有此字段的内容，一旦更新会把该字段内容置为 null，此时就可以使用此注解解决问题。 4.6.2 使用示例4.6.2.1 基于 POJO 属性的基本使用JSP: 12&lt;!-- ModelAttribute 注解的基本使用 --&gt;&lt;a href=\"springmvc/testModelAttribute?username=test\"&gt;测试 modelattribute&lt;/a&gt; 控制器: 1234567891011121314151617181920/** * 被 ModelAttribute 修饰的方法 * * @param user */@ModelAttributepublic void showModel(User user) &#123; System.out.println(\"执行了 showModel 方法\"+user.getUsername());&#125;/** * 接收请求的方法 * * @param user * @return */@RequestMapping(\"/testModelAttribute\")public String testModelAttribute(User user) &#123; System.out.println(\"执行了控制器的方法\"+user.getUsername()); return \"success\";&#125; 运行结果: 4.6.2.2 基于 Map 的应用场景1ModelAttribute 修饰方法带返回值 修改用户信息，要求用户的密码不能修改 JSP: 123456&lt;!-- 修改用户信息 --&gt;&lt;form action=\"springmvc/updateUser\" method=\"post\"&gt; 用户名称：&lt;input type=\"text\" name=\"username\" &gt;&lt;br/&gt; 用户年龄：&lt;input type=\"text\" name=\"age\" &gt;&lt;br/&gt; &lt;input type=\"submit\" value=\" 保存 \"&gt;&lt;/form&gt; 控制器: 1234567891011121314151617181920212223242526272829303132333435363738/** * 查询数据库中用户信息 * * @param user */@ModelAttributepublic User showModel(String username) &#123; //模拟去数据库查询 User abc = findUserByName(username); System.out.println(\"执行了 showModel 方法\"+abc); return abc;&#125;/** * 模拟修改用户方法 * * @param user * @return */@RequestMapping(\"/updateUser\")public String testModelAttribute(User user) &#123; System.out.println(\"控制器中处理请求的方法：修改用户：\"+user); return \"success\";&#125;/** * 模拟去数据库查询 * * @param username * @return */private User findUserByName(String username) &#123; User user = new User(); user.setUsername(username); user.setAge(19); user.setPassword(\"123456\"); return user;&#125; 运行结果: 4.6.2.3 基于 Map 的应用场景2ModelAttribute 修饰方法不带返回值 修改用户信息，要求用户的密码不能修改 JSP: 123456&lt;!-- 修改用户信息 --&gt;&lt;form action=\"springmvc/updateUser\" method=\"post\"&gt; 用户名称：&lt;input type=\"text\" name=\"username\" &gt;&lt;br/&gt; 用户年龄：&lt;input type=\"text\" name=\"age\" &gt;&lt;br/&gt; &lt;input type=\"submit\" value=\" 保存 \"&gt;&lt;/form&gt; 控制器: 1234567891011121314151617181920212223242526272829303132333435363738/** * 查询数据库中用户信息 * * @param user */@ModelAttributepublic void showModel(String username,Map&lt;String,User&gt; map) &#123; //模拟去数据库查询 User user = findUserByName(username); System.out.println(\"执行了 showModel 方法\"+user); map.put(\"abc\",user);&#125;/** * 模拟修改用户方法 * * @param user * @return */@RequestMapping(\"/updateUser\")public String testModelAttribute(@ModelAttribute(\"abc\")User user) &#123; System.out.println(\"控制器中处理请求的方法：修改用户：\"+user); return \"success\";&#125;/** * 模拟去数据库查询 * * @param username * @return */private User findUserByName(String username) &#123; User user = new User(); user.setUsername(username); user.setAge(19); user.setPassword(\"123456\"); return user;&#125; 运行结果: 4.7 SessionAttribute4.7.1 使用说明12345作用： 用于多次执行控制器方法间的参数共享。属性： value：用于指定存入的属性名称 type：用于指定存入的数据类型。 4.7.2 使用示例JSP: 123456&lt;!-- SessionAttribute 注解的使用 --&gt;&lt;a href=\"springmvc/testPut\"&gt;存入 SessionAttribute&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"springmvc/testGet\"&gt;取出 SessionAttribute&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"springmvc/testClean\"&gt;清除 SessionAttribute&lt;/a&gt; 控制器: 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 控制器 * * @author wgy */@Controller(\"sessionAttributeController\")@RequestMapping(\"/springmvc\")// 把数据存入到session域对象中@SessionAttributes(value =&#123;\"username\",\"password\"&#125;,types=&#123;Integer.class&#125;)public class SessionAttributeController &#123; /** * 把数据存入 SessionAttribute * * @param model * @return * Model 是 spring 提供的一个接口，该接口有一个实现类 ExtendedModelMap * 该类继承了 ModelMap，而 ModelMap 就是 LinkedHashMap 子类 */ @RequestMapping(\"/testPut\") public String testPut(Model model)&#123; System.out.println(\"存入了数据\"); model.addAttribute(\"username\", \"泰斯特\"); model.addAttribute(\"password\",\"123456\"); model.addAttribute(\"age\", 31); //跳转之前将数据保存到 username、password 和 age 中，因为注解@SessionAttribute 中有 这几个参数 return \"success\"; &#125; @RequestMapping(\"/testGet\") public String testGet(ModelMap model)&#123; System.out.print(\"获取了数据:\"); System.out.println(model.get(\"username\")+\";\"+model.get(\"password\")+\";\"+model.get(\"age\")); return \"success\"; &#125; @RequestMapping(\"/testClean\") public String complete(SessionStatus sessionStatus)&#123; System.out.print(\"清除了数据\"); sessionStatus.setComplete(); return \"success\"; &#125;&#125; 运行结果:","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://wgy1993.gitee.io/tags/SpringMVC/"}]},{"title":"MyBatis(四)","date":"2020-06-23T14:38:54.000Z","path":"archives/bf2fe2d.html","text":"1. Mybatis 延迟加载策略1.1 何为延迟加载就是在需要用到数据时才进行加载，不需要用到数据时就不加载数据。延迟加载也称懒加载。 好处：先从单表查询，需要时再从关联表去关联查询，大大提高数据库性能，因为查询单表要比关联查询多张表速度要快。 坏处 ：因为只有当需要用到数据时，才会进行数据库查询，这样在大批量数据查询时，因为查询工作也要消耗时间，所以可能造成用户等待时间变长，造成用户体验下降。 1.2 实现需求查询账户(Account)信息并且关联查询用户(User)信息。如果先查询账户(Account)信息即可满足要求，当我们需要查询用户(User)信息时再查询用户(User)信息。把对用户(User)信息的按需去查询就是延迟加载。 1.3 使用 assocation 实现延迟加载1.3.1 在 Account 实体类中加入 user 属性123456789101112131415/** * 账户实体类 * * @author wgy */public class Account implements Serializable &#123; private Integer id; private Integer uid; private Double money; //从表实体应该包含一个主表实体的对象引用 private User user; ...&#125; 1.3.2 账户的持久层 DAO 接口1234567891011121314/** * 账户持久层接口 * * @author wgy */public interface IAccountDao &#123; /** * 查询所有账户，同时还要获取到当前账户的所属用户信息 * * @return */ List&lt;Account&gt; findAll();&#125; 1.3.3 账户的持久层映射文件123456789101112131415161718192021222324&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wgy.dao.IAccountDao\"&gt; &lt;!-- 定义封装account和user的resultMap --&gt; &lt;resultMap id=\"accountUserMap\" type=\"account\"&gt; &lt;id property=\"id\" column=\"id\"/&gt; &lt;result property=\"uid\" column=\"uid\"/&gt; &lt;result property=\"money\" column=\"money\"/&gt; &lt;!-- 一对一的关系映射：配置封装user的内容 select属性指定的内容：查询用户的唯一标识： column属性指定的内容：用户根据id查询时，所需要的参数的值 --&gt; &lt;association property=\"user\" column=\"uid\" javaType=\"user\" select=\"com.wgy.dao.IUserDao.findById\"/&gt; &lt;/resultMap&gt; &lt;!-- 查询所有 --&gt; &lt;select id=\"findAll\" resultMap=\"accountUserMap\"&gt; select * from account &lt;/select&gt;&lt;/mapper&gt; 1.3.4 用户的持久层接口和映射文件123456789101112131415/** * 用户持久层接口 * * @author wgy */public interface IUserDao &#123; /** * 根据id查询用户信息 * * @param userId * @return */ User findById(Integer userId);&#125; 12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wgy.dao.IUserDao\"&gt; &lt;!-- 根据id查询用户 --&gt; &lt;select id=\"findById\" parameterType=\"int\" resultType=\"user\"&gt; select * from user where id = #&#123;uid&#125; &lt;/select&gt;&lt;/mapper&gt; 1.3.5 开启 Mybatis 的延迟加载策略进入 Mybaits 的官方文档，找到 settings 的说明信息： 我们需要在 Mybatis 的配置文件 SqlMapConfig.xml 文件中添加延迟加载的配置。 123456&lt;!--配置参数--&gt;&lt;settings&gt; &lt;!--开启Mybatis支持延迟加载--&gt; &lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt; &lt;setting name=\"aggressiveLazyLoading\" value=\"false\"&gt;&lt;/setting&gt;&lt;/settings&gt; 1.3.6 编写测试只查账户信息不查用户信息12345678/** * 测试查询所有 */@Testpublic void testFindAll() &#123; //5.执行操作 List&lt;Account&gt; accounts = accountDao.findAll();&#125; 测试结果如下： 我们发现，因为本次只是将Account对象查询出来放入List集合中，并没有涉及到User对象，所以就没有发出 SQL 语句查询账户所关联的 User 对象的查询。 1.4 使用 Collection 实现延迟加载1.4.1 在 User 实体类中加入List&lt;Account&gt;属性12345678910111213141516/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private Integer id; private String username; private String address; private String sex; private Date birthday; //一对多关系映射：主表实体应该包含从表实体的集合引用 private List&lt;Account&gt; accounts; ...&#125; 1.4.2 用户的持久层 DAO 接口123456/** * 查询所有用户，同时获取到用户下所有账户的信息 * * @return */List&lt;User&gt; findAll(); 1.4.3 用户的持久层映射文件12345678910111213141516&lt;!-- 定义User的resultMap--&gt;&lt;resultMap id=\"userAccountMap\" type=\"user\"&gt; &lt;id property=\"id\" column=\"id\"/&gt; &lt;result property=\"username\" column=\"username\"/&gt; &lt;result property=\"address\" column=\"address\"/&gt; &lt;result property=\"sex\" column=\"sex\"/&gt; &lt;result property=\"birthday\" column=\"birthday\"/&gt; &lt;!-- 配置user对象中accounts集合的映射 --&gt; &lt;collection property=\"accounts\" ofType=\"account\" select=\"com.wgy.dao.IAccountDao.findAccountByUid\" column=\"id\"/&gt;&lt;/resultMap&gt;&lt;!--配置查询所有--&gt;&lt;select id=\"findAll\" resultMap=\"userAccountMap\"&gt; select * from user&lt;/select&gt; 1.4.4 账户的持久层接口和映射文件1234567/** * 根据用户id查询账户信息 * * @param uid * @return */List&lt;Account&gt; findAccountByUid(Integer uid); 123456&lt;!-- 根据用户id查询账户列表 --&gt;&lt;select id=\"findAccountByUid\" resultType=\"account\" parameterType=\"int\"&gt; select * from account where uid = #&#123;id&#125;&lt;/select&gt; 1.4.5 编写测试只查用户信息不查账户信息12345678/** * 测试查询所有 */@Testpublic void testFindAll() &#123; //5.执行操作 List&lt;User&gt; users = userDao.findAll();&#125; 测试结果如下： 我们发现并没有加载 Account 账户信息。 2. Mybatis 缓存像大多数的持久化框架一样，Mybatis 也提供了缓存策略，通过缓存策略来减少数据库的查询次数，从而提高性能。 Mybatis 中缓存分为一级缓存，二级缓存。 2.1 Mybatis 一级缓存2.1.1 证明一级缓存的存在一级缓存是 SqlSession 级别的缓存，只要 SqlSession 没有 flush 或 close或clearCache，它就存在。 2.1.1.1 编写用户持久层 Dao 接口123456789101112131415/** * 用户持久层接口 * * @author wgy */public interface IUserDao &#123; /** * 根据id查询用户信息 * * @param userId * @return */ User findById(Integer userId);&#125; 2.1.1.2 编写用户持久层映射文件12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wgy.dao.IUserDao\"&gt; &lt;!-- 根据id查询用户 --&gt; &lt;select id=\"findById\" parameterType=\"int\" resultType=\"user\"&gt; select * from user where id = #&#123;uid&#125; &lt;/select&gt;&lt;/mapper&gt; 2.1.1.3 编写测试方法123456789101112/** * 测试一级缓存 */@Testpublic void testFirstLevelCache() &#123; //5.执行操作 User user1 = userDao.findById(41); System.out.println(user1); User user2 = userDao.findById(41); System.out.println(user2); System.out.println(user1 == user2);&#125; 测试结果如下： 我们可以发现，虽然在上面的代码中我们查询了两次，但最后只执行了一次数据库操作，这就是 Mybatis 提供给我们的一级缓存在起作用了。因为一级缓存的存在，导致第二次查询 id为41的记录时，并没有发出sql语句从数据库中查询数据，而是从一级缓存中查询。 2.1.2 一级缓存的分析一级缓存是 SqlSession 范围的缓存，当调用 SqlSession 的修改，添加，删除，commit()，close()等方法时，就会清空一级缓存。 第一次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，如果没有，从数据库查询用户信息。 得到用户信息，将用户信息存储到一级缓存中。 如果 sqlSession 去执行 commit 操作（执行插入、更新、删除），清空 SqlSession 中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。 第二次发起查询用户 id 为 1 的用户信息，先去找缓存中是否有 id 为 1 的用户信息，缓存中有，直接从缓存中获取用户信息。 2.1.3 测试一级缓存的清空12345678910111213141516/** * 测试一级缓存 */@Testpublic void testFirstLevelCache() &#123; User user1 = userDao.findById(41); System.out.println(user1);// sqlSession.close(); //再次获取SqlSession对象// sqlSession = factory.openSession(); sqlSession.clearCache();//此方法也可以清空缓存 userDao = sqlSession.getMapper(IUserDao.class); User user2 = userDao.findById(41); System.out.println(user2); System.out.println(user1 == user2);&#125; 1234567891011121314151617181920/** * 测试缓存的同步 */@Testpublic void testClearlCache() &#123; //1.根据id查询用户 User user1 = userDao.findById(41); System.out.println(user1); //2.更新用户信息 user1.setUsername(\"update user clear cache\"); user1.setAddress(\"北京市海淀区\"); userDao.updateUser(user1); //3.再次查询id为41的用户 User user2 = userDao.findById(41); System.out.println(user2); System.out.println(user1 == user2);&#125; 当执行sqlSession.close()后，再次获取sqlSession并查询id=41的User对象时，又重新执行了sql语句，从数据库进行了查询操作。 2.2 Mybatis 二级缓存二级缓存是 mapper 映射级别的缓存，多个 SqlSession 去操作同一个 Mapper 映射的 sql 语句，多个SqlSession 可以共用二级缓存，二级缓存是跨 SqlSession 的。 2.2.1 二级缓存结构图 首先开启 mybatis 的二级缓存。 sqlSession1 去查询用户信息，查询到用户信息会将查询数据存储到二级缓存中。 如果 SqlSession3 去执行相同 mapper 映射下 sql，执行 commit 提交，将会清空该 mapper 映射下的二级缓存区域的数据。 sqlSession2 去查询与 sqlSession1 相同的用户信息，首先会去缓存中找是否存在数据，如果存在直接从缓存中取出数据。 2.2.2 二级缓存的开启与关闭2.2.2.1 在 SqlMapConfig.xml 文件开启二级缓存123456&lt;settings&gt; &lt;!-- 开启二级缓存的支持 --&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt;&lt;/settings&gt;因为 cacheEnabled 的取值默认就为 true，所以这一步可以省略不配置。为 true 代表开启二级缓存；为false 代表不开启二级缓存。 2.2.2.2 配置相关的 Mapper 映射文件1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wgy.dao.IUserDao\"&gt; &lt;!--开启user支持二级缓存--&gt; &lt;cache/&gt;&lt;/mapper&gt;&lt;cache&gt;标签表示当前这个 mapper 映射将使用二级缓存，区分的标准就看 mapper 的 namespace 值。 2.2.2.3 配置 statement 上面的 useCache12345678&lt;!-- 根据id查询用户 --&gt;&lt;select id=\"findById\" parameterType=\"int\" resultType=\"user\" useCache=\"true\"&gt; select * from user where id = #&#123;uid&#125;&lt;/select&gt;&lt;select&gt;标签中设置 useCache=”true”代表当前这个 statement 要使用二级缓存，如果不使用二级缓存可以设置为 false。 2.2.3 二级缓存测试12345678910111213141516171819/** * 测试二级缓存 */@Testpublic void testFirstLevelCache() &#123; SqlSession sqlSession1 = factory.openSession(); IUserDao dao1 = sqlSession1.getMapper(IUserDao.class); User user1 = dao1.findById(41); System.out.println(user1); sqlSession1.close();//一级缓存消失 SqlSession sqlSession2 = factory.openSession(); IUserDao dao2 = sqlSession2.getMapper(IUserDao.class); User user2 = dao2.findById(41); System.out.println(user2); sqlSession2.close(); System.out.println(user1 == user2);&#125; 经过上面的测试，我们发现执行了两次查询，并且在执行第一次查询后，我们关闭了一级缓存，再去执行第二次查询时，我们发现并没有对数据库发出 sql 语句，所以此时的数据就只能是来自于我们所说的二级缓存。 2.2.4 二级缓存注意事项当我们在使用二级缓存时，所缓存的类一定要实现 java.io.Serializable 接口，这种就可以使用序列化方式来保存对象。 3. Mybatis 注解开发3.1 mybatis 的常用注解说明1234567891011@Insert:实现新增@Update:实现更新@Delete:实现删除@Select:实现查询@Result:实现结果集封装@Results:可以与@Result 一起使用，封装多个结果集@ResultMap:实现引用@Results 定义的封装@One:实现一对一结果集封装@Many:实现一对多结果集封装@SelectProvider: 实现动态 SQL 映射@CacheNamespace:实现注解二级缓存的使用 3.2 使用 Mybatis 注解实现基本 CRUD3.2.1 编写实体类1234567891011121314/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private Integer id; private String username; private String address; private String sex; private Date birthday; ...&#125; 3.2.2 使用注解方式开发持久层接口12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * 在mybatis中针对CRUD一共有四个注解 * * @author wgy * @Select @Insert @Update @Delete */public interface IUserDao &#123; /** * 查询所有用户 * * @return */ @Select(\"select * from user\") List&lt;User&gt; findAll(); /** * 保存用户 * * @param user */ @Insert(\"insert into user(username,address,sex,birthday)values(#&#123;username&#125;,#&#123;address&#125;,#&#123;sex&#125;,#&#123;birthday&#125;)\") void saveUser(User user); /** * 更新用户 * * @param user */ @Update(\"update user set username=#&#123;username&#125;,sex=#&#123;sex&#125;,birthday=#&#123;birthday&#125;,address=#&#123;address&#125; where id=#&#123;id&#125;\") void updateUser(User user); /** * 删除用户 * * @param userId */ @Delete(\"delete from user where id=#&#123;id&#125; \") void deleteUser(Integer userId); /** * 根据id查询用户 * * @param userId * @return */ @Select(\"select * from user where id=#&#123;id&#125; \") User findById(Integer userId); /** * 根据用户名称模糊查询 * * @param username * @return */ @Select(\"select * from user where username like #&#123;username&#125; \")// @Select(\"select * from user where username like '%$&#123;value&#125;%' \") List&lt;User&gt; findUserByName(String username); /** * 查询总用户数量 * * @return */ @Select(\"select count(*) from user \") int findTotalUser();&#125; 3.2.3 编写 SqlMapConfig 配置文件123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;!-- mybatis的主配置文件 --&gt;&lt;configuration&gt; &lt;!-- 配置properties--&gt; &lt;properties resource=\"jdbcConfig.properties\"/&gt; &lt;!--使用typeAliases配置别名，它只能配置domain中类的别名 --&gt; &lt;typeAliases&gt; &lt;package name=\"com.wgy.domain\"/&gt; &lt;/typeAliases&gt; &lt;!--配置环境--&gt; &lt;environments default=\"mysql\"&gt; &lt;!-- 配置mysql的环境--&gt; &lt;environment id=\"mysql\"&gt; &lt;!-- 配置事务 --&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;!--配置连接池--&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;jdbc.driver&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 配置映射文件的位置 --&gt; &lt;mappers&gt; &lt;!-- 配置 dao 接口的位置，它有两种方式 第一种：使用 mapper 标签配置 class 属性 第二种：使用 package 标签，直接指定 dao 接口所在的包 --&gt; &lt;package name=\"com.wgy.dao\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 3.2.4 编写测试方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * @author wgy */public class AnnotationCRUDTest &#123; private InputStream in; private SqlSessionFactory factory; private SqlSession session; private IUserDao userDao; @Before public void init() throws Exception &#123; in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); factory = new SqlSessionFactoryBuilder().build(in); session = factory.openSession(); userDao = session.getMapper(IUserDao.class); &#125; @After public void destroy() throws Exception &#123; session.commit(); session.close(); in.close(); &#125; @Test public void testSave() &#123; User user = new User(); user.setUsername(\"mybatis annotation\"); user.setAddress(\"北京市昌平区\"); userDao.saveUser(user); &#125; @Test public void testUpdate() &#123; User user = new User(); user.setId(57); user.setUsername(\"mybatis annotation update\"); user.setAddress(\"北京市海淀区\"); user.setSex(\"男\"); user.setBirthday(new Date()); userDao.updateUser(user); &#125; @Test public void testDelete() &#123; userDao.deleteUser(51); &#125; @Test public void testFindOne() &#123; User user = userDao.findById(57); System.out.println(user); &#125; @Test public void testFindByName() &#123; List&lt;User&gt; users = userDao.findUserByName(\"%mybatis%\");// List&lt;User&gt; users = userDao.findUserByName(\"mybatis\"); for (User user : users) &#123; System.out.println(user); &#125; &#125; @Test public void testFindTotal() &#123; int total = userDao.findTotalUser(); System.out.println(total); &#125;&#125; 3.3 使用注解实现复杂关系映射开发实现复杂关系映射之前我们可以在映射文件中通过配置&lt;resultMap&gt;来实现，在使用注解开发时我们需要借助@Results 注解，@Result 注解，@One 注解，@Many 注解。 3.3.1 复杂关系映射的注解说明123456789101112131415161718192021222324252627@Results 注解代替的是标签&lt;resultMap&gt;该注解中可以使用单个@Result 注解，也可以使用@Result 集合@Results（&#123;@Result（），@Result（）&#125;）或@Results（@Result（））@Resutl 注解代替了 &lt;id&gt; 标签和&lt;result&gt; 标签@Result 中 属性介绍： id 是否是主键字段 column 数据库的列名 property 需要装配的属性名 one 需要使用的@One 注解（@Result（one&#x3D;@One）（））） many 需要使用的@Many 注解（@Result（many&#x3D;@many）（）））@One 注解（一对一） 代替了&lt;assocation&gt; 标签，是多表查询的关键，在注解中用来指定子查询返回单一对象。@One 注解属性介绍： select 指定用的 来多表查询的 sqlmapper fetchType 会覆盖全局的配置参数 lazyLoadingEnabled。。 使用格式： @Result(column&#x3D;&quot; &quot;,property&#x3D;&quot;&quot;,one&#x3D;@One(select&#x3D;&quot;&quot;))@Many 注解（多对一） 代替了&lt;Collection&gt; 标签, 是是多表查询的关键，在注解中用来指定子查询返回对象集合。 注意：聚集元素用来处理“一对多”的关系。需要指定映射的 Java 实体类的属性，属性的 javaType（一般为 ArrayList）但是注解中可以不定义； 使用格式： @Result(property&#x3D;&quot;&quot;,column&#x3D;&quot;&quot;,many&#x3D;@Many(select&#x3D;&quot;&quot;)) 3.3.2 使用注解实现一对一 复杂关系映射及延迟加载加载账户信息时并且加载该账户的用户信息，根据情况可实现延迟加载。（注解方式实现） 3.3.2.1 添加 User 实体类及 Account 实体类1234567891011121314/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private Integer userId; private String userName; private String userAddress; private String userSex; private Date userBirthday; ...&#125; 1234567891011121314/** * 账户实体类 * * @author wgy */public class Account implements Serializable &#123; private Integer id; private Integer uid; private Double money; //多对一（mybatis中称之为一对一）的映射：一个账户只能属于一个用户 private User user; ...&#125; 3.3.2.2 添加账户的持久层接口并使用注解配置123456789101112131415161718192021/** * 账户持久层接口 * * @author wgy */public interface IAccountDao &#123; /** * 查询所有账户，并且获取每个账户所属的用户信息 * * @return */ @Select(\"select * from account\") @Results(id = \"accountMap\", value = &#123; @Result(id = true, column = \"id\", property = \"id\"), @Result(column = \"uid\", property = \"uid\"), @Result(column = \"money\", property = \"money\"), @Result(property = \"user\", column = \"uid\", one = @One(select = \"com.wgy.dao.IUserDao.findById\", fetchType = FetchType.EAGER)) &#125;) List&lt;Account&gt; findAll();&#125; 3.3.2.3 添加用户的持久层接口并使用注解配置123456789101112131415161718192021222324252627282930313233/** * 在mybatis中针对CRUD一共有四个注解 * * @author wgy * @Select @Insert @Update @Delete */public interface IUserDao &#123; /** * 查询所有用户 * * @return */ @Select(\"select * from user\") @Results(id = \"userMap\", value = &#123; @Result(id = true, column = \"id\", property = \"userId\"), @Result(column = \"username\", property = \"userName\"), @Result(column = \"address\", property = \"userAddress\"), @Result(column = \"sex\", property = \"userSex\"), @Result(column = \"birthday\", property = \"userBirthday\") &#125;) List&lt;User&gt; findAll(); /** * 根据id查询用户 * * @param userId * @return */ @Select(\"select * from user where id=#&#123;id&#125; \") @ResultMap(\"userMap\") User findById(Integer userId);&#125; 3.3.2.4 测试一对一关联及延迟加载1234@Testpublic void testFindAll() &#123; List&lt;Account&gt; accounts = accountDao.findAll();&#125; 3.3.3 使用注解实现一对多复杂关系映射查询用户信息时，也要查询他的账户列表。使用注解方式实现。 3.3.3.1 User 实体类加入List&lt;Account&gt;12345678910111213141516/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private Integer userId; private String userName; private String userAddress; private String userSex; private Date userBirthday; //一对多关系映射：一个用户对应多个账户 private List&lt;Account&gt; accounts; ...&#125; 3.3.3.2 编写用户的持久层接口并使用注解配置123456789101112131415/** * 查询所有用户 * * @return */@Select(\"select * from user\")@Results(id = \"userMap\", value = &#123; @Result(id = true, column = \"id\", property = \"userId\"), @Result(column = \"username\", property = \"userName\"), @Result(column = \"address\", property = \"userAddress\"), @Result(column = \"sex\", property = \"userSex\"), @Result(column = \"birthday\", property = \"userBirthday\"), @Result(property = \"accounts\", column = \"id\", many = @Many(select = \"com.wgy.dao.IAccountDao.findAccountByUid\", fetchType = FetchType.LAZY))&#125;)List&lt;User&gt; findAll(); 3.3.3.3 编写账户的持久层接口并使用注解配置12345678/** * 根据用户id查询账户信息 * * @param userId * @return */@Select(\"select * from account where uid = #&#123;userId&#125;\")List&lt;Account&gt; findAccountByUid(Integer userId); 3.3.3.4 测试一对多关联及延迟加载1234@Testpublic void testFindAll() &#123; List&lt;User&gt; users = userDao.findAll();&#125; 3.4 mybatis 基于注解的二级缓存3.4.1 在 SqlMapConfig 中开启二级缓存支持12345&lt;!-- 配置二级缓存 --&gt;&lt;settings&gt; &lt;!-- 开启二级缓存的支持 --&gt; &lt;setting name=\"cacheEnabled\" value=\"true\"/&gt;&lt;/settings&gt; 3.4.2 在持久层接口中使用注解配置二级缓存1234567/** * * @author wgy */@CacheNamespace(blocking = true)//mybatis 基于注解方式实现配置二级缓存public interface IUserDao &#123;&#125;","tags":[{"name":"ORM","slug":"ORM","permalink":"https://wgy1993.gitee.io/tags/ORM/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wgy1993.gitee.io/tags/MyBatis/"}]},{"title":"MyBatis(三)","date":"2020-06-22T04:21:01.000Z","path":"archives/f1b11122.html","text":"1. Mybatis 连接池与事务深入1.1 Mybatis 的连接池技术在 Mybatis 的 SqlMapConfig.xml 配置文件中，通过&lt;dataSource type=&quot;pooled&quot;&gt;来实现 Mybatis 中连接池的配置。 1.1.1 Mybatis 连接池的分类在 Mybatis 中我们将它的数据源 dataSource 分为以下几类： UNPOOLED：不使用连接池的数据源 POOLED：使用连接池的数据源 JNDI：使用 JNDI 实现的数据源 在这三种数据源中，我们一般采用的是 POOLED 数据源（很多时候我们所说的数据源就是为了更好的管理数据库连接，也就是我们所说的连接池技术）。 1.1.2 Mybatis 中数据源的配置1234567&lt;!-- 配置数据源（连接池）信息 --&gt;&lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;jdbc.driver&#125;\"/&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\"/&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\"/&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"/&gt;&lt;/dataSource&gt; MyBatis 在初始化时，根据&lt;dataSource&gt;的 type 属性来创建相应类型的的数据源 DataSource。 1.1.3 Mybatis 中 DataSource 的存取MyBatis 是通过工厂模式来创建数据源 DataSource 对象的， MyBatis 定义了抽象的工厂接口:org.apache.ibatis.datasource.DataSourceFactory,通过其 getDataSource()方法返回数据源DataSource。 MyBatis 创建了 DataSource 实例后，会将其放到 Configuration 对象内的 Environment 对象中， 供以后使用。 1.1.4 Mybatis 中连接的获取过程分析当我们需要创建 SqlSession 对象并需要执行 SQL 语句时，这时候 MyBatis 才会去调用 dataSource 对象来创建java.sql.Connection对象。也就是说，java.sql.Connection对象的创建一直延迟到执行SQL语句的时候。当我们用完了就再立即将数据库连接归还到连接池中。 1.2 Mybatis 的事务控制1.2.1 JDBC 中事务的回顾在 JDBC 中我们可以将事务的提交改为自动方式，通过 setAutoCommit()方法就可以调整。 1.2.2 Mybatis 中事务提交方式Mybatis 中事务的提交方式，本质上就是调用 JDBC 的 setAutoCommit()来实现事务控制。 123456789101112131415161718192021222324252627282930@Testpublic void testSaveUser() throws Exception &#123; User user = new User(); user.setUsername(\"mybatis user09\"); //6.执行操作 int res = userDao.saveUser(user); System.out.println(res); System.out.println(user.getId());&#125;@Before//在测试方法执行之前执行public void init()throws Exception &#123; //1.读取配置文件 in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //2.创建构建者对象 SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); //3.创建 SqlSession 工厂对象 factory = builder.build(in); //4.创建 SqlSession 对象 session = factory.openSession(); //5.创建 Dao 的代理对象 userDao = session.getMapper(IUserDao.class);&#125;@After//在测试方法执行完成之后执行public void destroy() throws Exception&#123; //7.提交事务 session.commit(); //8.释放资源 session.close(); in.close();&#125; 控制台输出的结果： Connection 的整个变化过程，通过分析我们能够发现之前的 CUD 操作过程中，我们都要手动进行事务的提交，原因是 setAutoCommit()方法，在执行时它的值被设置为 false 了，所以我们在 CUD 操作中，必须通过sqlSession.commit()方法来执行提交操作。 1.2.3 Mybatis 自动提交事务的设置12//4.创建 SqlSession 对象session = factory.openSession(true); 所对应的 DefaultSqlSessionFactory 类的源代码： 2. Mybatis 的动态 SQL 语句2.1 &lt;if&gt;标签123456789101112&lt;!-- 根据条件查询 --&gt;&lt;select id=\"findByUser\" resultType=\"user\" parameterType=\"user\"&gt; select * from user where 1=1 &lt;if test=\"username!=null and username != '' \"&gt; and username = #&#123;username&#125; &lt;/if&gt; &lt;if test=\"address != null\"&gt; and address like #&#123;address&#125; &lt;/if&gt;&lt;/select&gt; 2.2 &lt;where&gt;标签为了简化上面 where 1=1 的条件拼装，我们可以采用&lt;where&gt;标签来简化开发。 123456789101112&lt;select id=\"findUserByCondition\" resultMap=\"userMap\" parameterType=\"user\"&gt; select * from user &lt;where&gt; &lt;if test=\"userName != null\"&gt; and username = #&#123;userName&#125; &lt;/if&gt; &lt;if test=\"userSex != null\"&gt; and sex = #&#123;userSex&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 2.3 &lt;foreach&gt; 标签2.3.1 需求传入多个 id 查询用户信息，用下边两个 sql 实现： 12SELECT * FROM USERS WHERE username LIKE '%张%' AND (id =10 OR id =89 OR id=16)SELECT * FROM USERS WHERE username LIKE '%张%' AND id IN (10,89,16) 这样我们在进行范围查询时，就要将一个集合中的值，作为参数动态添加进来。 2.3.2 在 QueryVo 中加入一个 List 集合用于封装参数123456789/** * @author wgy */public class QueryVo &#123; private User user; private List&lt;Integer&gt; ids; ...&#125; 2.3.3 映射配置12345678910111213141516171819202122&lt;!-- 根据queryvo中的Id集合实现查询用户列表 --&gt;&lt;select id=\"findUserInIds\" resultMap=\"userMap\" parameterType=\"queryvo\"&gt; select * from user &lt;where&gt; &lt;if test=\"ids != null and ids.size()&gt;0\"&gt; &lt;foreach collection=\"ids\" open=\"and id in (\" close=\")\" separator=\",\" item=\"uid\"&gt; #&#123;uid&#125; &lt;/foreach&gt; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt;SQL 语句：select 字段 from user where id in (?)&lt;foreach&gt;标签用于遍历集合，它的属性： collection:代表要遍历的集合元素，注意编写时不要写#&#123;&#125; open:代表语句的开始部分 close:代表结束部分 item:代表遍历集合的每个元素，生成的变量名 sperator:代表分隔符 2.4 简化编写的 SQL 片段Sql 中可将重复的 sql 提取出来，使用时用 include 引用即可，最终达到 sql 重用的目的。 2.4.1 定义代码片段12345&lt;!-- 了解的内容：抽取重复的sql语句--&gt;&lt;sql id=\"defaultUser\"&gt; select * from user&lt;/sql&gt; 2.4.2 引用代码片段1234&lt;!--配置查询所有--&gt;&lt;select id=\"findAll\" resultMap=\"userMap\"&gt; &lt;include refid=\"defaultUser\"&gt;&lt;/include&gt;&lt;/select&gt; 3. Mybatis 多表查询之一对多用户为User 表，账户为Account表。一个用户（User）可以有多个账户（Account）。 3.1 一对一查询( 多对一)查询所有账户信息，关联查询下单用户信息。 3.1.1 方式一3.1.1.1 定义账户信息的实体类1234567public class Account implements Serializable &#123; private Integer id; private Integer uid; private Double money; ...&#125; 3.1.1.2 编写 Sql 语句1234select a.*, u.username, u.addressfrom account a, user uwhere u.id &#x3D; a.uid; 3.1.1.3 定义 AccountUser 类为了能够封装上面 SQL 语句的查询结果，定义 AccountUser 类中要包含账户信息同时还要包含用户信息，所以我们要在定义 AccountUser 类时可以继承 User 类。 123456789/** * @author wgy */public class AccountUser extends Account &#123; private String username; private String address; ...&#125; 3.1.1.4 映射配置1234567&lt;!--查询所有账户同时包含用户名和地址信息--&gt;&lt;select id=\"findAllAccount\" resultType=\"accountuser\"&gt; select a.*, u.username, u.address from account a, user u where u.id = a.uid;&lt;/select&gt; 定义专门的 po 类作为输出类型，其中定义了 sql 查询结果集所有的字段。此方法较为简单，企业中使用普遍。 3.1.2 方式二使用 resultMap，定义专门的 resultMap 用于映射一对一查询结果。 通过面向对象的(has a)关系可以得知，我们可以在 Account 类中加入一个 User 类的对象来代表这个账户是哪个用户的。 3.1.2.1 修改 Account 类在 Account 类中加入 User 类的对象作为 Account 类的一个属性。 123456789101112131415/** * 账户实体类 * * @author wgy */public class Account implements Serializable &#123; private Integer id; private Integer uid; private Double money; //从表实体应该包含一个主表实体的对象引用 private User user; ...&#125; 3.1.2.2 映射配置12345678910111213141516171819202122&lt;!-- 定义封装account和user的resultMap --&gt;&lt;resultMap id=\"accountUserMap\" type=\"account\"&gt; &lt;id property=\"id\" column=\"aid\"&gt;&lt;/id&gt; &lt;result property=\"uid\" column=\"uid\"&gt;&lt;/result&gt; &lt;result property=\"money\" column=\"money\"&gt;&lt;/result&gt; &lt;!-- 一对一的关系映射：配置封装user的内容--&gt; &lt;association property=\"user\" column=\"uid\" javaType=\"user\"&gt; &lt;id property=\"id\" column=\"id\"&gt;&lt;/id&gt; &lt;result column=\"username\" property=\"username\"&gt;&lt;/result&gt; &lt;result column=\"address\" property=\"address\"&gt;&lt;/result&gt; &lt;result column=\"sex\" property=\"sex\"&gt;&lt;/result&gt; &lt;result column=\"birthday\" property=\"birthday\"&gt;&lt;/result&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;!-- 查询所有 --&gt;&lt;select id=\"findAll\" resultMap=\"accountUserMap\"&gt; select u.*, a.id as aid, a.uid, a.money from account a, user u where u.id = a.uid;&lt;/select&gt; 3.2 一对多查询查询所有用户信息及用户关联的账户信息。 3.2.1 编写 SQL 语句123456SELECT u.*, acc.id id, acc.uid, acc.moneyFROM user u LEFT JOIN account acc ON u.id &#x3D; acc.uid 3.2.2 User类加入 List&lt;Account&gt;12345678910111213141516/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private Integer id; private String username; private String address; private String sex; private Date birthday; //一对多关系映射：主表实体应该包含从表实体的集合引用 private List&lt;Account&gt; accounts; ...&#125; 3.2.3 映射配置12345678910111213141516171819202122232425262728&lt;!-- 定义User的resultMap--&gt;&lt;resultMap id=\"userAccountMap\" type=\"user\"&gt; &lt;id property=\"id\" column=\"id\"&gt;&lt;/id&gt; &lt;result property=\"username\" column=\"username\"&gt;&lt;/result&gt; &lt;result property=\"address\" column=\"address\"&gt;&lt;/result&gt; &lt;result property=\"sex\" column=\"sex\"&gt;&lt;/result&gt; &lt;result property=\"birthday\" column=\"birthday\"&gt;&lt;/result&gt; &lt;!-- 配置user对象中accounts集合的映射 --&gt; &lt;collection property=\"accounts\" ofType=\"account\"&gt; &lt;id column=\"aid\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"uid\" property=\"uid\"&gt;&lt;/result&gt; &lt;result column=\"money\" property=\"money\"&gt;&lt;/result&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;!--配置查询所有--&gt;&lt;select id=\"findAll\" resultMap=\"userAccountMap\"&gt; select * from user u left outer join account a on u.id = a.uid&lt;/select&gt;collection 部分定义了用户关联的账户信息。表示关联查询结果集property=\"accounts\" ： 关联查询的结果集存储在 User 对象的上哪个属性。ofType=\"account\" ： 指定关联查询的结果集中的对象类型即List中的对象类型。此处可以使用别名，也可以使用全限定名。 4. Mybatis 多表查询之多对多多对多关系其实是双向的一对多关系。一个用户可以有多个角色，一个角色可以赋予多个用户。 4.1 实现 Role 到 User 多对多实现查询所有角色并且加载它所分配的用户信息。 4.1.1 编写 SQL 语句1234select u.*, r.id as rid, r.role_name, r.role_descfrom role r left outer join user_role ur on r.id &#x3D; ur.rid left outer join user u on u.id &#x3D; ur.uid 4.1.2 编写角色实体类1234567891011121314/** * 角色实体类 * * @author wgy */public class Role implements Serializable &#123; private Integer roleId; private String roleName; private String roleDesc; //多对多的关系映射：一个角色可以赋予多个用户 private List&lt;User&gt; users; ...&#125; 4.1.3 映射配置123456789101112131415161718192021&lt;!--定义role表的ResultMap--&gt;&lt;resultMap id=\"roleMap\" type=\"role\"&gt; &lt;id property=\"roleId\" column=\"rid\"&gt;&lt;/id&gt; &lt;result property=\"roleName\" column=\"role_name\"&gt;&lt;/result&gt; &lt;result property=\"roleDesc\" column=\"role_desc\"&gt;&lt;/result&gt; &lt;collection property=\"users\" ofType=\"user\"&gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"username\" property=\"username\"&gt;&lt;/result&gt; &lt;result column=\"address\" property=\"address\"&gt;&lt;/result&gt; &lt;result column=\"sex\" property=\"sex\"&gt;&lt;/result&gt; &lt;result column=\"birthday\" property=\"birthday\"&gt;&lt;/result&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;!--查询所有--&gt;&lt;select id=\"findAll\" resultMap=\"roleMap\"&gt; select u.*, r.id as rid, r.role_name, r.role_desc from role r left outer join user_role ur on r.id = ur.rid left outer join user u on u.id = ur.uid&lt;/select&gt; 4.2 实现 User 到 Role 的多对多4.2.1 编写 SQL 语句1234select u.*, r.id as rid, r.role_name, r.role_descfrom user u left outer join user_role ur on u.id &#x3D; ur.uid left outer join role r on r.id &#x3D; ur.rid 4.2.2 编写用户实体类12345678910111213141516/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private Integer id; private String username; private String address; private String sex; private Date birthday; //多对多的关系映射：一个用户可以具备多个角色 private List&lt;Role&gt; roles; ...&#125; 4.2.3 映射配置12345678910111213141516171819202122&lt;!-- 定义User的resultMap--&gt;&lt;resultMap id=\"userMap\" type=\"user\"&gt; &lt;id property=\"id\" column=\"id\"&gt;&lt;/id&gt; &lt;result property=\"username\" column=\"username\"&gt;&lt;/result&gt; &lt;result property=\"address\" column=\"address\"&gt;&lt;/result&gt; &lt;result property=\"sex\" column=\"sex\"&gt;&lt;/result&gt; &lt;result property=\"birthday\" column=\"birthday\"&gt;&lt;/result&gt; &lt;!-- 配置角色集合的映射 --&gt; &lt;collection property=\"roles\" ofType=\"role\"&gt; &lt;id property=\"roleId\" column=\"rid\"&gt;&lt;/id&gt; &lt;result property=\"roleName\" column=\"role_name\"&gt;&lt;/result&gt; &lt;result property=\"roleDesc\" column=\"role_desc\"&gt;&lt;/result&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;!--配置查询所有--&gt;&lt;select id=\"findAll\" resultMap=\"userMap\"&gt; select u.*, r.id as rid, r.role_name, r.role_desc from user u left outer join user_role ur on u.id = ur.uid left outer join role r on r.id = ur.rid&lt;/select&gt; 5. JNDI数据源JNDI：Java Naming and Directory Interface。是SUN公司推出的一套规范，属于JavaEE技术之一。目的是模仿windows系统中的注册表。在服务器中注册数据源 5.1 创建Maven的war工程并导入坐标123456789101112131415161718192021222324252627282930313233343536373839&lt;groupId&gt;com.wgy&lt;/groupId&gt;&lt;artifactId&gt;MyBatis10_JNDI&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;war&lt;/packaging&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 5.2 在webapp文件下创建META-INF目录 5.3 在META-INF目录中建立一个名为context.xml的配置文件1234567891011121314151617181920212223242526272829&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;Context&gt; &lt;!-- &lt;Resource name=\"jdbc/mybatis\" 数据源的名称 type=\"javax.sql.DataSource\" 数据源类型 auth=\"Container\" 数据源提供者 maxActive=\"20\" 最大活动数 maxWait=\"10000\" 最大等待时间 maxIdle=\"5\" 最大空闲数 username=\"root\" 用户名 password=\"root\" 密码 driverClassName=\"com.mysql.jdbc.Driver\" 驱动类 url=\"jdbc:mysql://localhost:3306/mybatis\" 连接url字符串 /&gt; --&gt; &lt;Resource name=\"jdbc/mybatis\" type=\"javax.sql.DataSource\" auth=\"Container\" maxActive=\"20\" maxWait=\"10000\" maxIdle=\"5\" username=\"root\" password=\"root\" driverClassName=\"com.mysql.jdbc.Driver\" url=\"jdbc:mysql://localhost:3306/mybatis\" /&gt;&lt;/Context&gt; 5.4 修改SqlMapConfig.xml中的配置1234567891011121314&lt;!--配置环境--&gt;&lt;environments default=\"dev\"&gt; &lt;!-- 配置dev的环境--&gt; &lt;environment id=\"dev\"&gt; &lt;!-- 配置事务控制的方式 --&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;!-- 配置连接数据库的必备信息 type属性表示是否使用数据源（连接池）--&gt; &lt;dataSource type=\"JNDI\"&gt; &lt;property name=\"data_source\" value=\"java:comp/env/jdbc/mybatis\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt;java:comp/env/为固定写法，后面为context.xml中设置的数据源的名称jdbc/mybatis","tags":[{"name":"ORM","slug":"ORM","permalink":"https://wgy1993.gitee.io/tags/ORM/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wgy1993.gitee.io/tags/MyBatis/"}]},{"title":"MyBatis(二)","date":"2020-06-21T14:11:23.000Z","path":"archives/8f423109.html","text":"1. 基于代理 Dao 实现 CRUD 操作使用要求： 持久层接口和持久层接口的映射配置必须在相同的包下 持久层映射配置中 mapper 标签的 namespace 属性取值必须是持久层接口的全限定类名 SQL 语句的配置标签&lt;select&gt;,&lt;insert&gt;,&lt;delete&gt;,&lt;update&gt;的 id 属性必须和持久层接口的方法名相同。 1.1 根据 ID 查询1.1.1 在持久层接口中添加查询方法1234567/** * 根据id查询用户信息 * * @param userId * @return */User findById(Integer userId); 1.1.2 在用户的映射配置文件中配置123456&lt;!-- 根据id查询用户 --&gt;&lt;select id=\"findById\" parameterType=\"int\" resultType=\"com.wgy.domain.User\"&gt; select * from user where id = #&#123;uid&#125;&lt;/select&gt; 细节： resultType 属性： 用于指定结果集的类型。 parameterType 属性： 用于指定传入参数的类型。 sql 语句中使用#{} 字符 ： 它代表占位符，相当于原来 jdbc 部分所学的?，都是用于执行语句时替换实际的数据。 具体的数据是由#{}里面的内容决定的。 #{} 中内容的写法： 由于数据类型是基本类型，所以此处可以随意写。 1.1.3 在测试类添加测试12345678910111213141516171819202122232425262728293031323334353637383940/** * 测试mybatis的crud操作 * * @author wgy */public class MyBatisTest &#123; private InputStream in; private SqlSession sqlSession; private IUserDao userDao; @Before//用于在测试方法执行之前执行 public void init() throws Exception &#123; //1.读取配置文件，生成字节输入流 in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //2.获取SqlSessionFactory SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); //3.获取SqlSession对象 sqlSession = factory.openSession(); //4.获取dao的代理对象 userDao = sqlSession.getMapper(IUserDao.class); &#125; @After//用于在测试方法执行之后执行 public void destroy() throws Exception &#123; //6.释放资源 sqlSession.close(); in.close(); &#125; /** * 测试查询一个操作 */ @Test public void testFindOne() &#123; //5.执行查询一个方法 User user = userDao.findById(49); System.out.println(user); &#125;&#125; 1.2 保存操作1.2.1 在持久层接口中添加新增方法123456/** * 保存用户 * * @param user */void saveUser(User user); 1.2.2 在用户的映射配置文件中配置12345&lt;!-- 保存用户 --&gt;&lt;insert id=\"saveUser\" parameterType=\"com.wgy.domain.User\"&gt; insert into user(username, address, sex, birthday) values (#&#123;userName&#125;, #&#123;userAddress&#125;, #&#123;userSex&#125;, #&#123;userBirthday&#125;);&lt;/insert&gt; 细节： parameterType 属性： 代表参数的类型，因为我们要传入的是一个类的对象，所以类型就写类的全名称。 sql 语句中使用#{}字符： 它代表占位符，相当于原来 jdbc 部分所学的?，都是用于执行语句时替换实际的数据。 具体的数据是由#{}里面的内容决定的。 #{}中内容的写法： 由于我们保存方法的参数是 一个 User 对象，此处要写 User 对象中的属性名称。 它用的是 ognl 表达式。 ognl 表达式： 它是 apache 提供的一种表达式语言，全称是：Object Graphic Navigation Language 对象图导航语言 它是按照一定的语法格式来获取数据的。 语法格式就是使用 #{对象.对象}的方式 #{user.username}它会先去找 user 对象，然后在 user 对象中找到 username 属性，并调用getUsername()方法把值取出来。但是我们在 parameterType 属性上指定了实体类名称，所以可以省略 user.而直接写 username。 1.2.3 在测试类添加测试12345678910111213141516/** * 测试保存操作 */@Testpublic void testSave() &#123; User user = new User(); user.setUserName(\"insert User property\"); user.setUserAddress(\"北京市顺义区\"); user.setUserSex(\"男\"); user.setUserBirthday(new Date()); System.out.println(\"保存操作之前：\" + user); //5.执行保存方法 userDao.saveUser(user); System.out.println(\"保存操作之后：\" + user);&#125; 打开 Mysql 数据库发现并没有添加任何记录，原因是什么？ 这一点和 jdbc 是一样的，我们在实现增删改时一定要去控制事务的提交，那么在 mybatis 中如何控制事务提交呢？ 可以使用:session.commit();来实现事务提交。加入事务提交后的代码如下： 12345678@After//用于在测试方法执行之后执行public void destroy() throws Exception &#123; //提交事务 sqlSession.commit(); //6.释放资源 sqlSession.close(); in.close();&#125; 1.2.4 问题扩展：新增用户 id 的返回值新增用户后，同时还要返回当前新增用户的 id 值，因为 id 是由数据库的自动增长来实现的，所以就相当于我们要在新增后将自动增长 auto_increment 的值返回。 123456789&lt;!-- 保存用户 --&gt;&lt;insert id=\"saveUser\" parameterType=\"com.wgy.domain.User\"&gt; &lt;!-- 配置插入操作后，获取插入数据的id --&gt; &lt;selectKey keyProperty=\"userId\" keyColumn=\"id\" resultType=\"int\" order=\"AFTER\"&gt; select last_insert_id(); &lt;/selectKey&gt; insert into user(username, address, sex, birthday) values (#&#123;userName&#125;, #&#123;userAddress&#125;, #&#123;userSex&#125;, #&#123;userBirthday&#125;);&lt;/insert&gt; 1.3 用户更新1.3.1 在持久层接口中添加更新方法123456/** * 更新用户 * * @param user */void updateUser(User user); 1.3.2 在用户的映射配置文件中配置123456789&lt;!-- 更新用户 --&gt;&lt;update id=\"updateUser\" parameterType=\"com.wgy.domain.User\"&gt; update user set username=#&#123;userName&#125;, address=#&#123;userAddress&#125;, sex=#&#123;userSex&#125;, birthday=#&#123;userBirthday&#125; where id = #&#123;userId&#125;&lt;/update&gt; 1.3.3 在测试类添加测试123456789101112131415/** * 测试更新操作 */@Testpublic void testUpdate() &#123; User user = new User(); user.setUserId(49); user.setUserName(\"mybastis update user\"); user.setUserAddress(\"北京市顺义区\"); user.setUserSex(\"女\"); user.setUserBirthday(new Date()); //5.执行保存方法 userDao.updateUser(user);&#125; 1.4 用户删除1.4.1 在持久层接口中添加删除方法123456/** * 根据Id删除用户 * * @param userId */void deleteUser(Integer userId); 1.4.2 在用户的映射配置文件中配置123456&lt;!-- 删除用户--&gt;&lt;delete id=\"deleteUser\" parameterType=\"java.lang.Integer\"&gt; delete from user where id = #&#123;uid&#125;&lt;/delete&gt; 1.4.3 在测试类添加测试12345678/** * 测试删除操作 */@Testpublic void testDelete() &#123; //5.执行删除方法 userDao.deleteUser(48);&#125; 1.5 用户模糊查询1.5.1 在持久层接口中添加模糊查询方法1234567/** * 根据名称模糊查询用户信息 * * @param username * @return */List&lt;User&gt; findByName(String username); 1.5.2 在用户的映射配置文件中配置123456&lt;!-- 根据名称模糊查询 --&gt;&lt;select id=\"findByName\" parameterType=\"string\" resultType=\"com.wgy.domain.User\"&gt; select * from user where username like #&#123;name&#125;&lt;/select&gt; 1.5.3 在测试类添加测试1234567891011/** * 测试模糊查询操作 */@Testpublic void testFindByName() &#123; //5.执行查询一个方法 List&lt;User&gt; users = userDao.findByName(\"%王%\"); for (User user : users) &#123; System.out.println(user); &#125;&#125; 在控制台输出的执行 SQL 语句如下： 我们在配置文件中没有加入%来作为模糊查询的条件，所以在传入字符串实参时，就需要给定模糊查询的标识%。配置文件中的#{username}也只是一个占位符，所以 SQL 语句显示为“？”。 1.5.4 模糊查询的另一种配置方式第一步：修改 SQL 语句的配置，配置如下： 123456&lt;!-- 根据名称模糊查询 --&gt;&lt;select id=\"findByName\" parameterType=\"string\" resultType=\"com.wgy.domain.User\"&gt; select * from user where username like '%$&#123;value&#125;%'&lt;/select&gt; 我们在上面将原来的#{}占位符，改成了${value}。注意如果用模糊查询的这种写法，那么${value}的写法就是固定的，不能写成其它名字。 第二步：测试，如下： 1234567891011/** * 测试模糊查询操作 */@Testpublic void testFindByName() &#123; //5.执行查询一个方法 List&lt;User&gt; users = userDao.findByName(\"王\"); for (User user : users) &#123; System.out.println(user); &#125;&#125; 在控制台输出的执行 SQL 语句如下： 可以发现，我们在程序代码中就不需要加入模糊查询的匹配符%了，这两种方式的实现效果是一样的，但执行的语句是不一样的。 1.5.5 #{} 与${} 的区别 #{} 表示一个占位符号 通过#{}可以实现 preparedStatement 向占位符中设置值，自动进行 java 类型和 jdbc 类型转换，#{}可以有效防止 sql 注入。 #{}可以接收简单类型值或 pojo 属性值。 如果 parameterType 传输单个简单类型值，#{}括号中可以是 value 或其它名称。 ${} 表示拼接 sql 串 通过${}可以将 parameterType 传入的内容拼接在 sql中且不进行 jdbc 类型转换， ${}可以接收简单类型值或 pojo 属性值，如果 parameterType 传输单个简单类型值，${}括号中只能是 value。 1.5.6 模糊查询的${value} 源码分析 这就说明了源码中指定了读取的 key 的名字就是”value”，所以我们在绑定参数时就只能叫 value 的名字了。 1.6 查询使用聚合函数1.6.1 在持久层接口中添加聚合函数查询方法123456/** * 查询总用户数 * * @return */int findTotal(); 1.6.2 在用户的映射配置文件中配置12345&lt;!-- 获取用户的总记录条数 --&gt;&lt;select id=\"findTotal\" resultType=\"int\"&gt; select count(id) from user;&lt;/select&gt; 1.6.3 在测试类添加测试123456789/** * 测试查询总记录条数 */@Testpublic void testFindTotal() &#123; //5.执行查询一个方法 int count = userDao.findTotal(); System.out.println(count);&#125; 2. Mybatis 的参数深入2.1 parameterType 配置参数2.1.1 使用说明 SQL 语句传参，使用标签的 parameterType 属性来设定。该属性的取值可以是基本类型，引用类型（例如:String 类型），还可以是实体类类型（POJO 类）。同时也可以使用实体类的包装类 2.1.2 注意事项基本类 型和 String 我们可以直接写类型名称 ，也可以使用包名 . 类名的方式 ，例如 ：java.lang.String。实体类类型，目前我们只能使用全限定类名。 2.2 传递 pojo 包装对象开发中通过 pojo 传递查询条件 ，查询条件是综合的查询条件，不仅包括用户查询条件还包括其它的查询条件（比如将用户购买商品信息也作为查询条件），这时可以使用包装对象传递输入参数。Pojo 类中包含 pojo。 需求：根据用户名查询用户信息，查询条件放到 QueryVo 的 user 属性中。 2.2.1 编写 QueryVo123456789101112131415/** * @author wgy */public class QueryVo &#123; private User user; public User getUser() &#123; return user; &#125; public void setUser(User user) &#123; this.user = user; &#125;&#125; 2.2.2 在持久层接口中添加查询方法123456/** * 根据queryVo中的条件查询用户 * @param vo * @return */List&lt;User&gt; findUserByVo(QueryVo vo); 2.2.3 在用户的映射配置文件中配置123456&lt;!-- 根据queryVo的条件查询用户 --&gt;&lt;select id=\"findUserByVo\" parameterType=\"com.wgy.domain.QueryVo\" resultType=\"com.wgy.domain.User\"&gt; select * from user where username like #&#123;user.userName&#125;&lt;/select&gt; 2.2.4 在测试类添加测试123456789101112131415/** * 测试使用QueryVo作为查询条件 */@Testpublic void testFindByVo() &#123; QueryVo vo = new QueryVo(); User user = new User(); user.setUserName(\"%王%\"); vo.setUser(user); //5.执行查询一个方法 List&lt;User&gt; users = userDao.findUserByVo(vo); for (User u : users) &#123; System.out.println(u); &#125;&#125; 3. Mybatis 的输出结果封装3.1 resultType 配置结果类型resultType 属性可以指定结果集的类型，它支持基本类型和实体类类型。 它和 parameterType 一样，如果注册过类型别名的，可以直接使用别名。没有注册过的必须使用全限定类名。 3.1.1 基本类型示例案例1.6 3.1.2 实体类类型示例案例1.1 3.1.3 特殊情况示例3.1.3.1 修改实体类实体类属性和数据库表的列名已经不一致 3.1.3.2 修改映射配置使用别名查询 12345&lt;!--配置查询所有--&gt;&lt;select id=\"findAll\" resultType=\"com.wgy.domain.User\"&gt; select id as userId, username as userName, address as userAddress, sex as userSex, birthday as userBirthday from user;&lt;/select&gt; 3.2 resultMap 结果类型resultMap 标签可以建立查询的列名和实体类的属性名称不一致时建立对应关系。从而实现封装。 在 select 标签中使用 resultMap 属性指定引用即可。同时 resultMap 可以实现将查询结果映射为复杂类型的 pojo，比如在查询结果映射对象中包括 pojo 和 list 实现一对一查询和一对多查询。 3.2.1 定义 resultMap12345678910111213141516&lt;!-- 建立 User 实体和数据库表的对应关系 type 属性：指定实体类的全限定类名 id 属性：给定一个唯一标识，是给查询 select 标签引用用的。--&gt;&lt;resultMap type&#x3D;&quot;com.wgy.domain.User&quot; id&#x3D;&quot;userMap&quot;&gt; &lt;id column&#x3D;&quot;id&quot; property&#x3D;&quot;userId&quot;&#x2F;&gt; &lt;result column&#x3D;&quot;username&quot; property&#x3D;&quot;userName&quot;&#x2F;&gt; &lt;result column&#x3D;&quot;sex&quot; property&#x3D;&quot;userSex&quot;&#x2F;&gt; &lt;result column&#x3D;&quot;address&quot; property&#x3D;&quot;userAddress&quot;&#x2F;&gt; &lt;result column&#x3D;&quot;birthday&quot; property&#x3D;&quot;userBirthday&quot;&#x2F;&gt;&lt;&#x2F;resultMap&gt;id 标签：用于指定主键字段result 标签：用于指定非主键字段column 属性：用于指定数据库列名property 属性：用于指定实体类属性名称 3.2.2 映射配置12345&lt;!--配置查询所有--&gt;&lt;select id=\"findAll\" resultMap=\"userMap\"&gt; select * from user&lt;/select&gt; 4. Mybatis 传统 DAO 层开发使用 Mybatis 开发 Dao，通常有两个方法，即原始 Dao开发方式和 Mapper 接口代理开发方式。而现在主流的开发方式是接口代理开发方式，这种方式总体上更加简便。 4.1 Mybatis 实现 DAO 的传统开发方式4.1.1 持久层 Dao 接口12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 用户持久层接口 * * @author wgy */public interface IUserDao &#123; /** * 查询所有操作 * * @return */ List&lt;User&gt; findAll(); /** * 保存用户 * * @param user */ void saveUser(User user); /** * 更新用户 * * @param user */ void updateUser(User user); /** * 根据Id删除用户 * * @param userId */ void deleteUser(Integer userId); /** * 根据id查询用户信息 * * @param userId * @return */ User findById(Integer userId); /** * 根据名称模糊查询用户信息 * * @param username * @return */ List&lt;User&gt; findByName(String username); /** * 查询总用户数 * * @return */ int findTotal();&#125; 4.1.2 持久层 Dao 实现类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * 持久层实现类 * * @author wgy */public class UserDaoImpl implements IUserDao &#123; private SqlSessionFactory factory; public UserDaoImpl(SqlSessionFactory factory) &#123; this.factory = factory; &#125; public List&lt;User&gt; findAll() &#123; //1.根据factory获取SqlSession对象 SqlSession session = factory.openSession(); //2.调用SqlSession中的方法，实现查询列表 //参数就是能获取配置信息的key List&lt;User&gt; users = session.selectList(\"com.wgy.dao.IUserDao.findAll\"); //3.释放资源 session.close(); return users; &#125; public void saveUser(User user) &#123; //1.根据factory获取SqlSession对象 SqlSession session = factory.openSession(); //2.调用方法实现保存 session.insert(\"com.wgy.dao.IUserDao.saveUser\", user); //3.提交事务 session.commit(); //4.释放资源 session.close(); &#125; public void updateUser(User user) &#123; //1.根据factory获取SqlSession对象 SqlSession session = factory.openSession(); //2.调用方法实现更新 session.update(\"com.wgy.dao.IUserDao.updateUser\", user); //3.提交事务 session.commit(); //4.释放资源 session.close(); &#125; public void deleteUser(Integer userId) &#123; //1.根据factory获取SqlSession对象 SqlSession session = factory.openSession(); //2.调用方法实现更新 session.update(\"com.wgy.dao.IUserDao.deleteUser\", userId); //3.提交事务 session.commit(); //4.释放资源 session.close(); &#125; public User findById(Integer userId) &#123; //1.根据factory获取SqlSession对象 SqlSession session = factory.openSession(); //2.调用SqlSession中的方法，实现查询一个 User user = session.selectOne(\"com.wgy.dao.IUserDao.findById\", userId); //3.释放资源 session.close(); return user; &#125; public List&lt;User&gt; findByName(String username) &#123; //1.根据factory获取SqlSession对象 SqlSession session = factory.openSession(); //2.调用SqlSession中的方法，实现查询列表 List&lt;User&gt; users = session.selectList(\"com.wgy.dao.IUserDao.findByName\", username); //3.释放资源 session.close(); return users; &#125; public int findTotal() &#123; //1.根据factory获取SqlSession对象 SqlSession session = factory.openSession(); //2.调用SqlSession中的方法，实现查询一个 Integer count = session.selectOne(\"com.wgy.dao.IUserDao.findTotal\"); //3.释放资源 session.close(); return count; &#125;&#125; 4.1.3 持久层映射配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wgy.dao.IUserDao\"&gt; &lt;!--配置查询所有--&gt; &lt;select id=\"findAll\" resultType=\"com.wgy.domain.User\"&gt; select * from user &lt;/select&gt; &lt;!-- 保存用户 --&gt; &lt;insert id=\"saveUser\" parameterType=\"com.wgy.domain.User\"&gt; &lt;!-- 配置插入操作后，获取插入数据的id --&gt; &lt;selectKey keyProperty=\"id\" keyColumn=\"id\" resultType=\"int\" order=\"AFTER\"&gt; select last_insert_id(); &lt;/selectKey&gt; insert into user(username, address, sex, birthday) values (#&#123;username&#125;, #&#123;address&#125;, #&#123;sex&#125;, #&#123;birthday&#125;); &lt;/insert&gt; &lt;!-- 更新用户 --&gt; &lt;update id=\"updateUser\" parameterType=\"com.wgy.domain.User\"&gt; update user set username=#&#123;username&#125;, address=#&#123;address&#125;, sex=#&#123;sex&#125;, birthday=#&#123;birthday&#125; where id = #&#123;id&#125; &lt;/update&gt; &lt;!-- 删除用户--&gt; &lt;delete id=\"deleteUser\" parameterType=\"java.lang.Integer\"&gt; delete from user where id = #&#123;uid&#125; &lt;/delete&gt; &lt;!-- 根据id查询用户 --&gt; &lt;select id=\"findById\" parameterType=\"int\" resultType=\"com.wgy.domain.User\"&gt; select * from user where id = #&#123;uid&#125; &lt;/select&gt; &lt;!-- 根据名称模糊查询 --&gt; &lt;select id=\"findByName\" parameterType=\"string\" resultType=\"com.wgy.domain.User\"&gt; select * from user where username like #&#123;name&#125; &lt;/select&gt; &lt;!-- 获取用户的总记录条数 --&gt; &lt;select id=\"findTotal\" resultType=\"int\"&gt; select count(id) from user; &lt;/select&gt;&lt;/mapper&gt; 4.1.4 测试类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112/** * 测试mybatis的crud操作 * * @author wgy */public class MyBatisTest &#123; private InputStream in; private IUserDao userDao; @Before//用于在测试方法执行之前执行 public void init() throws Exception &#123; //1.读取配置文件，生成字节输入流 in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //2.获取SqlSessionFactory SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(in); //3.使用工厂对象，创建dao对象 userDao = new UserDaoImpl(factory); &#125; @After//用于在测试方法执行之后执行 public void destroy() throws Exception &#123; //6.释放资源 in.close(); &#125; /** * 测试查询所有 */ @Test public void testFindAll() &#123; //5.执行查询所有方法 List&lt;User&gt; users = userDao.findAll(); for (User user : users) &#123; System.out.println(user); &#125; &#125; /** * 测试保存操作 */ @Test public void testSave() &#123; User user = new User(); user.setUsername(\"dao impl user\"); user.setAddress(\"北京市顺义区\"); user.setSex(\"男\"); user.setBirthday(new Date()); System.out.println(\"保存操作之前：\" + user); //5.执行保存方法 userDao.saveUser(user); System.out.println(\"保存操作之后：\" + user); &#125; /** * 测试更新操作 */ @Test public void testUpdate() &#123; User user = new User(); user.setId(50); user.setUsername(\"userdaoimpl update user\"); user.setAddress(\"北京市顺义区\"); user.setSex(\"女\"); user.setBirthday(new Date()); //5.执行保存方法 userDao.updateUser(user); &#125; /** * 测试删除操作 */ @Test public void testDelete() &#123; //5.执行删除方法 userDao.deleteUser(54); &#125; /** * 测试删除操作 */ @Test public void testFindOne() &#123; //5.执行查询一个方法 User user = userDao.findById(50); System.out.println(user); &#125; /** * 测试模糊查询操作 */ @Test public void testFindByName() &#123; //5.执行查询一个方法 List&lt;User&gt; users = userDao.findByName(\"%王%\"); for (User user : users) &#123; System.out.println(user); &#125; &#125; /** * 测试查询总记录条数 */ @Test public void testFindTotal() &#123; //5.执行查询一个方法 int count = userDao.findTotal(); System.out.println(count); &#125;&#125; 5. SqlMapConfig.xml 配置文件5.1 配置内容5.1.1 SqlMapConfig.xml 中配置的内容和顺序1234567891011121314151617-properties （属性） --property-settings（全局配置参数） --setting-typeAliases （类型别名） --typeAliase --package-typeHandlers（类型处理器）-objectFactory（对象工厂）-plugins（插件）-environments（环境集合属性对象） --environment（环境子属性对象） ---transactionManager（事务管理） ---dataSource（数据源）-mappers （映射器） --mapper --package 5.2 properties（属性）在使用 properties 标签配置时，我们可以采用两种方式指定属性配置。 5.2.1 第一种123456&lt;properties&gt; &lt;property name=\"jdbc.driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"jdbc.url\" value=\"jdbc:mysql://localhost:3306/mybatis\"/&gt; &lt;property name=\"jdbc.username\" value=\"root\"/&gt; &lt;property name=\"jdbc.password\" value=\"root\"/&gt;&lt;/properties&gt; 5.2.2 第二种5.2.2.1 在 classpath 下定义 db.properties1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/mybatisjdbc.username=rootjdbc.password=root 5.2.2.2 properties 标签配置12345678910111213141516&lt;!-- 配置properties 可以在标签内部配置连接数据库的信息。也可以通过属性引用外部配置文件信息 resource属性： 常用的 用于指定配置文件的位置，是按照类路径的写法来写，并且必须存在于类路径下。 resource=\"db.properties\" url属性： 是要求按照Url的写法来写地址 URL：Uniform Resource Locator 统一资源定位符。它是可以唯一标识一个资源的位置。 它的写法： http://localhost:8080/mybatisserver/demo1Servlet 协议 主机 端口 URI URI:Uniform Resource Identifier 统一资源标识符。它是在应用中可以唯一定位一个资源的。--&gt;&lt;properties url=\"file:///E:\\MyIDEAWorkSpace\\MyBatis\\MyBatis05_CRUD\\src\\main\\resources\\db.properties\"&gt;&lt;/properties&gt; 5.2.3 dataSource 标签引用配置1234567&lt;dataSource type=\"POOLED\"&gt; &lt;!-- 配置连接数据库的4个基本信息 --&gt; &lt;property name=\"driver\" value=\"$&#123;jdbc.driver&#125;\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"&gt;&lt;/property&gt;&lt;/dataSource&gt; 5.3 typeAliases（类型别名）除了前面 Mybatis 支持的默认别名，也可以采用自定义别名方式来开发。 5.3.1 自定义别名在 SqlMapConfig.xml 中配置： 123456&lt;typeAliases&gt; &lt;!-- 单个别名定义 --&gt; &lt;typeAlias alias=\"user\" type=\"com.wgy.domain.User\"/&gt; &lt;!-- 批量别名定义，扫描整个包下的类，别名为类名（首字母大写或小写都可以） --&gt; &lt;package name=\"com.wgy.domain\"/&gt;&lt;/typeAliases&gt; 5.4 mappers（映射器）5.4.1 &lt;mapper resource=&quot; &quot; /&gt;12使用相对于类路径的资源如：&lt;mapper resource=\"com/wgy/dao/IUserDao.xml\" /&gt; 5.4.2 &lt;mapper class=&quot; &quot; /&gt;123使用 mapper 接口类路径如：&lt;mapper class=\"com.wgy.dao.UserDao\"/&gt;注意：此种方法要求 mapper 接口名称和 mapper 映射文件名称相同，且放在同一个目录中。 5.4.3 &lt;package name=&quot;&quot;/&gt;123注册指定包下的所有 mapper 接口如：&lt;package name=\"com.wgy.dao\"/&gt;注意：此种方法要求 mapper 接口名称和 mapper 映射文件名称相同，且放在同一个目录中。","tags":[{"name":"ORM","slug":"ORM","permalink":"https://wgy1993.gitee.io/tags/ORM/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wgy1993.gitee.io/tags/MyBatis/"}]},{"title":"MyBatis(一)","date":"2020-06-20T07:25:51.000Z","path":"archives/2073aa6b.html","text":"1. 单独使用jdbc编程问题总结1.1 jdbc编程步骤 加载数据库驱动 创建并获取数据库链接 创建jdbc statement对象 设置sql语句 设置sql语句中的参数(使用preparedStatement) 通过statement执行sql并获取结果 对sql执行结果进行解析处理 释放资源(resultSet、preparedstatement、connection) 1.2 jdbc程序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Public static void main(String[] args) &#123; Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; //加载数据库驱动 Class.forName(\"com.mysql.jdbc.Driver\"); //通过驱动管理类获取数据库链接 connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/mybatis?characterEncoding=utf-8\", \"root\", \"root\"); //定义sql语句 ?表示占位符 String sql = \"select * from user where username = ?\"; //获取预处理statement preparedStatement = connection.prepareStatement(sql); //设置参数，第一个参数为sql语句中参数的序号（从1开始），第二个参数为设置的参数值 preparedStatement.setString(1, \"王五\"); //向数据库发出sql执行查询，查询出结果集 resultSet = preparedStatement.executeQuery(); //遍历查询结果集 while(resultSet.next())&#123; System.out.println(resultSet.getString(\"id\")+\" \"+resultSet.getString(\"username\")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; //释放资源 if(resultSet!=null)&#123; try &#123; resultSet.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if(preparedStatement!=null)&#123; try &#123; preparedStatement.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if(connection!=null)&#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 上边使用jdbc的原始方法（未经封装）实现了查询数据库表记录的操作。 1.3 jdbc问题总结如下1、数据库连接，使用时就创建，不使用立即释放，对数据库进行频繁连接开启和关闭，造成数据库资源浪费，影响数据库性能。 设想：使用数据库连接池管理数据库连接。 2、将sql语句硬编码到java代码中，如果sql 语句修改，需要重新编译java代码，不利于系统维护。 设想：将sql语句配置在xml配置文件中，即使sql变化，不需要对java代码进行重新编译。 3、向preparedStatement中设置参数，对占位符号位置和设置参数值，硬编码在java代码中，不利于系统维护。 设想：将sql语句及占位符号和参数全部配置在xml中。 4、从resutSet中遍历结果集数据时，存在硬编码，将获取表的字段进行硬编码，，不利于系统维护。 设想：将查询的结果集，自动映射成java对象。 2. MyBatis框架2.1 MyBatis是什么？mybatis是一个持久层的框架，它对jdbc的操作数据库的过程进行封装，不需要花费精力去处理例如注册驱动、创建connection、创建statement、手动设置参数、结果集检索等jdbc繁杂的过程代码。是apache下的顶级项目。 mybatis托管到goolecode下，再后来托管到github下(https://github.com/mybatis/mybatis-3/releases)。 mybatis让程序将主要精力放在sql上，通过mybatis提供的映射方式，自由灵活生成（半自动化，大部分需要程序员编写sql）满足需要sql语句。 mybatis可以将向 preparedStatement中的输入参数自动进行输入映射，将查询结果集灵活映射成java对象。（输出映射） 2.2 MyBatis框架 2.3 与hibernate不同Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句，不过mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。 总之，按照用户的需求在有限的资源环境下只要能做出维护性、扩展性良好的软件架构都是好架构，所以框架只有适合才是最好。 2.4 Mybatis与iBatis的主要差异对比他们都是优秀的持久层框架，MyBatis是现在最常用的持久层框架，可以动态地拼接sql语句，非常人性化，更适合编辑复杂的sql；iBatis就是MyBatis前身，他们有很多相似的地方 1、传入参数 iBatis是parameterClass，而MyBatis是可以不写的，也可以用parameterType,parameterClass iBatis的传出参数关键字是resultClass，而MyBatis是resultMap 2、判断语句 对于MyBatis的很简单，只要在where或者if标签里面添加test=“”就可以了，里面写判断条件。 但是iBatis的就麻烦了许多了，它将每个方法都进行了封装。例如isNull：判断字段是否为null 3、循环的使用 iBatis是使用Iterate，而MyBatis使用的是ForEach方法 4、MyBatis实现了DAO接口与xml映射文件的绑定，使用更加方便 3. MyBatis 框架快速入门3.1 Mybatis 框架开发的准备3.1.1 官网下载 Mybatis 框架从百度中“mybatis download”可以下载最新的 Mybatis 开发包。https://mybatis.org/mybatis-3/ 进入选择语言的界面，进入中文版本的开发文档。 下载相关的 jar 包或 maven 开发的坐标。 3.2 搭建 Mybatis 开发环境3.2.1 创建 maven 工程创建 MyBatis01 的工程，工程信息如下： 1234&lt;groupId&gt;com.wgy&lt;/groupId&gt;&lt;artifactId&gt;MyBatis01&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; 3.2.2 添加 Mybatis3.4.5 的坐标在 pom.xml 文件中添加 Mybatis3.4.5 的坐标，如下： 1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.2.3 log4j.properties123456789log4j.rootLogger=DEBUG,consolelog4j.additivity.org.apache=true# 控制台(console)log4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.Threshold=DEBUGlog4j.appender.console.ImmediateFlush=truelog4j.appender.console.Target=System.outlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n 3.2.4 编写 User 实体类1234567891011121314/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private Integer id; private String username; private Date birthday; private String sex; private String address; ...&#125; 3.2.5 编写持久层口接口 IUserDao1234567891011121314/** * 用户持久层接口 * * @author wgy */public interface IUserDao &#123; /** * 查询所有操作 * * @return */ List&lt;User&gt; findAll();&#125; 3.2.6 编写持久层接口的件映射文件 IUserDao.xml创建位置：必须和持久层接口在相同的包中。名称：必须以持久层接口名称命名文件名，扩展名是.xml 1234567891011&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace&#x3D;&quot;com.wgy.dao.IUserDao&quot;&gt; &lt;!--配置查询所有--&gt; &lt;select id&#x3D;&quot;findAll&quot; resultType&#x3D;&quot;com.wgy.domain.User&quot;&gt; select * from user &lt;&#x2F;select&gt;&lt;&#x2F;mapper&gt; 3.2.7 编写 SqlMapConfig.xml12345678910111213141516171819202122232425262728&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Config 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-config.dtd&quot;&gt;&lt;!-- mybatis的主配置文件 --&gt;&lt;configuration&gt; &lt;!-- 配置环境 --&gt; &lt;environments default&#x3D;&quot;dev&quot;&gt; &lt;!-- 配置mysql的环境--&gt; &lt;environment id&#x3D;&quot;dev&quot;&gt; &lt;!-- 配置事务的类型--&gt; &lt;transactionManager type&#x3D;&quot;JDBC&quot;&#x2F;&gt; &lt;!-- 配置数据源（连接池） --&gt; &lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;!-- 配置连接数据库的4个基本信息 --&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;com.mysql.jdbc.Driver&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mybatis&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; &lt;&#x2F;environment&gt; &lt;&#x2F;environments&gt; &lt;!-- 指定映射配置文件的位置，映射配置文件指的是每个dao独立的配置文件 --&gt; &lt;mappers&gt; &lt;mapper resource&#x3D;&quot;com&#x2F;wgy&#x2F;dao&#x2F;IUserDao.xml&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt;&lt;&#x2F;configuration&gt; 3.2.8 编写测试类1234567891011121314151617181920212223242526272829303132/** * mybatis的入门案例 * * @author wgy */public class MyBatisTest &#123; /** * 入门案例 * * @param args */ public static void main(String[] args) throws Exception &#123; //1.读取配置文件 InputStream in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //2.创建SqlSessionFactory工厂 SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); SqlSessionFactory factory = builder.build(in); //3.使用工厂生产SqlSession对象 SqlSession session = factory.openSession(); //4.使用SqlSession创建Dao接口的代理对象 IUserDao userDao = session.getMapper(IUserDao.class); //5.使用代理对象执行方法 List&lt;User&gt; users = userDao.findAll(); for (User user : users) &#123; System.out.println(user); &#125; //6.释放资源 session.close(); in.close(); &#125;&#125; 3.3 基于注解的 mybatis 使用3.3.1 在持久层接口中添加注解123456789101112131415/** * 用户持久层接口 * * @author wgy */public interface IUserDao &#123; /** * 查询所有操作 * * @return */ @Select(\"select * from user\") List&lt;User&gt; findAll();&#125; 3.3.2 修改 SqlMapConfig.xml123456&lt;!-- 指定映射配置文件的位置，映射配置文件指的是每个dao独立的配置文件 如果是用注解来配置的话，此处应该使用class属性指定被注解的dao全限定类名--&gt;&lt;mappers&gt; &lt;mapper class&#x3D;&quot;com.wgy.dao.IUserDao&quot;&#x2F;&gt;&lt;&#x2F;mappers&gt; 注意事项：在使用基于注解的 Mybatis 配置时，请移除 xml 的映射配置（IUserDao.xml） 。 4. 自定义 Mybatis 框架4.1 自定义 Mybatis 框架的分析4.1.1 涉及知识点介绍构建一个属于自己的持久层框架，将会涉及到的一些知识点：工厂模式（Factory 工厂模式） 、构造者模式（Builder 模式）、代理模式，反射，自定义注解，注解的反射，xml 解析，数据库元数据，元数据的反射等。 4.1.2 分析流程 4.2 前期准备4.2.1 创建 Maven 工程创建 MyBatis04_Design 的工程，工程信息如下： 1234&lt;groupId&gt;com.wgy&lt;/groupId&gt;&lt;artifactId&gt;MyBatis04_Design&lt;/artifactId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; 4.2.2 引入相关坐标12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;!-- mysql 驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 日志坐标 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.10&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 解析 xml 的 dom4j --&gt; &lt;dependency&gt; &lt;groupId&gt;dom4j&lt;/groupId&gt; &lt;artifactId&gt;dom4j&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- dom4j 的依赖包 jaxen --&gt; &lt;dependency&gt; &lt;groupId&gt;jaxen&lt;/groupId&gt; &lt;artifactId&gt;jaxen&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.2.3 引入 工具类到项目中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200/** * 用于解析配置文件 * * @author wgy */public class XMLConfigBuilder &#123; /** * 解析主配置文件，把里面的内容填充到DefaultSqlSession所需要的地方 * 使用的技术： * dom4j+xpath */ public static Configuration loadConfiguration(InputStream config) &#123; try &#123; //定义封装连接信息的配置对象（mybatis的配置对象） Configuration cfg = new Configuration(); //1.获取SAXReader对象 SAXReader reader = new SAXReader(); //2.根据字节输入流获取Document对象 Document document = reader.read(config); //3.获取根节点 Element root = document.getRootElement(); //4.使用xpath中选择指定节点的方式，获取所有property节点 List&lt;Element&gt; propertyElements = root.selectNodes(\"//property\"); //5.遍历节点 for (Element propertyElement : propertyElements) &#123; //判断节点是连接数据库的哪部分信息 //取出name属性的值 String name = propertyElement.attributeValue(\"name\"); if (\"driver\".equals(name)) &#123; //表示驱动 //获取property标签value属性的值 String driver = propertyElement.attributeValue(\"value\"); cfg.setDriver(driver); &#125; if (\"url\".equals(name)) &#123; //表示连接字符串 //获取property标签value属性的值 String url = propertyElement.attributeValue(\"value\"); cfg.setUrl(url); &#125; if (\"username\".equals(name)) &#123; //表示用户名 //获取property标签value属性的值 String username = propertyElement.attributeValue(\"value\"); cfg.setUsername(username); &#125; if (\"password\".equals(name)) &#123; //表示密码 //获取property标签value属性的值 String password = propertyElement.attributeValue(\"value\"); cfg.setPassword(password); &#125; &#125; //取出mappers中的所有mapper标签，判断他们使用了resource还是class属性 List&lt;Element&gt; mapperElements = root.selectNodes(\"//mappers/mapper\"); //遍历集合 for (Element mapperElement : mapperElements) &#123; //判断mapperElement使用的是哪个属性 Attribute attribute = mapperElement.attribute(\"resource\"); if (attribute != null) &#123; System.out.println(\"使用的是XML\"); //表示有resource属性，用的是XML //取出属性的值 //获取属性的值\"com/wgy/dao/IUserDao.xml\" String mapperPath = attribute.getValue(); //把映射配置文件的内容获取出来，封装成一个map Map&lt;String, Mapper&gt; mappers = loadMapperConfiguration(mapperPath); //给configuration中的mappers赋值 cfg.setMappers(mappers); &#125; else &#123; System.out.println(\"使用的是注解\"); //表示没有resource属性，用的是注解 //获取class属性的值 String daoClassPath = mapperElement.attributeValue(\"class\"); //根据daoClassPath获取封装的必要信息 Map&lt;String, Mapper&gt; mappers = loadMapperAnnotation(daoClassPath); //给configuration中的mappers赋值 cfg.setMappers(mappers); &#125; &#125; //返回Configuration return cfg; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; finally &#123; try &#123; config.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 根据传入的参数，解析XML，并且封装到Map中 * * @param mapperPath 映射配置文件的位置 * @return map中包含了获取的唯一标识（key是由dao的全限定类名和方法名组成） * 以及执行所需的必要信息（value是一个Mapper对象，里面存放的是执行的SQL语句和要封装的实体类全限定类名） */ private static Map&lt;String, Mapper&gt; loadMapperConfiguration(String mapperPath) throws IOException &#123; InputStream in = null; try &#123; //定义返回值对象 Map&lt;String, Mapper&gt; mappers = new HashMap&lt;String, Mapper&gt;(); //1.根据路径获取字节输入流 in = Resources.getResourceAsStream(mapperPath); //2.根据字节输入流获取Document对象 SAXReader reader = new SAXReader(); Document document = reader.read(in); //3.获取根节点 Element root = document.getRootElement(); //4.获取根节点的namespace属性取值 //是组成map中key的部分 String namespace = root.attributeValue(\"namespace\"); //5.获取所有的select节点 List&lt;Element&gt; selectElements = root.selectNodes(\"//select\"); //6.遍历select节点集合 for (Element selectElement : selectElements) &#123; //取出id属性的值 组成map中key的部分 String id = selectElement.attributeValue(\"id\"); //取出resultType属性的值 组成map中value的部分 String resultType = selectElement.attributeValue(\"resultType\"); //取出文本内容 组成map中value的部分 String queryString = selectElement.getText(); //创建Key String key = namespace + \".\" + id; //创建Value Mapper mapper = new Mapper(); mapper.setQueryString(queryString); mapper.setResultType(resultType); //把key和value存入mappers中 mappers.put(key, mapper); &#125; return mappers; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; finally &#123; in.close(); &#125; &#125; /** * 根据传入的参数，得到dao中所有被select注解标注的方法。 * 根据方法名称和类名，以及方法上注解value属性的值，组成Mapper的必要信息 * * @param daoClassPath * @return */ private static Map&lt;String, Mapper&gt; loadMapperAnnotation(String daoClassPath) throws Exception &#123; //定义返回值对象 Map&lt;String, Mapper&gt; mappers = new HashMap&lt;String, Mapper&gt;(); //1.得到dao接口的字节码对象 Class daoClass = Class.forName(daoClassPath); //2.得到dao接口中的方法数组 Method[] methods = daoClass.getMethods(); //3.遍历Method数组 for (Method method : methods) &#123; //取出每一个方法，判断是否有select注解 boolean isAnnotated = method.isAnnotationPresent(Select.class); if (isAnnotated) &#123; //创建Mapper对象 Mapper mapper = new Mapper(); //取出注解的value属性值 Select selectAnno = method.getAnnotation(Select.class); String queryString = selectAnno.value(); mapper.setQueryString(queryString); //获取当前方法的返回值，还要求必须带有泛型信息 //List&lt;User&gt; Type type = method.getGenericReturnType(); //判断type是不是参数化的类型 if (type instanceof ParameterizedType) &#123; //强转 ParameterizedType ptype = (ParameterizedType) type; //得到参数化类型中的实际类型参数 Type[] types = ptype.getActualTypeArguments(); //取出第一个 Class domainClass = (Class) types[0]; //获取domainClass的类名 String resultType = domainClass.getName(); //给Mapper赋值 mapper.setResultType(resultType); &#125; //组装key的信息 //获取方法的名称 String methodName = method.getName(); String className = method.getDeclaringClass().getName(); String key = className + \".\" + methodName; //给map赋值 mappers.put(key, mapper); &#125; &#125; return mappers; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * 责执行SQL语句，并且封装结果集 * * @author wgy */public class Executor &#123; public &lt;E&gt; List&lt;E&gt; selectList(Mapper mapper, Connection conn) &#123; PreparedStatement pstm = null; ResultSet rs = null; try &#123; //1.取出mapper中的数据 //select * from user String queryString = mapper.getQueryString(); //com.wgy.domain.User String resultType = mapper.getResultType(); Class domainClass = Class.forName(resultType); //2.获取PreparedStatement对象 pstm = conn.prepareStatement(queryString); //3.执行SQL语句，获取结果集 rs = pstm.executeQuery(); //4.封装结果集 //定义返回值 List&lt;E&gt; list = new ArrayList&lt;E&gt;(); while (rs.next()) &#123; //实例化要封装的实体类对象 E obj = (E) domainClass.newInstance(); //取出结果集的元信息：ResultSetMetaData ResultSetMetaData rsmd = rs.getMetaData(); //取出总列数 int columnCount = rsmd.getColumnCount(); //遍历总列数 for (int i = 1; i &lt;= columnCount; i++) &#123; //获取每列的名称，列名的序号是从1开始的 String columnName = rsmd.getColumnName(i); //根据得到列名，获取每列的值 Object columnValue = rs.getObject(columnName); //给obj赋值：使用Java内省机制（借助PropertyDescriptor实现属性的封装） //要求：实体类的属性和数据库表的列名保持一种 PropertyDescriptor pd = new PropertyDescriptor(columnName, domainClass); //获取它的写入方法 Method writeMethod = pd.getWriteMethod(); //把获取的列的值，给对象赋值 writeMethod.invoke(obj, columnValue); &#125; //把赋好值的对象加入到集合中 list.add(obj); &#125; return list; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; finally &#123; release(pstm, rs); &#125; &#125; private void release(PreparedStatement pstm, ResultSet rs) &#123; if (rs != null) &#123; try &#123; rs.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; if (pstm != null) &#123; try &#123; pstm.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 12345678910111213141516171819202122/** * 用于创建数据源的工具类 * * @author wgy */public class DataSourceUtil &#123; /** * 用于获取一个连接 * * @param cfg * @return */ public static Connection getConnection(Configuration cfg) &#123; try &#123; Class.forName(cfg.getDriver()); return DriverManager.getConnection(cfg.getUrl(), cfg.getUsername(), cfg.getPassword()); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 4.2.4 编写 SqlMapConfig.xml12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!-- mybatis的主配置文件 --&gt;&lt;configuration&gt; &lt;!-- 配置环境 --&gt; &lt;environments default=\"mysql\"&gt; &lt;!-- 配置mysql的环境--&gt; &lt;environment id=\"mysql\"&gt; &lt;!-- 配置事务的类型--&gt; &lt;transactionManager type=\"JDBC\"/&gt; &lt;!-- 配置数据源（连接池） --&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;!-- 配置连接数据库的4个基本信息 --&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/mybatis\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- 指定映射配置文件的位置，映射配置文件指的是每个dao独立的配置文件 --&gt; &lt;mappers&gt; &lt;mapper resource=\"com/wgy/dao/IUserDao.xml\"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 注意：此处我们直接使用的是 mybatis 的配置文件，但是由于我们没有使用 mybatis 的 jar 包，所以要把配置文件的约束删掉否则会报错（如果电脑能接入互联网，不删也行） 4.2.5 编写读取配置文件类1234567891011121314151617/** * 使用类加载器读取配置文件的类 * * @author wgy */public class Resources &#123; /** * 根据传入的参数，获取一个字节输入流 * * @param filePath * @return */ public static InputStream getResourceAsStream(String filePath) &#123; return Resources.class.getClassLoader().getResourceAsStream(filePath); &#125;&#125; 4.2.6 编写 Mapper 类12345678910111213141516171819202122232425262728/** * 用于封装执行的SQL语句和结果类型的全限定类名 * * @author wgy */public class Mapper &#123; //SQL private String queryString; //实体类的全限定类名 private String resultType; public String getQueryString() &#123; return queryString; &#125; public void setQueryString(String queryString) &#123; this.queryString = queryString; &#125; public String getResultType() &#123; return resultType; &#125; public void setResultType(String resultType) &#123; this.resultType = resultType; &#125;&#125; 4.2.7 编写 Configuration 配置类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 自定义mybatis的配置类 * * @author wgy */public class Configuration &#123; private String driver; private String url; private String username; private String password; private Map&lt;String, Mapper&gt; mappers = new HashMap&lt;String, Mapper&gt;(); public String getDriver() &#123; return driver; &#125; public void setDriver(String driver) &#123; this.driver = driver; &#125; public String getUrl() &#123; return url; &#125; public void setUrl(String url) &#123; this.url = url; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public Map&lt;String, Mapper&gt; getMappers() &#123; return mappers; &#125; public void setMappers(Map&lt;String, Mapper&gt; mappers) &#123; //此处需要使用追加的方式 this.mappers.putAll(mappers); &#125;&#125; 4.2.8 编写 User 实体类1234567891011121314/** * 用户实体类 * * @author wgy */public class User implements Serializable &#123; private Integer id; private String username; private Date birthday; private String sex; private String address; ...&#125; 4.3 基于 XML 的自定义 mybatis 框架4.3.1 编写持久层接口和 IUserDao.xml1234567891011121314/** * 用户持久层接口 * * @author wgy */public interface IUserDao &#123; /** * 查询所有操作 * * @return */ List&lt;User&gt; findAll();&#125; 12345678&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;mapper namespace=\"com.wgy.dao.IUserDao\"&gt; &lt;!--配置查询所有--&gt; &lt;select id=\"findAll\" resultType=\"com.wgy.domain.User\"&gt; select * from user &lt;/select&gt;&lt;/mapper&gt; 注意：此处我们使用的也是 mybatis 的配置文件，所以也要把约束删除了 4.3.2 编写构建者类123456789101112131415161718/** * 用于创建一个SqlSessionFactory对象 * * @author wgy */public class SqlSessionFactoryBuilder &#123; /** * 根据参数的字节输入流来构建一个SqlSessionFactory工厂 * * @param config * @return */ public SqlSessionFactory build(InputStream config) &#123; Configuration cfg = XMLConfigBuilder.loadConfiguration(config); return new DefaultSqlSessionFactory(cfg); &#125;&#125; 4.3.3 编写 SqlSessionFactory 接口和实现类123456789101112/** * @author wgy */public interface SqlSessionFactory &#123; /** * 用于打开一个新的SqlSession对象 * * @return */ SqlSession openSession();&#125; 12345678910111213141516171819202122/** * SqlSessionFactory接口的实现类 * * @author wgy */public class DefaultSqlSessionFactory implements SqlSessionFactory &#123; private Configuration cfg; public DefaultSqlSessionFactory(Configuration cfg) &#123; this.cfg = cfg; &#125; /** * 用于创建一个新的操作数据库对象 * * @return */ public SqlSession openSession() &#123; return new DefaultSqlSession(cfg); &#125;&#125; 4.3.4 编写 SqlSession 接口和实现类12345678910111213141516171819202122/** * 自定义Mybatis中和数据库交互的核心类 * 它里面可以创建dao接口的代理对象 * * @author wgy */public interface SqlSession &#123; /** * 根据参数创建一个代理对象 * * @param daoInterfaceClass dao的接口字节码 * @param &lt;T&gt; * @return */ &lt;T&gt; T getMapper(Class&lt;T&gt; daoInterfaceClass); /** * 释放资源 */ void close();&#125; 12345678910111213141516171819202122232425262728293031323334353637383940/** * SqlSession接口的实现类 * * @author wgy */public class DefaultSqlSession implements SqlSession &#123; private Configuration cfg; private Connection connection; public DefaultSqlSession(Configuration cfg) &#123; this.cfg = cfg; connection = DataSourceUtil.getConnection(cfg); &#125; /** * 用于创建代理对象 * * @param daoInterfaceClass dao的接口字节码 * @param &lt;T&gt; * @return */ public &lt;T&gt; T getMapper(Class&lt;T&gt; daoInterfaceClass) &#123; return (T) Proxy.newProxyInstance(daoInterfaceClass.getClassLoader(), new Class[]&#123;daoInterfaceClass&#125;, new MapperProxy(cfg.getMappers(), connection)); &#125; /** * 用于释放资源 */ public void close() &#123; if (connection != null) &#123; try &#123; connection.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 4.3.5 编写用于创建 Dao 接口代理对象的类123456789101112131415161718192021222324252627282930313233343536373839404142/** * 代理实现类 * * @author wgy */public class MapperProxy implements InvocationHandler &#123; //map的key是全限定类名+方法名 private Map&lt;String, Mapper&gt; mappers; private Connection conn; public MapperProxy(Map&lt;String, Mapper&gt; mappers, Connection conn) &#123; this.mappers = mappers; this.conn = conn; &#125; /** * 用于对方法进行增强的，我们的增强其实就是调用selectList方法 * * @param proxy * @param method * @param args * @return * @throws Throwable */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //1.获取方法名 String methodName = method.getName(); //2.获取方法所在类的名称 String className = method.getDeclaringClass().getName(); //3.组合key String key = className + \".\" + methodName; //4.获取mappers中的Mapper对象 Mapper mapper = mappers.get(key); //5.判断是否有mapper if (mapper == null) &#123; throw new IllegalArgumentException(\"传入的参数有误\"); &#125; //6.调用工具类执行查询所有 return new Executor().selectList(mapper, conn); &#125;&#125; 4.3.6 运行测试类1234567891011121314151617181920212223242526272829303132/** * mybatis的入门案例 * * @author wgy */public class MyBatisTest &#123; /** * 入门案例 * * @param args */ public static void main(String[] args) throws Exception &#123; //1.读取配置文件 InputStream in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //2.创建SqlSessionFactory工厂 SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); SqlSessionFactory factory = builder.build(in); //3.使用工厂生产SqlSession对象 SqlSession session = factory.openSession(); //4.使用SqlSession创建Dao接口的代理对象 IUserDao userDao = session.getMapper(IUserDao.class); //5.使用代理对象执行方法 List&lt;User&gt; users = userDao.findAll(); for (User user : users) &#123; System.out.println(user); &#125; //6.释放资源 session.close(); in.close(); &#125;&#125; 4.4 基于注解方式定义 Mybatis 框架4.4.1 自定义@Select 注解12345678910111213141516/** * 查询的注解 * * @author wgy */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface Select &#123; /** * 配置SQL语句的 * * @return */ String value();&#125; 4.4.2 修改持久层接口123456789101112131415/** * 用户持久层接口 * * @author wgy */public interface IUserDao &#123; /** * 查询所有操作 * * @return */ @Select(\"select * from user\") List&lt;User&gt; findAll();&#125; 4.4.3 修改 SqlMapConfig.xml1234&lt;!-- 指定映射配置文件的位置，映射配置文件指的是每个dao独立的配置文件 --&gt;&lt;mappers&gt; &lt;mapper class=\"com.wgy.dao.IUserDao\"/&gt;&lt;/mappers&gt;","tags":[{"name":"ORM","slug":"ORM","permalink":"https://wgy1993.gitee.io/tags/ORM/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://wgy1993.gitee.io/tags/MyBatis/"}]},{"title":"Maven(三)","date":"2020-06-18T16:03:44.000Z","path":"archives/898b4292.html","text":"1. maven 私服1.1 需求正式开发，不同的项目组开发不同的工程。 ssm_dao 工程开发完毕，发布到私服。 ssm_service 从私服下载 dao 1.2 分析公司在自己的局域网内搭建自己的远程仓库服务器，称为私服，私服服务器即是公司内部的 maven 远程仓库，每个员工的电脑上安装 maven 软件并且连接私服服务器，员工将自己开发的项目打成 jar 并发布到私服服务器，其它项目组从私服服务器下载所依赖的构件（jar）。 私服还充当一个代理服务器，当私服上没有 jar 包会从互联网中央仓库自动下载，如下图： 1.3 搭建私服环境1.3.1 下载 nexusNexus 是 Maven 仓库管理器，通过 nexus 可以搭建 maven 仓库，同时 nexus 还提供强 大的仓库管理功能，构件搜索功能等。 下载 Nexus， 下载地址：https://www.sonatype.com/download-oss-sonatype 1.3.2 安装 nexus解压 nexus-2.12.0-01-bundle.zip，本教程将它解压在 F 盘，进入 bin 目录： cmd进入 bin目录，执行 nexus.bat install 安装成功在服务中查看有 nexus 服务： 1.3.3 卸载 nexuscmd 进入 nexus 的 bin 目录，执行：nexus.bat uninstall 查看 window 服务列表 nexus 已被删除。 1.3.4 启动 nexus方法 1：cmd 进入 bin 目录，执行 nexus.bat start 方法 2：直接启动 nexus 服务 1.3.4.1 查看 nexus 的配置文件conf/nexus.properties 12345678# Jetty sectionapplication-port=8081 # nexus 的访问端口配置 application-host=0.0.0.0 # nexus 主机监听配置(不用修改) nexus-webapp=$&#123;bundleBasedir&#125;/nexus # nexus 工程目录 nexus-webapp-context-path=/nexus # nexus 的 web 访问路径# Nexus sectionnexus-work=$&#123;bundleBasedir&#125;/../sonatype-work/nexus # nexus 仓库目录runtime=$&#123;bundleBasedir&#125;/nexus/WEB-INF # nexus 运行程序目录 1.3.4.2 访问http://localhost:8081/nexus/ 1.3.4.3 使用 Nexus 内置账户登陆点击右上角的 Log in，输入账号和密码登陆 admin/admin123 登陆成功： 1.3.5 仓库类型1.3.5.1 查看 nexus 的仓库 1.3.5.2 nexus 的仓库有 4 种类型 1、hosted，宿主仓库，部署自己的 jar 到这个类型的仓库，包括 releases 和 snapshot 两部 分，Releases 公司内部发布版本仓库、 Snapshots 公司内部测试版本仓库 2、proxy，代理仓库，用于代理远程的公共仓库，如 maven 中央仓库，用户连接私服，私服自动去中央仓库下载 jar 包或者插件。 3、group，仓库组，用来合并多个 hosted/proxy 仓库，通常我们配置自己的 maven 连接仓库组。 4、virtual(虚拟)：兼容 Maven1 版本的 jar 或者插件 1.3.5.3 nexus 仓库目录默认在 sonatype-work 目录中： central：代理仓库，代理中央仓库 apache-snapshots：代理仓库，存储 snapshots 构件，代理地址 https://repository.apache.org/snapshots/ central-m1：virtual 类型仓库，兼容 Maven1 版本的 jar 或者插件 releases：本地仓库，存储 releases 构件。 snapshots：本地仓库，存储snapshots 构件。 thirdparty：第三方仓库 public：仓库组 1.4 将项目发布到私服1.4.1 需求企业中多个团队协作开发通常会将一些公用的组件、开发模块等发布到私服供其它团队 或模块开发人员使用。 本例子假设多团队分别开发 ssm_dao、ssm_service、ssm_web，某个团队开发完在 ssm_dao 会将 ssm_dao 发布到私服供 ssm_service 团队使用，本例子会将 ssm_dao 工程打成 jar 包发布到私服。 1.4.2 配置1.4.2.1 修改 settings.xml文件需要在客户端即部署 ssm_dao 工程的电脑上配置 maven 环境，并修改 settings.xml文件，配置连接私服的用户和密码 。 此用户名和密码用于私服校验，因为私服需要知道上传的账号和密码是否和私服中的账号和 密码一致。 12345678910111213141516&lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt;&lt;!-- releases 连接发布版本项目仓库 snapshots 连接测试版本项目仓库--&gt; 1.4.2.2 配置项目 pom.xml配置私服仓库的地址，本公司的自己的 jar 包会上传到私服的宿主仓库，根据工程的版本号决定上传到哪个宿主仓库，如果版本为 release 则上传到私服的 release 仓库，如果版本为 snapshot 则上传到私服的 snapshot 仓库 12345678910&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 注意：pom.xml 这里&lt;id&gt; 和 settings.xml 配置 &lt;id&gt; 对应！ 1.4.3 测试将项目 dao 工程打成 jar 包发布到私服： 1、首先启动 nexus 2、对 ssm_dao 工程执行 deploy 命令 根据本项目pom.xml中version定义决定发布到哪个仓库，如果 version定义为 snapshot， 执行 deploy 后查看 nexus 的 snapshot 仓库，如果 version 定义为 release 则项目将发布到 nexus 的 release 仓库。 1.5 从私服下载 jar 包1.5.1 需求没有配置 nexus 之前，如果本地仓库没有，去中央仓库下载，通常在企业中会在局域网 内部署一台私服服务器，有了私服本地项目首先去本地仓库找 jar，如果没有找到则连接私 服从私服下载 jar 包，如果私服没有 jar 包私服同时作为代理服务器从中央仓库下载 jar 包， 这样做的好处是一方面由私服对公司项目的依赖 jar 包统一管理，一方面提高下载速度，项 目连接私服下载 jar 包的速度要比项目连接中央仓库的速度快的多。 本例子测试从私服下载 ssm_dao 工程 jar 包。 1.5.2 管理仓库组nexus中包括很多仓库，hosted 中存放的是企业自己发布的jar包及第三方公司的 jar包， proxy 中存放的是中央仓库的 jar，为了方便从私服下载 jar 包可以将多个仓库组成一个仓库 组，每个工程需要连接私服的仓库组下载 jar 包。 打开 nexus 配置仓库组，如下图： 上图中仓库组包括了本地仓库、代理仓库等。 1.5.3 在 settings.xml 中配置仓库在客户端的 settings.xml 中配置私服的仓库，由于 settings.xml 中没有 repositories 的配置标签需要使用 profile 定义仓库。 1234567891011121314151617181920212223242526272829&lt;profile&gt; &lt;!--profile 的 id--&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;!--仓库 id，repositories 可以配置多个仓库，保证 id 不重复--&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;!--仓库地址，即 nexus 仓库组的地址--&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;!--是否下载 releases 构件--&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;!--是否下载 snapshots 构件--&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;!-- 插件仓库，maven 的运行依赖插件，也需要从私服下载插件 --&gt; &lt;pluginRepository&gt; &lt;!-- 插件仓库的 id 不允许重复，如果重复后边配置会覆盖前边 --&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt;&lt;/profile&gt; 使用 profile 定义仓库需要激活才可生效。 123&lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 2. 把第三方 jar 包放入本地仓库或私服2.1 导入本地库随便找一个 jar 包测试，可以先 CMD 进入到 jar 包所在位置，运行 1mvn install:install-file -DgroupId&#x3D;com.alibaba -DartifactId&#x3D;fastjson -Dversion&#x3D;1.1.37 -Dfile&#x3D;fastjson-1.1.37.jar -Dpackaging&#x3D;jar 2.2 导入私服需要在 maven 软件的核心配置文件 settings.xml 中配置第三方仓库的 server 信息 12345&lt;server&gt; &lt;id&gt;thirdparty&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; CMD执行命令 1mvn deploy:deploy-file -DgroupId&#x3D;com.alibaba -DartifactId&#x3D;fastjson -Dversion&#x3D;1.1.37 -Dpackaging&#x3D;jar -Dfile&#x3D;fastjson-1.1.37.jar -Durl&#x3D;http:&#x2F;&#x2F;localhost:8081&#x2F;nexus&#x2F;content&#x2F;repositories&#x2F;thirdparty&#x2F; -DrepositoryId&#x3D;thirdparty 2.3 参数说明 DgroupId 和 DartifactId 构成了该 jar 包在 pom.xml 的坐标，项目就是依靠这两个属性定位。 自己起名字也行。 Dfile 表示需要上传的 jar 包的绝对路径。 Durl 私服上仓库的位置，打开 nexus——&gt;repositories 菜单，可以看到该路径。 DrepositoryId 服务器的表示 id，在 nexus 的 configuration 可以看到。 Dversion 表示版本信息。 关于 jar 包准确的版本： 包的名字上一般会带版本号，如果没有那可以解压该包，会发现一个叫 MANIFEST.MF 的文件，这个文件就有描述该包的版本信息。","tags":[{"name":"Maven","slug":"Maven","permalink":"https://wgy1993.gitee.io/tags/Maven/"},{"name":"项目管理","slug":"项目管理","permalink":"https://wgy1993.gitee.io/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"}]},{"title":"Maven(二)","date":"2020-06-18T14:07:00.000Z","path":"archives/f77862b9.html","text":"1. maven 构建 SSM 工程1.1 需求实现 SSM 工程构建，规范依赖管理。场景：根据 id 展示商品信息 1.2 准备数据库1234567891011121314151617181920212223SET FOREIGN_KEY_CHECKS&#x3D;0;-- ------------------------------ Table structure for &#96;items&#96;-- ----------------------------DROP TABLE IF EXISTS &#96;items&#96;;CREATE TABLE &#96;items&#96; ( &#96;id&#96; int(10) NOT NULL auto_increment, &#96;name&#96; varchar(20) default NULL, &#96;price&#96; float(10,0) default NULL, &#96;pic&#96; varchar(40) default NULL, &#96;createtime&#96; datetime default NULL, &#96;detail&#96; varchar(200) default NULL, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;9 DEFAULT CHARSET&#x3D;utf8;-- ------------------------------ Records of items-- ----------------------------INSERT INTO &#96;items&#96; VALUES (&#39;1&#39;, &#39;插入测试1&#39;, &#39;1000&#39;, null, &#39;2018-03-13 09:29:30&#39;, &#39;插入测试&#39;);INSERT INTO &#96;items&#96; VALUES (&#39;2&#39;, &#39;插入测试2&#39;, null, null, &#39;2018-03-28 10:05:52&#39;, &#39;插入测试&#39;);INSERT INTO &#96;items&#96; VALUES (&#39;3&#39;, &#39;插入测试3&#39;, &#39;199&#39;, null, &#39;2018-03-07 10:08:04&#39;, &#39;插入测试&#39;);INSERT INTO &#96;items&#96; VALUES (&#39;4&#39;, &#39;插入测试4&#39;, null, null, null, null);INSERT INTO &#96;items&#96; VALUES (&#39;5&#39;, &#39;插入测试5&#39;, null, null, null, null); 1.3 创建一个 maven 工程1.3.1 新建 ssm_maven 项目,使用下图选中的骨架 1.3.2 填写坐标 1.3.3 查看是否使用的自己的私服 1.3.4 在 main 目录下新建 java 和 resources 文件夹把 java 和 resources 文件夹转成 Sources Root和Resources Root 1.3.5 修改编译版本，在 pom.xml 文件中添加123456789101112131415&lt;build&gt; &lt;plugins&gt; &lt;!-- 设置编译版本为1.8 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 1.4 知识点准备1.4.1 什么是依赖传递先添加 springmvc 的核心依赖的坐标 12345678&lt;!-- 项目依赖jar包 --&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 会发现出现除了 spring-webmvc 以外的其他 jar。因为我们的项目依赖 spring-webmv.jar，而 spring-webmv.jar 会依赖 spring-beans.jar 等等，所以 spring-beans.jar 这些 jar 包也出现在了我 们的 maven 工程中，这种现象我们称为依赖传递。从下图中可看到他们的关系：（请注意 spring-beans 的版本） 1.4.2 依赖冲突的解决接着添加一个依赖 1234567891011121314&lt;!-- 项目依赖jar包 --&gt;&lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 我们会发现这两个 jar 包同时都依赖了 spring-beans spring-webmvc 依赖 spirng-beans-4.2.4，spring-context 依赖 spring-beans-5.0.2，但是发现 spirng-beans-4.2.4 加入到工程中。而我们希望 spring-beans-5.0.2 加入工程。这就造成了依赖冲突。 1.4.2.1 依赖调解原则maven 自动按照下边的原则调解： 第一声明者优先原则： 在 pom 文件定义依赖，先声明的依赖为准。 测试： 如果将上边 spring-webmvc 和 spring-context 顺序颠倒，系统将导入 spring-beans-5.0.2。 分析：由于 spring-webmvc 在前边以 spring-webmvc 依赖的 spring-beans-5.0.2 为准，所以最终spring-beans-5.0.2 添加到了工程中。 路径近者优先原则： 例如：还是上述情况，spring-contex 和 spring-webmvc 都会传递过来 spirng-beans，那 如果直接把 spring-beans 的依赖直接写到 pom 文件中，那么项目就不会再使用其他依赖传 递来的 spring-beans，因为自己直接在 pom 中定义 spring-beans 要比其他依赖传递过来的路径要近。在本工程中的 pom 中加入 spirng-beans-5.0.2 的依赖，根据路径近者优先原则，系统将导入spirng-beans-5.0.2： 12345678910111213141516171819&lt;!-- 项目依赖jar包 --&gt;&lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.4.2.2 排除依赖上边的问题也可以通过排除依赖方法辅助依赖调解，如下： 比如在依赖 spring-webmvc 的设置中添加排除依赖，排除 spring-beans， 下边的配置表示：依赖 spring-webmvc，但排除 spring-webmvc 所依赖的 spring-beans。 1234567891011121314151617181920&lt;!-- 项目依赖jar包 --&gt;&lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.2.4.RELEASE&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.4.2.3 锁定版本面对众多的依赖，有一种方法不用考虑依赖路径、声明优化等因素可以采用直接锁定版 本的方法确定依赖构件的版本，版本锁定后则不考虑依赖的声明顺序或依赖的路径，以锁定的版本的为准添加到工程中，此方法在企业开发中常用。 如下的配置是锁定了 spring-beans 和 spring-context 的版本： 123456789101112131415&lt;!-- 锁定jar包版本 --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 还可以把版本号提取出来，使用&lt;properties&gt;标签设置成变量。 1234567891011121314151617181920&lt;!-- 统一管理jar包版本 --&gt;&lt;properties&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt;&lt;/properties&gt;&lt;!-- 锁定jar包版本 --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 注意：在工程中锁定依赖的版本并不代表在工程中添加了依赖，如果工程需要添加锁定版本的依赖则需要单独添加&lt;dependencies&gt;&lt;/dependencies&gt;标签，如下： 123456789101112&lt;!-- 项目依赖jar包 --&gt;&lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 上边添加的依赖并没有指定版本，原因是已在&lt;dependencyManagement&gt;中锁定了版本， 所以在&lt;dependency&gt;下不需要再指定版本。 1.5 定义 pom.xmlmaven 工程首先要识别依赖，web 工程实现 SSM 整合，需要依赖 spring-webmvc5.0.2、 spring5.0.2、mybatis3.4.5 等，在 pom.xml 添加工程如下依赖： （在实际企业开发中会有架构师专门来编写 pom.xml） 分两步： 锁定依赖版本 添加依赖 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222&lt;!-- 统一管理jar包版本 --&gt;&lt;properties&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt; &lt;mysql.version&gt;5.1.6&lt;/mysql.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt; &lt;spring.security.version&gt;5.0.1.RELEASE&lt;/spring.security.version&gt;&lt;/properties&gt;&lt;!-- 锁定jar包版本 --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;!-- 项目依赖jar包 --&gt;&lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jstl&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log start --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log end --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.security.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.security.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.security.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-taglibs&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.security.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.9&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- 添加tomcat7插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;80&lt;/port&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- 设置编译版本为1.8 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;configuration&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 1.6 Dao 层在 src/main/java 中定义 dao 接口，实现根据 id 查询商品信息： 1.6.1 pojo 模型类在 src/main/java 创建模型类 123456789101112131415/** * 实体类 * * @author wgy */public class Items implements Serializable &#123; private Integer id; private String name; private Double price; private String pic; private Date createtime; private String detail; ...&#125; 1.6.2 dao 层代码123456789101112131415/** * 持久接口 * * @author wgy */public interface ItemsDao &#123; /** * 根据id查询 * * @param id * @return */ public Items findById(Integer id);&#125; 1.6.3 配置文件1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.wgy.dao.ItemsDao\"&gt; &lt;select id=\"findById\" parameterType=\"int\" resultType=\"items\"&gt; select * from items where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 注意配置文件位置（resources下创建目录/com/wgy/dao）： 在 src/main/resources 配置 log4j.properties 12345678### direct log messages to stdout ###log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target=System.outlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d&#123;ABSOLUTE&#125; %5 p %c&#123;1&#125;:%L - %m%n### set log levels - for more verbose logging change 'info' to 'debug' ####在开发阶段日志级别使用 debuglog4j.rootLogger=debug, stdout 在 src/main/resources 创建 applicationContext.xml 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\"&gt; &lt;!--dao层配置文件开始--&gt; &lt;!--配置连接池--&gt; &lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql:///maven\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt; &lt;/bean&gt; &lt;!--配置生产SqlSession对象的工厂--&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!--扫描pojo包，给包下所有pojo对象起别名--&gt; &lt;property name=\"typeAliasesPackage\" value=\"com.wgy.domain\"/&gt; &lt;/bean&gt; &lt;!--扫描接口包路径，生成包下所有接口的代理对象，并且放入spring容器中--&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.wgy.dao\"/&gt; &lt;/bean&gt; &lt;!--dao层配置文件结束--&gt;&lt;/beans&gt; 1.6.4 单元测试在 src/test/java 创建单元测试类 12345678910111213141516171819/** * 测试 * * @author wgy */public class ItemsTest &#123; @Test public void findById() &#123; //获取spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //dao测试 //从容器中拿到所需的dao的代理对象 ItemsDao itemsDao = ac.getBean(ItemsDao.class); //调用方法 Items items = itemsDao.findById(1); System.out.println(items.getName()); &#125;&#125; 1.7 Service 层1.7.1 代码123456789101112131415/** * 业务层接口 * * @author wgy */public interface ItemsService &#123; /** * 根据id查询 * * @param id * @return */ public Items findById(Integer id);&#125; 123456789101112131415/** * 持久层实现类 * * @author wgy */@Servicepublic class ItemsServiceImpl implements ItemsService &#123; @Autowired private ItemsDao itemsDao; public Items findById(Integer id) &#123; return itemsDao.findById(id); &#125;&#125; 1.7.2 配置文件在 applicationContext.xml 中配置 service 123456789101112131415161718192021222324252627&lt;!--service层配置文件开始--&gt;&lt;!--组件扫描配置--&gt;&lt;context:component-scan base-package=\"com.wgy.service\"/&gt;&lt;!--aop面向切面编程，切面就是切入点和通知的组合--&gt;&lt;!--配置事务管理器--&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt;&lt;!--配置事务的通知--&gt;&lt;tx:advice id=\"advice\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"save*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"update*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"delete*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"find*\" read-only=\"true\" propagation=\"SUPPORTS\"/&gt; &lt;tx:method name=\"*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!--配置切面--&gt;&lt;aop:config&gt; &lt;aop:pointcut id=\"pointcut\" expression=\"execution(* com.wgy.service.impl.*.*(..))\"/&gt; &lt;aop:advisor advice-ref=\"advice\" pointcut-ref=\"pointcut\"/&gt;&lt;/aop:config&gt;&lt;!--service层配置文件结束--&gt; 1.7.3 单元测试123456789101112131415161718/** * 测试 * * @author wgy */public class ItemsTest &#123; @Test public void findById() &#123; //获取spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //service测试 ItemsService itemsService = ac.getBean(ItemsService.class); //调用方法 Items items = itemsService.findById(1); System.out.println(items.getName()); &#125;&#125; 1.8 Web 层1.8.1 代码12345678910111213141516171819/** * 控制器 * * @author wgy */@Controller@RequestMapping(\"/items\")public class ItemsController &#123; @Autowired private ItemsService itemsService; @RequestMapping(\"/findDetail\") public String findDetail(Model model) &#123; Items items = itemsService.findById(1); model.addAttribute(\"item\", items); return \"itemDetail\"; &#125;&#125; 1.8.2 配置文件在 src/main/resources 创建 springmvc.xml 1234567891011121314151617181920212223242526&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\"&gt; &lt;!--组件扫描--&gt; &lt;context:component-scan base-package=\"com.wgy.controller\"/&gt; &lt;!--处理器映射器，处理器适配器--&gt; &lt;mvc:annotation-driven/&gt; &lt;!--视图解析器--&gt; &lt;bean id=\"internalResourceViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/pages/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt; &lt;/bean&gt; &lt;!--释放静态资源--&gt; &lt;mvc:default-servlet-handler/&gt;&lt;/beans&gt; 配置web.xml，加载 spring 容器，配置 springmvc 前端控制器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;!DOCTYPE web-app PUBLIC \"-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN\" \"http://java.sun.com/dtd/web-app_2_3.dtd\" &gt;&lt;web-app xmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" version=\"3.0\"&gt; &lt;!--编码过滤器--&gt; &lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!--配置spring核心监听器--&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!--重新指定spring配置文件的路径--&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;!--springmvc的核心servlet--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 1.9 Jsp/WEB-INF/pages/itemDetail.jsp 12345678910111213141516171819202122232425262728293031323334&lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\" %&gt;&lt;%@ taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %&gt;&lt;%@ taglib uri=\"http://java.sun.com/jsp/jstl/fmt\" prefix=\"fmt\" %&gt;&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; &lt;title&gt;Insert title here&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form&gt; &lt;table width=\"100%\" border=1&gt; &lt;tr&gt; &lt;td&gt;商品名称&lt;/td&gt; &lt;td&gt; $&#123;item.name &#125; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;商品价格&lt;/td&gt; &lt;td&gt; $&#123;item.price &#125; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;生成日期&lt;/td&gt; &lt;td&gt;&lt;fmt:formatDate value=\"$&#123;item.createtime&#125;\" pattern=\"yyyy-MM-dd HH:mm:ss\"/&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;商品简介&lt;/td&gt; &lt;td&gt;$&#123;item.detail&#125; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 1.10 运行与调试添加 tomcat7 插件，双击右侧 tomcat7 运行 运行结果如下： 2. 分模块构建工程基于上边的三个工程分析 继承：创建一个 parent 工程将所需的依赖都配置在 pom 中 聚合：聚合多个模块运行。 2.1 需求2.1.1 需求描述将 SSM 工程拆分为多个模块开发： ssm_dao、ssm_service、ssm_web 2.1.2 理解继承和聚合通常继承和聚合同时使用 何为继承？ 继承是为了消除重复，如果将 dao、service、web 分开创建独立的工程则每个工程的 pom.xml 文件中的内容存在重复，比如：设置编译版本、锁定 spring 的版本的等，可以将这些重复的配置提取出来在父工程的 pom.xml 中定义。 何为聚合？ 项目开发通常是分组分模块开发，每个模块开发完成要运行整个工程需要将每个模块聚合在 一起运行，比如：dao、service、web 三个工程最终会打一个独立的 war 运行。 2.2 案例实现2.2.1 maven-parent 父模块2.2.1.1 创建父工程工程只有pom.xml文件 12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;maven05_parent&lt;/artifactId&gt; &lt;!-- 设置项目的打包方式 --&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/project&gt; 2.2.1.2 定义 pom.xml在父工程的 pom.xml 中抽取一些重复的配置的， 比如： 锁定 jar 包的版本、 设置编译版本等。 具体配置同上1.5 2.2.2 ssm_dao 子模块2.2.2.1 创建 dao 子模块在父工程上右击创建 maven 模块 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;maven05_parent&lt;/artifactId&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;maven05_dao&lt;/artifactId&gt;&lt;/project&gt; 2.2.2.2 dao 代码将 ssm_maven 工程中的 dao接口、映射文件及 pojo 类拷贝到 src/main/java 中： 2.2.2.3 配置文件将 applicationContext.xml拆分出一个applicationContext-dao.xml，此文件中只配置 dao 相关内容 12345678910111213141516171819202122232425262728&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--dao层配置文件开始--&gt; &lt;!--配置连接池--&gt; &lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"url\" value=\"jdbc:mysql:///maven\"/&gt; &lt;property name=\"username\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt; &lt;/bean&gt; &lt;!--配置生产SqlSession对象的工厂--&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!--扫描pojo包，给包下所有pojo对象起别名--&gt; &lt;property name=\"typeAliasesPackage\" value=\"com.wgy.domain\"/&gt; &lt;/bean&gt; &lt;!--扫描接口包路径，生成包下所有接口的代理对象，并且放入spring容器中--&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.wgy.dao\"/&gt; &lt;/bean&gt; &lt;!--dao层配置文件结束--&gt;&lt;/beans&gt; 2.2.3 ssm_service 子模块2.2.3.1 创建 service 子模块方法同 ssm_dao 模块创建方法。ssm_service 依赖 ssm_dao 模块 12345678910111213141516171819202122&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;maven05_parent&lt;/artifactId&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;maven05_service&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;maven05_dao&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.2.3.2 service 代码将 ssm_maven 工程中的service接口、实现类拷贝到 src/main/java中： 2.2.3.3 配置文件将 applicationContext.xml拆分出一个applicationContext-service.xml，此文件中只配置 service相关内容 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\"&gt; &lt;!--service层配置文件开始--&gt; &lt;!--组件扫描配置--&gt; &lt;context:component-scan base-package=\"com.wgy.service\"/&gt; &lt;!--aop面向切面编程，切面就是切入点和通知的组合--&gt; &lt;!--配置事务管理器--&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;/bean&gt; &lt;!--配置事务的通知--&gt; &lt;tx:advice id=\"advice\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"save*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"update*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"delete*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"find*\" read-only=\"true\" propagation=\"SUPPORTS\"/&gt; &lt;tx:method name=\"*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!--配置切面--&gt; &lt;aop:config&gt; &lt;aop:pointcut id=\"pointcut\" expression=\"execution(* com.wgy.service.impl.*.*(..))\"/&gt; &lt;aop:advisor advice-ref=\"advice\" pointcut-ref=\"pointcut\"/&gt; &lt;/aop:config&gt; &lt;!--service层配置文件结束--&gt;&lt;/beans&gt; 2.2.4 ssm_web 子模块2.2.4.1 创建 web 子模块 ssm_web 依赖 ssm_service 模块 1234567891011121314151617181920212223&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;maven05_parent&lt;/artifactId&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;maven05_web&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.wgy&lt;/groupId&gt; &lt;artifactId&gt;maven05_service&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.2.4.2 controller 代码将 ssm_web 工程中的controller 代码拷贝到src/main/java 中： 2.2.4.3 配置文件拷贝 ssm_web 工程中配置文件 将 applicationContext.xml引入server、dao配置： 123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;import resource=\"classpath:spring/applicationContext-dao.xml\"/&gt; &lt;import resource=\"classpath:spring/applicationContext-service.xml\"/&gt;&lt;/beans&gt; 2.2.5 运行调试 方法 1：在 ssm_web 工程的 pom.xml 中配置 tomcat 插件运行 运行 ssm_web 工程它会从本地仓库下载依赖的 jar 包，所以当 ssm_web 依赖的 jar 包内容修改了必须及时发布到本地仓库，比如：ssm_web 依赖的 ssm_service 修改了，需要及时将ssm_service 发布到本地仓库。 方法 2：在父工程的 pom.xml 中配置 tomcat 插件运行，自动聚合并执行 推荐方法 2，如果子工程都在本地，采用方法 2 则不需要子工程修改就立即发布到本地仓库， 父工程会自动聚合并使用最新代码执行。 注意：如果子工程和父工程中都配置了 tomcat 插件，运行的端口和路径以子工程为准。","tags":[{"name":"Maven","slug":"Maven","permalink":"https://wgy1993.gitee.io/tags/Maven/"},{"name":"项目管理","slug":"项目管理","permalink":"https://wgy1993.gitee.io/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"}]},{"title":"Maven(一)","date":"2020-06-17T08:21:58.000Z","path":"archives/5849f9db.html","text":"1. Maven 介绍1.1 什么是 Maven1.1.1 什么是 MavenMaven 是一个项目管理工具，它包含了一个项目对象模型 (POM：Project Object Model)，一组标准集合，一个项目生命周期(Project Lifecycle)，一个依赖管理系统(Dependency Management System)， 和用来运行定义在生命周期阶段(phase)中插件(plugin)目标(goal)的逻辑。 1.1.2 Maven能解决什么问题用更通俗的方式来说明，项目开发不仅仅是写写代码而已，期间会伴随着各种必不可少的事情要做，下面列举几个感受一下： 我们需要引用各种 jar包，尤其是比较大的工程，引用的 jar 包往往有几十个乃至上百个， 每用到一种 jar包，都需要手动引入工程目录，而且经常遇到各种让人抓狂的jar 包冲突，版本冲突。 我们辛辛苦苦写好了 Java文件，可是只懂 0 和 1 的白痴电脑却完全读不懂，需要将它编译成二进制字节码。好歹现在这项工作可以由各种集成开发工具帮我们完成，Eclipse、IDEA 等都可以将代码即时编译。当然，如果你嫌生命漫长，何不铺张，也可以用记事本来敲代码，然后用 javac 命令一个个地去编译，逗电脑玩。 世界上没有不存在 bug的代码，为了减少 bug，因此写完了代码，我们还要写一些单元测试，然后一个个的运行来检验代码质量。 再优雅的代码也是要出来卖的。我们后面还需要把代码与各种配置文件、资源整合到一起，定型打包，如果是 web项目，还需要将之发布到服务器，供人蹂躏。 试想，如果现在有一种工具，可以把你从上面的繁琐工作中解放出来，能帮你构建工程，管理 jar包，编译代码，还能帮你自动运行单元测试，打包，生成报表，甚至能帮你部署项目，生成 Web 站点，你会心动吗？Maven 就可以解决上面所提到的这些问题。 1.1.3 Maven 的优势举例我们通过 Web 阶段项目，要能够将项目运行起来，就必须将该项目所依赖的一些 jar 包添加到工程中，否则项目就不能运行。试想如果具有相同架构的项目有十个，那么我们就需要将这一份 jar包复制到十个不同的工程中。我们一起来看一个CRM项目的工程大小。 使用传统 Web 项目构建的 CRM 项目如下： 原因主要是因为上面的 WEB 程序要运行，我们必须将项目运行所需的 Jar 包复制到工程目录中，从而导致了工程很大。 同样的项目，如果我们使用Maven 工程来构建，会发现总体上工程的大小会少很多。如下图: 小结：可以初步推断它里面一定没有 jar 包，继续思考，没有 jar 包的项目怎么可能运行呢？ 1.2 Maven的两个精典作用1.2.1 Maven的依赖管理Maven的一个核心特性就是依赖管理。当我们涉及到多模块的项目（包含成百个模块或者子项目），管理依赖就变成一项困难的任务。Maven展示出了它对处理这种情形的高度控制。 传统的WEB项目中，我们必须将工程所依赖的jar包复制到工程中，导致了工程的变得很大。那么maven工程是如何使得工程变得很少呢？ 分析如下： 通过分析发现：maven工程中不直接将jar包导入到工程中，而是通过在pom.xml文件中添加所需jar包的坐标，这样就很好的避免了jar直接引入进来，在需要用到jar包的时候，只要查找pom.xml文件，再通过pom.xml文件中的坐标，到一个专门用于”存放jar包的仓库”(maven仓库)中根据坐标从而找到这些jar包，再把这些jar包拿去运行。 那么问题来了第一：”存放jar包的仓库”长什么样？第二：通过读取pom.xml 文件中的坐标，再到仓库中找到jar包，会不会很慢？从而导致这种方式不可行！ 第一个问题：存放jar包的仓库长什么样，这一点我们后期会分析仓库的分类，也会带大家去看我们的本地的仓库长什么样。第二个问题：通过pom.xml文件配置要引入的jar包的坐标，再读取坐标并到仓库中加载jar包，这样我们就可以直接使用jar包了，为了解决这个过程中速度慢的问题，maven中也有索引的概念，通过建立索引，可以大大提高加载jar包的速度，使得我们认为jar包基本跟放在本地的工程文件中再读取出来的速度是一样的。这个过程就好比我们查阅字典时，为了能够加快查找到内容，书前面的目录就好比是索引，有了这个目录我们就可以方便找到内容了，一样的在maven仓库中有了索引我们就可以认为可以快速找到jar包。 1.2.2 项目的一键构建我们的项目，往往都要经历编译、测试、运行、打包、安装 ，部署等一系列过程。 什么是构建：指的是项目从编译、测试、运行、打包、安装 ，部署整个过程都交给maven进行管理，这个过程称为构建。 一键构建：指的是整个构建过程，使用maven一个命令可以轻松完成整个工作。 Maven规范化构建流程如下： 2. Maven 的使用2.1 Maven的安装2.1.1 Maven软件的下载为了使用Maven管理工具，我们首先要到官网去下载它的安装软件。通过百度搜索“Maven“如下： 点击 Download 链接，就可以直接进入到 Maven 软件的下载页面： 2.1.2 Maven软件的安装Maven下载后，将Maven解压到一个没有中文没有空格的路径下，比如D:\\software\\maven下面。解压后目录结构如下： bin:存放了maven的命令，比如我们前面用到的mvn tomcat:runboot:存放了一些maven本身的引导程序，如类加载器等conf:存放了maven的一些配置文件，如setting.xml文件lib:存放了maven本身运行所需的一些jar包 至此我们的maven软件就可以使用了，前提是你的电脑上之前已经安装并配置好了JDK。 2.1.3 JDK的准备JDK 使用 JDK8版本 2.1.4 Maven及JDK配置配置 MAVEN_HOME （bin目录之前一级目录）： 配置JAVA_HOME： 添加%MAVEN_HOME%/bin;%JAVA_HOME%/bin;到patch中。 2.1.5 Maven软件版本测试通过 mvn -v命令检查 maven是否安装成功，看到maven的版本为3.5.2及java版本为1. 8 即为安装成功。 找开cmd命令，输入mvn –v命令，如下图： 2.2 Maven仓库2.2.1 Maven仓库的分类maven的工作需要从仓库下载一些jar包，如下图所示，本地的项目A、项目B等都会通过maven软件从远程仓库（可以理解为互联网上的仓库）下载jar包并存在本地仓库，本地仓库就是本地文件夹，当第二次需要此jar包时则不再从远程仓库下载，因为本地仓库已经存在了，可以将本地仓库理解为缓存，有了本地仓库就不用每次从远程仓库下载了。 下图描述了maven中仓库的类型： 本地仓库 ：用来存储从远程仓库或中央仓库下载的插件和 jar 包，项目使用一些插件或 jar 包，优先从本地仓库查找。默认本地仓库位置在 ${user.dir}/.m2/repository，${user.dir}表示windows 用户目录 远程仓库：如果本地需要插件或者 jar 包，本地仓库没有，默认去远程仓库下载。远程仓库可以在互联网内也可以在局域网内。 中央仓库 ：在 maven 软件中内置一个远程仓库地址 http://repo1.maven.org/maven2 ，它是中央仓库，服务于整个互联网，它是由 Maven 团队自己维护，里面存储了非常全的 jar 包，它包含了世界上大部分流行的开源项目构件。 2.2.2 Maven本地仓库的配置在MAVE_HOME/conf/settings.xml文件中配置本地仓库位置（maven的安装目录下）： 打开settings.xml文件，配置如下： 2.2.3 全局setting与用户settingmaven仓库地址、私服等配置信息需要在setting.xml文件中配置，分为全局配置和用户配置。 在maven安装目录下的有 conf/setting.xml文件，此setting.xml文件用于maven的所有project项目，它作为maven的全局配置。 如需要个性配置则需要在用户配置中设置，用户配置的setting.xml文件默认的位置在：${user.dir}/.m2/settings.xml目录中,${user.dir} 指windows 中的用户目录。 maven会先找用户配置，如果找到则以用户配置文件为准，否则使用全局配置文件。 2.3 Maven工程的认识2.3.1 Maven工程的目录结构 作为一个maven工程，它的src目录和pom.xml是必备的。 进入src目录后，我们发现它里面的目录结构如下： src/main/java —— 存放项目的.java文件src/main/resources —— 存放项目资源文件，如spring, hibernate配置文件src/test/java —— 存放所有单元测试.java文件，如JUnit测试类src/test/resources —— 测试资源文件target —— 项目输出位置，编译后的class文件会输出到此目录pom.xml——maven项目核心配置文件 注意：如果是普通的java项目，那么就没有webapp目录。 2.3.2 Maven工程的运行进入maven工程目录（当前目录有pom.xml文件），运行tomcat:run命令。 根据上边的提示信息，通过浏览器访问：http://localhost:8080/maven-helloworld/ 3. Maven常用命令我们可以在cmd中通过一系列的maven命令来对我们的maven-helloworld工程进行编译、测试、运行、打包、安装、部署。 3.1 compilecompile是maven工程的编译命令，作用是将src/main/java下的文件编译为class文件输出到target目录下。 cmd进入命令状态，执行mvn compile，如下图提示成功： 查看 target目录，class文件已生成，编译完成。 3.2 testtest是maven工程的测试命令 mvn test，会执行src/test/java下的单元测试类。 cmd执行mvn test执行src/test/java下单元测试类，下图为测试结果，运行 1 个测试用例，全部成功。 3.3 cleanclean是maven工程的清理命令，执行 clean会删除target目录及内容。 3.4 packagepackage是maven工程的打包命令，对于java工程执行package打成jar包，对于web工程打成war包。 3.5 installinstall是maven工程的安装命令，执行install将maven打成jar包或war包发布到本地仓库。 3.6 Maven指令的生命周期maven对项目构建过程分为三套相互独立的生命周期，请注意这里说的是“三套”，而且“相互独立”，这三套生命周期分别是： Clean Lifecycle ：在进行真正的构建之前进行一些清理工作。Default Lifecycle ：构建的核心部分，编译，测试，打包，部署等等。Site Lifecycle ：生成项目报告，站点，发布站点。 3.7 maven的概念模型Maven包含了一个项目对象模型 (Project Object Model)，一组标准集合，一个项目生命周期(Project Lifecycle)，一个依赖管理系统(Dependency Management System)，和用来运行定义在生命周期阶段(phase)中插件(plugin)目标(goal)的逻辑。 项目对象模型 (Project Object Model) 一个maven工程都有一个pom.xml文件，通过pom.xml文件定义项目的坐标、项目依赖、项目信息、插件目标等。 12345678910&lt;project&gt; ：文件的根节点 .&lt;modelversion&gt; ： pom.xml 使用的对象模型版本&lt;groupId&gt; ：项目名称，一般写项目的域名&lt;artifactId&gt; ：模块名称，子项目名或模块名称&lt;version&gt; ：产品的版本号 .&lt;packaging&gt; ：打包类型，一般有 jar、war、pom 等&lt;name&gt; ：项目的显示名，常用于 Maven 生成的文档。&lt;description&gt; ：项目描述，常用于 Maven 生成的文档&lt;dependencies&gt; ：项目依赖构件配置，配置项目依赖构件的坐标&lt;build&gt; ：项目构建配置，配置编译、运行插件等。 依赖管理系统(Dependency Management System) 通过maven的依赖管理对项目所依赖的jar 包进行统一管理。 比如：项目依赖junit4.9，通过在pom.xml中定义junit4.9的依赖即使用junit4.9，如下所示是junit4.9的依赖定义： 1234567891011121314&lt;!-- 依赖关系 --&gt;&lt;dependencies&gt; &lt;!-- 此项目运行使用 junit，所以此项目依赖 junit --&gt; &lt;dependency&gt; &lt;!-- junit 的项目名称 --&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;!-- junit 的模块名称 --&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;!-- junit 版本 --&gt; &lt;version&gt;4.9&lt;/version&gt; &lt;!-- 依赖范围：单元测试时使用 junit --&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 一个项目生命周期(Project Lifecycle) 使用maven完成项目的构建，项目构建包括：清理、编译、测试、部署等过程，maven将这些过程规范为一个生命周期，如下所示是生命周期的各各阶段： maven通过执行一些简单命令即可实现上边生命周期的各各过程，比如执行mvn compile执行编译、执行mvn clean执行清理。 一组标准集合 maven将整个项目管理过程定义一组标准，比如：通过maven构建工程有标准的目录结构，有标准的生命周期阶段、依赖管理有标准的坐标定义等。 插件(plugin)目标(goal) maven 管理项目生命周期过程都是基于插件完成的。 4. idea开发maven项目在实战的环境中，我们都会使用流行的工具来开发项目。 4.1 idea的maven配置依据图片指示，选择本地maven安装目录，指定maven安装目录下conf文件夹中settings配置文件。 4.2 idea中创建maven的web工程打开idea，选择创建一个新工程 选择idea提供好的maven的web工程模板 点击Next填写项目信息 点击Next，此处不做改动。 点击Next选择项目所在目录 点击Finish后开始创建工程，耐心等待，直到出现如下界面。 手动添加src/main/java目录，如下图右键main文件夹&gt;New&gt;Directory 创建一个新的文件夹命名为java 点击OK后，在新的文件夹java上右键&gt;Make Directory as&gt;Sources Root 4.2.1 创建一个Servletsrc/java/main创建了一个Servlet，但报错 要解决问题，就是要将servlet-api-xxx.jar包放进来，作为maven工程应当添加servlet的坐标，从而导入它的jar 4.2.2 在pom.xml文件添加坐标直接打开hello_maven工程的pom.xml文件，再添加坐标 添加jar包的坐标时，还可以指定这个jar包将来的作用范围。 每个maven工程都需要定义本工程的坐标，坐标是maven对jar包的身份定义，比如：入门程序的坐标定义如下： 12345678910&lt;!--项目名称，定义为组织名+项目名，类似包名--&gt;&lt;groupId&gt;com.wgy&lt;&#x2F;groupId&gt;&lt;!-- 模块名称 --&gt;&lt;artifactId&gt;hello_maven&lt;&#x2F;artifactId&gt;&lt;!-- 当前项目版本号，snapshot 为快照版本即非正式版本，release为正式发布版本 --&gt;&lt;version&gt;0.0.1-SNAPSHOT&lt;&#x2F;version&gt;&lt;packaging&gt; ：打包类型 jar：执行 package 会打成 jar 包 war：执行 package 会打成 war 包 pom ：用于 maven 工程的继承，通常父工程设置为 pom 4.2.3 坐标的来源方式添加依赖需要指定依赖jar包的坐标，但是很多情况我们是不知道jar包的的坐标，可以通过如下方式查询： http://search.maven.org/http://mvnrepository.com/ 4.3 依赖范围A依赖B，需要在A的pom.xml文件中添加B的坐标，添加坐标时需要指定依赖范围，依赖范围包括： compile：编译范围，指A在编译时依赖B，此范围为默认依赖范围。编译范围的依赖会用在编译、测试、运行，由于运行时需要所以编译范围的依赖会被打包。 provided：provided依赖只有在当JDK或者一个容器已提供该依赖之后才使用， provided依赖在编译和测试时需要，在运行时不需要，比如：servlet api被tomcat容器提供。 runtime：runtime 依赖在运行和测试系统的时候需要，但在编译的时候不需要。比如：jdbc的驱动包。由于运行时需要所以runtime范围的依赖会被打包。 test：test范围依赖 在编译和运行时都不需要，它们只有在测试编译和测试运行阶段可用，比如：junit。由于运行时不需要所以test范围依赖不会被打包。 system：system范围依赖与provided类似，但是你必须显式的提供一个对于本地系统中JAR文件的路径，需要指定systemPath磁盘路径，system依赖不推荐使用。 在maven-web工程中测试各各scop。 测试总结： 默认引入 的jar包 ——- compile 【默认范围 可以不写】（编译、测试、运行 都有效 ） servlet-api 、jsp-api ——- provided （编译、测试 有效， 运行时无效，防止和tomcat下jar冲突） jdbc驱动jar包 —- runtime （测试、运行 有效 ） junit —– test （测试有效） 依赖范围由强到弱的顺序是：compile&gt;provided&gt;runtime&gt;test 4.4 项目中添加的坐标1234567891011121314151617181920&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet.jsp-api&lt;/artifactId&gt; &lt;version&gt;2.3.3&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 4.5 设置jdk编译版本本教程使用jdk1. 8 ，需要设置编译版本为1. 8 ，这里需要使用maven的插件来设置，在pom.xml中加入： 12345678910111213&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 4.6 编写servlet在src/main/java中创建ServletTest 123456789101112public class MyServlet extends HttpServlet &#123; @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; this.doGet(request, response); &#125; @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; request.getRequestDispatcher(\"/hello.jsp\").forward(request, response); &#125;&#125; 4.7 编写jsp123456789&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; hello maven&lt;/body&gt;&lt;/html&gt; 4.8 在web.xml中配置servlet访问路径12345678&lt;servlet&gt; &lt;servlet-name&gt;servlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.wgy.servlet.MyServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;servlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 4.9 添加tomcat7插件在pom文件中添加如下内容 12345678&lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;80&lt;/port&gt; &lt;/configuration&gt;&lt;/plugin&gt; 此时点击idea最右侧Maven Projects，就可以看到我们新添加的tomcat7插件双击tomcat7插件下tomcat7:run命令直接运行项目 也可以直接点击如图按钮，手动输入tomc7:run命令运行项目 5. maven工程运行调试5.1 端口占用处理重新执行tomcat:run命令重启工程，重启之前需手动停止 tomcat，否则报下边的错误： 5.2 断点调试点击如图所示选项 在弹出框中点击如图加号按钮找到maven选项 在弹出窗口中填写如下信息 完成后先Apply再OK结束配置后，可以在主界面找到我们刚才配置的操作名称。 如上图红框选中的两个按钮，左侧是正常启动，右侧是debug启动。","tags":[{"name":"Maven","slug":"Maven","permalink":"https://wgy1993.gitee.io/tags/Maven/"},{"name":"项目管理","slug":"项目管理","permalink":"https://wgy1993.gitee.io/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"}]},{"title":"SSH","date":"2020-06-16T05:58:26.000Z","path":"archives/782a8ece.html","text":"1. SSH三大框架整合1.1 整合说明 独立式整合指的是三个框架都使用自己的配置文件。 引入式整合指的是hibernate主配置文件中的内容都配置到spring配置文件中 在整合过程中，确保每步都运行成功，然后在继续往下做。 整合中使用的案例是客户的保存和列表查询操作。 后面的三种整合方式都基于1.2中的环境准备。 1.2 环境准备1.2.1 创建java web工程1.2.2 创建数据库和表结构1234567891011121314create database ssh;use ssh;&#x2F;*创建客户表*&#x2F;CREATE TABLE &#96;cst_customer&#96; ( &#96;cust_id&#96; bigint(32) NOT NULL AUTO_INCREMENT COMMENT &#39;客户编号(主键)&#39;, &#96;cust_name&#96; varchar(32) NOT NULL COMMENT &#39;客户名称(公司名称)&#39;, &#96;cust_source&#96; varchar(32) DEFAULT NULL COMMENT &#39;客户信息来源&#39;, &#96;cust_industry&#96; varchar(32) DEFAULT NULL COMMENT &#39;客户所属行业&#39;, &#96;cust_level&#96; varchar(32) DEFAULT NULL COMMENT &#39;客户级别&#39;, &#96;cust_address&#96; varchar(128) DEFAULT NULL COMMENT &#39;客户联系地址&#39;, &#96;cust_phone&#96; varchar(64) DEFAULT NULL COMMENT &#39;客户联系电话&#39;, PRIMARY KEY (&#96;cust_id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8; 1.2.3 编写实体类1234567891011121314/** * 客户的实体类（数据模型） */public class Customer implements Serializable &#123; private Long custId; private String custName; private String custSource; private String custIndustry; private String custLevel; private String custAddress; private String custPhone; ...&#125; 1.2.4 编写业务层接口和实现类12345678910111213141516/** * 客户的业务层接口 */public interface ICustomerService &#123; /** * 查询所有客户 * @return */ List&lt;Customer&gt; findAllCustomer(); /** * @param customer */ void saveCustomer(Customer customer);&#125; 123456789101112131415161718192021/** * 客户的业务层实现类 */public class CustomerServiceImpl implements ICustomerService &#123; private ICustomerDao customerDao; public void setCustomerDao(ICustomerDao customerDao) &#123; this.customerDao = customerDao; &#125; @Override public List&lt;Customer&gt; findAllCustomer() &#123; return customerDao.findAllCustomer(); &#125; @Override public void saveCustomer(Customer customer) &#123; customerDao.saveCustomer(customer); &#125;&#125; 1.2.5 编写持久层接口和实现类1234567891011121314151617/** * 客户的持久层接口 */public interface ICustomerDao &#123; /** * 查询所有客户 * @return */ List&lt;Customer&gt; findAllCustomer(); /** * 保存客户 * @param customer */ void saveCustomer(Customer customer);&#125; 12345678910111213141516/** * 客户的持久层实现类 */public class CustomerDaoImpl implements ICustomerDao &#123; @Override public List&lt;Customer&gt; findAllCustomer() &#123; System.out.println(\"查询了所有用户\"); return null; &#125; @Override public void saveCustomer(Customer customer) &#123; System.out.println(\"保存了用户\"); &#125;&#125; 2. 基于XML的独立式整合2.1 保证spring框架在web工程中独立运行2.1.1 拷贝spring的ioc,aop和事务控制三组jar包ioc: aop: 事务控制: 2.1.2 编写spring配置文件并导入约束123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt;&lt;/beans&gt; 2.1.3 把业务层和持久层配置到文件中123456789&lt;!-- 把资源交给spring来管理 --&gt;&lt;!-- 配置dao --&gt;&lt;bean id=\"customerDao\" class=\"com.wgy.dao.impl.CustomerDaoImpl\"&gt;&lt;/bean&gt; &lt;!-- 配置service --&gt;&lt;bean id=\"customerService\" class=\"com.wgy.service.impl.CustomerServiceImpl\"&gt; &lt;!-- 注入dao --&gt; &lt;property name=\"customerDao\" ref=\"customerDao\"&gt;&lt;/property&gt;&lt;/bean&gt; 2.1.4 测试spring能否独立运行12345678910111213/** * 测试类，测试spring框架可以独立运行 */public class SpringTest &#123; public static void main(String[] args) &#123; //1.获取spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.跟Id获取bean对象 ICustomerService cs = (ICustomerService) ac.getBean(\"customerService\"); cs.findAllCustomer(); &#125;&#125; 2.2 保证hibernate框架能够在web工程中独立运行2.2.1 拷贝hibernate必备jar包hibernate基本jar包: c3p0: 2.2.2 编写实体类的映射文件1234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC \"-//Hibernate/Hibernate Mapping DTD 3.0//EN\" \"http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd\"&gt;&lt;hibernate-mapping package=\"com.wgy.domain\"&gt; &lt;class name=\"Customer\" table=\"cst_customer\"&gt; &lt;id name=\"custId\" column=\"cust_id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"custName\" column=\"cust_name\"/&gt; &lt;property name=\"custSource\" column=\"cust_source\"/&gt; &lt;property name=\"custIndustry\" column=\"cust_industry\"/&gt; &lt;property name=\"custLevel\" column=\"cust_level\"/&gt; &lt;property name=\"custAddress\" column=\"cust_address\"/&gt; &lt;property name=\"custPhone\" column=\"cust_phone\"/&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; 2.2.3 编写hibernate主配置文件12345678910111213141516171819202122232425262728293031&lt;?xml version='1.0' encoding='utf-8'?&gt;&lt;!DOCTYPE hibernate-configuration PUBLIC \"-//Hibernate/Hibernate Configuration DTD//EN\" \"http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd\"&gt;&lt;hibernate-configuration&gt; &lt;!-- 配置SessionFactory--&gt; &lt;session-factory&gt; &lt;!-- 第一部分：连接数据库的信息 --&gt; &lt;property name=\"hibernate.connection.driver_class\"&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name=\"hibernate.connection.url\"&gt;jdbc:mysql://localhost:3306/ssh&lt;/property&gt; &lt;property name=\"hibernate.connection.username\"&gt;root&lt;/property&gt; &lt;property name=\"hibernate.connection.password\"&gt;root&lt;/property&gt; &lt;!-- 数据库的方言 --&gt; &lt;property name=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQLDialect&lt;/property&gt; &lt;!-- 第二部分：hibernate的可选配置 --&gt; &lt;!-- 是否显示hibernate生成的SQL语句 --&gt; &lt;property name=\"hibernate.show_sql\"&gt;true&lt;/property&gt; &lt;!-- 是否使用格式化输出sql语句到控制台 --&gt; &lt;property name=\"hibernate.format_sql\"&gt;false&lt;/property&gt; &lt;!-- 配置hibernate采用何种方式生成DDL语句 --&gt; &lt;property name=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;!-- 设置hibernate的连接池提供商 --&gt; &lt;property name=\"hibernate.connection.provider_class\"&gt;org.hibernate.c3p0.internal.C3P0ConnectionProvider&lt;/property&gt; &lt;!-- 把session和线程绑定，从而实现一个线程只有一个Session --&gt; &lt;property name=\"hibernate.current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- 第三部分：映射配置文件的位置 --&gt; &lt;mapping resource=\"com/wgy/domain/Customer.hbm.xml\"/&gt; &lt;/session-factory&gt;&lt;/hibernate-configuration&gt; 2.2.4 编写测试类-测试保存客户123456789101112131415161718192021222324252627/** * 保证hibernate框架能够独立在web工程中运行 * * @author wgy */public class HibernateTest &#123; public static void main(String[] args) &#123; Customer c = new Customer(); c.setCustName(\"ssh整合Customer\"); //1.加载配置文件 Configuration cfg = new Configuration(); cfg.configure(); //2.根据配置文件创建SessionFactory SessionFactory factory = cfg.buildSessionFactory(); //3.获取session Session session = factory.getCurrentSession(); //4.开启事务 Transaction tx = session.beginTransaction(); //5.执行操作 session.save(c); //6.提交/回滚事务 tx.commit(); //7.释放资源 factory.close(); &#125;&#125; 2.3 整合spring和hibernate框架2.3.1 明确 Spring和Hibernate的整合就是spring接管SessionFactory的创建 Spring针对Hiberante的操作有一个封装的对象HibernateTemplate 和JdbcTemplate一样，HibernateTemplate也有一个HibernateDaoSupport HibernateTemplate和HibernateDaoSupport都在spring-orm-4.2.4.RELEASE.jar中 我们Dao采用继承HiberanteDaoSupport的方式编写，它一样不能用于注解配置。 2.3.2 整合步骤2.3.2.1 在spring配置文件中配置SessionFactory12345678&lt;!-- 配置sessionFactory：让spring接管sessionFactory的创建 用spring提供的一个SessionFactory：LocalSessionFactoryBean 创建SessionFactory有三部分必不可少的信息。三部分信息在hibernate主配置文件中都有 把hibernate主配置文件的位置注入进来--&gt;&lt;bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate5.LocalSessionFactoryBean\"&gt; &lt;property name=\"configLocation\" value=\"classpath:hibernate.cfg.xml\"/&gt;&lt;/bean&gt; 2.3.2.2 改造Dao继承HibernateDaoSupport123456789101112131415/** * 客户的持久层实现类 */public class CustomerDaoImpl extends HibernateDaoSupport implements ICustomerDao &#123; @Override public List&lt;Customer&gt; findAllCustomer() &#123; return (List&lt;Customer&gt;) getHibernateTemplate().find(\"from Customer\"); &#125; @Override public void saveCustomer(Customer customer) &#123; getHibernateTemplate().save(customer); &#125;&#125; 2.3.2.3 在spring配置文件中给Dao注入SessionFactory1234&lt;!-- 配置dao --&gt;&lt;bean id=\"customerDao\" class=\"com.wgy.dao.impl.CustomerDaoImpl\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"&gt;&lt;/property&gt;&lt;/bean&gt; 2.3.2.4 测试12345678910111213141516171819202122232425262728293031/** * 整合spring和hibernate的测试类 * spring整合Junit * 第一步：拷贝jar包 * spring-junit-4.2.4.jar * 第二步：使用注解替换运行器（原来junit的main方法） * @RunWith(支持spring的main方法) * @ContextConfiguration(指定spring的配置文件位置) */@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations=&#123;\"classpath:bean.xml\"&#125;)public class SpringHibernateTest &#123; @Autowired private ICustomerService cs; @Test public void testSave() &#123; Customer c = new Customer(); c.setCustName(\"spring hibernate customer123\"); cs.saveCustomer(c); &#125; @Test public void testFindAll() &#123; List list = cs.findAllCustomer(); for (Object o : list) &#123; System.out.println(o); &#125; &#125;&#125; 12345测试结果： 无论保存还是查询都运行失败！ 按常理来说，我们没有配置事务，保存失败是可以理解的。为什么查询也会失败呢？分析原因： 是由于spring的HibernateTemplate对象在使用Session时，spring创建了Session的代理对象，在这个过程中，spring对hibernate绑定Session到当前线程的配置不认识了，所以运行失败。 2.3.2.5 修改把Session绑定到当前线程上123456789&lt;!-- 是hibernate把session绑定到当前线程上的配置 &lt;property name=\"hibernate.current_session_context_class\"&gt;thread&lt;/property&gt;--&gt;&lt;!-- 是spring把sesion绑定到当前线程上的配置 --&gt;&lt;property name=\"hibernate.current_session_context_class\"&gt; org.springframework.orm.hibernate5.SpringSessionContext&lt;/property&gt;此时再运行刚才的测试： 查询可以使用了。保存不能使用，原因是没有事务。 2.3.3 配置Spring的事务2.3.3.1 配置事务管理器并注入SessionFactory1234&lt;!-- 配置事务管理器 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.orm.hibernate5.HibernateTransactionManager\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"/&gt;&lt;/bean&gt; 2.3.3.2 配置事务的通知及通知的属性1234567&lt;!-- 配置事务的通知 --&gt;&lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"*\" propagation=\"REQUIRED\" read-only=\"false\"/&gt; &lt;tx:method name=\"find*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 2.3.3.3 配置AOP建立切入点表达式和事务通知的关系12345678910&lt;!-- 配置aop --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点表达式 --&gt; &lt;aop:pointcut id=\"pt1\" expression=\"execution(* com.wgy.service.impl.*.*(..))\"/&gt; &lt;!-- 建立切入点表达式和事务通知的关联 --&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"pt1\"/&gt;&lt;/aop:config&gt;再次测试： 此时保存和查询都可以正常使用了。 2.4 保证struts2框架能够在web工程中独立运行2.4.1 拷贝struts2的必备jar包 2.4.2 编写struts.xml文件并导入约束123456789&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE struts PUBLIC &quot;-&#x2F;&#x2F;Apache Software Foundation&#x2F;&#x2F;DTD Struts Configuration 2.3&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;struts.apache.org&#x2F;dtds&#x2F;struts-2.3.dtd&quot;&gt;&lt;struts&gt; &lt;!-- 开启开发者模式 --&gt; &lt;constant name&#x3D;&quot;struts.devMode&quot; value&#x3D;&quot;true&quot;&#x2F;&gt;&lt;&#x2F;struts&gt; 2.4.3 在web.xml中配置struts2的核心过滤器123456789&lt;!-- 配置struts2和核心过滤器 --&gt;&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 2.4.4 导入jsp页面 2.4.5 修改menu.jsp123&lt;A class=style2 href=\"$&#123;pageContext.request.contextPath&#125;/customer/addUICustomer.action\" target=main&gt;－ 新增客户&lt;/A&gt;&lt;A class=style2 href=\"$&#123;pageContext.request.contextPath&#125;/customer/findAllCustomer.action\" target=main&gt;－ 客户列表&lt;/A&gt; 2.4.6 在struts.xml中配置action12345678910&lt;!-- 配置动作 --&gt;&lt;package name&#x3D;&quot;customer&quot; extends&#x3D;&quot;struts-default&quot; namespace&#x3D;&quot;&#x2F;customer&quot;&gt; &lt;action name&#x3D;&quot;addUICustomer&quot; class&#x3D;&quot;com.wgy.web.action.CustomerAction&quot; method&#x3D;&quot;addUICustomer&quot;&gt; &lt;result name&#x3D;&quot;addUICustomer&quot;&gt;&#x2F;jsp&#x2F;customer&#x2F;add.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;action name&#x3D;&quot;findAllCustomer&quot; class&#x3D;&quot;com.wgy.web.action.CustomerAction&quot; method&#x3D;&quot;findAllCustomer&quot;&gt; &lt;result name&#x3D;&quot;findAllCustomer&quot;&gt;&#x2F;jsp&#x2F;customer&#x2F;list.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt;&lt;&#x2F;package&gt; 2.4.7 编写动作类和方法12345678910111213141516171819202122232425262728/** * 客户的动作类*/public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private Customer customer = new Customer(); @Override public Customer getModel() &#123; return customer; &#125; /** * 获取添加客户页面 * @return */ public String addUICustomer()&#123; return \"addUICustomer\"; &#125; /** * 查询所有客户 * @return */ public String findAllCustomer() &#123; //调用业务层查询客户列表 &#125;&#125; 2.4.8 测试1运行结果：通过点击【新增客户】可以跳转到客户添加页面 2.5 整合spring和struts22.5.1 明确 spring整合struts2就是让spring接管action的创建 action是多例的，配置到spring中需要设置scope属性为多例 2.5.2 整合步骤2.5.2.1 拷贝spring整合struts的jar包 2.5.2.2 在action中使用构造函数获取Service对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 客户的动作类 */public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private Customer customer = new Customer(); private List&lt;Customer&gt; customers; private ICustomerService customerService; @Override public Customer getModel() &#123; return customer; &#125; public void setCustomerService(ICustomerService customerService) &#123; this.customerService = customerService; &#125;//此种方式根本不能用，因为由于动作类是多例的，每次都会创建新的容器，导致容器中的bean也会创建新的。// public CustomerAction()&#123;// ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\");// System.out.println(ac);// ICustomerService cs = (ICustomerService) ac.getBean(\"customerService\");// this.setCustomerService(cs);// &#125;//此种方式解决了容器多例的问题，保证了容器一个应用只有一个，但是我们的代码边臃肿了，每个action都需要这么写一下。 public CustomerAction()&#123; ServletContext application = ServletActionContext.getServletContext(); ApplicationContext ac = WebApplicationContextUtils.getWebApplicationContext(application); System.out.println(ac); ICustomerService cs = (ICustomerService) ac.getBean(\"customerService\"); this.setCustomerService(cs); &#125; /** * 获取添加客户页面 * * @return */ public String addUICustomer() &#123; return \"addUICustomer\"; &#125; /** * 查询所有客户 * @return */ public String findAllCustomer() &#123; //调用业务层查询客户列表 customers = customerService.findAllCustomer(); return \"findAllCustomer\"; &#125; public List&lt;Customer&gt; getCustomers() &#123; return customers; &#125; public void setCustomers(List&lt;Customer&gt; customers) &#123; this.customers = customers; &#125;&#125; 2.5.2.3 测试1运行结果：查询客户列表测试通过。 2.6 优化配置2.6.1 配置spring的监听器12345678910111213141516171819在上面2.5.2.2小节中有这么一句： 由于动作类是多例的，每次都会创建容器，导致资源的浪费。一个应用应该只有一个容器问题： 如何解决呢？答案： 只要让容器在应用加载时创建，应用卸载时销毁就可以。问题： 我们怎么知道应用何时加载了呢？答案： ServletContext对象创建了，就表示当前应用已经被服务器加载了。问题： 我们怎么知道ServletContext对象创建了呢？答案： ServletContextListener监听器可以监听到ServletContext对象的创建和销毁。Spring框架为我们提供了一个监听器：ContextLoaderListener。它是ServletContextListener接口的实现类，负责监听ServletContext对象的创建，为我们创建容器，监听ServletContext对象的销毁，销毁容器。我们只需要配置上即可。ContextLoaderListener在spring-web-4.2.4.RELEASE.jar中 在web.xml中配置监听器： 123456789&lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt;&lt;/listener&gt;当配置了此监听器后，就不需要使用Action的构造函数了，可以把构造函数那段删除了。此监听器只能读取WEB-INF目录中的名称为applicationContext.xml的配置文件。这显然限制了我们的配置。我们可以通过配置全局初始化参数的方式，指定spring配置文件的位置. 2.6.2 配置指定spring配置文件的位置12345&lt;!-- 手动指定spring的配置文件位置，需要使用ServletContext的初始化参数 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:config/spring/applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt; 2.6.3 分文件编写spring配置我们写到这里，其实搭建环境已经基本结束了，但是发现spring的配置文件杂乱无章，使我们在找配置的时候，很难一下找到。所以我们采用分配置文件编写的方式。 2.6.3.1 编写主配置文件引入其他配置文件1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 引入其他spring配置文件 --&gt; &lt;import resource=\"applicationContext-customer.xml\"/&gt; &lt;import resource=\"applicationContext-jdbc.xml\"/&gt; &lt;import resource=\"applicationContext-tx.xml\"/&gt;&lt;/beans&gt; 2.6.3.2 编写针对需求的配置文件applicationContext-customer.xml12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 配置service --&gt; &lt;bean id=\"customerService\" class=\"com.wgy.service.impl.CustomerServiceImpl\"&gt; &lt;property name=\"customerDao\" ref=\"customerDao\"/&gt; &lt;/bean&gt; &lt;!-- 配置dao --&gt; &lt;bean id=\"customerDao\" class=\"com.wgy.dao.impl.CustomerDaoImpl\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 2.6.3.3 编写数据库连接的配置文件applicationContext-jdbc.xml123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 配置SessionFactory --&gt; &lt;bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate5.LocalSessionFactoryBean\"&gt; &lt;!-- 使用的是hibernate主配置文件中的内容，我们只需要指定hibernate配置文件的位置 --&gt; &lt;property name=\"configLocation\" value=\"classpath:config/hibernate/hibernate.cfg.xml\"&gt;/&gt; &lt;/bean&gt;&lt;/beans&gt; 2.6.3.4 编写事务控制的配置文件applicationContext-tx.xml1234567891011121314151617181920212223242526272829303132333435&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt; &lt;!-- 和事务相关的配置 --&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.orm.hibernate5.HibernateTransactionManager\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"/&gt; &lt;/bean&gt; &lt;!-- 配置事务的通知 --&gt; &lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;!-- 配置事务的属性 --&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"*\" propagation=\"REQUIRED\" read-only=\"false\"/&gt; &lt;tx:method name=\"find*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 配置aop --&gt; &lt;aop:config&gt; &lt;!-- 配置切入点表达式 --&gt; &lt;aop:pointcut id=\"pt1\" expression=\"execution(* com.wgy.service.impl.*.*(..))\"/&gt; &lt;!-- 建立切入点表达式和事务通知的关联 --&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"pt1\"/&gt; &lt;/aop:config&gt;&lt;/beans&gt; 2.6.4 配置指定struts2配置文件位置我们的spring和hibernate配置文件都存到了src/config/的对应包中了，只有struts2配置文件还在类的根路径下，它也可以通过配置的方式指定struts.xml的位置。配置的是过滤器的初始化参数。初始化参数的name和value都是固定写法。 1234567891011121314&lt;!-- 配置struts2和核心过滤器 --&gt;&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;!-- 手动指定struts2配置文件的位置：此处的配置绝大多数都是固定的--&gt; &lt;init-param&gt; &lt;param-name&gt;config&lt;/param-name&gt; &lt;param-value&gt;struts-default.xml,struts-plugin.xml,config/struts/struts.xml&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 2.6.5 分文件编写struts2配置文件当我们后面做的模块越来越多，struts2一个配置文件写起来也会杂乱无章，所以我们也可以把struts2的配置文件分开编写。 2.6.5.1 编写struts2的主配置文件struts.xml123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE struts PUBLIC \"-//Apache Software Foundation//DTD Struts Configuration 2.3//EN\" \"http://struts.apache.org/dtds/struts-2.3.dtd\"&gt;&lt;struts&gt; &lt;!-- 开启开发者模式 --&gt; &lt;constant name=\"struts.devMode\" value=\"true\"/&gt; &lt;!-- 配置公共包，有公共的配置就写在此处--&gt; &lt;package name=\"myDefault\" extends=\"struts-default\" abstract=\"true\"&gt; &lt;/package&gt; &lt;!-- 引入其他struts2配置文件 --&gt; &lt;include file=\"config/struts/struts-customer.xml\"&gt;&lt;/include&gt;&lt;/struts&gt; 2.6.5.2 针对不同模块编写不同的配置文件struts-customer.xml1234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE struts PUBLIC \"-//Apache Software Foundation//DTD Struts Configuration 2.3//EN\" \"http://struts.apache.org/dtds/struts-2.3.dtd\"&gt;&lt;struts&gt; &lt;!-- 配置和客户管理相关的动作 --&gt; &lt;package name=\"customer\" extends=\"myDefault\" namespace=\"/customer\"&gt; &lt;action name=\"addUICustomer\" class=\"com.wgy.web.action.CustomerAction\" method=\"addUICustomer\"&gt; &lt;result name=\"addUICustomer\"&gt;/jsp/customer/add.jsp&lt;/result&gt; &lt;/action&gt; &lt;action name=\"findAllCustomer\" class=\"com.wgy.web.action.CustomerAction\" method=\"findAllCustomer\"&gt; &lt;result name=\"findAllCustomer\"&gt;/jsp/customer/list.jsp&lt;/result&gt; &lt;/action&gt; &lt;/package&gt;&lt;/struts&gt; 2.6.6 管理Action的两种方式2.6.6.1 让struts2自己来管理此种方式就是在action标签的class属性中提供动作类的全限定类名。 123&lt;action name=\"addUICustomer\" class=\"com.wgy.web.action.CustomerAction\" method=\"addUICustomer\"&gt; &lt;result name=\"addUICustomer\"&gt;/jsp/customer/add.jsp&lt;/result&gt;&lt;/action&gt; 2.6.6.2 让spring来管理（实际开发中采用的方式）此种方式就是在spring配置文件中配置Action，在struts2配置文件action标签的class属性里写bean的id。 spring配置文件： 1234&lt;!-- 配置action --&gt;&lt;bean id=\"customerAction\" class=\"com.wgy.web.action.CustomerAction\" scope=\"prototype\"&gt; &lt;property name=\"customerService\" ref=\"customerService\"&gt;&lt;/property&gt;&lt;/bean&gt; struts2配置文件： 123&lt;action name&#x3D;&quot;addUICustomer&quot; class&#x3D;&quot;customerAction&quot; method&#x3D;&quot;addUICustomer&quot;&gt; &lt;result name&#x3D;&quot;addUICustomer&quot;&gt;&#x2F;jsp&#x2F;customer&#x2F;add.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3. 基于XML的引入式整合3.1 明确引入式整合就是把hibernate.cfg.xml中的配置都挪到spring的配置文件中 3.2 配置方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;!-- 和jdbc配置相关的 --&gt;&lt;!-- 配置sessionFactory --&gt;&lt;bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate5.LocalSessionFactoryBean\"&gt; &lt;!-- 第一部分：连接数据库的 用连接池--&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!-- 第二部分：hibernate的可选配置 --&gt; &lt;property name=\"hibernateProperties\"&gt; &lt;props&gt; &lt;!-- 数据库的方言 --&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 是否显示hibernate生成的SQL语句 --&gt; &lt;prop key=\"hibernate.show_sql\"&gt;true&lt;/prop&gt; &lt;!-- 是否使用格式化输出sql语句到控制台 --&gt; &lt;prop key=\"hibernate.format_sql\"&gt;false&lt;/prop&gt; &lt;!-- 配置hibernate采用何种方式生成DDL语句 --&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;!-- 把session和线程绑定，从而实现一个线程只有一个Session --&gt; &lt;prop key=\"hibernate.current_session_context_class\"&gt; org.springframework.orm.hibernate5.SpringSessionContext &lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!-- 第三部分：映射文件的位置 mappingResources：它是一个注入string数组类型的数据。提供的是映射文件的位置。有几个映射文件，就需要写几个。 mappingDirectoryLocations：它是注入一个Resource类型的数组。提供的是映射文件所在的目录。此属性一般多用于一个项目有多个地方存放映射配置。 //服务端 server_domain //移动端 mobile_domain mappingLocations：它是注入一个Resource类型的数组。提供的映射文件的位置。它可以使用通配符。 --&gt; &lt;property name=\"mappingLocations\"&gt; &lt;array&gt; &lt;value&gt;classpath:com/wgy/domain/*.hbm.xml&lt;/value&gt; &lt;/array&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置连接池 --&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"/&gt; &lt;property name=\"jdbcUrl\" value=\"jdbc:mysql://localhost:3306/ssh\"/&gt; &lt;property name=\"user\" value=\"root\"/&gt; &lt;property name=\"password\" value=\"root\"/&gt;&lt;/bean&gt; 4. 基于注解的整合4.1 明确 注解整合仍然使用上面的环境，就是把xml的配置全部换成注解 spring的注解整合有两种方式，一种是用xml文件，一种是纯注解。 hibernate注解整合是把实体类映射改为JPA注解映射 4.2 整合步骤-spring使用xml文件4.2.1 spring配置使用注解实现4.2.1.1 在spring配置文件中配置要扫描的包12&lt;!-- 配置spring运行要扫描的包 --&gt;&lt;context:component-scan base-package=\"com.wgy\"/&gt; 4.2.1.2 把action,service和dao都用注解配置12345678910/** * 客户的动作类 */@Controller(\"customerAction\")@Scope(\"prototype\")public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; @Autowired private ICustomerService customerService; //action中的方法不变&#125; 123456789/** * 客户的业务层实现类 */@Service(\"customerService\")public class CustomerServiceImpl implements ICustomerService &#123; @Autowired private ICustomerDao customerDao; //service中的方法不变&#125; 12345678910/** * 客户的持久层实现类 */@Repository(\"customerDao\")public class CustomerDaoImpl implements ICustomerDao &#123; //dao中必须自己定义HibernateTemplate，不能继承HibernateDaoSupport了 @Autowired private HibernateTemplate hibernateTemplate; //dao中的方法不变&#125; 4.2.1.3 在spring配置文件中配置HiernateTemplate1234&lt;!-- 配置hibernateTemplate --&gt;&lt;bean id=\"hibernateTemplate\" class=\"org.springframework.orm.hibernate5.HibernateTemplate\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"/&gt;&lt;/bean&gt; 4.2.1.4 在spring配置文件中配置事务管理器1234&lt;!-- 配置事务管理器 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.orm.hibernate5.HibernateTransactionManager\"&gt; &lt;property name=\"sessionFactory\" ref=\"sessionFactory\"/&gt;&lt;/bean&gt; 4.2.1.5 在spring配置文件中开启spring对注解事务的支持12&lt;!-- 开启spring对注解事务的支持 --&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt; 4.2.1.6 在客户的业务层实现类上使用@Transactional注解123456789101112131415161718192021222324/** * 客户的业务层实现类 * * @author wgy */@Service(\"customerService\")@Transactional(readOnly = false, propagation = Propagation.REQUIRED)public class CustomerServiceImpl implements ICustomerService &#123; @Resource(name = \"customerDao\") private ICustomerDao customerDao; @Override @Transactional(readOnly = true, propagation = Propagation.SUPPORTS) public List&lt;Customer&gt; findAllCustomer() &#123; return customerDao.findAllCustomer(); &#125; @Override public void saveCustomer(Customer customer) &#123; customerDao.saveCustomer(customer); &#125;&#125; 4.2.2 hibernate映射使用注解配置实现4.2.2.1 实体类映射注解配置1234567891011121314151617181920212223242526272829/** * 客户的实体类 * JPA规范：java 持久化规范 * 注解全都是JPA规范的。 * 导包都需要导入javax.persistence包下的 * */@Entity@Table(name=\"cst_customer\")public class Customer implements Serializable &#123; @Id @GeneratedValue(strategy=GenerationType.IDENTITY) @Column(name=\"cust_id\") private Long custId; @Column(name=\"cust_name\") private String custName; @Column(name=\"cust_source\") private String custSource; @Column(name=\"cust_industry\") private String custIndustry; @Column(name=\"cust_level\") private String custLevel; @Column(name=\"cust_address\") private String custAddress; @Column(name=\"cust_phone\") private String custPhone; ...&#125; 4.2.2.2 spring中SessionFactory配置修改123456789101112131415161718192021222324252627282930&lt;!-- 配置sessionFactory --&gt;&lt;bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate5.LocalSessionFactoryBean\"&gt; &lt;!-- 第一部分：连接数据库的 用连接池--&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!-- 第二部分：hibernate的可选配置 --&gt; &lt;property name=\"hibernateProperties\"&gt; &lt;props&gt; &lt;!-- 数据库的方言 --&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQLDialect&lt;/prop&gt; &lt;!-- 是否显示hibernate生成的SQL语句 --&gt; &lt;prop key=\"hibernate.show_sql\"&gt;true&lt;/prop&gt; &lt;!-- 是否使用格式化输出sql语句到控制台 --&gt; &lt;prop key=\"hibernate.format_sql\"&gt;false&lt;/prop&gt; &lt;!-- 配置hibernate采用何种方式生成DDL语句 --&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;!-- 把session和线程绑定，从而实现一个线程只有一个Session --&gt; &lt;prop key=\"hibernate.current_session_context_class\"&gt; org.springframework.orm.hibernate5.SpringSessionContext &lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!-- 第三部分：指定实体类所在的包，当创建SessionFactory,会去该包中扫描实体类上的注解，从而生成映射配置--&gt; &lt;property name=\"packagesToScan\"&gt; &lt;array&gt; &lt;value&gt;com.wgy.domain&lt;/value&gt; &lt;/array&gt; &lt;/property&gt;&lt;/bean&gt; 4.2.3 struts2配置使用注解实现4.2.3.1 导入struts2注解的jar包 4.2.3.2 使用注解配置Action1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * 客户的动作类 */@Controller(\"customerAction\")@Scope(\"prototype\")//-------以下都是struts2的注解-----------@ParentPackage(\"struts-default\")//指定当前包的父包@Namespace(\"/customer\")//指定名称空间，访问当前action的所有方法都需要有名称空间@Results(&#123; @Result(name = \"addUICustomer\", type = \"dispatcher\", location = \"/jsp/customer/add.jsp\"), @Result(name = \"findAllCustomer\", type = \"dispatcher\", location = \"/jsp/customer/list.jsp\"), @Result(name = \"listCustomer\", type = \"redirectAction\", location = \"findAllCustomer\")&#125;)public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private Customer customer = new Customer(); private List&lt;Customer&gt; customers; @Resource(name = \"customerService\") private ICustomerService customerService; @Override public Customer getModel() &#123; return customer; &#125; /** * 查询所有客户 * @return */ @Action(\"findAllCustomer\") public String findAllCustomer()&#123; //调用业务层查询客户列表 customers = customerService.findAllCustomer(); return \"findAllCustomer\"; &#125; /** * 获取添加客户页面 * @return */ @Action(\"addUICustomer\") public String addUICustomer()&#123; return \"addUICustomer\"; &#125; /** * 添加客户 * @return */ @Action(\"addCustomer\") public String addCustomer()&#123; customerService.saveCustomer(customer); return \"listCustomer\"; &#125; public List&lt;Customer&gt; getCustomers() &#123; return customers; &#125; public void setCustomers(List&lt;Customer&gt; customers) &#123; this.customers = customers; &#125;&#125; 4.2.3.3 指定struts2开发模式12345678910111213&lt;!-- 配置struts2和核心过滤器 --&gt;&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;struts.devMode&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;","tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"https://wgy1993.gitee.io/tags/Hibernate/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Spring","slug":"Spring","permalink":"https://wgy1993.gitee.io/tags/Spring/"},{"name":"Struts2","slug":"Struts2","permalink":"https://wgy1993.gitee.io/tags/Struts2/"}]},{"title":"Spring(四)","date":"2020-06-14T09:18:35.000Z","path":"archives/924a9eab.html","text":"1. Spring中的JdbcTemplate1.1 JdbcTemplate概述它是spring框架中提供的一个对象，是对原始Jdbc API对象的简单封装。spring框架为我们提供了很多的操作模板类，入下图所示： 我们今天的主角在spring-jdbc-4.24.RELEASE.jar中，我们在导包的时候，除了要导入这个jar包外，还需要导入一个spring-tx-4.2.4.RELEASE.jar（它是和事务相关的）。 1.2 JdbcTemplate对象的创建我们可以参考它的源码，来一探究竟： 12345678910111213public JdbcTemplate() &#123;&#125;public JdbcTemplate(DataSource dataSource) &#123; setDataSource(dataSource); afterPropertiesSet();&#125;public JdbcTemplate(DataSource dataSource, boolean lazyInit) &#123; setDataSource(dataSource); setLazyInit(lazyInit); afterPropertiesSet();&#125; 除了默认构造函数之外，都需要提供一个数据源。既然有set方法，依据我们之前学过的依赖注入，我们可不可以在配置文件中配置呢？ 答案是肯定的。 1.3 Spring中配置数据源1.3.1 环境搭建 1.3.2 编写Spring的配置文件1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;/beans&gt; 1.3.3 配置数据源我们之前已经接触过了两个数据源，一个是C3P0，一个是DBCP。要想使用这两数据源都需要导入对应的jar包。 1.3.3.1 配置C3P0数据源导包: 在Spring的配置文件中配置： 123456&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"user\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt;&lt;/bean&gt; 1.3.3.2 配置DBCP数据源导包: 在Spring的配置文件中配置： 123456&lt;bean id=\"dataSource\" class=\"org.apache.commons.dbcp.BasicDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt;&lt;/bean&gt; 1.3.3.3 配置spring内置数据源spring框架也提供了一个内置数据源，我们也可以使用spring的内置数据源，它就在spring-jdbc-4.2.4.REEASE.jar包中： 123456&lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt;&lt;/bean&gt; 1.3.4 将数据库连接的信息配置到属性文件中:定义属性文件 1234jdbc.driverClass=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/springjdbc.username=rootjdbc.password=root 引入外部的属性文件 12345678一种方式:&lt;!-- 引入外部属性文件： --&gt;&lt;bean class&#x3D;&quot;org.springframework.beans.factory.config.PropertyPlaceholderConfigurer&quot;&gt; &lt;property name&#x3D;&quot;location&quot; value&#x3D;&quot;classpath:jdbc.properties&quot;&#x2F;&gt;&lt;&#x2F;bean&gt;二种方式:&lt;context:property-placeholder location&#x3D;&quot;classpath:jdbc.properties&quot;&#x2F;&gt; 1.4 JdbcTemplate的增删改查操作1.4.1 前期准备12345678910创建数据库：create database spring;use spring;创建表：create table account( id int primary key auto_increment, name varchar(40), money float)character set utf8 collate utf8_general_ci; 1.4.2 在spring配置文件中配置JdbcTemplate12345678910111213141516171819&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 配置一个数据库的操作模板：JdbcTemplate --&gt; &lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 1.4.3 最基本使用1234567891011public class JdbcTemplateDemo2 &#123; public static void main(String[] args) &#123; //1.获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 jt.execute(\"insert into account(name,money)values('eee',500)\"); &#125;&#125; 1.4.4 保存操作123456789101112public class JdbcTemplateDemo3 &#123; public static void main(String[] args) &#123; //1.获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 //保存 jt.update(\"insert into account(name,money)values(?,?)\",\"fff\",5000); &#125;&#125; 1.4.5 更新操作123456789101112public class JdbcTemplateDemo3 &#123; public static void main(String[] args) &#123; //1.获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 //修改 jt.update(\"update account set money = money-? where id = ?\",300,6); &#125;&#125; 1.4.6 删除操作123456789101112public class JdbcTemplateDemo3 &#123; public static void main(String[] args) &#123; //1.获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 //删除 jt.update(\"delete from account where id = ?\",6); &#125;&#125; 1.4.7 查询所有操作1234567891011121314151617181920212223242526public class JdbcTemplateDemo3 &#123; public static void main(String[] args) &#123; //1.获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 //查询所有 List&lt;Account&gt; accounts = jt.query(\"select * from account where money &gt; ? \",new AccountRowMapper(), 500); for(Account o : accounts)&#123; System.out.println(o); &#125; &#125;&#125;public class AccountRowMapper implements RowMapper&lt;Account&gt;&#123; @Override public Account mapRow(ResultSet rs, int rowNum) throws SQLException &#123; Account account = new Account(); account.setId(rs.getInt(\"id\")); account.setName(rs.getString(\"name\")); account.setMoney(rs.getFloat(\"money\")); return account; &#125;&#125; 123456789101112131415public class JdbcTemplateDemo3 &#123; public static void main(String[] args)&#123; //1.获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.跟id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 //查询所有 List&lt;Account&gt; accounts = jt.query(\"select * from account where money &gt; ?\",new BeanPropertyRowMapper&lt;Account&gt;(Account.class),1000); for(Account account : accounts)&#123; System.out.println(account); &#125; &#125;&#125; 1.4.8 查询一个操作1234567891011121314151617181920212223242526272829//使用RowMapper的方式：常用的方式public class JdbcTemplateDemo3 &#123; public static void main(String[] args) &#123; //1.获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 //查询一个 List&lt;Account&gt; as = jt.query(\"select * from account where id = ? \", new AccountRowMapper(), 55); System.out.println(as.isEmpty()?\"没有结果\":as.get(0)); &#125;&#125;//使用ResultSetExtractor的方式:不常用的方式public class JdbcTemplateDemo3 &#123; public static void main(String[] args) &#123; //1.获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 //查询一个 Account account = jt.query(\"select * from account where id = ?\",new AccountResultSetExtractor(),3); System.out.println(account); &#125;&#125; 1.4.9 查询返回一行一列操作1234567891011121314public class JdbcTemplateDemo3 &#123; public static void main(String[] args) &#123; //1.获取Spring容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据id获取bean对象 JdbcTemplate jt = (JdbcTemplate) ac.getBean(\"jdbcTemplate\"); //3.执行操作 //查询返回一行一列：使用聚合函数，在不使用group by字句时，都是返回一行一列。最长用的就是分页中获取总记录条数 //queryForObject是spring 3.x之后的新方法，在spring2.x的时候，它的方法是多个：queryForInt queryForLong queryForShort Integer total = jt.queryForObject(\"select count(*) from account where money &gt; ? \",Integer.class,500); System.out.println(total); &#125;&#125; 1.5 在dao中使用JdbcTemplate1.5.1 准备实体类12345678910/** * 账户的实体 */public class Account implements Serializable &#123; private Integer id; private String name; private Float money; ...&#125; 1.5.2 第一种方式：在dao中定义JdbcTemplate123456789101112131415161718192021222324252627282930313233343536/** * 账户的持久层实现类 * 此版本的dao，需要给dao注入JdbcTemplate */public class AccountDaoImpl implements IAccountDao &#123; private JdbcTemplate jdbcTemplate; public void setJdbcTemplate(JdbcTemplate jdbcTemplate) &#123; this.jdbcTemplate = jdbcTemplate; &#125; @Override public Account findAccountById(Integer id) &#123; List&lt;Account&gt; list = jdbcTemplate.query(\"select * from account where id = ? \",new AccountRowMapper(),id); return list.isEmpty()?null:list.get(0); &#125; @Override public Account findAccountByName(String name) &#123; List&lt;Account&gt; list = jdbcTemplate.query(\"select * from account where name = ? \",new AccountRowMapper(),name); if(list.isEmpty())&#123; return null; &#125; if(list.size()&gt;1)&#123; throw new RuntimeException(\"结果集不唯一，不是只有一个账户对象\"); &#125; return list.get(0); &#125; @Override public void updateAccount(Account account) &#123; jdbcTemplate.update(\"update account set money = ? where id = ? \",account.getMoney(),account.getId()); &#125;&#125; 配置文件： 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 配置一个dao --&gt; &lt;bean id=\"accountDao\" class=\"com.wgy.dao.impl.AccountDaoImpl\"&gt; &lt;!-- 注入jdbcTemplate --&gt; &lt;property name=\"jdbcTemplate\" ref=\"jdbcTemplate\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置一个数据库的操作模板：JdbcTemplate --&gt; &lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 1.5.3 第二种方式：让dao继承JdbcDaoSupportJdbcDaoSupport是spring框架为我们提供的一个类，该类中定义了一个JdbcTemplate对象，我们可以直接获取使用，但是要想创建该对象，需要为其提供一个数据源：具体源码如下： 12345678910111213141516171819202122232425public abstract class JdbcDaoSupport extends DaoSupport &#123; //定义对象 private JdbcTemplate jdbcTemplate; //set方法注入数据源，判断是否注入了，注入了就创建JdbcTemplate public final void setDataSource(DataSource dataSource) &#123; if (this.jdbcTemplate == null || dataSource != this.jdbcTemplate.getDataSource()) &#123; //如果提供了数据源就创建JdbcTemplate this.jdbcTemplate = createJdbcTemplate(dataSource); initTemplateConfig(); &#125; &#125; //使用数据源创建JdcbTemplate protected JdbcTemplate createJdbcTemplate(DataSource dataSource) &#123; return new JdbcTemplate(dataSource); &#125; //当然，我们也可以通过注入JdbcTemplate对象 public final void setJdbcTemplate(JdbcTemplate jdbcTemplate) &#123; this.jdbcTemplate = jdbcTemplate; initTemplateConfig(); &#125; //使用getJdbcTmeplate方法获取操作模板对象 public final JdbcTemplate getJdbcTemplate() &#123; return this.jdbcTemplate; &#125;&#125; 1234567891011121314151617181920212223242526272829303132/** * 账户的持久层实现类 * 此版本dao，只需要给它的父类注入一个数据源 */public class AccountDaoImpl2 extends JdbcDaoSupport implements IAccountDao &#123; @Override public Account findAccountById(Integer id) &#123; //getJdbcTemplate()方法是从父类上继承下来的。 List&lt;Account&gt; list = getJdbcTemplate().query(\"select * from account where id = ? \",new AccountRowMapper(),id); return list.isEmpty()?null:list.get(0); &#125; @Override public Account findAccountByName(String name) &#123; //getJdbcTemplate()方法是从父类上继承下来的。 List&lt;Account&gt; list = getJdbcTemplate().query(\"select * from account where name = ? \",new AccountRowMapper(),name); if(list.isEmpty())&#123; return null; &#125; if(list.size()&gt;1)&#123; throw new RuntimeException(\"结果集不唯一，不是只有一个账户对象\"); &#125; return list.get(0); &#125; @Override public void updateAccount(Account account) &#123; //getJdbcTemplate()方法是从父类上继承下来的。 getJdbcTemplate().update(\"update account set money = ? where id = ? \",account.getMoney(),account.getId()); &#125;&#125; 配置文件： 12345678910111213141516171819&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 配置dao2 --&gt; &lt;bean id=\"accountDao2\" class=\"com.wgy.dao.impl.AccountDaoImpl2\"&gt; &lt;!-- 注入dataSource --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 1.5.4 两种方式有什么区别呢？ 第一种在Dao类中定义JdbcTemplate的方式，适用于所有配置方式（xml和注解都可以）。 第二种让Dao继承JdbcDaoSupport的方式，只能用于基于XML的方式，注解用不了。 2. Spring中的事务控制2.1 Spring中事务控制的API介绍2.1.1 PlatformTransactionManager此接口是spring的事务管理器，它里面提供了我们常用的操作事务的方法，如下图： 我们在开发中都是使用它的实现类，如下图： 123真正管理事务的对象 org.springframework.jdbc.datasource.DataSourceTransactionManager 使用Spring JDBC或iBatis 进行持久化数据时使用 org.springframework.orm.hibernate3.HibernateTransactionManager 使用Hibernate版本进行持久化数据时使用 2.1.2 TransactionDefinition它是事务的定义信息对象，里面有如下方法： 2.1.2.1 事务的隔离级别 2.1.2.2 事务的传播行为1234567REQUIRED:如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。一般的选择（默认值）SUPPORTS:支持当前事务，如果当前没有事务，就以非事务方式执行（没有事务）MANDATORY：使用当前的事务，如果当前没有事务，就抛出异常REQUERS_NEW:新建事务，如果当前在事务中，把当前事务挂起。NOT_SUPPORTED:以非事务方式执行操作，如果当前存在事务，就把当前事务挂起NEVER:以非事务方式运行，如果当前存在事务，抛出异常NESTED:如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行REQUIRED类似的操作。 2.1.2.3 超时时间默认值是-1，没有超时限制。如果有，以秒为单位进行设置。 2.1.2.4 是否是只读事务建议查询时设置为只读 2.1.3 TransactionStatus此接口提供的是事务具体的运行状态，方法介绍如下图： 2.2 基于XML的声明式事务控制（配置方式）2.2.1 环境搭建2.2.1.1 拷贝必要的jar 2.2.1.2 创建spring的配置文件并导入约束123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt; &lt;/beans&gt; 2.2.1.3 准备实体类12345678910/** * 账户的实体 */public class Account implements Serializable &#123; private Integer id; private String name; private Float money; ...&#125; 2.2.1.4 编写业务层接口和实现类1234567891011121314151617181920/** * 账户的业务层接口 */public interface IAccountService &#123; /** * 根据id查询账户信息 * @param id * @return */ Account findAccountById(Integer id);//查 /** * 转账 * @param sourceName 转出账户名称 * @param targeName 转入账户名称 * @param money 转账金额 */ void transfer(String sourceName,String targeName,Float money);//增删改&#125; 123456789101112131415161718192021222324252627282930/** * 账户的业务层实现类 */public class AccountServiceImpl implements IAccountService &#123; private IAccountDao accountDao; public void setAccountDao(IAccountDao accountDao) &#123; this.accountDao = accountDao; &#125; @Override public Account findAccountById(Integer id) &#123; return accountDao.findAccountById(id); &#125; @Override public void transfer(String sourceName, String targeName, Float money) &#123; //1.根据名称查询两个账户 Account source = accountDao.findAccountByName(sourceName); Account target = accountDao.findAccountByName(targeName); //2.修改两个账户的金额 source.setMoney(source.getMoney()-money);//转出账户减钱 target.setMoney(target.getMoney()+money);//转入账户加钱 //3.更新两个账户 accountDao.updateAccount(source); int i=1/0; accountDao.updateAccount(target); &#125;&#125; 2.2.1.5 编写Dao接口和实现类123456789101112131415161718192021222324/** * 账户的持久层接口 */public interface IAccountDao &#123; /** * 根据id查询账户信息 * @param id * @return */ Account findAccountById(Integer id); /** * 根据名称查询账户信息 * @return */ Account findAccountByName(String name); /** * 更新账户信息 * @param account */ void updateAccount(Account account);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 账户的持久层实现类 * 此版本dao，只需要给它的父类注入一个数据源 */public class AccountDaoImpl extends JdbcDaoSupport implements IAccountDao &#123; @Override public Account findAccountById(Integer id) &#123; List&lt;Account&gt; list = getJdbcTemplate().query(\"select * from account where id = ? \",new AccountRowMapper(),id); return list.isEmpty()?null:list.get(0); &#125; @Override public Account findAccountByName(String name) &#123; List&lt;Account&gt; list = getJdbcTemplate().query(\"select * from account where name = ? \",new AccountRowMapper(),name); if(list.isEmpty())&#123; return null; &#125; if(list.size()&gt;1)&#123; throw new RuntimeException(\"结果集不唯一，不是只有一个账户对象\"); &#125; return list.get(0); &#125; @Override public void updateAccount(Account account) &#123; getJdbcTemplate().update(\"update account set money = ? where id = ? \",account.getMoney(),account.getId()); &#125;&#125;/** * 账户的封装类RowMapper的实现类 */public class AccountRowMapper implements RowMapper&lt;Account&gt;&#123; @Override public Account mapRow(ResultSet rs, int rowNum) throws SQLException &#123; Account account = new Account(); account.setId(rs.getInt(\"id\")); account.setName(rs.getString(\"name\")); account.setMoney(rs.getFloat(\"money\")); return account; &#125;&#125; 2.2.1.6 在配置文件中配置业务层和持久层123456789101112131415161718&lt;!-- 配置service --&gt;&lt;bean id=\"accountService\" class=\"com.wgy.service.impl.AccountServiceImpl\"&gt; &lt;property name=\"accountDao\" ref=\"accountDao\"&gt;&lt;/property&gt;&lt;/bean&gt; &lt;!-- 配置dao --&gt;&lt;bean id=\"accountDao\" class=\"com.wgy.dao.impl.AccountDaoImpl\"&gt; &lt;!-- 注入dataSource --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt;&lt;/bean&gt; &lt;!-- 配置数据源 --&gt;&lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt;&lt;/bean&gt; 2.2.2 配置步骤2.2.2.1 配置事务管理器12345&lt;!-- 配置一个事务管理器 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 注入DataSource --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt;&lt;/bean&gt; 2.2.2.2 配置事务的通知引用事务管理器123&lt;!-- 事务的配置 --&gt;&lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt;&lt;/tx:advice&gt; 2.2.2.3 配置事务的属性12345678910111213&lt;!--在tx:advice标签内部 配置事务的属性 --&gt;&lt;tx:attributes&gt; &lt;!-- 指定方法名称：是业务核心方法 read-only：是否是只读事务。默认false，不只读。 isolation：指定事务的隔离级别。默认值是使用数据库的默认隔离级别。 propagation：指定事务的传播行为。 timeout：指定超时时间。默认值为：-1。永不超时。 rollback-for：用于指定一个异常，当执行产生该异常时，事务回滚。产生其他异常，事务不回滚。没有默认值，任何异常都回滚。 no-rollback-for：用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时，事务回滚。没有默认值，任何异常都回滚。 --&gt; &lt;tx:method name=\"*\" read-only=\"false\" propagation=\"REQUIRED\"/&gt; &lt;tx:method name=\"find*\" read-only=\"true\" propagation=\"SUPPORTS\"/&gt;&lt;/tx:attributes&gt; 2.2.2.4 配置AOP-切入点表达式12345&lt;!-- 配置aop --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点表达式 --&gt; &lt;aop:pointcut expression=\"execution(* com.wgy.service.impl.*.*(..))\" id=\"pt1\"/&gt;&lt;/aop:config&gt; 2.2.2.5 配置切入点表达式和事务通知的对应关系12&lt;!-- 在aop:config标签内部：建立事务的通知和切入点表达式的关系 --&gt;&lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"pt1\"/&gt; 2.2.2.6 完整配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt; &lt;!-- 配置service --&gt; &lt;bean id=\"accountService\" class=\"com.wgy.service.impl.AccountServiceImpl\"&gt; &lt;property name=\"accountDao\" ref=\"accountDao\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置dao --&gt; &lt;bean id=\"accountDao\" class=\"com.wgy.dao.impl.AccountDaoImpl\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置SPRING内置数据源 --&gt; &lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- spring基于XML的声明式事务控制 --&gt; &lt;!-- 第一步：配置事务管理器 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 注入数据源 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 第二步：配置事务的通知 --&gt; &lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;!-- 第四步：配置事务的属性 isolation：配置事务的隔离级别。默认值：DEFAULT，使用数据库的默认隔离级别。mysql是REPEATABLE_READ propagation：配置事务的传播行为。默认值是：REQUIRED。 一般的选择。（增删改方法）。当是查询方法时，选择SUPPORTS timeout：指定事务的超时时间。默认值是：-1，永不超时。当指定其他值时，以秒为单位 read-only：配置是否只读事务。默认值是：false，读写型事务。 当指定为true时，表示只读，只能用于查询方法。 rollback-for：用于指定一个异常，当执行产生该异常时，事务回滚。产生其他异常时，事务不回滚。没有默认值，任何异常都回滚。 no-rollback-for：用于指定一个异常，当执行产生该异常时，事务不回滚。产生其他异常时，事务回滚。没有默认值，任何异常都回滚。 --&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"*\" propagation=\"REQUIRED\" read-only=\"false\"/&gt; &lt;tx:method name=\"find*\" propagation=\"SUPPORTS\" read-only=\"true\"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 第三步：配置aop 要配的是：切入点表达式 通知和切入点表达式的关联 --&gt; &lt;aop:config&gt; &lt;!-- 配置切入点表达式 --&gt; &lt;aop:pointcut expression=\"execution(* com.wgy.service.impl.*.*(..))\" id=\"pt1\"/&gt; &lt;!-- 配置事务通知和切入点表达式的关联 --&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"pt1\"/&gt; &lt;/aop:config&gt;&lt;/beans&gt; 2.3 基于XML和注解组合使用的整合方式2.3.1 环境搭建2.3.1.1 拷贝必备的jar 2.3.1.2 spring的配置文件导入约束并配置扫描的包1234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 配置spring要扫描的包 --&gt; &lt;context:component-scan base-package=\"com.wgy\"&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 2.3.1.3 准备实体类12345678910/** * 账户的实体 */public class Account implements Serializable &#123; private Integer id; private String name; private Float money; ...&#125; 2.3.1.4 业务层实现类使用注解让spring管理123456789101112131415161718192021222324252627/** * 账户的业务层实现类 */@Service(\"accountService\")public class AccountServiceImpl implements IAccountService &#123; @Autowired private IAccountDao accountDao; @Override public Account findAccountById(Integer id) &#123; return accountDao.findAccountById(id); &#125; @Override public void transfer(String sourceName, String targeName, Float money) &#123; //1.根据名称查询两个账户 Account source = accountDao.findAccountByName(sourceName); Account target = accountDao.findAccountByName(targeName); //2.修改两个账户的金额 source.setMoney(source.getMoney()-money);//转出账户减钱 target.setMoney(target.getMoney()+money);//转入账户加钱 //3.更新两个账户 accountDao.updateAccount(source); int i=1/0; accountDao.updateAccount(target); &#125;&#125; 2.3.1.5 Dao实现类使用注解让spring管理1234567891011121314151617181920212223242526272829303132@Repository(\"accountDao\")public class AccountDaoImpl implements IAccountDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public Account findAccountById(Integer accountId) &#123; List&lt;Account&gt; list = jdbcTemplate.query(\"select * from account where id = ? \", new BeanPropertyRowMapper&lt;Account&gt;(Account.class), accountId); return list.isEmpty() ? null : list.get(0); &#125; @Override public Account findAccountByName(String name) &#123; List&lt;Account&gt; list = jdbcTemplate.query(\"select * from account where name = ? \", new BeanPropertyRowMapper&lt;Account&gt;(Account.class), name); if (list.isEmpty()) &#123; //没有这个名称的账户 return null; &#125; if (list.size() &gt; 1) &#123; //结果集不唯一，不符合我们的约定 throw new RuntimeException(\"结果集不唯一，请检查数据\"); &#125; return list.get(0); &#125; @Override public void updateAccount(Account account) &#123; jdbcTemplate.update(\"update account set name=?,money=? where id=?\", account.getName(), account.getMoney(), account.getId()); &#125;&#125; 2.3.2 配置步骤2.3.2.1 配置数据源和JdbcTemplate123456789101112&lt;!-- 配置jdbcTemplate --&gt;&lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!-- 配置SPRING内置数据源 --&gt;&lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/spring\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt;&lt;/bean&gt; 2.3.2.2 配置事务管理器并注入数据源1234&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 注入数据源 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt;&lt;/bean&gt; 2.3.2.3 在业务层使用@Transactional注解1234567&lt;!-- 在需要事务的地方使用@Transactional注解 该注解可以写在接口上，类上和方法上。 写在接口上，表示该接口的所有实现类都有事务。 写在类上，表示该类中所有方法都有事务。 写在方法，表示该方法有事务。 优先级：就近原则。方法&gt;类&gt;接口--&gt; 12345678910111213141516171819202122232425262728@Service(\"accountService\")@Transactional(propagation = Propagation.SUPPORTS,readOnly = true)//只读型public class AccountServiceImpl implements IAccountService &#123; @Autowired private IAccountDao accountDao; @Override public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; @Override @Transactional(propagation=Propagation.REQUIRED,readOnly=false)//读写型 public void transfer(String sourceName, String targetName, Float money) &#123; //1.根据名称查询账户信息 Account source = accountDao.findAccountByName(sourceName); Account target = accountDao.findAccountByName(targetName); //2.转出账户减钱，转入账户加钱 source.setMoney(source.getMoney() - money); target.setMoney(target.getMoney() + money); //3.更新账户信息 accountDao.updateAccount(source); int i = 1 / 0; accountDao.updateAccount(target); &#125;&#125; 2.3.2.4 在配置文件中开启spring对注解事务的支持12&lt;!-- 开启spring对注解事务的支持 --&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt; 2.4 基于纯注解的声明式事务控制（配置方式）2.4.1 环境搭建2.4.1.1 拷贝必备的jar包 2.4.1.2 创建一个类用于加载spring的配置并指定要扫描的包12345678/** * 用于初始化spring容器的配置类 */@Configuration@ComponentScan(basePackages=\"com.wgy\")public class SpringConfiguration &#123;&#125; 2.4.1.3 准备实体类12345678910/** * 账户的实体 */public class Account implements Serializable &#123; private Integer id; private String name; private Float money; ...&#125; 2.4.1.4 业务层实现类使用注解让spring管理123456789101112131415161718192021222324252627/** * 账户的业务层实现类 */@Service(\"accountService\")public class AccountServiceImpl implements IAccountService &#123; @Autowired private IAccountDao accountDao; @Override public Account findAccountById(Integer id) &#123; return accountDao.findAccountById(id); &#125; @Override public void transfer(String sourceName, String targeName, Float money) &#123; //1.根据名称查询两个账户 Account source = accountDao.findAccountByName(sourceName); Account target = accountDao.findAccountByName(targeName); //2.修改两个账户的金额 source.setMoney(source.getMoney()-money);//转出账户减钱 target.setMoney(target.getMoney()+money);//转入账户加钱 //3.更新两个账户 accountDao.updateAccount(source); int i=1/0; accountDao.updateAccount(target); &#125;&#125; 2.4.1.5 Dao实现类使用注解让spring管理1234567891011121314151617181920212223242526272829303132@Repository(\"accountDao\")public class AccountDaoImpl implements IAccountDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public Account findAccountById(Integer accountId) &#123; List&lt;Account&gt; list = jdbcTemplate.query(\"select * from account where id = ? \", new BeanPropertyRowMapper&lt;Account&gt;(Account.class), accountId); return list.isEmpty() ? null : list.get(0); &#125; @Override public Account findAccountByName(String name) &#123; List&lt;Account&gt; list = jdbcTemplate.query(\"select * from account where name = ? \", new BeanPropertyRowMapper&lt;Account&gt;(Account.class), name); if (list.isEmpty()) &#123; //没有这个名称的账户 return null; &#125; if (list.size() &gt; 1) &#123; //结果集不唯一，不符合我们的约定 throw new RuntimeException(\"结果集不唯一，请检查数据\"); &#125; return list.get(0); &#125; @Override public void updateAccount(Account account) &#123; jdbcTemplate.update(\"update account set name=?,money=? where id=?\", account.getName(), account.getMoney(), account.getId()); &#125;&#125; 2.4.2 配置步骤2.4.2.1 使用@Bean注解配置数据源和JdbcTemplate12345678910111213141516171819202122/** * 连接数据库的配置类 * * @author wgy */public class JdbcConfig &#123; @Bean(name = \"jdbcTemplate\") public JdbcTemplate createJdbcTemplate(DataSource dataSource) &#123; return new JdbcTemplate(dataSource); &#125; @Bean(name = \"dataSource\") public DataSource createDataSource() &#123; DriverManagerDataSource ds = new DriverManagerDataSource(); ds.setDriverClassName(\"com.mysql.jdbc.Driver\"); ds.setUrl(\"jdbc:mysql://localhost:3306/spring\"); ds.setUsername(\"root\"); ds.setPassword(\"root\"); return ds; &#125;&#125; 2.4.2.2 使用@Bean注解配置配置事务管理器123456789101112/** * 事务控制的配置类 * * @author wgy */public class TransactionManager &#123; @Bean(name = \"transactionManager\") public PlatformTransactionManager createTransactionManager(DataSource dataSource) &#123; return new DataSourceTransactionManager(dataSource); &#125;&#125; 2.4.2.3 在业务层使用@Transactional注解1234567&lt;!-- 在需要事务的地方使用@Transactional注解 该注解可以写在接口上，类上和方法上。 写在接口上，表示该接口的所有实现类都有事务。 写在类上，表示该类中所有方法都有事务。 写在方法，表示该方法有事务。 优先级：就近原则。方法&gt;类&gt;接口--&gt; 12345678910111213141516171819202122232425262728@Service(\"accountService\")@Transactional(propagation = Propagation.SUPPORTS,readOnly = true)//只读型public class AccountServiceImpl implements IAccountService &#123; @Autowired private IAccountDao accountDao; @Override public Account findAccountById(Integer accountId) &#123; return accountDao.findAccountById(accountId); &#125; @Override @Transactional(propagation=Propagation.REQUIRED,readOnly=false)//读写型 public void transfer(String sourceName, String targetName, Float money) &#123; //1.根据名称查询账户信息 Account source = accountDao.findAccountByName(sourceName); Account target = accountDao.findAccountByName(targetName); //2.转出账户减钱，转入账户加钱 source.setMoney(source.getMoney() - money); target.setMoney(target.getMoney() + money); //3.更新账户信息 accountDao.updateAccount(source); int i = 1 / 0; accountDao.updateAccount(target); &#125;&#125; 2.4.2.4 使用@EnableTransactionManagement开启spring对注解事务的的支持123456789101112/** * spring的配置类，作用就是当bean.xml用 * * @author wgy */@Configuration@ComponentScan(\"com.wgy\")@Import(&#123;JdbcConfig.class, TransactionManager.class&#125;)@EnableTransactionManagementpublic class SpringConfiguration &#123;&#125;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Spring","slug":"Spring","permalink":"https://wgy1993.gitee.io/tags/Spring/"}]},{"title":"Spring(三)","date":"2020-06-13T10:27:26.000Z","path":"archives/680971a4.html","text":"1. AOP的相关概念1.1 AOP概述1.1.1 什么是AOP 在软件业，AOP为Aspect Oriented Programming的缩写，意为：面向切面编程，通过预编译方式和运行期间动态代理实现程序功能的统一维护的一种技术。AOP是OOP的延续，是软件开发中的一个热点，也是Spring框架中的一个重要内容，是函数式编程的一种衍生范型。利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 1.1.2 AOP的作用及优势作用： 在程序运行期间，不修改源码对已有方法进行增强。 优势： 减少重复代码 提高开发效率 维护方便 1.1.3 AOP的实现方式使用动态代理技术 1.2 AOP的具体应用1.2.1 案例中问题这是我们之前在struts2课程中做的一个完整的增删改查例子。下面是客户的业务层接口和实现类。 通过下面的代码，我们能看出什么问题吗? 123456789101112131415161718/** * 客户的业务层接口 */public interface ICustomerService &#123; /** * 保存客户 * @param customer */ void saveCustomer(Customer customer); /** * 查询所有客户 * @return */ List&lt;Customer&gt; findAllCustomer();&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041/** * 客户的业务层实现类 * 事务必须在此控制 * 业务层都是调用持久层的方法 */public class CustomerServiceImpl implements ICustomerService &#123; private ICustomerDao customerDao = new CustomerDaoImpl(); @Override public void saveCustomer(Customer customer) &#123; Session s = null; Transaction tx = null; try&#123; s = HibernateUtil.getCurrentSession(); tx = s.beginTransaction(); customerDao.saveCustomer(customer); tx.commit(); &#125;catch(Exception e)&#123; tx.rollback(); throw new RuntimeException(e); &#125; &#125; @Override public List&lt;Customer&gt; findAllCustomer() &#123; Session s = null; Transaction tx = null; try&#123; s = HibernateUtil.getCurrentSession(); tx = s.beginTransaction(); List&lt;Customer&gt; customers = customerDao.findAllCustomer(); tx.commit(); return customers; &#125;catch(Exception e)&#123; tx.rollback(); throw new RuntimeException(e); &#125; &#125;&#125; 上面代码的问题就是：我们的事务控制是重复性的代码。这还只是一个业务类，如果有多个业务了，每个业务类中都会有这些重复性的代码。 1.2.2 动态代理回顾1.2.2.1 动态代理的特点123字节码随用随创建，随用随加载。它与静态代理的区别也在于此。因为静态代理是字节码一上来就创建好，并完成加载。装饰者模式就是静态代理的一种体现。 1.2.2.2 动态代理常用的有两种方式123456基于接口的动态代理 提供者：JDK官方的Proxy类。 要求：被代理类最少实现一个接口。基于子类的动态代理 提供者：第三方的CGLib，如果报asmxxxx异常，需要导入asm.jar。 要求：被代理类不能用final修饰的类（最终类）。 1.2.2.3 使用JDK官方的Proxy类创建代理对象 此处我们使用的是一个演员的例子： 在很久以前，演员和剧组都是直接见面联系的。没有中间人环节。 而随着时间的推移，产生了一个新兴职业：经纪人（中间人），这个时候剧组再想找演员就需要通过经纪人来找了。下面我们就用代码演示出来。 1234567891011121314151617/** * 一个经纪公司的要求: * 能做基本的表演和危险的表演 */public interface IActor &#123; /** * 基本演出 * @param money */ public void basicAct(float money); /** * 危险演出 * @param money */ public void dangerAct(float money);&#125; 1234567891011121314/** * 一个演员 * 实现了接口，就表示具有接口中的方法实现。即：符合经纪公司的要求 */public class Actor implements IActor&#123; public void basicAct(float money)&#123; System.out.println(\"拿到钱，开始基本的表演：\"+money); &#125; public void dangerAct(float money)&#123; System.out.println(\"拿到钱，开始危险的表演：\"+money); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 模拟一个剧组 * * @author wgy */public class Client &#123; public static void main(String[] args) &#123; //一个剧组找演员 final Actor actor = new Actor(); /** * 动态代理： * 作用：不改变源码的基础上，对已有方法增强。（它是AOP思想的实现技术） * 分类： * 基于接口的动态代理： * 要求：被代理类最少实现一个接口 * 提供者：JDK官方 * 涉及的类：Proxy * 创建代理对象的方法：newProxyInstance(ClassLoader,Class[],InvocationHandler) * 参数的含义： * ClassLoader：类加载器。和被代理对象使用相同的类加载器。一般都是固定写法。 * Class[]：字节码数组。被代理类实现的接口。（要求代理对象和被代理对象具有相同的行为）。一般都是固定写法。 * InvocationHandler：它是一个接口，就是用于我们提供增强代码的。我们一般都是些一个该接口的实现类。实现类可以是匿名内部类。 * 它的含义就是：如何代理。此处的代码只能是谁用谁提供。 * 策略模式： * 使用要求：数据已经有了 * 目的明确 * 达成目标的过程就是策略。 * 在dbutils中的ResultSetHandler就是策略模式的具体应用。 * * 基于子类的动态代理 * */ IActor proxyActor = (IActor) Proxy.newProxyInstance(actor.getClass().getClassLoader(), actor.getClass().getInterfaces(), new InvocationHandler() &#123; /** * 执行被代理对象的任何方法都会经过该方法，该方法有拦截的功能 * 方法的参数 * Object proxy：代理对象的引用。不一定每次都会有。 * Method method：当前执行的方法 * Object[] args：当前执行方法所需的参数 * 返回值： * 当前执行方法的返回值 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object rtValue = null; //1.取出执行方法中的参数：给的多少钱 Float money = (Float)args[0]; //2.判断当前执行的是什么方法：每个经纪公司对不同演出收费不一样，此处开始判断 if(\"basicAct\".equals(method.getName()))&#123; //基本演出，没有10000不演 if(money &gt; 10000)&#123; //执行方法（开始表演） //看上去剧组是给了20000，实际到演员手里只有10000 //这就是我们没有修改原来basicAct方法源码，对方法进行了增强 rtValue = method.invoke(actor, money/2); &#125; &#125; if(\"dangerAct\".equals(method.getName()))&#123; //危险演出，没有50000不演 if(money &gt; 50000)&#123; //执行方法 //看上去剧组是给了60000，实际到演员手里只有30000 //这就是我们没有修改原来dangerAct方法源码，对方法进行了增强 rtValue = method.invoke(actor, money/2); &#125; &#125; return rtValue; &#125; &#125;);// 没有经纪公司的时候，直接找演员。// actor.basicAct(1000f);// actor.dangerAct(5000f);// 剧组无法直接联系演员，而是由经纪公司找的演员 proxyActor.basicAct(20000); proxyActor.dangerAct(60000); &#125;&#125; 1.2.2.4 使用CGLib的Enhancer类创建代理对象还是那个演员的例子，只不过不让他实现接口。 12345678910111213141516171819202122232425/** * 一个演员 * * @author wgy */public class Actor &#123; /** * 基本的演出 * * @param money */ public void basicAct(float money) &#123; System.out.println(\"CGLIB拿到钱，开始基本的表演：\" + money); &#125; /** * 危险的表演 * * @param money */ public void dangerAct(float money) &#123; System.out.println(\"CGLIB拿到钱，开始危险的表演：\" + money); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 模拟一个剧组 * * @author wgy */public class Client &#123; public static void main(String[] args) &#123; final Actor actor = new Actor(); /** * 动态代理： * 作用：不改变源码的基础上，对已有方法增强。（它是AOP思想的实现技术） * 分类： * 基于接口的动态代理： * * 基于子类的动态代理： * 要求：被代理类不能是最终类。不能被final修饰 * 提供者：第三方CGLib * 涉及的类：Enhancer * 创建代理对象的方法：create(Class,Callback); * 参数的含义： * Class：被代理对象的字节码 * Callback：如何代理。它和InvocationHandler的作用是一样的。它也是一个接口，我们一般使用该接口的子接口MethodInterceptor * 在使用时我们也是创建该接口的匿名内部类。 * */ Actor cglibActor = (Actor) Enhancer.create(actor.getClass(), new MethodInterceptor() &#123; /** * 执行被代理对象的任何方法，都会经过该方法。它和基于接口动态代理的invoke方法的作用是一模一样的。 * 方法的参数; * 前面三个和invoke方法的参数含义和作用都一样。 * MethodProxy methodProxy：当前执行方法的代理对象。一般不用 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; Object rtValue = null; //1.取出执行方法中的参数：给的多少钱 Float money = (Float)args[0]; //2.判断当前执行的是什么方法 if(\"basicAct\".equals(method.getName()))&#123; //基本演出 if(money &gt; 10000)&#123; //执行方法（开始表演） rtValue = method.invoke(actor, money/2); &#125; &#125; if(\"dangerAct\".equals(method.getName()))&#123; //危险演出 if(money &gt; 50000)&#123; //执行方法 rtValue = method.invoke(actor, money/2); &#125; &#125; return rtValue; &#125; &#125;); cglibActor.basicAct(50000); cglibActor.dangerAct(100000); &#125;&#125; 1.2.3 解决案例中的问题思路只有一个：使用动态代理技术创建客户业务层的代理对象，在执行CustomerServiceImpl时，对里面的方法进行增强，加入事务的支持。 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 用于创建客户业务层对象工厂（当然也可以创建其他业务层对象，只不过我们此处不做那么繁琐）*/public class BeanFactory &#123; /** * 获取客户业务层对象的代理对象 * @return */ public static ICustomerService getCustomerService() &#123; //定义客户业务层对象 final ICustomerService customerService = new CustomerServiceImpl(); //生成它的代理对象 ICustomerService proxyCustomerService = (ICustomerService) Proxy.newProxyInstance(customerService.getClass().getClassLoader() ,customerService.getClass().getInterfaces(), new InvocationHandler() &#123; //执行客户业务层任何方法，都会在此处被拦截，我们对那些方法增强，加入事务。 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String name = method.getName(); Object rtValue = null; try&#123; //开启事务 HibernateUtil.beginTransaction(); //执行操作 rtValue = method.invoke(customerService, args); //提交事务 HibernateUtil.commit(); &#125;catch(Exception e)&#123; //回滚事务 HibernateUtil.rollback(); e.printStackTrace(); &#125;finally&#123; //释放资源.hibernate在我们事务操作（提交/回滚）之后，已经帮我们关了。 //如果他没关，我们在此处关 &#125; return rtValue; &#125; &#125;); return proxyCustomerService; &#125;&#125; 1.3 Spring中的AOP1.3.1 关于代理的选择在spring中，框架会根据目标类是否实现了接口来决定采用哪种动态代理的方式。 1.3.2 AOP相关术语123456789101112131415161718Joinpoint(连接点): 所谓连接点是指那些被拦截到的点。在spring中,这些点指的是方法,因为spring只支持方法类型的连接点。Pointcut(切入点): 所谓切入点是指我们要对哪些Joinpoint进行拦截的定义。Advice(通知&#x2F;增强): 所谓通知是指拦截到Joinpoint之后所要做的事情就是通知。 通知的类型：前置通知,后置通知,异常通知,最终通知,环绕通知。Introduction(引介): 引介是一种特殊的通知在不修改类代码的前提下, Introduction可以在运行期为类动态地添加一些方法或Field。Target(目标对象): 代理的目标对象。Weaving(织入): 是指把增强应用到目标对象来创建新的代理对象的过程。 spring采用动态代理织入，而AspectJ采用编译期织入和类装载期织入。Proxy（代理）: 一个类被AOP织入增强后，就产生一个结果代理类。Aspect(切面): 是切入点和通知（引介）的结合。 1.3.3 学习spring中的AOP要明确的事123456a、开发阶段（我们做的） 编写核心业务代码（开发主线）：大部分程序员来做，要求熟悉业务需求。 把公用代码抽取出来，制作成通知。（开发阶段最后再做）：AOP编程人员来做。 在配置文件中，声明切入点与通知间的关系，即切面。：AOP编程人员来做。b、运行阶段（Spring框架完成的） Spring框架监控切入点方法的执行。一旦监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置，将通知对应的功能织入，完成完整的代码逻辑运行。 2. 基于XML的AOP配置2.1 环境搭建2.1.1 准备客户的业务层和接口（需要增强的类）12345678910111213141516/** * 客户的业务层接口*/public interface ICustomerService &#123; /** * 保存客户 */ void saveCustomer(); /** * 修改客户 * @param i */ void updateCustomer(int i);&#125; 123456789101112131415/** * 客户的业务层实现类 */public class CustomerServiceImpl implements ICustomerService &#123; @Override public void saveCustomer() &#123; System.out.println(\"调用持久层，执行保存客户\"); &#125; @Override public void updateCustomer(int i) &#123; System.out.println(\"调用持久层，执行修改客户\"); &#125;&#125; 2.1.2 拷贝必备的jar包 2.1.3 创建spring的配置文件并导入约束12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt;&lt;/beans&gt; 2.1.4 把客户的业务层配置到spring容器中12&lt;!-- 把资源交给spring来管理 --&gt;&lt;bean id=\"customerService\" class=\"com.wgy.service.impl.CustomerServiceImpl\"/&gt; 2.1.5 制作通知（增强的类）123456789101112131415/** * 一个用于记录日志的类 * * @author wgy */public class Logger &#123; /** * 记录日志的操作 * 计划让其在业务核心方法（切入点方法）执行之前执行 */ public void beforePrintLog() &#123; System.out.println(\"Logger中的printLog方法开始记录日志了。。。。\"); &#125;&#125; 2.2 配置步骤2.2.1 把通知类用bean标签配置起来12&lt;!-- 把通知类也交给spring来管理 --&gt;&lt;bean id=\"logger\" class=\"com.wgy.util.Logger\"&gt;&lt;/bean&gt; 2.2.2 使用aop:config声明aop配置1234&lt;!-- aop的配置 --&gt;&lt;aop:config&gt; &lt;!-- 配置的代码都写在此处 --&gt; &lt;/aop:config&gt; 2.2.3 使用aop:aspect配置切面1234567&lt;!-- 配置切面 ：此标签要出现在aop:config内部 id：给切面提供一个唯一标识 ref：引用的是通知类的bean的id--&gt;&lt;aop:aspect id=\"logAdvice\" ref=\"logger\"&gt; &lt;!--配置通知的类型要写在此处--&gt;&lt;/aop:aspect&gt; 2.2.4 使用aop:before配置前置通知12345&lt;!-- 用于配置前置通知：指定增强的方法在切入点方法之前执行 method:用于指定通知类中的增强方法名称 ponitcut-ref：用于指定切入点的表达式的引用 --&gt;&lt;aop:before method=\"beforePrintLog\" pointcut-ref=\"pt1\"/&gt; 2.2.5 使用aop:pointcut配置切入点表达式1&lt;aop:pointcut expression=\"execution(public void com.wgy.service.impl.CustomerServiceImpl.saveCustomer())\" id=\"pt1\"/&gt; 2.3 切入点表达式说明123456789101112131415161718192021222324252627execution: 匹配方法的执行(常用) execution(表达式)表达式语法：execution([修饰符] 返回值类型 包名.类名.方法名(参数))写法说明： 全匹配方式： public void com.wgy.service.impl.CustomerServiceImpl.saveCustomer() 访问修饰符可以省略 void com.wgy.service.impl.CustomerServiceImpl.saveCustomer() 返回值可以使用*号，表示任意返回值 * com.wgy.service.impl.CustomerServiceImpl.saveCustomer() 包名可以使用*号，表示任意包，但是有几级包，需要写几个* * *.*.*.*.CustomerServiceImpl.saveCustomer() 使用..来表示当前包，及其子包 * com..CustomerServiceImpl.saveCustomer() 类名可以使用*号，表示任意类 * com..*.saveCustomer() 方法名可以使用*号，表示任意方法 * com..*.*() 参数列表可以使用*，表示参数可以是任意数据类型，但是必须有参数 * com..*.*(*) 参数列表可以使用..表示有无参数均可，有参数可以是任意类型 * com..*.*(..) 全通配方式： * *..*.*(..) 实际开发中，我们一般情况下，我们都是对业务层方法进行增强： 所以写法：* com.wgy.service.impl.*.*(..) 2.4 完整配置123456789101112131415161718192021222324252627282930&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt; &lt;!-- 配置service --&gt; &lt;bean id=\"customerService\" class=\"com.wgy.service.impl.CustomerServiceImpl\"&gt;&lt;/bean&gt; &lt;!-- 基于xml的aop配置步骤 ：要想使用spring的aop，必须导入aop的jar包--&gt; &lt;!-- 第一步：把通知类交给spring来管理 --&gt; &lt;bean id=\"logger\" class=\"com.wgy.utils.Logger\"&gt;&lt;/bean&gt; &lt;!-- 第二步：导入aop名称空间，并且使用aop:config开始aop的配置 --&gt; &lt;aop:config&gt; &lt;!-- 定义通用的切入点表达式，如果写在aop:aspct标签外部，则表示所有切面可用 --&gt; &lt;aop:pointcut expression=\"execution(* com.wgy.service.impl.*.*(..))\" id=\"pt1\"/&gt; &lt;!-- 第三步：使用aop:aspect配置切面 --&gt; &lt;aop:aspect id=\"logAdvice\" ref=\"logger\"&gt; &lt;!-- 第四步：配置通知的类型，指定增强的方法何时执行。--&gt; &lt;aop:before method=\"beforePrintLog\" pointcut-ref=\"pt1\"&gt;&lt;/aop:before&gt; &lt;!-- 定义通用的切入点表达式：如果是写在了aop:aspect标签内部，则表示只有当前切面可用 &lt;aop:pointcut expression=\"execution(* com.wgy.service.impl.*.*(..))\" id=\"pt1\"/&gt;--&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 2.5 通知的类型2.5.1 类型说明1234567891011121314151617&lt;!-- 配置通知的类型 aop:before： 用于配置前置通知。前置通知的执行时间点：切入点方法执行之前执行 aop:after-returning： 用于配置后置通知。后置通知的执行时间点：切入点方法正常执行之后。它和异常通知只能有一个执行 aop:after-throwing 用于配置异常通知。异常通知的执行时间点：切入点方法执行产生异常后执行。它和后置通知只能执行一个。 aop:after 用于配置最终通知。最终通知的执行时间点：无论切入点方法执行时是否有异常，它都会在其后面执行。 aop:around 用于配置环绕通知。他和前面四个不一样，他不是用于指定通知方法何时执行的。--&gt; &lt;aop:before method=\"beforePrintLog\" pointcut-ref=\"pt1\"/&gt;&lt;aop:after-returning method=\"afterReturningPrintLog\" pointcut-ref=\"pt1\"/&gt;&lt;aop:after-throwing method=\"afterThrowingPrintLog\" pointcut-ref=\"pt1\"/&gt;&lt;aop:after method=\"afterPrintLog\" pointcut-ref=\"pt1\"/&gt;&lt;aop:around method=\"aroundPringLog\" pointcut-ref=\"pt1\"/&gt; 2.5.2 环绕通知的特殊说明1234567891011121314151617181920212223242526272829/** * 环绕通知 * 问题： * 当我们配置了环绕通知之后，切入点方法没有执行，而环绕通知里的代码执行了。 * 分析： * 由动态代理可知，环绕通知指的是invoke方法，并且里面有明确的切入点方法调用。而我们现在的环绕通知没有明确切入点方法调用。 * 解决： * spring为我们提供了一个接口：ProceedingJoinPoint。该接口可以作为环绕通知的方法参数来使用。 * 在程序运行时，spring框架会为我们提供该接口的实现类，供我们使用。 * 该接口中有一个方法，proceed()，它的作用就等同于method.invoke方法，就是明确调用业务层核心方法（切入点方法） * * 环绕通知： * 它是spring框架为我们提供的一种可以在代码中手动控制通知方法什么时候执行的方式。 */public Object aroundPrintLog(ProceedingJoinPoint pjp)&#123; Object rtValue = null; try &#123; System.out.println(\"Logger中的aroundPrintLog方法开始记录日志了。。。。前置\"); rtValue = pjp.proceed(); System.out.println(\"Logger中的aroundPrintLog方法开始记录日志了。。。。后置\"); &#125; catch (Throwable e) &#123; System.out.println(\"Logger中的aroundPrintLog方法开始记录日志了。。。。异常\"); e.printStackTrace(); &#125;finally&#123; System.out.println(\"Logger中的aroundPrintLog方法开始记录日志了。。。。最终\"); &#125; return rtValue;&#125; 3. 基于注解的AOP配置3.1 环境搭建3.1.1 准备客户的业务层和接口（需要增强的类）12345678910111213141516/** * 客户的业务层接口*/public interface ICustomerService &#123; /** * 保存客户 */ void saveCustomer(); /** * 修改客户 * @param i */ void updateCustomer(int i);&#125; 123456789101112131415/** * 客户的业务层实现类 */public class CustomerServiceImpl implements ICustomerService &#123; @Override public void saveCustomer() &#123; System.out.println(\"调用持久层，执行保存客户\"); &#125; @Override public void updateCustomer(int i) &#123; System.out.println(\"调用持久层，执行修改客户\"); &#125;&#125; 3.1.2 拷贝必备的jar包 3.1.3 创建spring的配置文件并导入约束12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;/beans&gt; 3.1.4 把资源使用注解让spring来管理123456789101112131415/** * 客户的业务层实现类 */@Service(\"customerService\")public class CustomerServiceImpl implements ICustomerService &#123; @Override public void saveCustomer() &#123; System.out.println(\"调用持久层，执行保存客户\"); &#125; @Override public void updateCustomer(int i) &#123; System.out.println(\"调用持久层，执行修改客户\"); &#125;&#125; 3.1.5 在配置文件中指定spring要扫描的包12&lt;!-- 告知spring，在创建容器时要扫描的包 --&gt;&lt;context:component-scan base-package=\"com.wgy\"&gt;&lt;/context:component-scan&gt; 3.2 配置步骤3.2.1 把通知类也使用注解配置1234567/** * 一个记录日志的工具类 */@Component(\"logger\")public class Logger &#123;&#125; 3.2.2 在通知类上使用@Aspect注解声明为切面12345678/** * 一个记录日志的工具类 */@Component(\"logger\")@Aspect//表明当前类是一个切面类public class Logger &#123; &#125; 3.2.3 在增强的方法上使用注解配置通知1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * 一个用于记录日志的类 * * @author wgy */@Component(\"logger\")@Aspect//配置了切面public class Logger &#123; /** * 指定切入点表达式 */ @Pointcut(\"execution(* com.wgy.service.impl.*.*(..))\") private void pt1()&#123;&#125; /** * 前置通知 */// @Before(\"execution(* com.wgy.service.impl.*.*(..))\") public void beforePrintLog() &#123; System.out.println(\"前置：Logger中的beforePrintLog方法开始记录日志了。。。。\"); &#125; /** * 后置通知 */// @AfterReturning(\"pt1()\") public void afterReturningPrintLog() &#123; System.out.println(\"后置：Logger中的afterReturningPrintLog方法开始记录日志了。。。。\"); &#125; /** * 异常通知 */// @AfterThrowing(\"pt1()\") public void afterThrowingPrintLog() &#123; System.out.println(\"异常：Logger中的afterThrowingPrintLog方法开始记录日志了。。。。\"); &#125; /** * 最终通知 */// @After(\"pt1()\") public void afterPrintLog() &#123; System.out.println(\"最终：Logger中的afterPrintLog方法开始记录日志了。。。。\"); &#125; /** * 环绕通知 */ @Around(\"pt1()\") public Object aroundPrintLog(ProceedingJoinPoint pjp)&#123; Object rtValue = null; try &#123; System.out.println(\"Logger中的aroundPrintLog方法开始记录日志了。。。。前置\"); rtValue = pjp.proceed(); System.out.println(\"Logger中的aroundPrintLog方法开始记录日志了。。。。后置\"); &#125; catch (Throwable e) &#123; System.out.println(\"Logger中的aroundPrintLog方法开始记录日志了。。。。异常\"); e.printStackTrace(); &#125;finally&#123; System.out.println(\"Logger中的aroundPrintLog方法开始记录日志了。。。。最终\"); &#125; return rtValue; &#125;&#125; 3.2.4 在spring配置文件中开启spring对注解AOP的支持12&lt;!-- 开启spring对注解AOP的支持 --&gt;&lt;aop:aspectj-autoproxy/&gt; 3.3 不使用XML的配置方式123456@Configuration@ComponentScan(basePackages=\"com.wgy\")@EnableAspectJAutoProxypublic class SpringConfiguration &#123; &#125;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Spring","slug":"Spring","permalink":"https://wgy1993.gitee.io/tags/Spring/"}]},{"title":"Spring(二)","date":"2020-06-11T06:53:10.000Z","path":"archives/16fa518f.html","text":"1. 基于注解的IOC配置1.1 写在最前学习基于注解的IoC配置，大家脑海里首先得有一个认知，即注解配置和xml配置要实现的功能都是一样的，都是要降低程序间的耦合。只是配置的形式不一样。 关于实际的开发中到底使用xml还是注解，每家公司有着不同的使用习惯。所以这两种配置方式我们都需要掌握。 1.2 环境搭建1.2.1 拷贝必备jar包注意：在基于xml的配置中，多拷贝一个aop的jar包。如下图： 1.2.2 在类的根路径下创建任意名称的xml文件12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt;&lt;/beans&gt; 1.2.3 使用@Component注解配置管理的资源12345678910111213/** * 客户的业务层实现类 * * @author wgy */@Component(value=\"customerService\")public class CustomerServiceImpl implements ICustomerService &#123; @Override public void saveCustomer() &#123; System.out.println(\"执行了保存客户\"); &#125;&#125; 1.2.4 在Spring的配置文件中开启Spring对注解ioc的支持12345678910111213&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 告知spring在创建容器时要扫描的包。当配置了此标签之后，spring创建容器就会去指定的包及其子包下找对应的注解 标签是在一个context的名称空间里，所以必须先导入context名称空间 --&gt; &lt;context:component-scan base-package=\"com.wgy\"/&gt;&lt;/beans&gt; 1.3 常用注解1.3.1 用于创建bean对象1234567891011121314相当于：&lt;bean id&#x3D;&quot;&quot; class&#x3D;&quot;&quot;&gt;@Component 作用：就相当于配置了一个bean标签。 它能出现的位置：类上面 属性： value：含义是指定bean的id。当不写时，它有默认值，默认值是：当前类的短名首字母改小写。 由此注解衍生的三个注解： @Controller 一般用于表现的注解 @Service 一般用于业务层 @Repository 一般用于持久层 他们和@Component的作用及属性都是一模一样 细节：如果注解中有且只有一个属性要赋值时，且名称是value，value在赋值是可以不写。 1.3.2 用于注入数据的1234567891011121314151617181920212223242526相当于：&lt;property name&#x3D;&quot;&quot; ref&#x3D;&quot;&quot;&gt; &lt;property name&#x3D;&quot;&quot; value&#x3D;&quot;&quot;&gt;@Autowired 作用：自动按照类型注入。只要有唯一的类型匹配就能注入成功。 如果注入的bean在容器中类型不唯一时，它会把变量名称作为bean的id，在容器中查找，找到后也能注入成功。 如果没有找到一致的bean的id，则报错。 当我们使用注解注入时，set方法就不是必须的了。 @Qualifier 作用：在自动按照类型注入的基础之上，再按照bean的id注入。 它在给类成员注入数据时，不能独立使用。但是再给方法的形参注入数据时，可以独立使用。 属性： value：用于指定bean的id。 @Resource 作用：直接按照bean的id注入。 属性： name：用于指定bean的id。 以上三个注解都是用于注入其他bean类型的。用于注入基本类型和String类型需要使用Value@Value: 作用：用于注入基本类型和String类型数据。它可以借助Spring的el表达式读取properties文件中的配置。 属性： value：用于指定要注入的数据 1.3.3 用于改变作用范围的1234567相当于：&lt;bean id&#x3D;&quot;&quot; class&#x3D;&quot;&quot; scope&#x3D;&quot;&quot;&gt;@Scope 作用：用于改变bean的作用范围 属性： value：用于指定范围的取值。 取值和xml中scope属性的取值是一样的。singleton prototype request session globalsession 1.3.4 和生命周期相关的123456789相当于：&lt;bean id&#x3D;&quot;&quot; class&#x3D;&quot;&quot; init-method&#x3D;&quot;&quot; destroy-method&#x3D;&quot;&quot; &#x2F;&gt;@PostConstruct 作用： 用于指定初始化方法。@PreDestroy 作用： 用于指定销毁方法。 1.3.5 代码示例持久层 123456789101112131415161718192021222324/** * 客户的业务层实现类 * * @author wgy */@Service(\"customerService\")@Scope(\"singleton\")public class CustomerServiceImpl implements ICustomerService &#123; @Value(\"泰斯特\") private String name;// @Autowired// @Qualifier(\"customerDao1\") @Resource(name=\"customerDao\") private ICustomerDao customerDao = null; @Override public void saveCustomer() &#123; System.out.println(\"业务层调用持久层......\"+name); customerDao.saveCustomer(); &#125;&#125; 持久层 1234567891011121314/** * 模拟客户的持久层实现类 * * @author wgy */@Repository(\"customerDao\")public class CustomerDaoImpl implements ICustomerDao &#123; @Override public void saveCustomer() &#123; System.out.println(\"持久层保存了客户\"); &#125;&#125; 1.3.6 关于Spring注解和XML的选择问题1234注解的优势： 配置简单，维护方便（我们找到类，就相当于找到了对应的配置）。XML的优势： 修改时，不用改源码。不涉及重新编译和部署。 Spring管理Bean方式的比较： 1.4 Spring的纯注解配置1.4.1 待改造的问题我们发现，之所以我们现在离不开xml配置文件，是因为我们有一句很关键的配置： 1234&lt;!-- 告知spring在创建容器时要扫描的包。当配置了此标签之后，spring创建容器就会去指定的包及其子包下找对应的注解 标签是在一个context的名称空间里，所以必须先导入context名称空间--&gt;&lt;context:component-scan base-package&#x3D;&quot;com.wgy&quot;&#x2F;&gt; 如果他要也能用注解配置，那么我们就可以脱离xml文件了。 1.4.2 使用注解配置要扫描的包创建配置类： 1234567891011/** * 一个spring的配置类 * 它的作用就相当于bean.xml * * @author wgy */@Configuration//它就是把当前类看成是spring的配置类@ComponentScan(&#123;\"com.wgy\"&#125;)//配置要扫描的包public class SpringConfiguration &#123; &#125; 测试类获取容器: 123456789public static void main(String[] args) &#123; //1.获取容器：由于我们已经没有了xml文件，所以再用读取xml方式就不能用了。 //这时需要指定加载哪个类上的注解 ApplicationContext ac = new AnnotationConfigApplicationContext(SpringConfiguration.class); //2.根据id获取对象 ICustomerService cs = (ICustomerService) ac.getBean(\"customerService\"); cs.saveCustomer();&#125; 1.4.3 新注解说明1.4.3.1 @Configuration12345@Configuration 作用： 用于指定当前类是一个spring配置类，当创建容器时会从该类上加载注解。获取容器时需要使用AnnotationApplicationContext(有@Configuration注解的类.class)。 属性： value:用于指定配置类的字节码 1.4.3.2 @ComponentScan123456@ComponentScan 作用： 用于指定spring在初始化容器时要扫描的包。作用和在spring的xml配置文件中的： &lt;context:component-scan base-package&#x3D;&quot;com.wgy&quot;&#x2F;&gt;是一样的。 属性： basePackages：用于指定要扫描的包。和该注解中的value属性作用一样。 1.4.3.3 @PropertySource12345@PropertySource 作用： 用于加载.properties文件中的配置。例如我们配置数据源时，可以把连接数据库的信息写到properties配置文件中，就可以使用此注解指定properties配置文件的位置。 属性： value[]：用于指定properties文件位置。如果是在类路径下，需要写上classpath: 1.4.3.4 @Import12345@Import 作用： 用于导入其他配置类，在引入其他配置类时，可以不用再写@Configuration注解。当然，写上也没问题。 属性： value[]：用于指定其他配置类的字节码。 1.4.3.5 @Bean12345@Bean 作用： 该注解只能写在方法上，表明使用此方法创建一个对象，并且放入spring容器。它就相当于我们之前在xml配置中介绍的factory-bean和factory-method。 属性： name：给当前@Bean注解方法创建的对象指定一个名称(即bean的id）。 1.4.3.6 代码示例jdbcConfig.properties: 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/springjdbc.username=rootjdbc.password=root Jdbc的配置类: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * Jdbc的配置类 * * @author wgy */public class JdbcConfig &#123; @Value(\"$&#123;jdbc.driver&#125;\") private String driver; @Value(\"$&#123;jdbc.url&#125;\") private String url; @Value(\"$&#123;jdbc.username&#125;\") private String username; @Value(\"$&#123;jdbc.password&#125;\") private String password; @Bean(name = \"runner\")//它是把方法的返回值存入Spring容器中。该注解有一个属性，name：用于指定bean的id。当不指定时它有默认值，默认值是方法的名称。 public QueryRunner createQueryRunner(@Qualifier(\"ds1\") DataSource dataSource) &#123; return new QueryRunner(dataSource); &#125; @Bean(name = \"ds\") public DataSource createDataSource() &#123; try &#123; System.out.println(driver);//com.mysql.jdbc.Driver ComboPooledDataSource ds = new ComboPooledDataSource(); ds.setDriverClass(driver); ds.setJdbcUrl(url); ds.setUser(username); ds.setPassword(password); return ds; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; @Bean(name = \"ds1\") public DataSource createDataSource1() &#123; try &#123; System.out.println(url); ComboPooledDataSource ds = new ComboPooledDataSource(); ds.setDriverClass(driver); ds.setJdbcUrl(url); ds.setUser(username); ds.setPassword(password); return ds; &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 配置类: 123456789101112131415161718192021222324252627/** * 一个spring的配置类 * 它的作用就相当于bean.xml * * @author wgy */@Configuration//它就是把当前类看成是spring的配置类@ComponentScan(&#123;\"com.wgy\"&#125;)@Import(&#123;JdbcConfig.class&#125;)//导入其他配置类@PropertySource(&#123;\"classpath:config/jdbcConfig.properties\"&#125;)public class SpringConfiguration &#123; /** * Spring EL表达式失效问题： * 目前使用的版本是4.2.4,在spring4.3以前都需要提供一个占位符配置器： * PropertySourcesPlaceholderConfigurer * 而在spring4.3以后，则不需要提供。 * 提供的方式如下：（在SpringConfiguration或JdbcConfig中配置均可） * * @return */ @Bean public static PropertySourcesPlaceholderConfigurer createPropertySourcesPlaceholderConfigurer()&#123; return new PropertySourcesPlaceholderConfigurer(); &#125;&#125; 2. Spring整合Junit2.1 准备测试环境2.1.1 创建业务层接口实现类123456789101112/** * 客户的业务层接口 */public interface ICustomerService &#123; /** * 查询所有客户 * @return */ List&lt;Customer&gt; findAllCustomer();&#125; 1234567891011121314151617/** * 客户的业务层实现类 */public class CustomerServiceImpl implements ICustomerService &#123; private ICustomerDao customerDao; public void setCustomerDao(ICustomerDao customerDao) &#123; this.customerDao = customerDao; &#125; @Override public List&lt;Customer&gt; findAllCustomer() &#123; return customerDao.findAllCustomer(); &#125;&#125; 2.1.2 创建持久层接口实现类123456789101112/** * 客户的持久层接口 */public interface ICustomerDao &#123; /** * 查询所有客户 * @return */ List&lt;Customer&gt; findAllCustomer();&#125; 123456789101112/** * 客户的持久层实现类 */public class CustomerDaoImpl implements ICustomerDao &#123; @Override public List&lt;Customer&gt; findAllCustomer() &#123; System.out.println(\"查询了所有客户\"); return null; &#125;&#125; 2.1.3 导入junit的jar包 2.1.4 编写测试类123456789101112/** * 测试客户的业务层和持久层 */public class CustomerServiceTest &#123; private ICustomerService customerService; @Test public void testFindAll()&#123; customerService.findAllCustomer(); &#125;&#125; 2.2 使用xml配置步骤2.2.1 xml文件中的配置123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 把资源交给spring来管理 --&gt; &lt;bean id=\"customerDao\" class=\"com.wgy.dao.impl.CustomerDaoImpl\"&gt;&lt;/bean&gt; &lt;bean id=\"customerService\" class=\"com.wgy.service.impl.CustomerServiceImpl\"&gt; &lt;property name=\"customerDao\" ref=\"customerDao\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 2.2.2 拷贝整合junit的必备jar包 2.2.3 使用@RunWith注解替换原有运行器1234567891011@RunWith(SpringJUnit4ClassRunner.class)public class CustomerServiceTest &#123; private ICustomerService customerService; @Test public void testFindAll()&#123; customerService.findAllCustomer(); &#125; &#125; 2.2.4 使用@ContextConfiguration指定spring配置文件的位置123456789101112@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations=&#123;\"classpath:bean.xml\"&#125;)public class CustomerServiceTest &#123; private ICustomerService customerService; @Test public void testFindAll()&#123; customerService.findAllCustomer(); &#125; &#125; 2.2.5 使用@Autowired给测试类中的变量注入数据12345678910111213@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations=&#123;\"classpath:bean.xml\"&#125;)public class CustomerServiceTest &#123; @Autowired private ICustomerService customerService; @Test public void testFindAll()&#123; customerService.findAllCustomer(); &#125;&#125; 2.3 使用纯注解配置步骤2.3.1 拷贝整合junit的必备jar 2.3.2 把资源都用注解管理123456789101112131415/** * 客户的业务层实现类 */@Service(\"customerService\")public class CustomerServiceImpl implements ICustomerService &#123; @Autowired private ICustomerDao customerDao; @Override public List&lt;Customer&gt; findAllCustomer() &#123; return customerDao.findAllCustomer(); &#125;&#125; 12345678910111213/** * 客户的持久层实现类 */@Repository(\"customerDao\")public class CustomerDaoImpl implements ICustomerDao &#123; @Override public List&lt;Customer&gt; findAllCustomer() &#123; System.out.println(\"查询了所有客户\"); return null; &#125;&#125; 2.3.3 使用注解配置方式创建Spring容器12345678910111213@Configuration@ComponentScan(basePackages=&#123;\"com.wgy\"&#125;)public class CustomerServiceTest &#123; @Autowired private ICustomerService customerService; @Test public void testFindAll()&#123; customerService.findAllCustomer(); &#125;&#125; 2.3.4 使用RunWith注解和ContextConfiguration注解配置123456789101112131415@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes=&#123;CustomerServiceTest.class&#125;)@Configuration@ComponentScan(basePackages=&#123;\"com.wgy\"&#125;)public class CustomerServiceTest &#123; @Autowired private ICustomerService customerService; @Test public void testFindAll()&#123; customerService.findAllCustomer(); &#125; &#125; 2.4 为什么不把测试类配到xml中 当我们在xml中配置了一个bean，Spring加载配置文件创建容器时，就会创建对象。 测试类只是我们在测试功能时使用，而在项目中它并不参与程序逻辑，也不会解决需求上的问题，所以创建完了，并没有使用。那么存在容器中就会造成资源的浪费。","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Spring","slug":"Spring","permalink":"https://wgy1993.gitee.io/tags/Spring/"}]},{"title":"Spring(一)","date":"2020-06-10T15:26:00.000Z","path":"archives/b9cbcaed.html","text":"1. Spring概述1.1 Spring概述1.1.1 Spring介绍Spring是分层的Java SE/EE应用 full-stack轻量级开源框架，以IoC（Inverse Of Control：反转控制）和AOP（Aspect Oriented Programming：面向切面编程）为内核，提供了展现层Spring MVC和持久层Spring JDBC以及业务层事务管理等众多的企业级应用技术，还能整合开源世界众多著名的第三方框架和类库，逐渐成为使用最多的Java EE企业应用开源框架。 1.1.2 Spring的发展历程1997年IBM提出了EJB的思想 1998年，SUN制定开发标准规范EJB1.0 1999年，EJB1.1发布 2001年，EJB2.0发布 2003年，EJB2.1发布 2006年，EJB3.0发布 Rod Johnson（spring之父） ​ Expert One-to-One J2EE Design and Development(2002) ​ 阐述了J2EE使用EJB开发设计的优点及解决方案 ​ Expert One-to-One J2EE Development without EJB(2004) ​ 阐述了J2EE开发不使用EJB的解决方式（Spring雏形） 1.1.3 Spring的优势方便解耦，简化开发 通过Spring提供的IoC容器，可以将对象间的依赖关系交由Spring进行控制，避免硬编码所造成的过度程序耦合。用户也不必再为单例模式类、属性文件解析等这些很底层的需求编写代码，可以更专注于上层的应用。 AOP编程的支持 通过Spring的AOP功能，方便进行面向切面的编程，许多不容易用传统OOP实现的功能可以通过AOP轻松应付。 声明式事务的支持 可以将我们从单调烦闷的事务管理代码中解脱出来，通过声明式方式灵活的进行事务的管理，提高开发效率和质量。 方便程序的测试 可以用非容器依赖的编程方式进行几乎所有的测试工作，测试不再是昂贵的操作，而是随手可做的事情。 方便集成各种优秀框架 Spring可以降低各种框架的使用难度，提供了对各种优秀框架（Struts、Hibernate、Hessian、Quartz等）的直接支持。 降低JavaEE API的使用难度 Spring对JavaEE API（如JDBC、JavaMail、远程调用等）进行了薄薄的封装层，使这些API的使用难度大为降低。 Java源码是经典学习范例 Spring的源代码设计精妙、结构清晰、匠心独用，处处体现着大师对Java设计模式灵活运用以及对Java技术的高深造诣。它的源代码无意是Java技术的最佳实践的范例。 1.1.4 Spring的体系结构 1.2 程序的耦合和解耦1.2.1 什么是程序的耦合我们在开发中，会写很多的类，而有些类之间不可避免的产生依赖关系，这种依赖关系称之为耦合。 有些依赖关系是必须的，有些依赖关系可以通过优化代码来解除的。请看下面的示例代码： 1234567/** * 客户的业务层实现类 */public class CustomerServiceImpl implements ICustomerService &#123; private ICustomerDao customerDao = new CustomerDaoImpl(); &#125; 上面的代码表示：业务层调用持久层，并且此时业务层在依赖持久层的接口和实现类。如果此时没有持久层实现类，编译将不能通过。这种依赖关系就是我们可以通过优化代码解决的。 再比如： 下面的代码中，我们的类依赖了MySQL的具体驱动类，如果这时候更换了数据库品牌，我们需要改源码来修改数据库驱动。这显然不是我们想要的。 12345678910111213141516171819202122232425public class JdbcDemo1 &#123; /** * JDBC操作数据库的基本入门中存在什么问题？ * 导致驱动注册两次是个问题，但不是严重的。 * 严重的问题：是当前类和mysql的驱动类有很强的依赖关系。 * 当我们没有驱动类的时候，连编译都不让。 * 那这种依赖关系，就叫做程序的耦合 * * 我们在开发中，理想的状态应该是： * 我们应该尽力达到的：编译时不依赖，运行时才依赖。 * * @param args * @throws Exception */ public static void main(String[] args) throws Exception &#123; //1.注册驱动 DriverManager.registerDriver(new com.mysql.jdbc.Driver()); //Class.forName(\"com.mysql.jdbc.Driver\"); //2.获取连接 //3.获取预处理sql语句对象 //4.获取结果集 //5.遍历结果集 &#125;&#125; 1.2.2 解决程序耦合的思路当是我们讲解jdbc时，是通过反射来注册驱动的，代码如下： 1Class.forName(\"com.mysql.jdbc.Driver\"); 这时的好处是，我们的类中不再依赖具体的驱动类，此时就算删除mysql的驱动jar包，依然可以编译。但是因为没有驱动类，所以不能运行。 不过，此处也有个问题，就是我们反射类对象的全限定类名字符串是在java类中写死的，一旦要改还是要修改源码。 解决这个问题也很简单，使用配置文件配置。 1.2.3 工厂模式解耦在实际开发中我们可以把所有的dao和service和action对象使用配置文件配置起来，当启动服务器应用加载的时候，通过读取配置文件，把这些对象创建出来并存起来。在接下来的使用的时候，直接拿过来用就好了。 1.2.4 控制反转-Inversion Of Control上面解耦的思路有2个问题： 1、存哪去？ 分析：由于我们是很多对象，肯定要找个集合来存。这时候有Map和List供选择。 ​ 到底选Map还是List就看我们有没有查找需求。有查找需求，选Map。 所以我们的答案就是： ​ 在应用加载时，创建一个Map，用于存放action，Service和dao对象。 ​ 我们把这个map称之为容器。 2、还是没解释什么是工厂？ 工厂就是负责给我们从容器中获取指定对象的类。这时候我们获取对象的方式发生了改变。 原来： ​ 我们在获取对象时，都是采用new的方式。是主动的。 现在： ​ 我们获取对象时，同时跟工厂要，有工厂为我们查找或者创建对象。是被动的。 这种被动接收的方式获取对象的思想就是控制反转，它是Spring框架的核心之一。 它的作用只有一个：削减计算机程序的耦合。 控制反转（Inversion of Control，缩写为IoC），是面向对象编程中的一种设计原则，可以用来减低计算机代码之间的耦合度。其中最常见的方式叫做依赖注入（Dependency Injection，简称DI），还有一种方式叫“依赖查找”（Dependency Lookup）。通过控制反转，对象在被创建的时候，由一个调控系统内所有对象的外界实体将其所依赖的对象的引用传递给它。也可以说，依赖被注入到对象中。 2. 使用Spring的IOC解决程序耦合2.1 案例的前期准备本章我们使用的案例是，客户的业务层和持久层的依赖关系解决。在开始Spring的配置之前，我们要先准备一下环境。由于我们是使用Spring解决依赖关系，并不是真正的要做增伤改查操作，所以此时我们没必要写实体类。并且我们在此处使用的是java工程，不是java web工程。 2.1.1 准备Spring的开发包1234567官网：http:&#x2F;&#x2F;spring.io&#x2F; 下载地址：http:&#x2F;&#x2F;repo.springsource.org&#x2F;libs-release-local&#x2F;org&#x2F;springframework&#x2F;spring解压:(Spring目录结构:) * docs :API和开发规范. * libs :jar包和源码. * schema :约束. 2.1.2 创建业务层接口和实现类123456789101112/** * 模拟：客户的业务层接口 * * @author wgy */public interface ICustomerService &#123; /** * 保存客户 */ void saveCustomer();&#125; 12345678910111213141516/** * 客户的业务层实现类 * * @author wgy */public class CustomerServiceImpl implements ICustomerService &#123; private ICustomerDao customerDao = new CustomerDaoImpl();//此处有依赖关系 @Override public void saveCustomer() &#123; System.out.println(\"业务层调用持久层\"); customerDao.saveCustomer(); &#125;&#125; 2.1.3 创建持久层接口和实现类12345678910111213/** * 模拟一个客户dao * * @author wgy */public interface ICustomerDao &#123; /** * 保存客户 */ void saveCustomer();&#125; 12345678910111213/** * 模拟客户的持久层实现类 * * @author wgy */public class CustomerDaoImpl implements ICustomerDao &#123; @Override public void saveCustomer() &#123; System.out.println(\"持久层保存了客户\"); &#125;&#125; 2.2 基于XML的配置（入门案例）2.2.1 拷贝必备的jar包 2.2.2 在类根路径下创建任意名称的xml文件12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!-- 导入schema约束约束的位置在: ..\\spring-framework-4.2.4.RELEASE\\docs\\spring-framework-reference\\html\\xsd-configuration.html文件中。--&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt;&lt;/beans&gt; 2.2.3 把资源交给spring来管理12345678910&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- 配置资源：把对象的创建交给spring来管理 --&gt; &lt;bean id=\"customerService\" class=\"com.wgy.service.impl.CustomerServiceImpl\"/&gt; &lt;bean id=\"customerDao\" class=\"com.wgy.dao.impl.CustomerDaoImpl\"/&gt;&lt;/beans&gt; 2.2.4 测试配置是否成功123456789101112131415161718192021/** * spring的入门案例 * * @author wgy */public class Client &#123; /** * ClassPahtXmlApplicationContext：它是只能加载类路径下的配置文件 我们用这个 * FileSystemXmlApplicationContext：它是可以加载磁盘任意位置的配置文件 * * @param args */ public static void main(String[] args) &#123; //1.获取容器 ApplicationContext ac = new ClassPathXmlApplicationContext(\"bean.xml\"); //2.根据bean的id获取对象 ICustomerService cs = (ICustomerService) ac.getBean(\"customerService\"); cs.saveCustomer(); &#125;&#125; 2.3 Spring基于XML的IOC细节2.3.1 Spring中工厂的类结构图 2.3.1.1 Bean创建的两种规则1234BeanFactory: 提供的是一种延迟加载思想来创建bean对象。bean对象什么时候用什么时候创建ApplicationContext 提供的是一种立即加载思想来创建bean对象。只要一解析完配置文件，就立马创建bean对象。 123456789public static void main(String[] args) &#123; //1.获取容器 Resource resource = new ClassPathResource(\"bean.xml\"); BeanFactory factory = new XmlBeanFactory(resource); //2.根据bean的id获取对象 ICustomerService cs = (ICustomerService) factory.getBean(\"customerService\"); cs.saveCustomer();&#125; 2.3.2 IOC中bean标签和管理对象细节2.3.2.1 bean标签12345678910111213作用： 用于配置对象让Spring来创建的。 默认情况下它调用的是类中的无参构造函数。如果没有无参构造函数则不能创建成功。属性： id：给对象在容器中提供一个唯一标识。用于获取对象。 class：指定类的全限定类名。用于反射创建对象。默认情况下调用无参构造函数。 scope：指定对象的作用范围。 init-method：指定类中的初始化方法名称。 destroy-method：指定类中销毁方法名称。 &lt;bean id&#x3D;&quot;customerService&quot; class&#x3D;&quot;com.wgy.service.impl.CustomerServiceImpl&quot; scope&#x3D;&quot;singleton&quot; init-method&#x3D;&quot;init&quot; destroy-method&#x3D;&quot;destroy&quot;&gt;&lt;&#x2F;bean&gt; 12345678910111213141516171819202122232425/** * 客户的业务层实现类 * * @author wgy */public class CustomerServiceImpl implements ICustomerService &#123; public CustomerServiceImpl() &#123; System.out.println(\"bean对象创建了\"); &#125; public void init()&#123; System.out.println(\"对象初始化了\"); &#125; public void destroy()&#123; System.out.println(\"对象销毁了\"); &#125; @Override public void saveCustomer() &#123; System.out.println(\"业务层调用持久层\"); &#125;&#125; 2.3.2.2 Bean的作用范围123456789Bean的作用范围： 它是可以通过配置的方式来调整作用范围。 配置的属性：bean标签的scope属性。 属性的取值： singleton：单例的（默认值） prototype：多例的（当我们让spring接管struts2的action创建时，action必须配置此值） request：作用范围是一次请求，和当前请求的转发。 session：作用范围是一次会话。 globalsession ：作用范围是一次全局会话。 2.3.2.3 Bean的生命周期123456789101112Bean的生命周期： 涉及bean标签的两个属性： init-method destroy-method 单例： 出生：容器创建，对象就出生了。 活着：只要容器在，对象就一直存在。 死亡：容器销毁，对象消亡。 多例： 出生：每次使用时，创建对象 活着：只要对象在使用中，就一直活着 死亡：当对象长时间不使用，并且也没有别的对象引用时，由java的垃圾回收器回收。 2.3.2.4 Bean的三种创建方式第一种方式：调用默认无参构造函数创建 此种方式用的最多 1234&lt;!--在默认情况下： 它会根据默认无参构造函数来创建类对象。如果bean中没有默认无参构造函数，则创建失败，会报异常 --&gt;&lt;bean id=\"customerService\" class=\"com.wgy.service.impl.CustomerServiceImpl\"/&gt; 第二种方式：使用静态工厂中的方法创建对象 1234567891011/** * 模拟一个静态工厂 * * @author wgy */public class StaticFactory &#123; public static ICustomerService getCustomerService() &#123; return new CustomerServiceImpl(); &#125;&#125; 123456789&lt;!-- 此种方式是: 使用StaticFactory类中的静态方法getCustomerService创建对象，并存入Spring容器 id属性：指定bean的id，用于从容器中获取 class属性：指定静态工厂的全限定类名 factory-method属性：指定生产对象的静态方法 --&gt;&lt;bean id=\"staticCustomerService\" class=\"com.wgy.factory.StaticFactory\" factory-method=\"getCustomerService\"&gt;&lt;/bean&gt; 第三种方式：使用实例工厂中的方法创建 1234567891011/** * 模拟一个实例工厂 * * @author wgy */public class InstanceFactory &#123; public ICustomerService getCustomerService() &#123; return new CustomerServiceImpl(); &#125;&#125; 12345678910&lt;!-- 此种方式是： 先把工厂的创建交给Spring来管理。 然后在使用工厂的bean来调用里面的方法 factory-bean属性：用于指定实例工厂bean的id。 factory-method属性：用于指定实例工厂中创建对象的方法。--&gt;&lt;bean id=\"instancFactory\" class=\"com.wgy.factory.InstanceFactory\"&gt;&lt;/bean&gt;&lt;bean id=\"instanceCustomerService\" factory-bean=\"instancFactory\" factory-method=\"getCustomerService\"&gt;&lt;/bean&gt; 2.3.3 Spring的依赖注入它是spring框架核心ioc的具体实现方式。简单的说，就是坐等框架把对象传入，而不用我们自己去获取。 123456789Spring的依赖注入： 注入的方式有3三种： 第一种：使用构造函数注入 第二种：使用set方法注入 第三种：使用注解注入 注入的数据类型有3类： 第一类：基本类型和String类型 第二类：其他bean类型（必须是在spring的配置文件中出现过的bean） 第三类：复杂类型（集合类型） 2.3.3.1 构造函数注入就是使用类中的构造函数，给成员变量赋值。注意，赋值的操作不是我们自己做的，而是通过配置的方式，让Spring框架来为我们注入。具体代码如下： 123456789101112131415161718192021222324/** * 客户的业务层实现类 * * @author wgy */public class CustomerServiceImpl implements ICustomerService &#123; private String driver; private Integer port; private Date today; //以上三个类成员，没有具体的实际意义，只是用于演示注入。 public CustomerServiceImpl(String driver, Integer port, Date today) &#123; this.driver = driver; this.port = port; this.today = today; &#125; @Override public void saveCustomer() &#123; System.out.println(\"业务层调用持久层\"+driver+\",\"+port+\",\"+today); &#125;&#125; 12345678910111213141516171819&lt;!-- 构造函数注入： 涉及的标签：constructor-arg 标签的属性： type:指定参数的类型。 index:指定参数的索引位置，从0开始。 name:指定参数的名称。 一般用它 ========上面三个属性是指定给哪个参数赋值的，下面两个属性是指定赋什么值的============== value:指定基本数据类型或String类型的数据 ref:指定其他bean类型数据 标签出现的位置： 写在bean标签内部 --&gt;&lt;bean id=\"customerService\" class=\"com.wgy.service.impl.CustomerServiceImpl\"&gt; &lt;constructor-arg name=\"driver\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=\"port\" value=\"3306\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg name=\"today\" ref=\"now\"&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;bean id=\"now\" class=\"java.util.Date\"&gt;&lt;/bean&gt; 2.3.3.2 set方法注入就是在类中提供需要注入成员的set方法。具体代码如下： 12345678910111213141516171819202122232425262728293031/** * 客户的业务层实现类 * * @author wgy */public class CustomerServiceImpl2 implements ICustomerService &#123; private String driver; private Integer port; private Date today; //以上三个类成员，没有具体的实际意义，只是用于演示注入。 public void setDriver(String driver) &#123; this.driver = driver; &#125; public void setPort(Integer port) &#123; this.port = port; &#125; public void setToday(Date today) &#123; this.today = today; &#125; @Override public void saveCustomer() &#123; System.out.println(\"业务层调用持久层\" + driver + \",\" + port + \",\" + today); &#125;&#125; 1234567891011121314151617&lt;!-- set方法注入 涉及的标签：property 标签的属性： name:指定参数的set方法名称。 =========上面三个属性是指定给哪个参数赋值的，下面两个属性是指定赋什么值的============ value:指定基本数据类型或String类型的数据 ref:指定其他bean类型数据 标签出现的位置： 写在bean标签内部 --&gt;&lt;bean id=\"customerService2\" class=\"com.wgy.service.impl.CustomerServiceImpl2\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"port\" value=\"3307\"&gt;&lt;/property&gt; &lt;property name=\"today\" ref=\"now\"&gt;&lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"now\" class=\"java.util.Date\"&gt;&lt;/bean&gt; 2.3.3.3 复杂类型的注入就是给类中的集合成员传值，它用的也是set方法注入的方式，只不过变量的数据类型都是集合。我们这里介绍注入数组，List,Set,Map,Properties。具体代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 客户的业务层实现类 * * @author wgy */public class CustomerServiceImpl3 implements ICustomerService &#123; private String[] myStrs; private List&lt;String&gt; myList; private Set&lt;String&gt; mySet; private Map&lt;String, String&gt; myMap; private Properties myProps; public void setMyStrs(String[] myStrs) &#123; this.myStrs = myStrs; &#125; public void setMyList(List&lt;String&gt; myList) &#123; this.myList = myList; &#125; public void setMySet(Set&lt;String&gt; mySet) &#123; this.mySet = mySet; &#125; public void setMyMap(Map&lt;String, String&gt; myMap) &#123; this.myMap = myMap; &#125; public void setMyProps(Properties myProps) &#123; this.myProps = myProps; &#125; @Override public void saveCustomer() &#123; System.out.println(Arrays.toString(myStrs)); System.out.println(myList); System.out.println(mySet); System.out.println(myMap); System.out.println(myProps); &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!-- 复杂类型的注入 结构相同，标签可以互换 List结构的： array,list,set Map结构的 map,entry,props,prop --&gt;&lt;bean id=\"customerService3\" class=\"com.wgy.service.impl.CustomerServiceImpl3\"&gt; &lt;property name=\"myStrs\"&gt; &lt;set&gt; &lt;value&gt;AAA&lt;/value&gt; &lt;value&gt;BBB&lt;/value&gt; &lt;value&gt;CCC&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=\"myList\"&gt; &lt;array&gt; &lt;value&gt;AAA&lt;/value&gt; &lt;value&gt;BBB&lt;/value&gt; &lt;value&gt;CCC&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name=\"mySet\"&gt; &lt;list&gt; &lt;value&gt;AAA&lt;/value&gt; &lt;value&gt;BBB&lt;/value&gt; &lt;value&gt;CCC&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=\"myMap\"&gt; &lt;props&gt; &lt;prop key=\"testF\"&gt;FFF&lt;/prop&gt; &lt;prop key=\"testG\"&gt;GGG&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name=\"myProps\"&gt; &lt;map&gt; &lt;entry key=\"testD\" value=\"DDD\"&gt;&lt;/entry&gt; &lt;entry key=\"testE\"&gt; &lt;value&gt;EEE&lt;/value&gt; &lt;/entry&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Spring","slug":"Spring","permalink":"https://wgy1993.gitee.io/tags/Spring/"}]},{"title":"Struts2(四)","date":"2020-06-08T07:27:44.000Z","path":"archives/bb3a06c4.html","text":"1. Struts2中的拦截器1.1 Struts2的拦截器基本概念1.1.1 拦截器概述在Webwork的中文文档的解释为——拦截器是动态拦截Action调用的对象。它提供了一种机制可以使开发者在定义的action执行的前后加入执行的代码，也可以在一个action执行前阻止其执行。也就是说它提供了一种可以提取action中可重用代码，统一管理和执行的方式。 拦截器链 （Interceptor Chain，在Struts 2中称为拦截器栈Interceptor Stack）。拦截器链就是将拦截器按一定的顺序联结成一条链。在访问被拦截的方法或字段时，拦截器链中的拦截器就会按其之前定义的顺序被调用。 拦截器和过滤器是有几分相似，但是也有区别： 过滤器是servlet规范中的一部分，任何java web工程都可以使用。 拦截器是struts2框架自己的，只有使用了struts2框架的工程才能用。 过滤器在url-pattern中配置了/*之后，可以对所有要访问的资源拦截。 拦截器它是只有进入struts2核心内部之后，才会起作用，如果访问的是jsp，html,css,image或者js是不会进行拦截的。 同时，拦截器还是AOP编程思想的具体体现形式。AOP（Aspect-Oriented Programming）简单的说就是： 在不修改源码的基础上，已有的方法进行动态增强。 在struts2中，拦截器它就是对我们的动作方法进行增强。（其实就是把重复性的代码提取出来，然后放到拦截器中，统一管理，统一调用） 1.1.2 拦截器的执行时机在访问struts2核心内部时，在动作方法执行之前先正序执行，然后执行动作方法，执行完动作方法和结果视图之后，再倒序执行。所以它是先进后出，是个栈的结构。具体可参考下图： 1.2 自定义拦截器直接或间接的实现Interceptor接口 12345public interface Interceptor extends Serializable &#123; void init(); void destroy(); String intercept(ActionInvocation invocation) throws Exception;&#125; 该接口提供了三个方法，其具体介绍如下。 void init()：该方法在拦截器被创建后会立即被调用, 它在拦截器的生命周期内只被调用一次. 可以在该方法中对相关资源进行必要的初始化。 void destroy()：该方法与init方法相对应，在拦截器实例被销毁之前，将调用该方法来释放和拦截器相关的资源。它在拦截器的生命周期内，也只被调用一次。 String intercept(ActionInvocation invocation) throws Exception：该方法是拦截器的核心方法，用来添加真正执行拦截工作的代码，实现具体的拦截操作。它返回一个字符串作为逻辑视图，系统根据返回的字符串跳转到对应的视图资源。每拦截一个动作请求, 该方法就会被调用一次。该方法的ActionInvocation参数包含了被拦截的Action的引用，可以通过该参数的invoke()方法，将控制权转给下一个拦截器或者转给Action的execute()方法。 继承抽象拦截器类AbstractIntercepter 12345public abstract class AbstractInterceptor implements Interceptor &#123; public void init() &#123;&#125; public void destroy() &#123;&#125; public abstract String intercept(ActionInvocation invocation) throws Exception;&#125; AbstractInterceptor有一个子类MethodFilterInterceptor，该类中提供了两个属性，可以告知拦截器对哪些方法进行拦截或者对哪些方法排除。 1.2.1 第一步：编写普通java类，继承AbstractInterceptor123456789101112131415161718192021222324252627282930313233/** * 自定义拦截器 * 步骤： * 第一步：编写一个普通类，继承AbstractInterceptor（也可以实现Interceptor接口） * 第二步：配置拦截器 * &lt;!-- 声明一个拦截器 --&gt; * &lt;interceptors&gt; * &lt;interceptor name=\"myInterceptor\" class=\"com.itheima.web.interceptors.MyInterceptor\"&gt;&lt;/interceptor&gt; * &lt;/interceptors&gt; * &lt;!-- 引用拦截器：写在action标签的内部。当我们写了自己的拦截器引用时，默认的拦截器栈就失效了 --&gt; * &lt;interceptor-ref name=\"myInterceptor\"&gt;&lt;/interceptor-ref&gt; * 拦截器的放行： * invocation.invoke(); * 返回值的内容是： * 动作方法的返回值 * 关于结果视图的执行时机以及拦截器的返回值问题 * 在放行之前，拦截器的返回值可以控制显示哪个结果视图。一旦放行之后，它一定会显示动作方法返回值所匹配的结果视图，此时已经不管拦截器返回什么内容。 * 多个拦截器的执行顺序问题： * 是由引用顺序决定的，与声明顺序无关 * * @author wgy */public class MyInterceptor extends AbstractInterceptor &#123; @Override public String intercept(ActionInvocation invocation) throws Exception &#123; System.out.println(\"访问Action之前：MyInterceptor拦截了。。。。\"); //放行 String rtValue = invocation.invoke(); System.out.println(\"访问Action之后：MyInterceptor拦截了。。。。\"); return rtValue; &#125;&#125; 1.2.2 第二步：在struts.xml中配置拦截器1234567891011121314151617181920&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE struts PUBLIC &quot;-&#x2F;&#x2F;Apache Software Foundation&#x2F;&#x2F;DTD Struts Configuration 2.3&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;struts.apache.org&#x2F;dtds&#x2F;struts-2.3.dtd&quot;&gt;&lt;struts&gt; &lt;constant name&#x3D;&quot;struts.devMode&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;package name&#x3D;&quot;p1&quot; extends&#x3D;&quot;struts-default&quot;&gt; &lt;!-- 声明一个拦截器 --&gt; &lt;interceptors&gt; &lt;interceptor name&#x3D;&quot;myIntercepter&quot; class&#x3D;&quot;com.wgy.web.interceptors.MyInterceptor&quot;&#x2F;&gt; &lt;&#x2F;interceptors&gt; &lt;action name&#x3D;&quot;demo1&quot; class&#x3D;&quot;com.wgy.web.action.Demo1Action&quot; method&#x3D;&quot;demo1&quot;&gt; &lt;!-- 引用拦截器：当我们写了自己的拦截器引用时，默认的拦截器栈就失效了 --&gt; &lt;interceptor-ref name&#x3D;&quot;myIntercepter&quot;&#x2F;&gt; &lt;result&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;&#x2F;package&gt;&lt;&#x2F;struts&gt; 1.3 案例-检查登录拦截器1.3.1 定义拦截器123456789101112131415161718/** * 检查登录拦截器 * * @author wgy */public class CheckLoginInterceptor2 extends MethodFilterInterceptor &#123; @Override protected String doIntercept(ActionInvocation invocation) throws Exception &#123; Object obj = ServletActionContext.getRequest().getSession().getAttribute(\"userinfo\"); if(obj == null)&#123; //没登录，去登录 return \"login\"; &#125; //已登录，就放行 return invocation.invoke(); &#125;&#125; 1.3.2 配置拦截器12345678910111213141516171819&lt;package name&#x3D;&quot;myDefault&quot; extends&#x3D;&quot;struts-default&quot; abstract&#x3D;&quot;true&quot;&gt; &lt;!-- 声明拦截器 --&gt; &lt;interceptors&gt; &lt;interceptor name&#x3D;&quot;checkLogin&quot; class&#x3D;&quot;com.wgy.web.interceptors.CheckLoginInterceptor2&quot;&#x2F;&gt; &lt;!-- 定义一个自己的拦截器栈 --&gt; &lt;interceptor-stack name&#x3D;&quot;myDefaultStack&quot;&gt; &lt;interceptor-ref name&#x3D;&quot;checkLogin&quot;&gt; &lt;!-- 告知拦截器，哪些方法需要拦截，哪些方法不需要拦截 --&gt; &lt;param name&#x3D;&quot;excludeMethods&quot;&gt;userLogin&lt;&#x2F;param&gt; &lt;&#x2F;interceptor-ref&gt; &lt;interceptor-ref name&#x3D;&quot;defaultStack&quot;&#x2F;&gt; &lt;&#x2F;interceptor-stack&gt; &lt;&#x2F;interceptors&gt; &lt;!-- 把我们自定义的拦截器栈声明为默认拦截器栈 --&gt; &lt;default-interceptor-ref name&#x3D;&quot;myDefaultStack&quot;&#x2F;&gt; &lt;global-results&gt; &lt;result name&#x3D;&quot;login&quot;&gt;&#x2F;login.jsp&lt;&#x2F;result&gt; &lt;&#x2F;global-results&gt;&lt;&#x2F;package&gt; 1.3.3 编写和配置Action动作类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class BBSAction extends ActionSupport implements ServletRequestAware &#123; private HttpServletRequest request; @Override public void setServletRequest(HttpServletRequest request) &#123; this.request = request; &#125; /** * 娱乐 * @return */ public String demo1()&#123; System.out.println(request); return SUCCESS; &#125; /** * 体育 * @return */ public String demo2()&#123; System.out.println(request); return SUCCESS; &#125; /** * 军事 * @return */ public String demo3()&#123; System.out.println(request); return SUCCESS; &#125; /** * 登录方法 * @return */ public String userLogin()&#123; //往session域中存入一个登录标记 ServletActionContext.getRequest().getSession().setAttribute(\"userinfo\", \"\"); return SUCCESS; &#125;&#125; 配置 1234567891011121314151617&lt;package name&#x3D;&quot;p1&quot; extends&#x3D;&quot;myDefault&quot;&gt; &lt;action name&#x3D;&quot;demo1&quot; class&#x3D;&quot;com.wgy.web.action.BBSAction&quot; method&#x3D;&quot;demo1&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;demo1.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;action name&#x3D;&quot;demo2&quot; class&#x3D;&quot;com.wgy.web.action.BBSAction&quot; method&#x3D;&quot;demo2&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;demo2.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;action name&#x3D;&quot;demo3&quot; class&#x3D;&quot;com.wgy.web.action.BBSAction&quot; method&#x3D;&quot;demo3&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;demo3.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;action name&#x3D;&quot;login&quot; class&#x3D;&quot;com.wgy.web.action.BBSAction&quot; method&#x3D;&quot;userLogin&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;redirect&quot;&gt;&#x2F;main.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt;&lt;&#x2F;package&gt; 2. Struts2的注解配置2.1 使用前提Struts2框架，它不仅支持基于XML的配置方式，同时也支持基于注解配置的方式。 注解和XML的配置，都是告知struts2框架，当我们jsp页面发送请求，根据配置执行对应动作类的方法，并根据返回值，前往指定的结果视图（jsp页面或者其他动作）。它们只是配置的形式不一样。 其次要想使用struts2的注解，必须要导入一个新的jar包。该jar包是： struts2-convention-plugin-2.3.24.jar 2.2 常用注解2.2.1 @NameSpace1234567891011121314151617出现的位置： 它只能出现在package上或者Action类上。一般情况下都是写在Action类上。作用： 指定当前Action中所有动作方法的名称空间。属性： value：指定名称空间的名称。写法和xml配置时一致。不指定的话，默认名称空间是&quot;&quot;。示例：@Namespace(&quot;&#x2F;customer&quot;)public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private Customer customer &#x3D; new Customer(); @Override public Customer getModel() &#123; return customer; &#125;&#125; 2.2.2 @ParentPackage1234567891011121314151617出现的位置： 它只能出现在package上或者Action类上。一般情况下都是写在Action类上。作用： 指定当前动作类所在包的父包。由于我们已经是在类中配置了，所以无需在指定包名了。属性： value：指定父包的名称。示例：@ParentPackage(&quot;struts-default&quot;)public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private Customer customer &#x3D; new Customer(); @Override public Customer getModel() &#123; return customer; &#125;&#125; 2.2.3 @Action12345678910111213141516171819出现的位置： 它只能出现在Action类上或者动作方法上。一般情况下都是写在动作方法上。作用： 指定当前动作方法的动作名称。也就是xml配置时action标签的name属性。属性： value：指定动作名称。 results[]：它是一个数组，数据类型是注解。用于指定结果视图。此属性可以没有，当没有该属性时，表示不返回任何结果视图。即使用response输出响应正文。 interceptorRefs[]：它是一个数组，数据类型是注解。用于指定引用的拦截器。示例：&#x2F;** * 获取添加客户页面 * @return *&#x2F;@Action(value&#x3D;&quot;addUICustomer&quot;,results&#x3D;&#123; @Result(name&#x3D;&quot;addUICustomer&quot;,location&#x3D;&quot;&#x2F;jsp&#x2F;customer&#x2F;add.jsp&quot;)&#125;)public String addUICustomer()&#123; return &quot;addUICustomer&quot;;&#125; 2.2.4 @Result123456789101112131415161718192021出现的位置： 它可以出现在动作类上，也可以出现在Action注解中。作用： 出现在类上，表示当前动作类中的所有动作方法都可以用此视图。 出现在Action注解中，表示当前Action可用此视图。属性： name：指定逻辑结果视图名称。 type：指定前往视图的方式。例如：请求转发，重定向，重定向到另外的动作。 location：指定前往的地址。可以是一个页面，也可以是一个动作。示例：&#x2F;** * 保存客户 * @return *&#x2F;@Action(value&#x3D;&quot;addCustomer&quot;,results&#x3D;&#123; @Result(name&#x3D;&quot;addCustomer&quot;,type&#x3D;&quot;redirect&quot;,location&#x3D;&quot;&#x2F;jsp&#x2F;success.jsp&quot;)&#125;)public String addCustomer()&#123; customerService.saveCustomer(customer); return &quot;addCustomer&quot;;&#125; 2.2.5 @Results1234567891011121314151617181920出现的位置： 它可以出现在动作类上，也可以出现在Action注解中。作用： 用于配置多个结果视图。属性： value：它是一个数组，数据类型是result注解。示例：@Results(&#123; @Result(name&#x3D;&quot;login&quot;,location&#x3D;&quot;&#x2F;login.jsp&quot;), @Result(name&#x3D;&quot;error&quot;,location&#x3D;&quot;&#x2F;error.jsp&quot;)&#125;)public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private Customer customer &#x3D; new Customer(); @Override public Customer getModel() &#123; return customer; &#125;&#125; 2.2.6 @InterceptorRef123456789101112131415161718192021222324252627282930313233343536出现的位置： 它可以出现在动作类上或者Action注解中。作用： 用于配置要引用的拦截器或者拦截器栈属性： value：用于指定拦截器或者拦截器栈示例：出现在动作方法上：&#x2F;** * 查询所有客户 * @return *&#x2F;@Action(value&#x3D;&quot;findAllCustomer&quot;, results&#x3D;&#123; @Result(name&#x3D;&quot;findAllCustomer&quot;,location&#x3D;&quot;&#x2F;jsp&#x2F;customer&#x2F;list.jsp&quot;) &#125;, interceptorRefs&#x3D;&#123; @InterceptorRef(&quot;myDefaultStack&quot;) &#125;)public String findAllCustomer()&#123; customers &#x3D; customerService.findAllCustomer(); return &quot;findAllCustomer&quot;;&#125;出现在动作类上：@InterceptorRef(&quot;myDefaultStack&quot;)public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private Customer customer &#x3D; new Customer(); @Override public Customer getModel() &#123; return customer; &#125;&#125; 2.3 案例-注解实现客户保存和查询列表2.3.1 拷贝必备jar包导入jar包： struts2-convention-plugin-2.3.24.jar 2.3.2 使用注解配置Action1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 客户的动作类 * * @author wgy */@ParentPackage(\"struts-default\")//指定当前包的父包@Namespace(\"/customer\")@Results(&#123; @Result(name=\"customerList\",type=\"redirect\",location=\"findAllCustomer.action\"), @Result(name=\"error\",location=\"/jsp/error.jsp\"), @Result(name=\"addUICustomer\",location=\"/jsp/customer/add.jsp\"), @Result(name=\"findAllCustomer\",location=\"/jsp/customer/list.jsp\")&#125;)public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private ICustomerService customerService = new CustomerServiceImpl(); private Customer customer = new Customer(); private List&lt;Customer&gt; customers; @Override public Customer getModel() &#123; return customer; &#125; /** * 查询所有客户 * @return */ @Action(\"findAllCustomer\") public String findAllCustomer()&#123; //1.调用service查询客户 List&lt;Customer&gt; customers = customerService.findAllCustomer(); //2.返回 return \"findAllCustomer\"; &#125; /** * 获取添加客户页面 * @return */ @Action(\"addUICustomer\") public String addUICustomer()&#123; return \"addUICustomer\"; &#125; /** * 添加客户 * @return */ @Action(\"addCustomer\") public String addCustomer()&#123; customerService.saveCustomer(customer); return \"customerList\"; &#125; /** * 删除客户 * @return */ @Action(\"deleteCustomer\") public String deleteCustomer()&#123; customerService.deleteCustomer(customer); return \"customerList\"; &#125; //------getters and setters-------------- public List&lt;Customer&gt; getCustomers() &#123; return customers; &#125; public void setCustomers(List&lt;Customer&gt; customers) &#123; this.customers = customers; &#125;&#125;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Struts2","slug":"Struts2","permalink":"https://wgy1993.gitee.io/tags/Struts2/"}]},{"title":"Struts2(三)","date":"2020-06-07T11:48:27.000Z","path":"archives/4179e9cb.html","text":"1. OGNL表达式1.1 OGNL表达式概述1.1.1 什么是OGNL表达式OGNL的全称是对象图导航语言（Object-Graph Navigation Language），它是一种功能强大的开源表达式语言，使用这种表达式语言，可以通过某种表达式语法，存取Java对象的任意属性，调用Java对象的方法，同时能够自动实现必要的类型转换。如果把表达式看作是一个带有语义的字符串，那么OGNL无疑成为了这个语义字符串与Java对象之间沟通的桥梁。 1.1.2 OGNL表达式的由来它原本是xwork2中的默认表达式语言，当年OpenSymphony和apache在合作开发struts2框架时，把这个表达式也引进来了，所以就变成了struts2的默认表达式语言。 1.1.3 OGNL表达式的使用要求要想使用ognl表达式，一般情况下都得需要使用struts2的标签库。 1&lt;%@ taglib uri=\"/struts-tags\" prefix=\"s\" %&gt; 1.1.4 它的特点它不仅可以用于取值，显示。还可以赋值。取值是我们程序员使用框架做的事情。赋值是框架为我们做的。 1.2 OGNL表达式的基本用法1.2.1 s:property标签输出内容到浏览器1.2.1.1 s:property的用法12345678&lt;%--要想使用OGNL表达式获取数据，此时需要借助struts2的标签库 s:property标签实现把数据输出到浏览器上 &lt;s:property value=\"\"/&gt; value属性的取值是一个OGNL表达式。 标签会把value属性取值所对应的内容输出到浏览器上 如果没有任何对应内容，则什么都不显示--%&gt;OGNL的最基本用法：&lt;s:property value=\"OGNLExpression\"/&gt; 1.2.1.2 OGNL表达式和字符串的转换12345678910111213141516&lt;%--OGNL表达式和字符串的转换 表达式转成字符串 %&#123;''&#125; | %&#123;\"\"&#125; 可以把%&#123;&#125;去掉 --%&gt;OGNL转成一个普通的字符串：&lt;s:property value=\"%&#123;'OGNLExpression1'&#125;\"/&gt;&lt;br/&gt;OGNL转成一个普通的字符串：&lt;s:property value='%&#123;\"OGNLExpression2\"&#125;'/&gt;&lt;br/&gt;OGNL转成一个普通的字符串：&lt;s:property value='\"OGNLExpression3\"'/&gt;&lt;br/&gt;OGNL转成一个普通的字符串：&lt;s:property value=\"'OGNLExpression4'\"/&gt;&lt;br/&gt;&lt;!-- 字符串转成表达式 %&#123;&#125;把字符串套起来--&gt;&lt;!-- user.name 看上去是字符串，当它执行user对象的getName方法时，表示用OGNL表达式解释。 --&gt;字符串转成一个OGNL表达式：&lt;s:textfield name=\"username\" value=\"%&#123;user.name&#125;\"/&gt; 1.2.2 OGNL表达式访问对象的方法1234&lt;%--OGNL表达式访问对象的方法 --%&gt;调用字符串的长度方法：&lt;s:property value=\"'OGNLExpression1'.length()\"/&gt;&lt;br/&gt;调用字符串的转大写方法：&lt;s:property value=\"'OGNLExpression1'.toUpperCase()\"/&gt;&lt;br/&gt;调用字符串的分隔方法：&lt;s:property value=\"'OGNLExpression1'.split('E')\"/&gt;&lt;br/&gt; 1.2.3 OGNL表达式访问类的静态属性和静态方法123456&lt;%--OGNL表达式访问类的静态成员(静态属性) 访问静态属性需要按照固定的书写规范来写。 规范是： @包名.包名...类名@静态属性名称--%&gt;OGNL表达式访问静态属性：&lt;s:property value=\"@java.lang.Integer@MAX_VALUE\"/&gt; 123456&lt;%--OGNL表达式访问类的静态方法 访问静态方法需要按照固定的书写规范来写。 规范是： @包名.包名...类名@静态方法名称--%&gt;OGNL表达式访问静态方法：&lt;s:property value=\"@java.lang.Math@random()\"/&gt; 1.2.4 OGNL表达式操作集合1.2.4.1 list集合1234567891011&lt;%--操作List集合 s:radio标签的list取值就是一个OGNL表达式。 &#123;&#125;就表示创建了一个List集合 &#123;'男','女'&#125;=== List list = new ArrayList(); list.add(\"男\"); list.add(\"女\");--%&gt;Struts2的单选按钮：&lt;br/&gt;&lt;s:radio list=\"&#123;'男','女'&#125;\" name=\"gender2\" label=\"性别\"&gt;&lt;/s:radio&gt;HTML的单选按钮：&lt;br/&gt;性别：&lt;input type=\"radio\" name=\"gender1\" value=\"男\"&gt;男&lt;input type=\"radio\" name=\"gender1\" value=\"女\"&gt;女 1.2.4.2 map集合1234567891011&lt;%--操作Map集合 #&#123;&#125;就表示创建了一个Map集合。 #&#123;key:value,key:value&#125; #&#123;'male':'男','female':'女'&#125;=== Map map = new HashMap(); map.put(\"male\",\"男\"); map.put(\"female\",\"女\");--%&gt;Struts2的单选按钮：&lt;br/&gt;&lt;s:radio list=\"#&#123;'male':'男','female':'女'&#125;\" name=\"gender4\" label=\"性别\"&gt;&lt;/s:radio&gt;HTML的单选按钮：&lt;br/&gt;性别：&lt;input type=\"radio\" name=\"gender3\" value=\"male\"&gt;男&lt;input type=\"radio\" name=\"gender3\" value=\"female\"&gt;女 2. OGNL上下文2.1 ContextMap2.1.1 ContextMap概述它是OGNL上下文对象，是struts2中封装数据最大的对象。我们一次请求中所有用到的信息都可以在它里面找到。它是一个Map结构的对象，其中key是字符串，value是一个Object。 2.1.2 ContextMap中封装的数据 我们把这些内容拿出来逐个分析一下，得到下面的表格： Map的key（类型是String） Map的Value （类型是Object） 说明信息 application Java.util.Map&lt;String,Object&gt; 封装的应用域中的所有数据 session Java.util.Map&lt;String,Object&gt; 封装的会话域中的所有数据 request Java.util.Map&lt;String,Object&gt; 封装的请求域中的所有数据 valueStack(特殊) com.opensymphony.xwork2.ognl.OgnlValueStack 它是List结构 parameters Java.util.Map&lt;String,String[]&gt; 封装的是请求参数 attr Java.util.Map&lt;String,Object&gt; 封装的是四大域的组合数据，从最小的域开始搜索 action com.opensymphony.xwork2.ActionSupport 当前执行的动作类对象 2.2 ActionContext2.2.1 ActionContext对象概述它是一个工具类，是struts2框架提供给我们的，可以让我们调用其中的方法，快速的操作ContextMap。用它操作OGNL上下文对象，比直接操作ContextMap要方便很多。 2.2.2 ActionContext对象与ContextMap的关系ActionContext就相当于对ContextMap进行了一次再封装。 2.2.3 ActionContext何时创建由于ActionContext是操作的ContextMap，而ContextMap中封了我们一次请求的所有数据，所以它的创建应该是每次请求访问Action时，即核心控制器(StrutsPrepareAndExecuteFilter)的doFilter方法执行时，下图是代码截取： 2.2.4 ActionContext的线程安全我们都知道，java的web工程是多线程的，那么每个线程在访问Action时，都会创建自己的ActionContext,那么是如何保证在获取ActionContext时，每个线程都能获取到自己的那个呢？ 答案就是，每次创建ActionContext时，把对象绑定到当前线程上。下图是代码截取： 2.2.5 ActionContext的获取使用ActionContext类中的静态方法getContext()从当前线程上获取 2.2.6 获取ContextMap中的数据2.2.6.1 s:debug标签的使用123456&lt;%-- 引入标签库 --%&gt;&lt;%@ taglib uri=\"/struts-tags\" prefix=\"s\" %&gt;&lt;%--1、struts2的debug标签 它是一个用于开发阶段的标签，查看我们OGNL上下文中内容的标签 --%&gt;&lt;s:debug/&gt; 2.2.6.2 使用OGNL表达式获取Map中的数据动作类存数据： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * ActionContext的数据存取 * * @author wgy */public class Demo1Action extends ActionSupport &#123; /** * 通过ActionContext往ContextMap中存入数据 * contextMap hello context map * * 往应用域中存入数据：用两种方式实现 * applicationMap hello application map * applicationAttr hello application attr * * 往会话域中存入数据：同上用两种方式 * @return */ public String demo1()&#123; //1.获取ActionContext //从当前线程上获取 ActionContext context = ActionContext.getContext(); //2.存入数据 context.put(\"contextMap\", \"hello context map\"); //3.往应用域中存入数据 //第一种方式：使用原始ServletAPI对象ServletContext ServletContext applicationAttr = ServletActionContext.getServletContext(); applicationAttr.setAttribute(\"applicationAttr\", \"hello application attr\"); //第二种方式：根据key从ActionContext中获取应用域的map，往map中存入数据 Map&lt;String,Object&gt; applicationMap = context.getApplication(); applicationMap.put(\"applicationMap\",\"hello application map\"); //4.往会话域中存入数据 //第一种：使用ServletAPI的HttpSession HttpSession session = ServletActionContext.getRequest().getSession(); session.setAttribute(\"sessionAttr\", \"hello session attr\"); //第二种：获取key为session的map Map&lt;String,Object&gt; sessionMap = context.getSession(); sessionMap.put(\"sessionMap\",\"hello session map\"); return SUCCESS; &#125;&#125; 在页面中使用OGNL表达式获取： 1234567891011&lt;%--借助struts2的s:property标签和OGNL表达式获取ActionContext存入的数据 我们现在获取的数据，都是在map中。 获取Map中的数据，OGNL表达式的写法： #key 如果还想继续向下获取，使用.key的方式--%&gt;&lt;s:property value=\"#contextMap\"/&gt;&lt;br/&gt;&lt;s:property value=\"#application.applicationMap\"/&gt;&lt;br/&gt;&lt;s:property value=\"#session.sessionAttr\"/&gt;&lt;s:property value=\"#session.sessionMap\"/&gt; 2.3 ValueStack对象2.3.1 ValueStack对象概述ValueStack是Struts的一个接口，字面意义为值栈，OgnlValueStack是ValueStack的实现类，客户端发起一个请求struts2架构会创建一个action实例同时创建一个OgnlValueStack值栈实例，OgnlValueStack贯穿整个 Action 的生命周期。 它是ContextMap中的一部分，里面的结构是一个List，是我们可以快速访问数据一个容器。它的封装是由struts2框架完成的。 通常情况下我们是从页面上获取数据。它实现了栈的特性（先进后出）。 2.3.2 ValueStack的内部结构在 OnglValueStack 中包含了一个CompoundRoot的对象，该对象继承了ArrayList，并且提供了只能操作集合第一个元素的方法，所以我们说它实现了栈的特性。同时，它里面定义了一个ContextMap的引用，也就是说，我们有值栈对象，也可以通过值栈来获取ContextMap。 2.3.3 获取ValueStack中的数据2.3.3.1 值栈中都有什么首先我们要明确，值栈中存的都是对象。因为它本质就是一个List，List中只能存对象。 值栈中包含了我们通过调用push方法压栈的对象，当前执行的动作了和一个名称为DefaultTextProvider的类。值栈中的内容如下图： 2.3.3.2 在动作类中往值栈中存入数据12345678910111213141516171819202122232425262728293031/** * ValueStack的数据存取 * * @author wgy */public class Demo2Action extends ActionSupport &#123; //把私有成员放入值栈中 private String name = \"泰斯特\"; public String getName() &#123; return name; &#125; /** * 获取ValueStack，并且压栈操作 * @return */ public String demo2()&#123; //1.获取ActionContext //从当前线程上获取 ActionContext context = ActionContext.getContext(); //2.获取ValueStack对象 ValueStack vs = context.getValueStack(); //3.压栈操作 Student s = new Student(\"张三\",18,\"male\"); vs.push(s); return SUCCESS; &#125;&#125; 2.3.3.3 我们可以获取值栈中的什么一般情况下，我们都是根据debug标签中显示的Property Name来获取Property Value。 当然我们也可以获取栈顶对象。 2.3.3.5 在页面上使用OGNL表达式获取数据123456789101112131415&lt;%--获取值栈的数据也需要借助于struts2的标签库 使用s:property获取 获取值栈的数据，是直接写属性名称，得到的就是属性的值。 OGNL表达式的找法，是从栈顶逐个属性名称开始查找，只要找到之后，就不再继续查找，而是返回结果。 --%&gt;姓名：&lt;s:property value=\"name\"/&gt;&lt;br/&gt;年龄：&lt;s:property value=\"age\"/&gt;&lt;br/&gt;性别：&lt;s:property value=\"gender\"/&gt;&lt;br/&gt;&lt;%--获取指定位置的属性 --%&gt;获取第一个name:&lt;s:property value=\"[0].name\"/&gt;&lt;br/&gt;获取第二个name:&lt;s:property value=\"[1].name\"/&gt;&lt;%--如果使用s:property标签，没有写value属性，取的是栈顶对象 --%&gt;&lt;s:property/&gt; 2.3.3.6 OGNL表达式执行时调用的方法12345678910111213&lt;%--s:property在通过OGNL表达式获取数据时，所调用的方法：ValueStack中的findValue(String expr); --%&gt;&lt;% ActionContext context = ActionContext.getContext(); ValueStack vs = context.getValueStack(); Object o1 = vs.findValue(\"[0].name\"); out.println(o1); out.println(\"&lt;br/&gt;\"); Object o2 = vs.findValue(\"[1].name\"); out.print(o2); out.println(\"&lt;br/&gt;\"); Object o3 = vs.findValue(\"#application.applicationMap\"); out.print(o3);%&gt; 3. Struts2中使用EL表达式3.1 EL表达式回顾EL表达式的写法：${表达式}。 它是从四大域中，由小到大逐个域搜索，根据名称获取值。只要找到了，就不再继续搜索。 它的原理：使用的是PageContext类中的findValue方法。 3.2 Struts2对EL表达式的改变Struts2框架中对EL表达式做了如下改变： 1234EL表达式原来的搜素顺序： page Scope——&gt;request Scope——&gt;session Scope——&gt;application ScopeEL表达式改变后的搜索顺序： page Scope—&gt;request Scope—&gt;valueStack—&gt;contextMap—&gt;session Scope—&gt;application Scope struts2框架对request对象进行了包装，并且对getAttribute方法进行了增强，代码如下： 4. OGNL表达式中的各种符号总结4.1 %121、把OGNL表达式转成普通字符串 %&#123;&quot;&quot;&#125;2、把字符串转成OGNL表达式%&#123;&#125; 4.2 #121、获取ContextMap中的数据。#key2、在页面中可以创建Map集合。 #&#123;&#125; 4.3 $121、EL表达式使用2、可以在struts2的配置中使用OGNL表达式（配置可以是xml文件，也可以是注解）$&#123;&#125; 5. 案例-优化客户列表的展示5.1 改造Action我们把之前查询所有客户的动作方法改造一下，之前我们是把查询结果存入请求域中了，而此时我们只需要在Action中定义一个集合，并且提供get/set方法，它就会出现在值栈中。就可以在页面中使用OGNL表达式获取。 1234567891011121314151617181920/** * 查询所有客户 * @return */private List&lt;Customer&gt; customers;public String findAllCustomer()&#123; //1.调用service查询客户 List&lt;Customer&gt; customers = customerService.findAllCustomer(); //2.返回 return \"findAllCustomer\";&#125;public List&lt;Customer&gt; getCustomers() &#123; return customers;&#125;public void setCustomers(List&lt;Customer&gt; customers) &#123; this.customers = customers;&#125; 5.2 改造jsp在显示客户列表时，我们之前采用的是jstl标签库的c:forEach标签，今天我们将使用struts2提供的迭代标签s:iterator。 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;%-- &lt;c:forEach items=\"$&#123;customers&#125;\" var=\"customer\"&gt; &lt;TR&gt; &lt;TD&gt;$&#123;customer.custName &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custLevel &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custSource &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custIndustry &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custAddress &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custPhone &#125;&lt;/TD&gt; &lt;/TR&gt; &lt;/c:forEach&gt; --%&gt;&lt;%-- struts2中的迭代标签： 属性： value：它的取值是一个OGNL表达式 var：写了该属性：它会把var的值作为key，把当前遍历的对象作为value，存入contextMap中 没写该属性：它会把每次遍历的对象压入栈顶--%&gt;&lt;%-- &lt;s:iterator value=\"customers\" var=\"cust\"&gt; &lt;TR&gt; &lt;TD&gt;&lt;s:property value=\"#cust.custName\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"#cust.custLevel\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"#cust.custSource\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"#cust.custIndustry\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"#cust.custAddress\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"#cust.custPhone\"/&gt;&lt;/TD&gt; &lt;/TR&gt; &lt;/s:iterator&gt; --%&gt;&lt;s:iterator value=\"customers\"&gt; &lt;TR&gt; &lt;TD&gt;&lt;s:property value=\"custName\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"custLevel\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"custSource\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"custIndustry\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"custAddress\"/&gt;&lt;/TD&gt; &lt;TD&gt;&lt;s:property value=\"custPhone\"/&gt;&lt;/TD&gt; &lt;/TR&gt;&lt;/s:iterator&gt;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Struts2","slug":"Struts2","permalink":"https://wgy1993.gitee.io/tags/Struts2/"}]},{"title":"Struts2(二)","date":"2020-06-05T09:14:53.000Z","path":"archives/3f8ac9e0.html","text":"1. 结果视图的配置1.1 result标签在sturts.xml文件中，Result的配置非常简单，使用元素来配置Result逻辑视图与物理视图之间的映射，元素可以有name和type属性，但这两种属性都不是必选的。 1234567891011121314151617181920212223242526&lt;action name&#x3D;&quot;demo1&quot; class&#x3D;&quot;com.wgy.web.action.Demo1Action&quot; method&#x3D;&quot;demo1&quot;&gt; &lt;!-- result标签： 作用：用于配置结果视图（结果视图可以是一个jsp&#x2F;html，也可以是一个action） 属性： name：指定逻辑结果视图。作用就是和动作方法的返回值进行比较，当一致时，前往配置的页面或者action。不写的话:默认值是success——&gt;去哪 type：指定前往结果视图的方式。以何种方式前往。 ——&gt;怎么去 type取值都是来源于struts-default.xml文件中package名称是struts-default包中定义类型 常用的结果类型： dispatcher：请求转发 （默认值） redirect：重定向(可以是重定向到另外一个动作或者是重定向到一个jsp) redirectAction：重定向到另外一个动作(它由于会自动在后面为我们拼接url后缀，所以只能重定向到动作) 请求转发和重定向的区别： 请求转发：一次请求 地址栏不变 请求域中数据不丢失 服务器行为 只能是在当前应用中转发 重定向： 两次请求 地址栏改变 请求域中数据丢失 浏览器行为 可以定向到当前应用的外部 响应浏览器的三种方式： 请求转发 重定向 使用流输出（如果只有一种方式，那就是此种方式） --&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;redirectAction&quot;&gt;demo2&lt;&#x2F;result&gt; &lt;result name&#x3D;&quot;error&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;error.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt;&lt;action name&#x3D;&quot;demo2&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;result name&#x3D;&quot;login&quot;&gt;&#x2F;login.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 1.2 配置全局结果视图1.2.1 局部结果视图和全局结果视图配置在action标签内的result，我们成为局部结果视图，它只能由当前action使用。 而在实际开发中，有很多页面，每个action可能都会用到。比如：success.jsp,error.jsp,login.jsp等等。当我们很多action都用到了login.jsp，在每个action标签中都配置一次，显然是不合理的，这个时候我们就用到了全局结果视图。 1.2.2 配置方式12345678910111213141516171819202122&lt;!-- 定义一个公共包 全局结果视图和局部结果视图 定义是放在action标签外面， 在global-results标签内部的结果视图。 可以在多个action中使用 优先级：先找局部，再找全局。--&gt;&lt;package name&#x3D;&quot;myDefault&quot; extends&#x3D;&quot;struts-default&quot; abstract&#x3D;&quot;true&quot;&gt; &lt;global-results&gt; &lt;result name&#x3D;&quot;login&quot;&gt;&#x2F;login.jsp&lt;&#x2F;result&gt; &lt;&#x2F;global-results&gt;&lt;&#x2F;package&gt;&lt;package name&#x3D;&quot;p1&quot; extends&#x3D;&quot;myDefault&quot;&gt; &lt;action name&#x3D;&quot;demo1&quot; class&#x3D;&quot;com.wgy.web.action.Demo1Action&quot; method&#x3D;&quot;demo1&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;redirectAction&quot;&gt;demo2&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;action name&#x3D;&quot;demo2&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;result name&#x3D;&quot;login&quot;&gt;&#x2F;login.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt;&lt;&#x2F;package&gt; 2. 访问Servlet的API的两种方式2.1 使用ServletActionContext12345678910111213141516171819202122232425262728293031323334353637383940/** * 访问ServletAPI: * 有两种方式： * 第一种方式： * 使用struts2框架提供的一个工具类，该类中包含了相应的静态方法，可以直接获取 * 工具类是：ServletActionContext * 此种方式是我们实际开发中用的最多的方式 * * 输出结果之后，找出其中一个和其他三个不一样： * org.apache.struts2.dispatcher.StrutsRequestWrapper@1c6e453 它和其他三个不一样，它是struts2提供的 * org.apache.catalina.connector.ResponseFacade@b846ae * org.apache.catalina.core.ApplicationContextFacade@287809 * org.apache.catalina.session.StandardSessionFacade@e0d480 * * @author wgy */public class Demo1Action extends ActionSupport &#123; private HttpServletRequest request; private HttpServletResponse response; private HttpSession session; private ServletContext application; /** * 动作方法 * * @return */ public String demo1()&#123; request = ServletActionContext.getRequest(); response = ServletActionContext.getResponse(); application = ServletActionContext.getServletContext(); session = request.getSession(); System.out.println(request); System.out.println(response); System.out.println(application); System.out.println(session); return SUCCESS; &#125;&#125; 2.2 通过实现接口的方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 访问ServletAPI: * 有两种方式： * 第二种方式： * 通过实现不同的接口，获取不同的对象。 * 要想使用request，需要实现ServletRequestAware * 要想使用response，需要实现ServletResponseAware * 要想使用servletContext，需要实现SerlvetContextAware * 输出结果之后，找出其中一个和其他三个不一样： * org.apache.struts2.dispatcher.StrutsRequestWrapper@1c6e453 它和其他三个不一样，它是struts2提供的 * org.apache.catalina.connector.ResponseFacade@b846ae * org.apache.catalina.core.ApplicationContextFacade@287809 * org.apache.catalina.session.StandardSessionFacade@e0d480 * * * 如果说是一种方式获取ServletAPI对象：ActionContext中的get(key) * 如果说是三种方式获取ServletAPI对象，除了我们讲的两种之外，也可以使用ActionContext获取 * * 通过分析源码，我们得知，ActionContext看上去是一个类似Map的结构。 * map的key是String类型，Map的value是Object类型 * * @author wgy */public class Demo2Action extends ActionSupport implements ServletRequestAware, ServletResponseAware, ServletContextAware &#123; private HttpServletRequest request = null; private HttpServletResponse response = null; private ServletContext application = null; /** * 动作方法 * * @return */ public String demo2()&#123; System.out.println(request); System.out.println(response); System.out.println(application); return SUCCESS; &#125; @Override public void setServletRequest(HttpServletRequest request) &#123; this.request = request; &#125; @Override public void setServletResponse(HttpServletResponse response) &#123; this.response = response; &#125; @Override public void setServletContext(ServletContext application) &#123; this.application = application; &#125;&#125; 3. 请求参数的封装3.1 请求参数封装概述封装请求参数就是把我们通过浏览器发送请求时，要转递给服务器的数据封装到指定的对象中。这个对象一般都是实体类。但是有时就是Action中的一个属性。也就是说，我们封装请求参数时，可以有实体类，也可以没有。同时，我们还需要知道，请求参数的封装和请求方式无关。无论get还是post都可以封装。 3.2 属性驱动3.2.1 没有实体类此种情况，我们一般也称为动作类和模型在一起，也就是说我们在action中定义一些私有成员，并且提供它们的公有get/set方法。具体代码如下 动作类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * 请求参数封装 * 第一种情况： * 属性驱动：没有实体类 * 表单数据的接收都定义在动作类中，所以称为动作类和模型数据写在一起 * 要想封装成功，需要按照要求书写： * 要求是：表单元素的name属性取值，必须和动作类中成员get/set方法后面的部分保持一致 * * 细节： * 1、struts2框架会我们解决post请求的中文乱码问题，但是get请求不解决。 * 2、struts2框架会自动为我们转换数据类型： * 基本类型自动转换 * 字符串数组会按照逗号+空格的方式拼接成字符串 * 日期类型会按照本地格式转成日期对象 * 本地格式：yyyy-MM-dd * * 执行参数封装，是一个名称为params的拦截器实现的。 * 封装的规则只有一个，它要去指定位置找属性，找到之后调用set方法赋值。 * * @author wgy */public class Demo1Action extends ActionSupport &#123; private String username; private Integer age; private Date birthday; private String hobby; /** * 动作方法 * * @return */ public String demo1()&#123; System.out.println(username+\"===\"+age+\"===\"+birthday+\"===\"+hobby); return SUCCESS; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public Date getBirthday() &#123; return birthday; &#125; public void setBirthday(Date birthday) &#123; this.birthday = birthday; &#125; public String getHobby() &#123; return hobby; &#125; public void setHobby(String hobby) &#123; this.hobby = hobby; &#125;&#125; jsp页面： 1234567891011&lt;%--请求参数封装：第一种情况：属性驱动-没有实体类 --%&gt;&lt;form action=\"$&#123;pageContext.request.contextPath&#125;/demo1.action\" method=\"post\"&gt; 姓名：&lt;input type=\"text\" name=\"username\"/&gt;&lt;br/&gt; 年龄：&lt;input type=\"text\" name=\"age\"/&gt;&lt;br/&gt; 生日：&lt;input type=\"text\" name=\"birthday\"/&gt;&lt;br/&gt; 爱好：&lt;input type=\"checkbox\" name=\"hobby\" value=\"吃饭\"/&gt;吃饭 &lt;input type=\"checkbox\" name=\"hobby\" value=\"睡觉\"/&gt;睡觉 &lt;input type=\"checkbox\" name=\"hobby\" value=\"写代码\"/&gt;写代码 &lt;br/&gt; &lt;input type=\"submit\" value=\"提交\"/&gt;&lt;/form&gt; struts.xml： 123&lt;action name&#x3D;&quot;demo1&quot; class&#x3D;&quot;com.wgy.web.action.Demo1Action&quot; method&#x3D;&quot;demo1&quot;&gt; &lt;result&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3.2.2 有实体类此种情况是，我们有独立的实体类，在action中定义的是实体类对象，并且提供get/set方法。代码如下： 动作类： 1234567891011121314151617181920212223242526272829303132333435363738/** * 请求参数封装 * 第二种情况： * 属性驱动：有实体类 * 表单数据的接收都定义在实体类中，把实体类定义在动作类中。 * 要想封装成功，需要按照要求书写： * 此时需要使用OGNL表达式来指定表单元素的name取值 * OGNL表达式全称：Object Graphic Navigation Language * 对象 图 导航 语言 * 写法： * user.username user.age * * 执行参数封装，是一个名称为params的拦截器实现的。 * 封装的规则只有一个，它要去指定位置找属性，找到之后调用set方法赋值。 * * @author wgy */public class Demo2Action extends ActionSupport &#123; private User user; public User getUser() &#123; return user; &#125; public void setUser(User user) &#123; this.user = user; &#125; /** * 动作方法 * @return */ public String demo2()&#123; System.out.println(user); return SUCCESS; &#125;&#125; jsp页面： 1234567891011&lt;%--请求参数封装：第二种情况：属性驱动-有实体类 --%&gt;&lt;form action=\"$&#123;pageContext.request.contextPath&#125;/demo2.action\" method=\"post\"&gt; 姓名：&lt;input type=\"text\" name=\"user.username\"/&gt;&lt;br/&gt; 年龄：&lt;input type=\"text\" name=\"user.age\"/&gt;&lt;br/&gt; 生日：&lt;input type=\"text\" name=\"user.birthday\"/&gt;&lt;br/&gt; 爱好：&lt;input type=\"checkbox\" name=\"user.hobby\" value=\"吃饭\"/&gt;吃饭 &lt;input type=\"checkbox\" name=\"user.hobby\" value=\"睡觉\"/&gt;睡觉 &lt;input type=\"checkbox\" name=\"user.hobby\" value=\"写代码\"/&gt;写代码 &lt;br/&gt; &lt;input type=\"submit\" value=\"提交\"/&gt;&lt;/form&gt; struts.xml： 123&lt;action name&#x3D;&quot;demo2&quot; class&#x3D;&quot;com.wgy.web.action.Demo2Action&quot; method&#x3D;&quot;demo2&quot;&gt; &lt;result&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3.3 模型驱动在Struts2中，Action处理请求参数还有另外一种方式，叫做模型驱动（ModelDriven）。通过实现ModelDriven接口来接收请求参数。 动作类： 12345678910111213141516171819202122232425262728293031323334/** * 请求参数封装 * 第三种情况： 我们在后面用的最多的方式 * 模型驱动 * 要想封装成功，需要按照要求书写： * 1、动作类必须实现ModelDriven接口 * 2、动作类中需要定义模型，并且必须实例化出来 * 3、提供接口抽象方法的实现，返回值必须是模型对象 * * 执行参数封装，是一个名称为params的拦截器实现的。 * 模型驱动的实现，除了params拦截器之外，还需要一个叫modelDriven的拦截器配合 * 封装的规则只有一个，它要去指定位置找属性，找到之后调用set方法赋值。 * * @author wgy */public class Demo3Action extends ActionSupport implements ModelDriven&lt;User&gt; &#123; private User user = new User(); @Override public User getModel() &#123; return user; &#125; /** * 动作方法 * * @return */ public String demo3()&#123; System.out.println(user); return SUCCESS; &#125;&#125; jsp页面： 1234567891011&lt;%--请求参数封装：第三种情况：模型驱动 --%&gt;&lt;form action=\"$&#123;pageContext.request.contextPath&#125;/demo3.action\" method=\"post\"&gt; 姓名：&lt;input type=\"text\" name=\"username\"/&gt;&lt;br/&gt; 年龄：&lt;input type=\"text\" name=\"age\"/&gt;&lt;br/&gt; 生日：&lt;input type=\"text\" name=\"birthday\"/&gt;&lt;br/&gt; 爱好：&lt;input type=\"checkbox\" name=\"hobby\" value=\"吃饭\"/&gt;吃饭 &lt;input type=\"checkbox\" name=\"hobby\" value=\"睡觉\"/&gt;睡觉 &lt;input type=\"checkbox\" name=\"hobby\" value=\"写代码\"/&gt;写代码 &lt;br/&gt; &lt;input type=\"submit\" value=\"提交\"/&gt;&lt;/form&gt; struts.xml： 123&lt;action name&#x3D;&quot;demo3&quot; class&#x3D;&quot;com.wgy.web.action.Demo3Action&quot; method&#x3D;&quot;demo3&quot;&gt; &lt;result&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3.4 封装集合数据在实际的开发中，有些时候我们需要批量插入用户或者批量插入其他的对象，在Action中需要接受到这多个Action中封装的对象，然后传递给业务层。那么这个时候就需要将表单的数据封装到集合中。 3.4.1 封装到List动作类： 12345678910111213141516171819202122232425262728/** * 请求参数封装 * 复杂类型的封装：List集合封装 * 复杂类型的封装都需要基于第二种情况实现 * * @author wgy */public class Demo4Action extends ActionSupport &#123; private List&lt;User&gt; users; public List&lt;User&gt; getUsers() &#123; return users; &#125; public void setUsers(List&lt;User&gt; users) &#123; this.users = users; &#125; /** * 动作方法 * * @return */ public String demo4()&#123; System.out.println(users); return SUCCESS; &#125;&#125; jsp页面： 123456789101112131415161718&lt;%--请求参数封装：List集合类型的封装 --%&gt;&lt;form action=\"$&#123;pageContext.request.contextPath&#125;/demo4.action\" method=\"post\"&gt; 姓名：&lt;input type=\"text\" name=\"users[0].username\"/&gt;&lt;br/&gt; 年龄：&lt;input type=\"text\" name=\"users[0].age\"/&gt;&lt;br/&gt; 生日：&lt;input type=\"text\" name=\"users[0].birthday\"/&gt;&lt;br/&gt; 爱好：&lt;input type=\"checkbox\" name=\"users[0].hobby\" value=\"吃饭\"/&gt;吃饭 &lt;input type=\"checkbox\" name=\"users[0].hobby\" value=\"睡觉\"/&gt;睡觉 &lt;input type=\"checkbox\" name=\"users[0].hobby\" value=\"写代码\"/&gt;写代码 &lt;br/&gt; 姓名：&lt;input type=\"text\" name=\"users[1].username\"/&gt;&lt;br/&gt; 年龄：&lt;input type=\"text\" name=\"users[1].age\"/&gt;&lt;br/&gt; 生日：&lt;input type=\"text\" name=\"users[1].birthday\"/&gt;&lt;br/&gt; 爱好：&lt;input type=\"checkbox\" name=\"users[1].hobby\" value=\"吃饭\"/&gt;吃饭 &lt;input type=\"checkbox\" name=\"users[1].hobby\" value=\"睡觉\"/&gt;睡觉 &lt;input type=\"checkbox\" name=\"users[1].hobby\" value=\"写代码\"/&gt;写代码 &lt;br/&gt; &lt;input type=\"submit\" value=\"提交\"/&gt;&lt;/form&gt; struts.xml： 123&lt;action name&#x3D;&quot;demo4&quot; class&#x3D;&quot;com.wgy.web.action.Demo4Action&quot; method&#x3D;&quot;demo4&quot;&gt; &lt;result&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3.4.2 封装到Map动作类： 1234567891011121314151617181920212223242526272829/** * 请求参数封装 * 复杂类型的封装：Map集合封装 * 复杂类型的封装都需要基于第二种情况实现 * * @author wgy */public class Demo5Action extends ActionSupport &#123; private Map&lt;String, User&gt; users; public Map&lt;String, User&gt; getUsers() &#123; return users; &#125; public void setUsers(Map&lt;String, User&gt; users) &#123; this.users = users; &#125; /** * 动作方法 * * @return */ public String demo5()&#123; System.out.println(users); return SUCCESS; &#125;&#125; jsp页面： 123456789101112131415161718&lt;%--请求参数封装：Map集合类型的封装 --%&gt;&lt;form action=\"$&#123;pageContext.request.contextPath&#125;/demo5.action\" method=\"post\"&gt; 姓名：&lt;input type=\"text\" name=\"users['key1'].username\"/&gt;&lt;br/&gt; 年龄：&lt;input type=\"text\" name=\"users['key1'].age\"/&gt;&lt;br/&gt; 生日：&lt;input type=\"text\" name=\"users['key1'].birthday\"/&gt;&lt;br/&gt; 爱好：&lt;input type=\"checkbox\" name=\"users['key1'].hobby\" value=\"吃饭\"/&gt;吃饭 &lt;input type=\"checkbox\" name=\"users['key1'].hobby\" value=\"睡觉\"/&gt;睡觉 &lt;input type=\"checkbox\" name=\"users['key1'].hobby\" value=\"写代码\"/&gt;写代码 &lt;br/&gt; 姓名：&lt;input type=\"text\" name=\"users['abc'].username\"/&gt;&lt;br/&gt; 年龄：&lt;input type=\"text\" name=\"users['abc'].age\"/&gt;&lt;br/&gt; 生日：&lt;input type=\"text\" name=\"users['abc'].birthday\"/&gt;&lt;br/&gt; 爱好：&lt;input type=\"checkbox\" name=\"users['abc'].hobby\" value=\"吃饭\"/&gt;吃饭 &lt;input type=\"checkbox\" name=\"users['abc'].hobby\" value=\"睡觉\"/&gt;睡觉 &lt;input type=\"checkbox\" name=\"users['abc'].hobby\" value=\"写代码\"/&gt;写代码 &lt;br/&gt; &lt;input type=\"submit\" value=\"提交\"/&gt;&lt;/form&gt; struts.xml： 123&lt;action name&#x3D;&quot;demo5&quot; class&#x3D;&quot;com.wgy.web.action.Demo5Action&quot; method&#x3D;&quot;demo5&quot;&gt; &lt;result&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 4. 案例：Struts2+Hibernate实现保存删除客户4.1 Struts24.1.1 修改jspmenu.jsp 12345&lt;TR&gt; &lt;TD class=menuSmall&gt; &lt;A class=style2 href=\"$&#123;pageContext.request.contextPath&#125;/customer/addUICustomer.action\" target=main&gt;－ 新增客户&lt;/A&gt; &lt;/TD&gt;&lt;/TR&gt; add.jsp 123&lt;FORM id=form1 name=form1 action=\"$&#123;pageContext.request.contextPath &#125;/customer/addCustomer.action\" method=post&gt; ...&lt;/FORM&gt; list.jsp 123456789&lt;SCRIPT language=javascript&gt; function delOne(custId)&#123; var sure = window.confirm(\"确定删除吗？\"); if(sure)&#123; window.location.href = \"$&#123;pageContext.request.contextPath&#125;/customer/deleteCustomer?custId=\"+custId; &#125; &#125;&lt;/SCRIPT&gt;&lt;a href=\"javascript:delOne('$&#123;customer.custId&#125;')\" &gt;删除&lt;/a&gt; 4.1.2 配置xml并编写Actionstruts.xml的配置 12345678910111213141516171819202122232425262728293031&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE struts PUBLIC &quot;-&#x2F;&#x2F;Apache Software Foundation&#x2F;&#x2F;DTD Struts Configuration 2.3&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;struts.apache.org&#x2F;dtds&#x2F;struts-2.3.dtd&quot;&gt;&lt;struts&gt; &lt;!-- 开启开发者模式 --&gt; &lt;constant name&#x3D;&quot;struts.devMode&quot; value&#x3D;&quot;true&quot;&gt;&lt;&#x2F;constant&gt; &lt;!-- 动作配置 --&gt; &lt;package name&#x3D;&quot;customer&quot; extends&#x3D;&quot;struts-default&quot; namespace&#x3D;&quot;&#x2F;customer&quot;&gt; &lt;!-- 查询所有客户 --&gt; &lt;action name&#x3D;&quot;findAllCustomer&quot; class&#x3D;&quot;com.wgy.web.action.CustomerAction&quot; method&#x3D;&quot;findAllCustomer&quot;&gt; &lt;result name&#x3D;&quot;findAllCustomer&quot;&gt;&#x2F;jsp&#x2F;customer&#x2F;list.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;!-- 获取添加客户页面 --&gt; &lt;action name&#x3D;&quot;addUICustomer&quot; class&#x3D;&quot;com.wgy.web.action.CustomerAction&quot; method&#x3D;&quot;addUICustomer&quot;&gt; &lt;result name&#x3D;&quot;addUICustomer&quot;&gt;&#x2F;jsp&#x2F;customer&#x2F;add.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;!-- 添加客户 --&gt; &lt;action name&#x3D;&quot;addCustomer&quot; class&#x3D;&quot;com.wgy.web.action.CustomerAction&quot; method&#x3D;&quot;addCustomer&quot;&gt; &lt;!-- &lt;result name&#x3D;&quot;addCustomer&quot; type&#x3D;&quot;redirect&quot;&gt;&#x2F;jsp&#x2F;success.jsp&lt;&#x2F;result&gt; --&gt; &lt;result name&#x3D;&quot;addCustomer&quot; type&#x3D;&quot;redirect&quot;&gt;findAllCustomer&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;!-- 删除客户 --&gt; &lt;action name&#x3D;&quot;deleteCustomer&quot; class&#x3D;&quot;com.wgy.web.action.CustomerAction&quot; method&#x3D;&quot;deleteCustomer&quot;&gt; &lt;result name&#x3D;&quot;deleteCustomer&quot; type&#x3D;&quot;redirect&quot;&gt;findAllCustomer&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;&#x2F;package&gt;&lt;&#x2F;struts&gt; 动作类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 客户的动作类 * * @author wgy */public class CustomerAction extends ActionSupport implements ModelDriven&lt;Customer&gt; &#123; private ICustomerService customerService = new CustomerServiceImpl(); private Customer customer = new Customer(); @Override public Customer getModel() &#123; return customer; &#125; /** * 查询所有客户 * @return */ public String findAllCustomer()&#123; //1.调用service查询客户 List&lt;Customer&gt; customers = customerService.findAllCustomer(); //2.获取request对象 HttpServletRequest request = ServletActionContext.getRequest(); //3.把查询的结果存入请求域中 request.setAttribute(\"customers\", customers); //4.返回 return \"findAllCustomer\"; &#125; /** * 获取添加客户页面 * @return */ public String addUICustomer()&#123; return \"addUICustomer\"; &#125; /** * 添加客户 * @return */ public String addCustomer()&#123; customerService.saveCustomer(customer); return \"addCustomer\"; &#125; /** * 删除客户 * @return */ public String deleteCustomer()&#123; customerService.deleteCustomer(customer); return \"deleteCustomer\"; &#125;&#125; 4.2 Hibernate4.2.1 编写业务层接口及实现类123456789101112131415161718192021222324252627/** * 客户的业务层接口 * * @author wgy */public interface ICustomerService &#123; /** * 查询所有客户 * * @return */ List&lt;Customer&gt; findAllCustomer(); /** * 添加客户 * * @param customer */ void saveCustomer(Customer customer); /** * 删除客户 * @param customer */ void deleteCustomer(Customer customer);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * 客户的业务层实现类 * 事务控制在业务层的 * * @author wgy */public class CustomerServiceImpl implements ICustomerService &#123; private ICustomerDao customerDao = new CustomerDaoImpl(); @Override public List&lt;Customer&gt; findAllCustomer() &#123; Session s = null; Transaction tx = null; try&#123; //1.获取Session s = HibernateUtil.getCurrentSession(); //2.开启事务 tx = s.beginTransaction(); //3.执行操作 List&lt;Customer&gt; customers = customerDao.findAllCustomer(); //4.提交事务 tx.commit(); //5.返回结果 return customers; &#125;catch(Exception e)&#123; //回滚事务 tx.rollback(); throw new RuntimeException(e); &#125; &#125; @Override public void saveCustomer(Customer customer) &#123; Session s = null; Transaction tx = null; try&#123; //1.获取Session s = HibernateUtil.getCurrentSession(); //2.开启事务 tx = s.beginTransaction(); //3.执行操作 customerDao.saveCustomer(customer); //4.提交事务 tx.commit(); &#125;catch(Exception e)&#123; //回滚事务 tx.rollback(); throw new RuntimeException(e); &#125; &#125; @Override public void deleteCustomer(Customer customer) &#123; Session s = null; Transaction tx = null; try&#123; //1.获取Session s = HibernateUtil.getCurrentSession(); //2.开启事务 tx = s.beginTransaction(); //3.执行操作 customerDao.deleteCustomer(customer); //4.提交事务 tx.commit(); &#125;catch(Exception e)&#123; //回滚事务 tx.rollback(); throw new RuntimeException(e); &#125; &#125;&#125; 4.2.2 编写持久层接口及实现类123456789101112131415161718192021222324252627282930313233/** * 客户的持久层接口 * * @author wgy */public interface ICustomerDao &#123; /** * 查询所有客户 * * @return */ List&lt;Customer&gt; findAllCustomer(); /** * 添加客户 * @param customer */ void saveCustomer(Customer customer); /** * 删除客户 * @param customer */ void deleteCustomer(Customer customer); /** * 根据id查询客户 * @param custID * @return */ Customer findCustomerById(Long custID);&#125; 123456789101112131415161718192021222324252627/** * 客户的持久层实现类 * * @author wgy */public class CustomerDaoImpl implements ICustomerDao &#123; @Override public List&lt;Customer&gt; findAllCustomer() &#123; return HibernateUtil.getCurrentSession().createQuery(\"from Customer\").list(); &#125; @Override public void saveCustomer(Customer customer) &#123; HibernateUtil.getCurrentSession().save(customer); &#125; @Override public void deleteCustomer(Customer customer) &#123; HibernateUtil.getCurrentSession().delete(findCustomerById(customer.getCustId())); &#125; @Override public Customer findCustomerById(Long custID) &#123; return HibernateUtil.getCurrentSession().get(Customer.class,custID); &#125;&#125; 5. 请求参数封装失败后处理办法5.1 配置input结果视图视图路径应该是从哪来回哪去 1234&lt;action name&#x3D;&quot;demo3&quot; class&#x3D;&quot;com.wgy.web.action.Demo3Action&quot; method&#x3D;&quot;demo3&quot;&gt; &lt;result&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;result name&#x3D;&quot;input&quot;&gt;&#x2F;user.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 5.2 提示错误信息引入struts2标签库 1&lt;%@ taglib uri=\"/struts-tags\" prefix=\"s\" %&gt; 1234567891011&lt;%--请求参数封装：类型转换失败的处理方式 --%&gt;&lt;form action=\"$&#123;pageContext.request.contextPath&#125;/demo3.action\" method=\"post\"&gt; 姓名：&lt;input type=\"text\" name=\"username\"/&gt;&lt;s:fielderror fieldName=\"username\"/&gt;&lt;br/&gt; 年龄：&lt;input type=\"text\" name=\"age\"/&gt;&lt;s:fielderror fieldName=\"age\"/&gt;&lt;br/&gt; 生日：&lt;input type=\"text\" name=\"birthday\"/&gt;&lt;s:fielderror fieldName=\"birthday\"/&gt;&lt;br/&gt; 爱好：&lt;input type=\"checkbox\" name=\"hobby\" value=\"吃饭\"/&gt;吃饭 &lt;input type=\"checkbox\" name=\"hobby\" value=\"睡觉\"/&gt;睡觉 &lt;input type=\"checkbox\" name=\"hobby\" value=\"写代码\"/&gt;写代码 &lt;s:fielderror fieldName=\"hobby\"/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"提交\"/&gt;&lt;/form&gt; 5.3 把提交的数据回显回来html标签和struts2标签都可以实现 123456&lt;s:form action=\"demo3\"&gt; &lt;s:textfield name=\"username\" label=\"姓名\"/&gt; &lt;s:textfield name=\"age\" label=\"年龄\"/&gt; &lt;s:textfield name=\"birthday\" label=\"生日\"/&gt; &lt;s:submit value=\"提交\"/&gt;&lt;/s:form&gt; 5.4 关于中文提示的问题I18N ： 国际化 Internationalization 同实体类创建.properties文件 1invalid.fieldvalue.birthday=请输入正确的日期格式。正确的格式是：yyyy-MM-dd","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Struts2","slug":"Struts2","permalink":"https://wgy1993.gitee.io/tags/Struts2/"}]},{"title":"Struts2(一)","date":"2020-06-04T15:33:21.000Z","path":"archives/90bb5282.html","text":"1. Struts2基本概念1.1 三层架构和三大框架各自的位置1.1.1 三层架构我们的开发架构一般都是基于两种形式，一种是C/S架构，也就是客户端/服务器，另一种是B/S架构，也就是浏览器/服务器。在JavaEE开发中，几乎全都是基于B/S架构的开发。那么在B/S架构中，系统标准的三层架构包括：表现层、业务层、持久层。三层架构在我们的实际开发中使用的非常多。 三层架构中，每一层各司其职，接下来我们就说说每层都负责哪些方面： 表现层： 也就是我们常说的web层。它负责接收客户端请求，向客户端响应结果，通常客户端使用http协议请求web层，web需要接收http请求，完成http响应。 表现层包括展示层和控制层：控制层负责接收请求，展示层负责结果的展示。 表现层依赖业务层，接收到客户端请求一般会调用业务层进行业务处理，并将处理结果响应给客户端。 表现层的设计一般都使用MVC模型。（MVC是表现层的设计模型，和其他层没有关系） 业务层： 也就是我们常说的service层。它负责业务逻辑处理，和我们开发项目的需求息息相关。web层依赖业务层，但是业务层不依赖web层。 业务层在业务处理时可能会依赖持久层，如果要对数据持久化需要保证事务一致性。（也就是我们说的，事务应该放到业务层来控制） 持久层： 也就是我们是常说的dao层。负责数据持久化，包括数据层即数据库和数据访问层，数据库是对数据进行持久化的载体，数据访问层是业务层和持久层交互的接口，业务层需要通过数据访问层将数据持久化到数据库中。 通俗的讲，持久层就是和数据库交互，对数据库表进行曾删改查的。 1.1.2 三大框架和三层架构的关系 1.2 Struts2概述Struts2是一种基于MVC模式的轻量级Web框架，它自问世以来，就受到了广大Web开发者的关注，并广泛应用于各种企业系统的开发中。目前掌握Struts2框架几乎成为Web开发者的必备技能之一。 在介绍Struts2之前，先来认识一下Struts1。Struts1是最早的基于MVC模式的轻量级Web框架，它能够合理的划分代码结构，并包含验证框架、国际化框架等多种实用工具框架。但是随着技术的进步，Struts1的局限性也越来越多的暴露出来。为了符合更加灵活、高效的开发需求，Struts2框架应运而生。 Struts2是Struts1的下一代产品，是在 Struts1和WebWork技术的基础上进行合并后的全新框架（WebWork是由OpenSymphony组织开发的，致力于组件化和代码重用的J2EE Web框架，它也是一个MVC框架）。虽然Struts2的名字与Struts1相似，但其设计思想却有很大不同。实质上，Struts2是以WebWork为核心的，它采用拦截器的机制来处理用户的请求。这样的设计也使得业务逻辑控制器能够与ServletAPI完全脱离开，所以Struts2可以理解为WebWork的更新产品。 Struts2拥有优良的设计和功能，其优势具体如下： 项目开源，使用及拓展方便，天生优势。 提供Exception处理机制。 Result方式的页面导航，通过Result标签很方便的实现重定向和页面跳转。 通过简单、集中的配置来调度业务类，使得配置和修改都非常容易。 提供简单、统一的表达式语言来访问所有可供访问的数据。 提供标准、强大的验证框架和国际化框架。 提供强大的、可以有效减少页面代码的标签。 提供良好的Ajax支持。 拥有简单的插件，只需放入相应的JAR包，任何人都可以扩展Struts2框架，比如自定义拦截器、自定义结果类型、自定义标签等，为Struts2定制需要的功能，不需要什么特殊配置，并且可以发布给其他人使用。 拥有智能的默认设置，不需要另外进行繁琐的设置。使用默认设置就可以完成大多数项目程序开发所需要的功能。 2. Struts2的入门2.1 Struts2环境搭建2.1.1 下载struts2开发包Struts2的官网: https://struts.apache.org/ 2.1.2 Struts2开发包目录介绍解压后的目录结构如下： 2.1.3 搭建步骤2.1.3.1 第一步：拷贝struts2必备jar包到web工程的lib目录要进行struts2的基本的开发，可以参考struts-2.3.24中的apps下的一些示例代码，其中struts2-blank.war是一个struts2的空的工程。我们只需要将struts2-blank.war解压后进入到WEB-INF下的lib中查看。 2.1.3.2 第二步：在类的根路径下创建一个名称为struts.xml的文件，并导入约束在开发中需要将struts.xml文件引入到工程的src下，因为src下内容发布到web服务器中就是WEB-INF下的classes中。 12345678910&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!-- 导入约束： 约束的位置：在struts2的核心jar包中 struts2-core-2.3.24.jar中包含一个名称为： struts-2.3.dtd的约束文件--&gt;&lt;!DOCTYPE struts PUBLIC &quot;-&#x2F;&#x2F;Apache Software Foundation&#x2F;&#x2F;DTD Struts Configuration 2.3&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;struts.apache.org&#x2F;dtds&#x2F;struts-2.3.dtd&quot;&gt;&lt;struts&gt;&lt;&#x2F;struts&gt; 2.1.3.3 第三步：在web.xml配置struts2的核心控制器Struts2框架要想执行，所有的请求都需要经过这个前端控制器（核心过滤器） 12345678910111213141516&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;web-app xmlns&#x3D;&quot;http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee&#x2F;web-app_4_0.xsd&quot; version&#x3D;&quot;4.0&quot;&gt; &lt;!-- 配置struts2的核心过滤器 --&gt; &lt;filter&gt; &lt;filter-name&gt;struts2&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;&#x2F;filter-class&gt; &lt;&#x2F;filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt;&lt;&#x2F;web-app&gt; 2.1.3.4 验证搭建成功与否把应用部署到tomcat中，启动tomcat，不报异常则表示搭建成功。 2.2 Struts2入门案例2.2.1 案例需求通过点击超链接发送请求，由Struts2中类来负责接收，并且在控制台输出接收到了的语句。 2.2.2 案例实现2.2.2.1 第一步：编写index.jsp 1234567891011121314&lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\" %&gt;&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; &lt;title&gt;struts2的入门案例&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;%--struts2的核心控制默认会处理以.action为后缀的url，或者是没有任何后缀的url --%&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/hello\"&gt;访问第一个struts2应用&lt;/a&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/hello.action\"&gt;访问第一个struts2应用.action&lt;/a&gt;&lt;/body&gt;&lt;/html&gt; 2.2.2.2 第二步：编写Class12345678910111213141516171819202122232425/** * 我们的第一个动作类。 * 动作类： * 它就是一个概念。它就是struts2框架中用于处理请求的类。 * 我们以后处理请求都写动作类。 * * @author wgy */public class HelloAction &#123; /** * 我们的第一个动作方法 * 动作方法： * 动作类中用于处理请求的方法 * 动作方法有编写规范： * 1、访问修饰符都是public * 2、方法的返回值一般都是String(但是可以是void) * 3、方法都没有参数 * @return */ public String sayHello()&#123; System.out.println(\"HelloAction的sayHello方法执行了。。。。\"+this); return \"success\"; &#125;&#125; 2.2.2.3 第三步：在配置文件中配置我们的动作类123456789101112&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE struts PUBLIC &quot;-&#x2F;&#x2F;Apache Software Foundation&#x2F;&#x2F;DTD Struts Configuration 2.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;struts.apache.org&#x2F;dtds&#x2F;struts-2.0.dtd&quot;&gt;&lt;struts&gt; &lt;!-- 配置文件 --&gt; &lt;package name&#x3D;&quot;p1&quot; extends&#x3D;&quot;struts-default&quot;&gt; &lt;action name&#x3D;&quot;hello&quot; class&#x3D;&quot;com.wgy.web.action.HelloAction&quot; method&#x3D;&quot;sayHello&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;&#x2F;package&gt;&lt;&#x2F;struts&gt; 2.2.2.4 第四步：编写success.jsp1234567891011&lt;%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\" %&gt;&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\"&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt; &lt;title&gt;执行结果页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt;执行成功！&lt;/body&gt;&lt;/html&gt; 2.2.2.5 第五步：启动tomcat测试访问结果2.3 Struts2的执行过程2.3.1 执行时序首先是，启动tomcat服务器，这时候会加载web.xml，当读到filter标签时，会创建过滤器对象。 Struts2的核心过滤器（StrutsPrepareAndExecuteFilter）会负责加载类路径下的struts.xml配置文件。 接下来，从客户端发送请求过来 先经过前端控制器（核心过滤器StrutsPrepareAndExecuteFilter），前端控制器会根据请求的名称在struts.xml中找到对应的配置，创建我们的动作类对象（每次访问时都会创建新的Action对象），然后执行指定的方法，根据方法的返回值找到Result的配置进行页面的跳转.最后响应浏览器。 2.3.2 内部执行流程Struts2框架在默认情况下核心控制器（StrutsPrepareAndExecuteFilter）默认会拦截以.action为后缀的请求，或者是没有任何后缀的请求。当拦截下来后，送入Struts2的核心内部。如下图所示： 我们通过上面的图解，应该明确在实际开发中我们用struts2要做哪些： 写动作类 写jsp 写配置文件 其中，又以配置文件为重。 3. Struts2的配置文件详解3.1 Struts2中的配置文件3.1.1 配置文件说明在struts2中给我们提供了6个配置文件。他们的加载时机是tomcat启动服务一加载我们的应用时，就加载struts2的配置文件。 他们的加载顺序入如下： 顺序 配置文件名 所在位置 说明 1 default.properties struts2-core-2.3.15.3.jar\\org\\apache\\struts2 不能修改 2 struts-default.xml struts2-core-2.3.15.3.jar 不能修改 3 strtuts-plugin.xml 在struts2提供的插件jar包中 不能修改 4 struts.xml 我们的应用中 我们修改的：推荐 5 struts.properties 我们的应用中 我们修改的 6 web.xml 我们的应用中 我们修改的，可以给过滤器配置参数 3.1.2 配置文件的注意事项 Struts2提供了两种配置的方式。一种是key=value的方式，即使用.properties文件。另一种是xml文件配置。我们推荐使用xml文件（它能描述层级关系）。 当多个配置文件中，有相同的参数，后加载的会把前面的值给覆盖了。 3.1.3 Struts2中的常用常量常量定义在了default.properties配置文件中，体现形式都是key=value。所有的struts2应用都会用到这些常量。 常用的： 常量名 常量值 说明 struts.i18n.encoding UTF-8 应用中使用的编码 struts.objectFactory.spring.autoWire name 和spring框架整合有关 struts.multipart.parser jakarta 指定文件上传用的组件 struts.multipart.maxSize 2097152 文件上传总文件大小限制：2M struts.action.extension action,, 能进入Struts2框架内部的url地址后缀名。多个值用逗号分隔 struts.enable.DynamicMethodInvocation false 是否允许动态方法调用 struts.devMode false 是否是开发模式。开发模式：改了配置文件，不需要重启。输出更多的错误信息。开发阶段建议为true struts.ui.theme xhtml 页面展示用的主题 3.2 Struts.xml中的标签详解3.2.1 constant标签123456789&lt;!-- constant标签： 作用： 用于修改struts2中的常量 属性： name：指定常量的key value：指定常量的值--&gt;&lt;!-- 开启开发者模式 --&gt;&lt;constant name&#x3D;&quot;struts.devMode&quot; value&#x3D;&quot;true&quot;&gt;&lt;&#x2F;constant&gt; 3.2.2 package标签1234567891011121314151617181920&lt;!-- package标签： 作用：给访问的action进行分包管理。把配置文件按照面向对象的思想来管理。 属性： name：指定包的名称。必须写，并且必须唯一。 extends：指定当前包的父包。子包自动具备父包所定义的配置。我们的包一般都需要继承struts-default包。 该包在struts-defaul.xml文件中定义着。如果不继承该包，则不能使用struts2的核心功能。 abstract：把当前包声明为抽象包。抽象包就是用来被继承的。里面定义一般都是公共的配置。 只有没有action标签的包，才能定义为抽象包。 namespace：指定当前包的名称空间。它可以让我们的访问URL模块化。当我们指定了该属性，访问URL就变成了：名称空间+&#x2F;hello 名称空间的写法：第一个字符必须是&#x2F;，后面紧跟的字符必须是一个字母。其余内容可以是字母，也可以是数字。 例如：我们访问用户 &#x2F;user&#x2F;addUser.action &#x2F;user&#x2F;updateUser.action 名称空间有默认值。默认值是：&quot;&quot;--&gt;&lt;package name&#x3D;&quot;p1&quot; extends&#x3D;&quot;struts-default&quot; namespace&#x3D;&quot;&#x2F;n1&quot;&gt; &lt;action name&#x3D;&quot;hello&quot; class&#x3D;&quot;com.wgy.web.action.HelloAction&quot; method&#x3D;&quot;sayHello&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt;&lt;&#x2F;package&gt; 3.2.3 action标签12345678910&lt;!-- action标签： 作用：建立动作名称，动作类和动作方法的对应关系 属性： name:指定动作名称。它是唯一的 class:指定动作类的全限定类名 method：指定动作方法名称--&gt;&lt;action name&#x3D;&quot;hello&quot; class&#x3D;&quot;com.wgy.web.action.HelloAction&quot; method&#x3D;&quot;sayHello&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3.3 action的三种创建方式3.3.1 第一种：无侵入式创建(实际开发中基本不用)12345678910111213/** * 我们的第一个动作类 * 动作类的第一种创建方式： * 无侵入式的创建。 */public class HelloAction &#123; public String sayHello()&#123; System.out.println(this); System.out.println(\"HelloAction中的sayHello方法执行了。。。。\"); return \"success\"; &#125;&#125; 123&lt;action name&#x3D;&quot;hello&quot; class&#x3D;&quot;com.wgy.web.action.HelloAction&quot; method&#x3D;&quot;sayHello&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3.3.2 第二种：实现Action接口的定义方式 (实际开发中用的也不多)12345678910111213/** * 通过实现接口的方式创建动作类 * * @author wgy */public class Hello2Action implements Action &#123; @Override public String execute() throws Exception &#123; System.out.println(\"Hello2Action的execute方法执行了。。。。\"); return SUCCESS; &#125;&#125; 1234&lt;!-- 默认动作方法：当我们要是执行的是execute方法时，method属性可以不写。 --&gt;&lt;action name&#x3D;&quot;hello2&quot; class&#x3D;&quot;com.wgy.web.action.Hello2Action&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; Action接口中的常量 SUCCESS：一般多用于成功 ERROR：一般多用于动作方法执行失败 LOGIN：一般多用于返回登录页面 NONE：一般用于不返回任何结果视图，和return null作用是一样的 INPUT：一般多用于数据回显，也是struts2中数据回显时的默认返回值。 3.3.3 第三种：继承ActionSupport (实际开发中采用的方式)12345678/** * 通过继承ActionSupport的方式创建动作类 * * @author wgy */public class Hello3Action extends ActionSupport &#123;&#125; 123&lt;action name&#x3D;&quot;hello3&quot; class&#x3D;&quot;com.wgy.web.action.Hello3Action&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3.3.4 默认动作类：ActionSupport类1234&lt;!-- 通过struts-default.xml中我们知道默认的动作类是ActionSupport，所以如果实现该类的execute方法，则可以不用指定class和method属性 --&gt;&lt;action name&#x3D;&quot;hello4&quot;&gt; &lt;result name&#x3D;&quot;success&quot; type&#x3D;&quot;dispatcher&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt;&lt;&#x2F;action&gt; 3.4 action的三种访问方式12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 用户的动作类 * * @author wgy */public class UserAction extends ActionSupport &#123; /** * 保存 * @return */ public String addUser() &#123; System.out.println(\"保存了用户\"); return SUCCESS; &#125; /** * 更新 * @return */ public String updateUser() &#123; System.out.println(\"更新了用户\"); return SUCCESS; &#125; /** * 查询 * @return */ public String deleteUser() &#123; System.out.println(\"删除了用户\"); return SUCCESS; &#125; /** * 删除 * @return */ public String findUser() &#123; System.out.println(\"查询了用户\"); return SUCCESS; &#125;&#125; 3.4.1 第一种：全匹配配置访问方式1234567&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/addUser\"&gt;添加用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/updateUser\"&gt;更新用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/deleteUser\"&gt;删除用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/findUser\"&gt;查询用户&lt;/a&gt; 1234567891011121314&lt;package name&#x3D;&quot;user&quot; extends&#x3D;&quot;struts-default&quot;&gt; &lt;action name&#x3D;&quot;addUser&quot; class&#x3D;&quot;com.wgy.web.action.UserAction&quot; method&#x3D;&quot;addUser&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;action name&#x3D;&quot;updateUser&quot; class&#x3D;&quot;com.wgy.web.action.UserAction&quot; method&#x3D;&quot;updateUser&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;action name&#x3D;&quot;deleteUser&quot; class&#x3D;&quot;com.wgy.web.action.UserAction&quot; method&#x3D;&quot;deleteUser&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;action name&#x3D;&quot;findUser&quot; class&#x3D;&quot;com.wgy.web.action.UserAction&quot; method&#x3D;&quot;findUser&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt;&lt;&#x2F;package&gt; 3.4.2 第二种：使用通配符的方式* 通配符基本用法 1234567&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/addUser\"&gt;添加用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/updateUser\"&gt;更新用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/deleteUser\"&gt;删除用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/findUser\"&gt;查询用户&lt;/a&gt; 123456&lt;!-- 通配符基本用法 --&gt;&lt;package name&#x3D;&quot;user&quot; extends&#x3D;&quot;struts-default&quot;&gt; &lt;action name&#x3D;&quot;*&quot; class&#x3D;&quot;com.wgy.web.action.UserAction&quot; method&#x3D;&quot;&#123;1&#125;&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt;&lt;&#x2F;package&gt; 通配符的高级用法 1234567&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/add_User\"&gt;添加用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/update_User\"&gt;更新用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/delete_User\"&gt;删除用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/find_User\"&gt;查询用户&lt;/a&gt; 123456&lt;!-- 通配符的高级用法 --&gt;&lt;package name&#x3D;&quot;user&quot; extends&#x3D;&quot;struts-default&quot;&gt; &lt;action name&#x3D;&quot;*_*&quot; class&#x3D;&quot;com.wgy.web.action.&#123;2&#125;Action&quot; method&#x3D;&quot;&#123;1&#125;&#123;2&#125;&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt;&lt;&#x2F;package&gt; 3.4.3 第三种：使用动态方法调用的方式1234567&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/user!addUser\"&gt;添加用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/user!updateUser\"&gt;更新用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/user!deleteUser\"&gt;删除用户&lt;/a&gt;&lt;hr/&gt;&lt;a href=\"$&#123;pageContext.request.contextPath&#125;/user!findUser\"&gt;查询用户&lt;/a&gt; 1234567&lt;!-- 使用动态方法调用的方式 --&gt;&lt;constant name&#x3D;&quot;struts.enable.DynamicMethodInvocation&quot; value&#x3D;&quot;true&quot;&#x2F;&gt;&lt;package name&#x3D;&quot;user&quot; extends&#x3D;&quot;struts-default&quot;&gt; &lt;action name&#x3D;&quot;user&quot; class&#x3D;&quot;com.wgy.web.action.UserAction&quot;&gt; &lt;result name&#x3D;&quot;success&quot;&gt;&#x2F;success.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt;&lt;&#x2F;package&gt; 4. 案例：Strut2+Hibernate查询客户列表4.1 第一步：搭建hibernate开发环境并准备实体类和映射配置可参考Hibernate环境搭建。 Hibernate开发环境 1234567891011121314151617181920212223242526272829303132&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE hibernate-configuration PUBLIC &quot;-&#x2F;&#x2F;Hibernate&#x2F;Hibernate Configuration DTD 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.hibernate.org&#x2F;dtd&#x2F;hibernate-configuration-3.0.dtd&quot;&gt;&lt;hibernate-configuration&gt; &lt;!-- 配置SessionFactory--&gt; &lt;session-factory&gt; &lt;!-- 第一部分：连接数据库的信息 --&gt; &lt;property name&#x3D;&quot;hibernate.connection.driver_class&quot;&gt;com.mysql.jdbc.Driver&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.url&quot;&gt;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;struts2&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.username&quot;&gt;root&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.password&quot;&gt;root&lt;&#x2F;property&gt; &lt;!-- 数据库的方言 --&gt; &lt;property name&#x3D;&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.MySQLDialect&lt;&#x2F;property&gt; &lt;!-- 第二部分：hibernate的可选配置 --&gt; &lt;!-- 是否显示hibernate生成的SQL语句 --&gt; &lt;property name&#x3D;&quot;hibernate.show_sql&quot;&gt;true&lt;&#x2F;property&gt; &lt;!-- 是否使用格式化输出sql语句到控制台 --&gt; &lt;property name&#x3D;&quot;hibernate.format_sql&quot;&gt;false&lt;&#x2F;property&gt; &lt;!-- 配置hibernate采用何种方式生成DDL语句 --&gt; &lt;!-- update表示检测实体类的映射配置和数据库的表结构是否一致，如果不一致，更新表结构 --&gt; &lt;property name&#x3D;&quot;hibernate.hbm2ddl.auto&quot;&gt;update&lt;&#x2F;property&gt; &lt;!-- 设置hibernate的连接池提供商 --&gt; &lt;property name&#x3D;&quot;hibernate.connection.provider_class&quot;&gt;org.hibernate.c3p0.internal.C3P0ConnectionProvider&lt;&#x2F;property&gt; &lt;!-- 把session和线程绑定，从而实现一个线程只有一个Session --&gt; &lt;property name&#x3D;&quot;hibernate.current_session_context_class&quot;&gt;thread&lt;&#x2F;property&gt; &lt;!-- 第三部分：映射配置文件的位置 --&gt; &lt;mapping class&#x3D;&quot;com.wgy.domain.Customer&quot;&#x2F;&gt; &lt;&#x2F;session-factory&gt;&lt;&#x2F;hibernate-configuration&gt; 实体类和映射配置 123456789101112131415161718192021222324252627282930313233/** * 客户的实体类 * * @author wgy */@Entity@Table(name = \"cst_customer\")public class Customer implements Serializable &#123; @Id @Column(name = \"cust_id\") @GeneratedValue(strategy = GenerationType.IDENTITY) private Long custId; @Column(name = \"cust_name\") private String custName; @Column(name = \"cust_source\") private String custSource; @Column(name = \"cust_industry\") private String custIndustry; @Column(name = \"cust_level\") private String custLevel; @Column(name = \"cust_address\") private String custAddress; @Column(name = \"cust_phone\") private String custPhone; ...&#125; 4.2 第二步：搭建struts2环境参考2.1.3章节的步骤去做。 4.3 第三步：导入crm的jsp页面把下图中红框内文件夹下所有内容都拷贝到WebContent目录中 4.4 第四步：修改menu.jsp12345&lt;TR&gt; &lt;TD class=menuSmall&gt; &lt;A class=style2 href=\"$&#123;pageContext.request.contextPath&#125;/customer/findAllCustomer.action\" target=main&gt;－ 客户列表&lt;/A&gt; &lt;/TD&gt;&lt;/TR&gt; 4.5 第五步：在配置文件中配置findAllCustomer.action1234567891011121314&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE struts PUBLIC &quot;-&#x2F;&#x2F;Apache Software Foundation&#x2F;&#x2F;DTD Struts Configuration 2.3&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;struts.apache.org&#x2F;dtds&#x2F;struts-2.3.dtd&quot;&gt;&lt;struts&gt; &lt;!-- 开启开发者模式 --&gt; &lt;constant name&#x3D;&quot;struts.devMode&quot; value&#x3D;&quot;true&quot;&gt;&lt;&#x2F;constant&gt; &lt;!-- 动作配置 --&gt; &lt;package name&#x3D;&quot;customer&quot; extends&#x3D;&quot;struts-default&quot; namespace&#x3D;&quot;&#x2F;customer&quot;&gt; &lt;action name&#x3D;&quot;findAllCustomer&quot; class&#x3D;&quot;com.wgy.web.action.CustomerAction&quot; method&#x3D;&quot;findAllCustomer&quot;&gt; &lt;result name&#x3D;&quot;findAllCustomer&quot;&gt;&#x2F;jsp&#x2F;customer&#x2F;list.jsp&lt;&#x2F;result&gt; &lt;&#x2F;action&gt; &lt;&#x2F;package&gt;&lt;&#x2F;struts&gt; 4.6 第六步：创建动作类并提供动作方法123456789101112131415161718192021222324/** * 客户的动作类 * * @author wgy */public class CustomerAction extends ActionSupport &#123; private ICustomerService customerService = new CustomerServiceImpl(); /** * 查询所有客户 * @return */ public String findAllCustomer()&#123; //1.调用service查询客户 List&lt;Customer&gt; customers = customerService.findAllCustomer(); //2.获取request对象 HttpServletRequest request = ServletActionContext.getRequest(); //3.把查询的结果存入请求域中 request.setAttribute(\"customers\", customers); //4.返回 return \"findAllCustomer\"; &#125;&#125; 4.7 第七步：编写service接口和实现类1234567891011121314/** * 客户的业务层接口 * * @author wgy */public interface ICustomerService &#123; /** * 查询所有客户 * * @return */ List&lt;Customer&gt; findAllCustomer();&#125; 1234567891011121314151617181920212223242526272829303132/** * 客户的业务层实现类 * 事务控制在业务层的 * * @author wgy */public class CustomerServiceImpl implements ICustomerService &#123; private ICustomerDao customerDao = new CustomerDaoImpl(); @Override public List&lt;Customer&gt; findAllCustomer() &#123; Session s = null; Transaction tx = null; try&#123; //1.获取Session s = HibernateUtil.getCurrentSession(); //2.开启事务 tx = s.beginTransaction(); //3.执行操作 List&lt;Customer&gt; customers = customerDao.findAllCustomer(); //4.提交事务 tx.commit(); //5.返回结果 return customers; &#125;catch(Exception e)&#123; //回滚事务 tx.rollback(); throw new RuntimeException(e); &#125; &#125;&#125; 4.8 第八步：编写dao接口和实现类1234567891011121314/** * 客户的持久层接口 * * @author wgy */public interface ICustomerDao &#123; /** * 查询所有客户 * * @return */ List&lt;Customer&gt; findAllCustomer();&#125; 123456789101112/** * 客户的持久层实现类 * * @author wgy */public class CustomerDaoImpl implements ICustomerDao &#123; @Override public List&lt;Customer&gt; findAllCustomer() &#123; return HibernateUtil.getCurrentSession().createQuery(\"from Customer\").list(); &#125;&#125; 4.9 第九步：页面展示jsp/customer/list.jsp12345678910&lt;c:forEach items=\"$&#123;customers&#125;\" var=\"customer\"&gt; &lt;TR&gt; &lt;TD&gt;$&#123;customer.custName &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custLevel &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custSource &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custIndustry &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custAddress &#125;&lt;/TD&gt; &lt;TD&gt;$&#123;customer.custPhone &#125;&lt;/TD&gt; &lt;/TR&gt; &lt;/c:forEach&gt;","tags":[{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"},{"name":"Struts2","slug":"Struts2","permalink":"https://wgy1993.gitee.io/tags/Struts2/"}]},{"title":"Hibernate(四)","date":"2020-06-03T14:33:02.000Z","path":"archives/c4c4045f.html","text":"1. JPA相关概念1.1 JPA概述全称是：Java Persistence API。是SUN公司推出的一套基于ORM的规范。Hibernate框架中提供了JPA的实现。 JPA通过JDK 5.0注解或XML描述对象—关系表的映射关系，并将运行期的实体对象持久化到数据库中。 1.2 JPA的优势 标准化 JPA 是 JCP 组织发布的 Java EE 标准之一，因此任何声称符合 JPA 标准的框架都遵循同样的架构，提供相同的访问API，这保证了基于JPA开发的企业应用能够经过少量的修改就能够在不同的JPA框架下运行。 容器级特性的支持 JPA框架中支持大数据集、事务、并发等容器级事务，这使得 JPA 超越了简单持久化框架的局限，在企业应用发挥更大的作用。 简单方便 JPA的主要目标之一就是提供更加简单的编程模型：在JPA框架下创建实体和创建Java 类一样简单，没有任何的约束和限制，只需要使用 javax.persistence.Entity进行注释，JPA的框架和接口也都非常简单，没有太多特别的规则和设计模式的要求，开发者可以很容易的掌握。JPA基于非侵入式原则设计，因此可以很容易的和其它框架或者容器集成。 查询能力 JPA的查询语言是面向对象而非面向数据库的，它以面向对象的自然语法构造查询语句，可以看成是Hibernate HQL的等价物。JPA定义了独特的JPQL（Java Persistence Query Language），JPQL是EJB QL的一种扩展，它是针对实体的一种查询语言，操作对象是实体，而不是关系数据库的表，而且能够支持批量更新和修改、JOIN、GROUP BY、HAVING 等通常只有 SQL 才能够提供的高级查询特性，甚至还能够支持子查询。 高级特性 JPA 中能够支持面向对象的高级特性，如类之间的继承、多态和类之间的复杂关系，这样的支持能够让开发者最大限度的使用面向对象的模型设计企业应用，而不需要自行处理这些特性在关系数据库的持久化。 1.3 学习JPA要明确的 JPA是一套ORM规范，Hibernate实现了JPA规范 hibernate中有自己的独立ORM操作数据库方式，也有JPA规范实现的操作数据库方式。 2. JPA入门2.1 需求介绍本章节我们实现基于JPA注解的对象关系映射，配置实体类和数据库表的对应关系。并且使用JPA规范中的方法实现CRUD操作。 2.2 JPA环境搭建2.2.1 第一步：拷贝jar包 2.2.2 第二步：创建配置文件在src下面的META-INF文件夹下面创建一个名称为persistence.xml的文件。 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;persistence xmlns&#x3D;&quot;http:&#x2F;&#x2F;java.sun.com&#x2F;xml&#x2F;ns&#x2F;persistence&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;java.sun.com&#x2F;xml&#x2F;ns&#x2F;persistence http:&#x2F;&#x2F;java.sun.com&#x2F;xml&#x2F;ns&#x2F;persistence&#x2F;persistence_2_0.xsd&quot; version&#x3D;&quot;2.0&quot;&gt; &lt;!-- 配置持久化单元，可以配置多个，但是名称不能重复 name:用于指定持久化单元名称 transaction-type:指定事务的类型。 JTA：Java Transaction API RESOURCE_LOCAL:指的是本地代码事务。（我们用这个） --&gt; &lt;persistence-unit name&#x3D;&quot;myJPAUnit&quot; transaction-type&#x3D;&quot;RESOURCE_LOCAL&quot;&gt; &lt;!-- JPA规范的提供商 可以不写。--&gt; &lt;provider&gt;org.hibernate.jpa.HibernatePersistenceProvider&lt;&#x2F;provider&gt; &lt;!-- 指定由Jpa注解的实体类位置 可以不写。--&gt; &lt;class&gt;com.wgy.domain.Customer&lt;&#x2F;class&gt; &lt;!-- 连接库相关的一些配置 --&gt; &lt;properties&gt; &lt;!-- 第一部分：连接数据库的信息 --&gt; &lt;property name&#x3D;&quot;hibernate.connection.driver_class&quot; value&#x3D;&quot;com.mysql.jdbc.Driver&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;hibernate.connection.url&quot; value&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;hibernate01&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;hibernate.connection.username&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;hibernate.connection.password&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;!-- 数据库的方言 --&gt; &lt;property name&#x3D;&quot;hibernate.dialect&quot; value&#x3D;&quot;org.hibernate.dialect.MySQLDialect&quot;&#x2F;&gt; &lt;!-- 第二部分：hibernate的可选配置 --&gt; &lt;!-- 是否显示hibernate生成的SQL语句 --&gt; &lt;property name&#x3D;&quot;hibernate.show_sql&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;!-- 是否使用格式化输出sql语句到控制台 --&gt; &lt;property name&#x3D;&quot;hibernate.format_sql&quot; value&#x3D;&quot;false&quot;&#x2F;&gt; &lt;!-- 配置hibernate采用何种方式生成DDL语句 --&gt; &lt;!-- update表示检测实体类的映射配置和数据库的表结构是否一致，如果不一致，更新表结构 --&gt; &lt;property name&#x3D;&quot;hibernate.hbm2ddl.auto&quot; value&#x3D;&quot;update&quot;&#x2F;&gt; &lt;&#x2F;properties&gt; &lt;&#x2F;persistence-unit&gt;&lt;&#x2F;persistence&gt; 2.2.3 第三步：编写工具类，用于获取JPA的操作数据库对象1234567891011121314151617181920212223/** * JPA的工具类 * * @author wgy */public class JPAUtil &#123; //它就相当于SessionFactory private static EntityManagerFactory factory; static &#123; //注意：该方法参数必须和persistence.xml中persistence-unit标签name属性取值一致 factory = Persistence.createEntityManagerFactory(\"myJPAUnit\"); &#125; /** * 获取EntityManager对象 * * @return */ public static EntityManager createEntityManager() &#123; return em.createEntityManager(); &#125;&#125; 2.2.4 第四步：编写实体类并使用注解配置12345678910111213141516171819202122232425262728293031323334/** * 客户实体类 * 使用的注解都是JPA规范，所以导包，都需要导入javax.persistence包下的 * * @author wgy */@Entity//表明该类是一个实体类@Table(name = \"cst_customer\")//建立当前类和数据库表的对应关系public class Customer implements Serializable &#123; @Id//表明当前字段是主键 @Column(name = \"cust_id\")//表明对应数据库的主键字段是cust_id @GeneratedValue(strategy = GenerationType.IDENTITY)//指定主键生成策略。 private Long custId; @Column(name = \"cust_name\") private String custName; @Column(name = \"cust_source\") private String custSource; @Column(name = \"cust_industry\") private String custIndustry; @Column(name = \"cust_level\") private String custLevel; @Column(name = \"cust_address\") private String custAddress; @Column(name = \"cust_phone\") private String custPhone; ...&#125; 2.3 JPA的CRUD操作2.3.1 保存123456789101112131415161718192021/** * 保存 */@Testpublic void test1() &#123; //创建客户对象 Customer c = new Customer(); c.setCustName(\"JPA Customer\"); //1.获取EntityManager对象 EntityManager em = JPAUtil.createEntityManager(); //2.获取事务对象，并开启事务 EntityTransaction tx = em.getTransaction(); tx.begin(); //3.执行保存操作 em.persist(c); //4.提交事务 tx.commit(); //5.关闭资源 em.close();&#125; 2.3.2 快照更新12345678910111213141516171819/** * 更新操作 */@Testpublic void test3() &#123; //1.获取EntityManager对象 EntityManager em = JPAUtil.createEntityManager(); //2.获取事务对象，并开启事务 EntityTransaction tx = em.getTransaction(); tx.begin(); //3.执行更新操作（需要把更新的对象先查询出来） Customer c = em.find(Customer.class, 1L); //修改客户的地址为：顺义区 c.setCustAddress(\"顺义区\"); //4.提交事务 tx.commit(); //5.关闭资源 em.close();&#125; 2.3.3 merge更新12345678910111213141516171819202122/** * 更新的另一种操作方式 * merge是合并 （两个实体合并） */@Testpublic void test4() &#123; //1.获取EntityManager对象 EntityManager em = JPAUtil.createEntityManager(); //2.获取事务对象，并开启事务 EntityTransaction tx = em.getTransaction(); tx.begin(); //3.执行更新操作（需要把更新的对象先查询出来） Customer c = em.find(Customer.class, 1L); //修改客户的地址为：顺义区 c.setCustAddress(\"北京市顺义区\"); em.merge(c); //4.提交事务 tx.commit(); //5.关闭资源 em.close();&#125; 2.3.4 删除1234567891011121314151617181920/** * 删除操作 */@Testpublic void test5() &#123; //1.获取EntityManager对象 EntityManager em = JPAUtil.createEntityManager(); //2.获取事务对象，并开启事务 EntityTransaction tx = em.getTransaction(); tx.begin(); //3.执行更新操作（需要把更新的对象先查询出来） Customer c = em.find(Customer.class, 1L); //删除操作 em.remove(c); //4.提交事务 tx.commit(); //5.关闭资源 em.close();&#125; 2.3.5 查询一个12345678910111213141516171819202122232425262728293031323334353637/** * 查询一个实体 立即加载 */@Testpublic void test2() &#123; //1.获取EntityManager对象 EntityManager em = JPAUtil.createEntityManager(); //2.获取事务对象，并开启事务 EntityTransaction tx = em.getTransaction(); tx.begin(); //3.执行更新操作（需要把更新的对象先查询出来） Customer c = em.find(Customer.class, 1L); System.out.println(c); //4.提交事务 tx.commit(); //5.关闭资源 em.close();&#125;/** * 查询一个实体 延迟加载 */@Testpublic void test2_1() &#123; //1.获取EntityManager对象 EntityManager em = JPAUtil.createEntityManager(); //2.获取事务对象，并开启事务 EntityTransaction tx = em.getTransaction(); tx.begin(); //3.执行更新操作（需要把更新的对象先查询出来） Customer c = em.getReference(Customer.class, 1L); System.out.println(c); //4.提交事务 tx.commit(); //5.关闭资源 em.close();&#125; 2.3.6 查询所有12345678910111213141516171819202122232425262728293031323334353637/** * 查询所有 * * 涉及的对象是： * JPA的Query * 如何获取该对象： * EntityManager的createQuery(String jpql) * 参数含义： * JPQL：Java Persistence Query Language * 他的写法和HQL很相似。也是把表名换成类名，把字段名换成属性名称 * 它在写查询所有时，不能直接用 from 实体类 * 需要使用select关键字 * select c from Customer c */@Testpublic void test6() &#123; //1.获取EntityManager对象 EntityManager em = JPAUtil.createEntityManager(); //2.获取事务对象，并开启事务 EntityTransaction tx = em.getTransaction(); tx.begin(); //3.获取JPA的查询对象Query Query query = em.createQuery(\"select c from Customer c where custName like ? and custLevel = ? \"); //给占位符赋值 query.setParameter(1, \"%集%\"); query.setParameter(2, \"23\"); //执行方法获取结果集 //getSingleResult()：查询结果是一个对象 List list = query.getResultList(); for (Object o : list) &#123; System.out.println(o); &#125; //4.提交事务 tx.commit(); //5.关闭资源 em.close();&#125; 3. JPA的多表操作3.1 一对多关系配置及操作3.1.1 配置123456789101112131415161718192021222324252627282930313233343536373839404142/** * 客户实体类 * * @author wgy */@Entity@Table(name = \"cst_customer\")public class Customer implements Serializable &#123; @Id @Column(name = \"cust_id\") @GeneratedValue(strategy = GenerationType.IDENTITY) private Long custId; @Column(name = \"cust_name\") private String custName; @Column(name = \"cust_source\") private String custSource; @Column(name = \"cust_industry\") private String custIndustry; @Column(name = \"cust_level\") private String custLevel; @Column(name = \"cust_address\") private String custAddress; @Column(name = \"cust_phone\") private String custPhone; /** * 一对多关系映射：一个客户可以有多个联系人 * mappedBy 取消维护 * cascade 级联保存 * fetch 查询加载时机 */ @OneToMany(targetEntity = LinkMan.class,mappedBy = \"customer\",cascade = CascadeType.ALL,fetch = FetchType.EAGER) private Set&lt;LinkMan&gt; linkmans = new HashSet&lt;LinkMan&gt;(0); ...&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 联系人的实体类 * * @author wgy */@Entity@Table(name = \"cst_linkman\")public class LinkMan implements Serializable &#123; @Id @Column(name = \"lkm_id\") @GeneratedValue(strategy = GenerationType.IDENTITY) private Long lkmId; @Column(name = \"lkm_name\") private String lkmName; @Column(name = \"lkm_gender\") private String lkmGender; @Column(name = \"lkm_phone\") private String lkmPhone; @Column(name = \"lkm_mobile\") private String lkmMobile; @Column(name = \"lkm_email\") private String lkmEmail; @Column(name = \"lkm_position\") private String lkmPosition; @Column(name = \"lkm_memo\") private String lkmMemo; /** * 一对多关系映射，多的一方。 * 从表实体包含主表实体的对象引用 */ @ManyToOne(targetEntity = Customer.class,fetch = FetchType.LAZY) @JoinColumn(name = \"lkm_cust_id\",referencedColumnName = \"cust_id\") private Customer customer; ...&#125; 3.1.2 操作3.1.2.1 保存123456789101112131415161718192021222324/** * 保存操作 * 创建一个客户和一个联系人 * 建立客户和联系人的双向关联关系 * 先保存客户，再保存联系人 */@Testpublic void test1() &#123; Customer c = new Customer(); LinkMan l = new LinkMan(); c.setCustName(\"JPA One To Many Customer\"); l.setLkmName(\"JPA One To Many LinkMan\"); c.getLinkmans().add(l); l.setCustomer(c); EntityManager em = JPAUtil.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); em.persist(c); em.persist(l); tx.commit(); em.close();&#125; 3.1.2.2 更新1234567891011121314151617181920212223/** * 更新操作 * 创建一个联系人 * 查询id为5的客户 * 为5这个客服分配该联系人 * 更新客户 */@Testpublic void test2() &#123; LinkMan l = new LinkMan(); l.setLkmName(\"JPA One To Many LinkMan 2\"); EntityManager em = JPAUtil.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); Customer c = em.find(Customer.class, 5L); c.getLinkmans().add(l); l.setCustomer(c); tx.commit(); em.close();&#125; 3.1.2.3 删除123456789101112131415/** * 删除操作 */@Testpublic void test3() &#123; EntityManager em = JPAUtil.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); Customer c = em.find(Customer.class, 5L); em.remove(c); tx.commit(); em.close();&#125; 3.1.2.4 查询12345678910111213141516171819202122232425262728293031323334353637/** * 根据客户查询客户下的联系人 */@Testpublic void test1() &#123; EntityManager em = JPAUtil.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); //1.查询id为1的客户 Customer c = em.find(Customer.class, 1l); System.out.println(c); //查询当前客户下的联系人 Set&lt;LinkMan&gt; linkmans = c.getLinkmans(); System.out.println(linkmans); tx.commit(); em.close();&#125;/** * 根据联系人，查询联系人的所属客户 */@Testpublic void test2() &#123; EntityManager em = JPAUtil.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); //1.查询id为1的客户 LinkMan l = em.find(LinkMan.class, 1l); System.out.println(l); //查询当前客户下的联系人 Customer c = l.getCustomer(); System.out.println(c); tx.commit(); em.close();&#125; 3.2 多对多关系配置及操作3.2.1 配置12345678910111213141516171819202122232425262728293031/** * 角色的实体类 * * @author wgy */@Entity@Table(name = \"sys_role\")public class SysRole implements Serializable &#123; @Id @Column(name = \"role_id\") @GenericGenerator(name = \"uuid\", strategy = \"uuid\")//声明一个主键生成器 name属性：给生成器起个名字。strategy：指定的就是hibernate中包含的生成策略 @GeneratedValue(generator = \"uuid\") private String roleId; @Column(name = \"role_name\") private String roleName; @Column(name = \"role_memo\") private String roleMemo; //多对多关系映射：一个角色可以赋予多个用户 @ManyToMany(cascade=CascadeType.ALL) //加入一张表 @JoinTable(name=\"user_role_ref\", joinColumns = &#123;@JoinColumn(name=\"role_id\",referencedColumnName=\"role_id\")&#125;,//写的是当前实体在中间表的外键字段 inverseJoinColumns=&#123;@JoinColumn(name=\"user_id\",referencedColumnName=\"user_id\")&#125;//写的是对方实体在中间表的外键字段 ) private Set&lt;SysUser&gt; users = new HashSet&lt;SysUser&gt;(0); ...&#125; 1234567891011121314151617181920212223242526272829/** * 用户的实体类 * * @author wgy */@Entity@Table(name = \"sys_user\")public class SysUser implements Serializable &#123; @Id @Column(name = \"user_id\") @GenericGenerator(name = \"uuid\", strategy = \"uuid\") @GeneratedValue(generator = \"uuid\") private String userId; @Column(name = \"user_name\") private String userName; @Column(name = \"user_password\") private String userPassword; @Column(name = \"user_state\") private Integer userState; //多对多关系映射：一个用户可以具备多个角色 @ManyToMany(mappedBy=\"users\",cascade= CascadeType.ALL) private Set&lt;SysRole&gt; roles = new HashSet&lt;SysRole&gt;(0); ...&#125; 3.2.2 操作3.2.2.1 保存1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 保存操作 * 创建两个用户 * 创建三个角色 * 让1号用户具备1号和2号角色 * 让2号用户具备2号和3号角色 * 保存用户和角色 */@Testpublic void test1() &#123; SysUser u1 = new SysUser(); SysUser u2 = new SysUser(); u1.setUserName(\"JPA Many to Many u1\"); u2.setUserName(\"JPA Many to Many u2\"); SysRole r1 = new SysRole(); SysRole r2 = new SysRole(); SysRole r3 = new SysRole(); r1.setRoleName(\"JPA Many to Many r1\"); r2.setRoleName(\"JPA Many to Many r2\"); r3.setRoleName(\"JPA Many to Many r3\"); //建立用户和角色的关联关系 u1.getRoles().add(r1); u1.getRoles().add(r2); u2.getRoles().add(r2); u2.getRoles().add(r3); r1.getUsers().add(u1); r2.getUsers().add(u1); r2.getUsers().add(u2); r3.getUsers().add(u2); EntityManager em = JPAUtil.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); //保存操作 em.persist(u1); tx.commit(); em.close();&#125; 3.2.2.2 删除123456789101112131415/** * 删除操作 * 双向级联删除，不管是hibernate还是Jpa,多对多中都不能配置 */@Testpublic void test2() &#123; EntityManager em = JPAUtil.createEntityManager(); EntityTransaction tx = em.getTransaction(); tx.begin(); //查询用户 SysUser u1 = em.find(SysUser.class, \"8a7e83cc5fbf19b6015fbf19bada0000\"); em.remove(u1); tx.commit(); em.close();&#125; 4. JPA的其他说明4.1 JPA中使用C3P0连接池4.1.1 第一步：拷贝C3P0所必须的3个jar包 4.1.2 第二步：在persistence.xml配置文件中配置12&lt;!-- 配置使用C3P0数据源 --&gt;&lt;property name&#x3D;&quot;hibernate.connection.provider_class&quot; value&#x3D;&quot;org.hibernate.connection.C3P0ConnectionProvider&quot;&#x2F;&gt; 4.1.3 验证是否配置成功1234567891011121314151617/** * 验证c3p0连接池是否配置成功 */@Testpublic void test1() &#123; //1.获取jpa中的操作对象 EntityManager em = JPAUtil.createEntityManager(); //2. Session session = em.unwrap(Session.class); //3.执行session的doWork方法 session.doWork(new Work() &#123; @Override public void execute(Connection conn) throws SQLException &#123; System.out.println(conn.getClass().getName()); &#125; &#125;);&#125; 12345678@Testpublic void test2() &#123; //1.获取jpa中的操作对象 EntityManager em1 = JPAUtil.createEntityManager(); EntityManager em2 = JPAUtil.createEntityManager(); //false System.out.println(em1 == em2);&#125; 4.2 JPA与Hibernate中操作数据的方法对照 5. Hibernate中使用JPA注解映射配置5.1 编写主配置文件(hibernate.cfg.xml)1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!-- 在类的根路径下创建名称为hibernate.cfg.xml的配置文件导入约束：dtd约束 --&gt;&lt;!DOCTYPE hibernate-configuration PUBLIC &quot;-&#x2F;&#x2F;Hibernate&#x2F;Hibernate Configuration DTD 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.hibernate.org&#x2F;dtd&#x2F;hibernate-configuration-3.0.dtd&quot;&gt;&lt;hibernate-configuration&gt; &lt;!-- 配置SessionFactory SessionFactory的作用就是用于创建Session对象的。 Session对象就是hibernate中操作数据库的核心对象。 此处的配置不要求背，但是要求记住创建SessionFactory必须的三部分信息 第一部分： 连接数据库的信息 第二部分： hibernate的可选配置 第三部分： 映射文件的位置 --&gt; &lt;session-factory&gt; &lt;!-- 第一部分：连接数据库的信息 --&gt; &lt;property name&#x3D;&quot;hibernate.connection.driver_class&quot;&gt;com.mysql.jdbc.Driver&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.url&quot;&gt;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;hibernate01&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.username&quot;&gt;root&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.password&quot;&gt;root&lt;&#x2F;property&gt; &lt;!-- 数据库的方言 --&gt; &lt;property name&#x3D;&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.MySQLDialect&lt;&#x2F;property&gt; &lt;!-- 第二部分：hibernate的可选配置 --&gt; &lt;!-- 是否显示hibernate生成的SQL语句 --&gt; &lt;property name&#x3D;&quot;hibernate.show_sql&quot;&gt;true&lt;&#x2F;property&gt; &lt;!-- 是否使用格式化输出sql语句到控制台 --&gt; &lt;property name&#x3D;&quot;hibernate.format_sql&quot;&gt;true&lt;&#x2F;property&gt; &lt;!-- update表示检测实体类的映射配置和数据库的表结构是否一致，如果不一致，更新表结构 --&gt; &lt;property name&#x3D;&quot;hibernate.hbm2ddl.auto&quot;&gt;update&lt;&#x2F;property&gt; &lt;!-- 设置hibernate的连接池提供商 --&gt; &lt;property name&#x3D;&quot;hibernate.connection.provider_class&quot;&gt;org.hibernate.c3p0.internal.C3P0ConnectionProvider &lt;&#x2F;property&gt; &lt;!-- 把session和线程绑定，从而实现一个线程只有一个Session --&gt; &lt;property name&#x3D;&quot;hibernate.current_session_context_class&quot;&gt;thread&lt;&#x2F;property&gt; &lt;!-- 第三部分：映射配置文件的位置 --&gt; &lt;mapping class&#x3D;&quot;com.wgy.domain.Customer&quot;&#x2F;&gt; &lt;&#x2F;session-factory&gt;&lt;&#x2F;hibernate-configuration&gt; 5.2 编写实体类并使用JPA注解配置123456789101112131415161718192021222324252627282930313233/** * 客户实体类 * * @author wgy */@Entity@Table(name = \"cst_customer\")public class Customer implements Serializable &#123; @Id @Column(name=\"cust_id\") @GeneratedValue(strategy=GenerationType.IDENTITY) private Long custId; @Column(name = \"cust_name\") private String custName; @Column(name = \"cust_source\") private String custSource; @Column(name = \"cust_industry\") private String custIndustry; @Column(name = \"cust_level\") private String custLevel; @Column(name = \"cust_address\") private String custAddress; @Column(name = \"cust_phone\") private String custPhone; ...&#125; 5.2 操作5.2.1 保存12345678910111213/** * 保存 */@Testpublic void test1() &#123; Customer c = new Customer(); c.setCustName(\"hibernate jpa customer\"); Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); s.save(c); tx.commit();&#125; 5.2.2 查询1234567891011/** * 查询一个 */@Testpublic void test2() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); Customer c = s.get(Customer.class, 6L); System.out.println(c); tx.commit();&#125; 5.2.3 修改1234567891011/** * 修改 */@Testpublic void test3() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); Customer c = s.get(Customer.class, 6L); c.setCustAddress(\"顺义区\"); tx.commit();&#125; 5.2.4 删除1234567891011/** * 删除操作 */@Testpublic void test4() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); Customer c = s.get(Customer.class, 6L); s.delete(c); tx.commit();&#125;","tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"https://wgy1993.gitee.io/tags/Hibernate/"},{"name":"ORM","slug":"ORM","permalink":"https://wgy1993.gitee.io/tags/ORM/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"}]},{"title":"Hibernate(三)","date":"2020-05-31T08:27:38.000Z","path":"archives/3e87eb50.html","text":"1. 多表设计1.1 表之间的关系划分 一对一 一对多（多对一） 多对多 2. 如何确立和实现数据库中的表关系2.1 一对多的表关系在数据库中如何实现?使用外键约束。我们一般习惯把一的方称为主表，把多的一方称为从表。 什么是外键： 从表中有一列，该列的取值除了null之外，只能来源于主表的主键。默认情况下，外键字段的值是可以重复的。 2.2 多对多的表关系在数据库中如何实现？使用中间表。中间表中只有两个外键，引用两个多对多表的主键。不能有其他字段信息，至于中间表的主键，应该采用联合主键。 任何一个多方的表和中间表去比较，都是一对多的关系。 2.3 一对一的表关系在数据库中如何实现？有两种： 第一种：建立外键的方式： 使用外键约束，唯一约束，非空约束。它是把外键字段加了非空和唯一约束。从而实现了一对一。 第二种：使用主键的方式： 让其中一张表既是主键，又是外键。 2.4 如何确立两张表之间的关系：找外键。 3. 学习多表映射配置要遵循的步骤 第一步：确定两张表之间的关系 第二步：在数据库中实现两张表之间的关系建立 第三步：在实体类中描述出两个实体之间的关系 第四步：在映射配置文件中建立两个实体和两张表之间的关系 4. 一对多关系映射配置示例：客户和联系人两张表 4.1 确定两张表之间的关系一个客户可以包含多个联系人，多个联系人可以属于同一个客户。所以：客户和联系人之间的关系是一对多。 4.2 在数据库中实现两张表之间的关系建立实现一对多的关系，靠外键。客户表是主表，联系人表是从表。我们需要在联系人表中添加外键。 4.3 在实体类中描述出两个实体之间的关系主表的实体类应该包含从表实体类的集合引用，从表的实体类应该包含主表实体类的对象引用 123456789101112131415161718192021/** * 客户实体类 * * @author wgy */public class Customer implements Serializable &#123; private Long custId; private String custName; private String custSource; private String custIndustry; private String custLevel; private String custAddress; private String custPhone; /** * 一对多关系映射：一的一方 * 主表实体应该包含从表实体的集合引用 */ private Set&lt;LinkMan&gt; linkmans = new HashSet&lt;LinkMan&gt;(0); ...&#125; 1234567891011121314151617181920212223/** * 联系人的实体类 * * @author wgy */public class LinkMan implements Serializable &#123; private Long lkmId; private String lkmName; private String lkmGender; private String lkmPhone; private String lkmMobile; private String lkmEmail; private String lkmPosition; private String lkmMemo; /** * 一对多关系映射，多的一方。 * 从表实体包含主表实体的对象引用 */ private Customer customer; ...&#125; 4.4 在映射配置文件中建立两个实体和两张表之间的关系客户配置文件： 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!-- 在实体类所在的包下，创建一个xml文件。该文件建议名称为：实体类名称+.hbm+.xml导入约束：dtd约束 --&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC &quot;-&#x2F;&#x2F;Hibernate&#x2F;Hibernate Mapping DTD 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.hibernate.org&#x2F;dtd&#x2F;hibernate-mapping-3.0.dtd&quot;&gt;&lt;hibernate-mapping package&#x3D;&quot;com.wgy.domain&quot;&gt; &lt;class name&#x3D;&quot;Customer&quot; table&#x3D;&quot;cst_customer&quot; lazy&#x3D;&quot;true&quot;&gt; &lt;id name&#x3D;&quot;custId&quot; column&#x3D;&quot;cust_id&quot;&gt; &lt;generator class&#x3D;&quot;identity&quot;&#x2F;&gt; &lt;&#x2F;id&gt; &lt;property name&#x3D;&quot;custName&quot; column&#x3D;&quot;cust_name&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;custSource&quot; column&#x3D;&quot;cust_source&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;custIndustry&quot; column&#x3D;&quot;cust_industry&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;custLevel&quot; column&#x3D;&quot;cust_level&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;custAddress&quot; column&#x3D;&quot;cust_address&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;custPhone&quot; column&#x3D;&quot;cust_phone&quot;&#x2F;&gt; &lt;!-- 一对多关系映射：主表实体的映射配置 涉及的标签： set： 作用：用于配置set集合属性。 属性： name：指定实体类中set集合的属性名称。 table：指定从表的名称。在一对多配置时可以不写。 key: 作用：用于映射外键字段。 属性： column：指定外键字段名称 one-to-many: 作用：用于建立一对多的映射配置 属性： class：用于指定从表实体的名称 --&gt; &lt;set name&#x3D;&quot;linkmans&quot; table&#x3D;&quot;cst_linkman&quot;&gt; &lt;key column&#x3D;&quot;lkm_cust_id&quot;&#x2F;&gt; &lt;one-to-many class&#x3D;&quot;LinkMan&quot;&#x2F;&gt; &lt;&#x2F;set&gt; &lt;&#x2F;class&gt;&lt;&#x2F;hibernate-mapping&gt; 联系人配置文件： 1234567891011121314151617181920212223242526272829&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC &quot;-&#x2F;&#x2F;Hibernate&#x2F;Hibernate Mapping DTD 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.hibernate.org&#x2F;dtd&#x2F;hibernate-mapping-3.0.dtd&quot;&gt;&lt;hibernate-mapping package&#x3D;&quot;com.wgy.domain&quot;&gt; &lt;class name&#x3D;&quot;LinkMan&quot; table&#x3D;&quot;cst_linkman&quot;&gt; &lt;id name&#x3D;&quot;lkmId&quot; column&#x3D;&quot;lkm_id&quot;&gt; &lt;generator class&#x3D;&quot;identity&quot;&#x2F;&gt; &lt;&#x2F;id&gt; &lt;property name&#x3D;&quot;lkmName&quot; column&#x3D;&quot;lkm_name&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;lkmGender&quot; column&#x3D;&quot;lkm_gender&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;lkmPhone&quot; column&#x3D;&quot;lkm_phone&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;lkmMobile&quot; column&#x3D;&quot;lkm_mobile&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;lkmEmail&quot; column&#x3D;&quot;lkm_email&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;lkmPosition&quot; column&#x3D;&quot;lkm_position&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;lkmMemo&quot; column&#x3D;&quot;lkm_memo&quot;&#x2F;&gt; &lt;!-- 一对多关系映射：从表实体的映射配置 涉及的标签： many-to-one： 作用：建立多对一的映射配置 属性： name：从表实体中引用主表实体对象引用的名称 class：指定属性所对应的实体类名称 column：指定从表中外键字段的名称 --&gt; &lt;many-to-one name&#x3D;&quot;customer&quot; class&#x3D;&quot;Customer&quot; column&#x3D;&quot;lkm_cust_id&quot;&#x2F;&gt; &lt;&#x2F;class&gt;&lt;&#x2F;hibernate-mapping&gt; 5. 多对多关系映射配置示例：用户和角色 5.1 确定两张表之间的关系一个用户可以有多个角色，一个角色可以赋给多个用户，所以用户和角色之间是多对多。 5.2 在数据库中实现两张表之间的关系建立在数据库中实现多对多要靠中间表。中间表中只能出现用户和角色主键。 5.3 在实体类中描述出两个实体之间的关系各自包含对方一个集合引用 12345678910111213141516/** * 用户的实体类 * * @author wgy */public class SysUser implements Serializable &#123; private Long userId; private String userName; private String userPassword; private Integer userState; //多对多关系映射：一个用户可以具备多个角色 private Set&lt;SysRole&gt; roles = new HashSet&lt;SysRole&gt;(0); ...&#125; 123456789101112131415/** * 角色的实体类 * * @author wgy */public class SysRole implements Serializable &#123; private Long roleId; private String roleName; private String roleMemo; //多对多关系映射：一个角色可以赋予多个用户 private Set&lt;SysUser&gt; users = new HashSet&lt;SysUser&gt;(0); ...&#125; 5.4 在映射配置文件中建立两个实体和两张表之间的关系用户配置文件： 123456789101112131415161718192021222324252627282930313233343536&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC &quot;-&#x2F;&#x2F;Hibernate&#x2F;Hibernate Mapping DTD 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.hibernate.org&#x2F;dtd&#x2F;hibernate-mapping-3.0.dtd&quot;&gt;&lt;hibernate-mapping package&#x3D;&quot;com.wgy.domain&quot;&gt; &lt;class name&#x3D;&quot;SysUser&quot; table&#x3D;&quot;sys_user&quot;&gt; &lt;id name&#x3D;&quot;userId&quot; column&#x3D;&quot;user_id&quot;&gt; &lt;generator class&#x3D;&quot;identity&quot;&gt;&lt;&#x2F;generator&gt; &lt;&#x2F;id&gt; &lt;property name&#x3D;&quot;userName&quot; column&#x3D;&quot;user_name&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;userPassword&quot; column&#x3D;&quot;user_password&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;userState&quot; column&#x3D;&quot;user_state&quot;&gt;&lt;&#x2F;property&gt; &lt;!-- 多对多关系映射 涉及的标签： set: 作用：用于映射set集合属性 属性： name：指定集合的名称 table：指定的是中间表的名称 key: 作用：用于映射外键字段 属性： column：指定的是当前实体在中间表的外键字段名称 many-to-many 作用：用于映射多对多的关系 属性： class：对方的实体类名称 column：对方在中间表的外键字段名称 --&gt; &lt;set name&#x3D;&quot;roles&quot; table&#x3D;&quot;user_role_ref&quot;&gt; &lt;key column&#x3D;&quot;user_id&quot;&gt;&lt;&#x2F;key&gt; &lt;many-to-many class&#x3D;&quot;SysRole&quot; column&#x3D;&quot;role_id&quot;&#x2F;&gt; &lt;&#x2F;set&gt; &lt;&#x2F;class&gt;&lt;&#x2F;hibernate-mapping&gt; 角色配置文件： 12345678910111213141516171819&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC &quot;-&#x2F;&#x2F;Hibernate&#x2F;Hibernate Mapping DTD 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.hibernate.org&#x2F;dtd&#x2F;hibernate-mapping-3.0.dtd&quot;&gt;&lt;hibernate-mapping package&#x3D;&quot;com.wgy.domain&quot;&gt; &lt;class name&#x3D;&quot;SysRole&quot; table&#x3D;&quot;sys_role&quot;&gt; &lt;id name&#x3D;&quot;roleId&quot; column&#x3D;&quot;role_id&quot;&gt; &lt;generator class&#x3D;&quot;identity&quot;&gt;&lt;&#x2F;generator&gt; &lt;&#x2F;id&gt; &lt;property name&#x3D;&quot;roleName&quot; column&#x3D;&quot;role_name&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;roleMemo&quot; column&#x3D;&quot;role_memo&quot;&gt;&lt;&#x2F;property&gt; &lt;!-- 多对多关系映射 --&gt; &lt;set name&#x3D;&quot;users&quot; table&#x3D;&quot;user_role_ref&quot; inverse&#x3D;&quot;true&quot;&gt; &lt;key column&#x3D;&quot;role_id&quot;&gt;&lt;&#x2F;key&gt; &lt;many-to-many class&#x3D;&quot;SysUser&quot; column&#x3D;&quot;user_id&quot;&#x2F;&gt; &lt;&#x2F;set&gt; &lt;&#x2F;class&gt;&lt;&#x2F;hibernate-mapping&gt; 6. 多表增删改操作6.1 一对多关系的操作6.1.1 保存操作6.1.1.1 正常保存12345678910111213141516171819/** * 保存操作 * 正常的保存：创建一个新的联系人，需要关联一个客户 */@Testpublic void test1() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1.查询一个客户 Customer c1 = s.get(Customer.class, 1L); //2.创建一个新的联系人 LinkMan l = new LinkMan(); l.setLkmName(\"一对多的联系人\"); //3.建立客户和联系人的关联关系（让联系人知道属于哪个客户即可） l.setCustomer(c1); //4.保存联系人 s.save(l); tx.commit();&#125; 6.1.1.2 特殊情况1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 特殊的情况： * 创建一个客户和一个联系人 * 建立联系人和客户的双向关联关系 * 使用符合原则的保存 * 原则是：先保存主表实体，再保存从表实体 * * 此时保存会有问题： * 我们保存两个实体，应该只有两条insert语句。 * 而执行结果却是多了一条update的语句。 * * 解决办法： * 让客户在执行操作的时候，放弃维护关联关系的权利。 * 配置的方式： * 在Customer的映射配置文件中的set标签上使用inverse属性。 * inverse含义：是否放弃维护关联关系的权利 * true：放弃 * false：不放弃（默认值） * */@Testpublic void test2()&#123; //1.创建一个客户 //瞬时态 Customer c1 = new Customer(); c1.setCustName(\"一对多的客户_4\"); //2.创建一个新的联系人 //瞬时态 LinkMan l = new LinkMan(); l.setLkmName(\"一对多的联系人_4\"); //3.建立客户和联系人的关联关系(双向) l.setCustomer(c1); c1.getLinkmans().add(l); Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //4.保存，要符合原则 //持久态 有一级缓存和快照 s.save(c1); //持久态 有一级缓存和快照 s.save(l); tx.commit();&#125; 1234&lt;set name&#x3D;&quot;linkmans&quot; table&#x3D;&quot;cst_linkman&quot; inverse&#x3D;&quot;true&quot;&gt; &lt;key column&#x3D;&quot;lkm_cust_id&quot;&#x2F;&gt; &lt;one-to-many class&#x3D;&quot;LinkMan&quot;&#x2F;&gt;&lt;&#x2F;set&gt; 6.1.1.3 级联保存级联操作是指当主控方执行保存、更新或者删除操作时，其关联对象（被控方）也执行相同的操作。 保存客户 123456789101112131415161718192021222324252627282930/** * 保存操作： * 级联保存 * 使用级联保存，配置的方式，仍然是找到Customer的映射配置文件的Set标签， * 也可以配置在many-to-one上。 * 在上面加入cascade属性 * cascade：配置级联操作 * 级联保存更新的取值：save-update */@Testpublic void test3()&#123; //1.创建一个客户 //瞬时态 Customer c1 = new Customer(); c1.setCustName(\"一对多的客户_5\"); //2.创建一个新的联系人 //瞬时态 LinkMan l = new LinkMan(); l.setLkmName(\"一对多的联系人_5\"); //3.建立客户和联系人的关联关系(双向) l.setCustomer(c1); c1.getLinkmans().add(l); Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //4.保存，要符合原则 s.save(c1); tx.commit();&#125; 1234&lt;set name&#x3D;&quot;linkmans&quot; table&#x3D;&quot;cst_linkman&quot; inverse&#x3D;&quot;true&quot; cascade&#x3D;&quot;save-update&quot;&gt; &lt;key column&#x3D;&quot;lkm_cust_id&quot;&#x2F;&gt; &lt;one-to-many class&#x3D;&quot;LinkMan&quot;&#x2F;&gt;&lt;&#x2F;set&gt; 保存联系人 123456789101112131415161718192021@Testpublic void test4()&#123; //1.创建一个客户 //瞬时态 Customer c1 = new Customer(); c1.setCustName(\"一对多的客户_6\"); //2.创建一个新的联系人 //瞬时态 LinkMan l = new LinkMan(); l.setLkmName(\"一对多的联系人_6\"); //3.建立客户和联系人的关联关系(双向) l.setCustomer(c1); c1.getLinkmans().add(l); Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //4.保存，要符合原则 s.save(l); tx.commit();&#125; 1&lt;many-to-one name&#x3D;&quot;customer&quot; class&#x3D;&quot;Customer&quot; column&#x3D;&quot;lkm_cust_id&quot; cascade&#x3D;&quot;save-update&quot;&#x2F;&gt; 6.1.2 更新操作123456789101112131415161718192021222324/** * 更新操作 * 需求： * 创建一个新的联系人，查询一个已有客户 * 联系人新联系人和已有客户的双向关联关系 * 更新客户 */@Testpublic void test5()&#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1.查询一个客户 Customer c1 = s.get(Customer.class, 1L); //2.创建一个新的联系人 //瞬时态 LinkMan l = new LinkMan(); l.setLkmName(\"一对多的联系人\"); //3.建立客户和联系人的关联关系（双向） l.setCustomer(c1); c1.getLinkmans().add(l); //4.更新客户 s.update(c1); tx.commit();&#125; 6.1.3 删除操作123456789101112131415161718192021/** * 删除操作 * 删除从表数据就是单表 * 删除主表数据： * 看有没有从表数据引用 * 有引用： * 在删除是，hibernate会把从表中的外键字段置为null，然后再删除主表数据。 * 如果外键字段有非空约束，则hibernate不能更新外键字段为null，会报错。 * 如果仍然想删除，此时需要使用级联删除。同时必须配置inverse属性是true。 * 没有引用： 就是单表，直接删 */@Testpublic void test6()&#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1.查询一个客户 Customer c9 = s.get(Customer.class, 5L); //2.删除id为5的客户 s.delete(c9); tx.commit();&#125; 1234&lt;set name&#x3D;&quot;linkmans&quot; table&#x3D;&quot;cst_linkman&quot; inverse&#x3D;&quot;true&quot; cascade&#x3D;&quot;save-update,delete&quot;&gt; &lt;key column&#x3D;&quot;lkm_cust_id&quot;&#x2F;&gt; &lt;one-to-many class&#x3D;&quot;LinkMan&quot;&#x2F;&gt;&lt;&#x2F;set&gt; 6.2 多对多关系的操作6.2.1 保存操作12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 保存操作 * 需求： * 创建2个用户和3个角色 * 让1号用户具备1号和2号角色 * 让2号用户具备2号和3号角色 * 保存用户和角色 */@Testpublic void test1()&#123; SysUser u1 = new SysUser(); u1.setUserName(\"用户1\"); SysUser u2 = new SysUser(); u2.setUserName(\"用户2\"); SysRole r1 = new SysRole(); r1.setRoleName(\"角色1\"); SysRole r2 = new SysRole(); r2.setRoleName(\"角色2\"); SysRole r3 = new SysRole(); r3.setRoleName(\"角色3\"); //建立双向关联关系 //先建立用户的 u1.getRoles().add(r1); u1.getRoles().add(r2); u2.getRoles().add(r2); u2.getRoles().add(r3); //再建立角色 r1.getUsers().add(u1); r2.getUsers().add(u1); r2.getUsers().add(u2); r3.getUsers().add(u2); Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); s.save(u1); s.save(u2); s.save(r1); s.save(r2); s.save(r3); tx.commit();&#125; 6.2.2 删除操作12345678910111213141516/** * 删除操作 * 实际开发中：多对多的双向级联删除是禁止使用的 */@Testpublic void test2()&#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1查询id为3的用户 SysUser u3 = s.get(SysUser.class, 3L); //删除 s.delete(u3); tx.commit();&#125; 7. Hibernate中的多表查询7.1 对象导航查询7.1.1 概述对象图导航检索方式是根据已经加载的对象，导航到他的关联对象。它利用类与类之间的关系来检索对象。 例如：我们通过OID查询方式查出一个客户，可以调用Customer类中的getLinkMans()方法来获取该客户的所有联系人。 对象导航查询的使用要求是：两个对象之间必须存在关联关系。 7.1.2 对象导航检索示例查询联系人 123456789101112131415/** * 查询id为1的客户下所属联系人 * 一对多时，根据一的一方查询多的一方时，需要使用延迟加载。（默认配置即可） */@Testpublic void test1() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //查询id为1的客户 Customer c = s.get(Customer.class, 1L); System.out.println(c); Set&lt;LinkMan&gt; linkmans = c.getLinkmans(); System.out.println(linkmans); tx.commit();&#125; 查询客户 12345678910111213141516171819/** * 查询id为5的联系人属于哪个客户 * 多对一时，根据多的一方查询一的一方时，不需要使用延迟加载，而是使用立即加载，需要配置一下 * 需要找到联系人的映射配置文件：在many-to-one标签上使用lazy属性。 * 取值有： * false：使用立即加载。 * proxy：是看load方法是延迟加载还是立即加载 * no-proxy：不管 */@Testpublic void test2()&#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); LinkMan l = s.get(LinkMan.class, 5L); System.out.println(l); Customer c = l.getCustomer(); System.out.println(c); tx.commit();&#125; 1&lt;many-to-one name&#x3D;&quot;customer&quot; class&#x3D;&quot;Customer&quot; column&#x3D;&quot;lkm_cust_id&quot; cascade&#x3D;&quot;save-update&quot; lazy&#x3D;&quot;false&quot;&#x2F;&gt; load方法加载 1234567891011121314/** * 关于load方法改为立即加载的方式 * 找到查询实体的映射配置文件，它的class标签上也有一个lazy属性。含义是：是否延迟加载 * true：延迟加载(默认值) * false：立即加载 */@Testpublic void test3()&#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); Customer c = s.load(Customer.class, 1L); System.out.println(c); tx.commit();&#125; class标签的lazy：它只能管load方法是否是延迟加载。 set标签的lazy：它管查询关联的集合对象是否是延迟加载。 many-to-one的lazy：它管查询关联的主表实体是否是立即加载。","tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"https://wgy1993.gitee.io/tags/Hibernate/"},{"name":"ORM","slug":"ORM","permalink":"https://wgy1993.gitee.io/tags/ORM/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"}]},{"title":"Hibernate(二)","date":"2020-05-28T16:09:27.000Z","path":"archives/4074cb7b.html","text":"1. Hibernate的持久化类和对象标识符1.1 持久化类的编写规范1.1.1 什么是持久化类Hibernate是持久层的ORM映射框架，专注于数据的持久化工作。 持久化就是将内存中的数据永久存储到关系型数据库中。 持久化类指的是一个Java类与数据库表建立了映射关系，那么这个类称为是持久化类。其实你可以简单的理解为持久化类就是一个Java类有了一个映射文件与数据库的表建立了关系。 1.1.2 持久化类的编写规范应该遵循JavaBean的编写规范: Bean：在软件开发中是指的可重用的组件。 JavaBean：指的是用java语言编写的可重用组件。在我们的实际项目中：domain,service,dao都可以看成是JavaBean。 编写规范： 类都是public的 一般实现序列化接口 类成员（字段）都是私有的 私有类成员都有公有get/set方法 类都有默认无参构造函数 细节： 数据类型的选择问题： 基本类型和包装类，选择哪个？由于包装类可以有null值。所以实际开发中都是用包装类。 1.2 Hibernate中对象标识符（OID）OID全称是Object Identifier，又叫做对象标识符。 它是hibernate用于区分两个对象是否是同一个对象的标识。 我们都知道，虚拟机内存区分两个对象看的是内存的地址是否一致。数据库区分两个对象，靠的是表的主键。Hibernate负责把内存中的对象持久化到数据库表中，靠的就是对象标识符来区分两个对象是否是同一个。实体类中映射主键的字段就是OID，如下图所示： 1.3 Hibernate的主键生成策略 2. Hibernate的一级缓存和对象状态2.1 Hibernate的一级缓存什么缓存？ 它就是内存中的临时数据。 什么样的数据适用于缓存，什么样的数据不适用缓存？ 适用缓存的数据： 经常查询的，并且不经常修改的。同时数据一旦出现问题，对最终结果影响不大的。 不适用缓存的数据： 不管是否经常查询，只要是经常修改的，都可以不用缓存。并且如果数据由于使用缓存，产生了异常数据，对最终结果影响很大，则不能使用。例如：股市的牌价，银行的汇率，商品的库存等等。 2.1.1 Hibernate中的一级缓存Hibernate的一级缓存就是指Session缓存，Session缓存是一块内存空间，用来存放相互管理的java对象，在使用Hibernate查询对象的时候，首先会使用对象属性的OID值在Hibernate的一级缓存中进行查找，如果找到匹配OID值的对象，就直接将该对象从一级缓存中取出使用，不会再查询数据库；如果没有找到相同OID值的对象，则会去数据库中查找相应数据。当从数据库中查询到所需数据时，该数据信息也会放置到一级缓存中。Hibernate的一级缓存的作用就是减少对数据库的访问次数。 在 Session 接口的实现中包含一系列的 Java 集合, 这些 Java 集合构成了 Session 缓存。只要 Session 实例没有结束生命周期，存放在它缓存中的对象也不会结束生命周期。固一级缓存也被称为是Session基本的缓存。 Hibernate的一级缓存有如下特点： 当应用程序调用Session接口的save()、update()、saveOrUpdate时，如果Session缓存中没有相应的对象，Hibernate就会自动的把从数据库中查询到的相应对象信息加入到一级缓存中去。 当调用Session接口的load()、get()方法，以及Query接口的list()、iterator()方法时，会判断缓存中是否存在该对象，有则返回，不会查询数据库，如果缓存中没有要查询对象，再去数据库中查询对应对象，并添加到一级缓存中。 当调用Session的close()方法时，Session缓存会被清空。 2.1.2 测试一级缓存123456789101112131415161718192021/** * 证明一级缓存确实存在 */@Testpublic void test1() &#123; Session s = HibernateUtil.openSession(); Transaction tx = s.beginTransaction(); //1.根据id查询客户 //先去数据库查询，并且把查询的结果存入了一级缓存之中 Customer c1 = s.get(Customer.class, 1L); System.out.println(c1); //2.根据id再次查询客户 //先去一级缓存中看看有没有，如果有的话，直接拿过来用，如果没有的话，再去查询。 Customer c2 = s.get(Customer.class, 1L); System.out.println(c2); //true 一级缓存缓存的是对象的地址 System.out.println(c1 == c2); tx.commit(); //session关闭，一级缓存就消失了 s.close();&#125; 2.1.3 快照机制Hibernate 向一级缓存放入数据时，同时复制一份数据放入到Hibernate快照中，当使用commit()方法提交事务时，同时会清理Session的一级缓存，这时会使用OID判断一级缓存中的对象和快照中的对象是否一致，如果两个对象中的属性发生变化，则执行update语句，将缓存的内容同步到数据库，并更新快照；如果一致，则不执行update语句。Hibernate快照的作用就是确保一级缓存中的数据和数据库中的数据一致。 1234567891011121314151617181920212223@Testpublic void test2() &#123; Session s = HibernateUtil.openSession(); Transaction tx = s.beginTransaction(); //1.根据id查询客户 Customer c5 = s.get(Customer.class, 2L); //输出客户的地址：北京市顺义区 System.out.println(c5.getCustAddress()); //2.修改客户的地址为 湖北省 c5.setCustAddress(\"湖北省\"); //输出客户的地址：湖北省 System.out.println(c5.getCustAddress()); //没有写update语句 tx.commit(); //session关闭，一级缓存就消失了 s.close(); //打印c5的address，这个c5能不能用 答案是可以使用。 //此时输出是什么 北京市顺义区 | 湖北省 System.out.println(c5.getCustAddress());&#125; 2.2 对象的状态2.2.1 对象的状态说明Hibernate为了更好的来管理持久化类，特将持久化类分成了三种状态。在Hibernate中持久化的对象可以划分为四种状态，分别是瞬时态、持久态、脱管态和删除状态，一个持久化类的实例可能处于四种不同状态中的某一种，四种状态的详细介绍如下。 瞬时态（transient） 瞬时态也称为临时态或者自由态，瞬时态的实例是由new命令创建、开辟内存空间的对象，不存在持久化标识OID（相当于主键值），尚未与Hibernate Session关联，在数据库中也没有记录，失去引用后将被JVM回收。瞬时状态的对象在内存中是孤立存在的，与数据库中的数据无任何关联，仅是一个信息携带的载体。 持久态（persistent） 持久态的对象存在持久化标识OID ，加入到了Session缓存中，并且相关联的Session没有关闭，在数据库中有对应的记录，每条记录只对应唯一的持久化对象，需要注意的是，持久态对象是在事务还未提交前变成持久态的。 脱管态（detached） 脱管态也称离线态或者游离态，当某个持久化状态的实例与Session的关联被关闭时就变成了脱管态。脱管态对象存在持久化标识OID，并且仍然与数据库中的数据存在关联，只是失去了与当前Session的关联，脱管状态对象发生改变时Hibernate不能检测到。 删除状态（了解） 2.2.2 学习对象状态我们要明确的 是为了更好的掌握hibernate中操作的方法。 区分状态只有两个标识：一是否有OID，二是否和Session建立的关系 瞬时状态（临时状态） 标志：没有OID，和Session没有关系。 持久化状态 标志：有OID，和Session有关系。只有持久化状态的对象才会有一级缓存的概念。 脱管状态（游离状态） 标志：有OID，和Session没有关系。 删除状态（了解）： 标志：有OID，和Session有关系。同时已经调用了删除方法，即将从数据库中把记录删除。但是事务还没有提交，此时的对象状态是删除态。 1234567891011@Testpublic void test2() &#123; //瞬时状态 Customer c = new Customer(); c.setCustName(\"测试_saveOrUpdate\"); Session s1 = HibernateUtil.openSession(); Transaction tx1 = s1.beginTransaction(); s1.saveOrUpdate(c); tx1.commit(); s1.close();&#125; 1234567891011121314151617181920@Testpublic void test3() &#123; Session s1 = HibernateUtil.openSession(); Transaction tx1 = s1.beginTransaction(); //持久化状态 Customer c = s1.get(Customer.class, 9L); tx1.commit(); s1.close(); c.setCustAddress(\"顺义校区\"); //脱管状态 System.out.println(c); Session session = HibernateUtil.openSession(); Transaction tx = session.beginTransaction(); //持久化状态 session.saveOrUpdate(c); tx.commit(); session.close();&#125; 3. Hibernate的事务控制3.1 配置Session和线程绑定保证在Service中开启的事务时使用的Session对象和DAO中多个操作使用的是同一个Session对象。 在hibernate.cfg.xml文件中配置 12&lt;!-- 把session和线程绑定，从而实现一个线程只有一个Session --&gt;&lt;property name&#x3D;&quot;hibernate.current_session_context_class&quot;&gt;thread&lt;&#x2F;property&gt; 获取Session时使用的方法 1234567891011121314151617181920212223242526272829/** * 抽取hibernate的工具类 * * @author wgy */public class HibernateUtil &#123; private static SessionFactory factory; //了解：hibernate把可以预见的异常都转成了运行时异常 static &#123; try &#123; Configuration configuration = new Configuration(); configuration.configure(); factory = configuration.buildSessionFactory(); &#125; catch (ExceptionInInitializerError e) &#123; throw new ExceptionInInitializerError(\"初始化SessionFactory失败,请检查配置文件\"); &#125; &#125; /** * 从当前线程上获取Session对象 * @return session */ public static Session getCurrentSession()&#123; //只有配置了把session和线程绑定之后，才能使用此方法，否则返回值是null return factory.getCurrentSession(); &#125;&#125; 验证session和线程绑定的配置 12345678910111213141516171819202122@Testpublic void test1() &#123; Session s1 = HibernateUtil.getCurrentSession(); Session s2 = HibernateUtil.getCurrentSession(); // true System.out.println(s1 == s2);&#125;/** * 当我们把session和线程绑定之后，hibernate就会在提交或者回滚事务之后，自动帮我们关闭session */@Testpublic void test2() &#123; //瞬时状态 Customer c = new Customer(); c.setCustName(\"测试_getCurrentSession\"); Session s1 = HibernateUtil.getCurrentSession(); Transaction tx1 = s1.beginTransaction(); s1.saveOrUpdate(c); tx1.commit(); //s1.close();&#125; 4. Hibernate中的查询方式hibernate中一共有5种查询方式 OID查询： 根据id查询一个实体。涉及的方法：get和load SQL查询： 使用SQL语句查询数据库。涉及两种方式： 第一种：SQLQuery（一般不怎么用） 1234567891011121314151617@Testpublic void testFindAll() &#123; Session s = HibernateUtil.openSession(); Transaction tx = s.beginTransaction(); //使用session对象，获取一个查询对象Query SQLQuery sqlquery = s.createSQLQuery(\"select * from cst_customer\"); //使用sqlquery对象获取结果集 List&lt;Object[]&gt; list = sqlquery.list(); for (Object[] os : list) &#123; System.out.println(\"------------数组中的内容-----------\"); for (Object o : os) &#123; System.out.println(o); &#125; &#125; tx.commit(); s.close();&#125; 第二种：session的doWork方法，它可以拿到Connection 1234567891011121314151617181920/** * hibernate中如何使用原始JDBC API * JDBC的API： * Connection * Statement * PreparedStatement * ResultSet */@Testpublic void test1() &#123; //1.获取Session对象 Session s = HibernateUtil.openSession(); //2.调用doWork方法 s.doWork(new Work() &#123; @Override public void execute(Connection conn) throws SQLException &#123; System.out.println(conn.getClass().getName()); &#125; &#125;);&#125; HQL查询： 使用HQL语句查询数据库 QBC查询： 使用Criteria对象查询数据库 对象导航查询： 对象图导航检索方式是根据已经加载的对象，导航到他的关联对象。它利用类与类之间的关系来检索对象。 5. Hibernate查询对象的API5.1 Query5.1.1 概述Query代表面向对象的一个Hibernate查询操作。 在Hibernate中，通常使用session.createQuery()方法接受一个HQL语句，然后调用Query的list()或uniqueResult()方法执行查询。 所谓的HQL是Hibernate Query Language缩写，其语法很像SQL语法，但它是完全面向对象的。 在Hibernate中使用Query对象的步骤，具体所示： 获得Hibernate的Session对象 编写HQL语句 调用session.createQuery 创建查询对象 如果HQL语句包含参数，则调用Query的setXxx设置参数 调用Query对象的方法执行查询 HQL的说明： ​ 把表的名称换成实体类名称。把表字段名称换成实体类属性名称。 例如： 12345SQL：select * from cst_customer where cust_name like ?HQL：select * from Customer where custName &#x3D; ?其中select * 可以省略，写为：from Customer where custName &#x3D; ? 5.1.2 常用查询5.1.2.1 基本查询12345678910111213141516/** * 基本查询 */@Testpublic void test1() &#123; Session session = HibernateUtil.getCurrentSession(); Transaction transaction = session.beginTransaction(); //1.获取Query对象 Query query = session.createQuery(\"from Customer\"); //2.获取结果集 List list = query.list(); for (Object object : list) &#123; System.out.println(object); &#125; transaction.commit();&#125; 5.1.2.2 条件查询12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 条件查询 */@Testpublic void test2() &#123; Session session = HibernateUtil.getCurrentSession(); Transaction transaction = session.beginTransaction(); //1.获取Query对象 Query query = session.createQuery(\"from Customer where custLevel = ? and custName like ?\"); //给参数占位符赋值 //hibernate的参数占位符是从0开始的 query.setString(0, \"23\"); query.setString(1, \"%集%\"); //2.获取结果集 List list = query.list(); for (Object object : list) &#123; System.out.println(object); &#125; transaction.commit();&#125;/** * 条件查询 * 给参数占位符提供一个具体的名称 * 参数占位符的写法： * :名称 * 赋值的时候不需要写冒号，直接写名称 */@Testpublic void test3()&#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1.获取Query对象 Query query = s.createQuery(\"from Customer where custName like :custName and custLevel = :custLevel\"); //2.给参数占位符赋值 //query.setString(\"custName\", \"%集%\"); //query.setString(\"custLevel\", \"普通客户\"); query.setParameter(\"custLevel\", \"23\"); query.setParameter(\"custName\", \"%集%\"); //3.执行对象的方法，获取结果集 List list = query.list(); for(Object o : list)&#123; System.out.println(o); &#125; tx.commit();&#125; 5.1.2.3 分页查询123456789101112131415161718192021222324252627282930/** * 分页查询 * mysql分页关键字 * limit * Limit的两个参数含义 * 第一个：查询的开始记录索引 * 第二个：每次查询的条数 * hibernate为我们提供了两个方法： * setFirstResult：设置查询的开始记录索引 * setMaxResults：设置每次查询的条数 * * 不管是用什么数据库，涉及分页的都是这两个方法。 * 因为SQL语句的生成已经是hibernate的事了。 */@Testpublic void test5() &#123; Session session = HibernateUtil.getCurrentSession(); Transaction transaction = session.beginTransaction(); //1.获取Query对象 Query query = session.createQuery(\"from Customer\"); //设置分页的方法 query.setFirstResult(2); query.setMaxResults(2); //2.获取结果集 List list = query.list(); for (Object object : list) &#123; System.out.println(object); &#125; transaction.commit();&#125; 5.1.2.4 排序查询12345678910111213141516/** * 排序查询 */@Testpublic void test4() &#123; Session session = HibernateUtil.getCurrentSession(); Transaction transaction = session.beginTransaction(); //1.获取Query对象 Query query = session.createQuery(\"from Customer order by custId desc\"); //2.获取结果集 List list = query.list(); for (Object object : list) &#123; System.out.println(object); &#125; transaction.commit();&#125; 5.1.2.5 统计查询12345678910111213141516171819202122232425/** * HQL使用聚合函数： * 统计查询 * 聚合函数： * count sum max min avg * * sql语句使用聚合函数时，在不使用group by子句的情况下，返回的结果，永远只有一行一列的情况。 * * 在SQL语句时： * select count(*) from table 它是统计所有字段，效率没有只统计主键字段高 * select count(主键) from table 它和第一个的结果是一样的，但是效率更高 * select count(非主键) from table 只统计不为null的字段 */@Testpublic void test1()&#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1.获取Query对象 Query query = s.createQuery(\"select count(*) from Customer\"); //2.获取结果集 //当返回的结果唯一时，可以使用此方法。如果返回的结果不唯一，使用了此方法会抛异常。 Long total = (Long)query.uniqueResult(); System.out.println(total); tx.commit();&#125; 5.1.2.6 投影查询12345678910111213141516171819202122/** * 投影查询 * 当我们在查询实体时，只需要部分字段，而是全部。并且希望它的返回结果使用实体类来封装，而不是Object[] * 这个时候我们称之为创建实体类的投影 * * 投影查询的用法： * 1.查询语句需要时使用new关键字 * 2.在实体类中添加对应参数列表的构造函数 */@Testpublic void test7() &#123; Session session = HibernateUtil.getCurrentSession(); Transaction transaction = session.beginTransaction(); //1.获取Query对象 Query query = session.createQuery(\"select new com.wgy.domain.Customer(custId,custName) from Customer\"); //2.获取结果集 List list = query.list(); for (Object object : list) &#123; System.out.println(object); &#125; transaction.commit();&#125; 123456789101112131415161718192021222324252627/** * 客户实体类 * * @author wgy */public class Customer implements Serializable &#123; private Long custId; private String custName; private String custSource; private String custIndustry; private String custLevel; private String custAddress; private String custPhone; public Customer() &#123; &#125; /** * //提供对应参数列表的构造函数 * @param custId * @param custName */ public Customer(Long custId, String custName) &#123; this.custId = custId; this.custName = custName; &#125; ..... 5.1.3 Query中的方法说明 list方法：该方法用于查询语句，返回的结果是一个list集合。 uniqueResult方法：该方法用于查询，返回的结果是一个Object对象。 setter方法：Query接口中提供了一系列的setter方法用于设置查询语句中的参数，针对不同的数据类型，需要用到不同的setter方法。 uniqueResult()方法：该方法用于返回唯一的结果，在确保只有一条记录的查询时可以使用该方法。 setFirstResult()方法：该方法可以设置获取第一个记录的位置，也就是它表示从第几条记录开始查询，默认从0开始计算。 setMaxResult()方法：该方法用于设置结果集的最大记录数，通常与setFirstResult()方法结合使用，用于限制结果集的范围，以实现分页功能。 5.2 Criteria5.2.1 概述Criteria是一个完全面向对象，可扩展的条件查询API，通过它完全不需要考虑数据库底层如何实现，以及SQL语句如何编写，它是Hibernate框架的核心查询对象。 Criteria 查询，又称为QBC查询（Query By Criteria），它是Hibernate的另一种对象检索方式。 通常，使用Criteria对象查询数据的主要步骤，具体如下： 获得Hibernate的Session对象 通过Session获得Criteria对象 使用Restrictions的静态方法创建Criterion条件对象。Restrictions类中提供了一系列用于设定查询条件的静态方法，这些静态方法都返回Criterion实例，每个Criterion实例代表一个查询条件 向Criteria对象中添加Criterion 查询条件。Criteria的add()方法用于加入查询条件 执行Criterita的 list() 或uniqueResult() 获得结果 细节： HQL能查的，QBC都能查，反之亦然。 5.2.2 常用查询5.2.2.1 基本查询1234567891011121314151617/** * 基本查询 */@Testpublic void test1() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1. 获取Criteria对象 //它就相当于HQL的from Customer Criteria c = s.createCriteria(Customer.class); //2. 获取结果集 List list = c.list(); for (Object o : list) &#123; System.out.println(o); &#125; tx.commit();&#125; 5.2.2.2 条件查询12345678910111213141516171819/** * 条件查询 */@Testpublic void test2() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1. 获取Criteria对象 Criteria c = s.createCriteria(Customer.class); //使用criteria对象的add方法来添加条件 c.add(Restrictions.eq(\"custLevel\", \"23\")); c.add(Restrictions.like(\"custName\", \"%集%\")); //2. 获取结果集 List list = c.list(); for (Object o : list) &#123; System.out.println(o); &#125; tx.commit();&#125; 5.2.2.3 分页查询1234567891011121314151617181920/** * 分页查询 * QBC的分页查询和HQL的分页查询所用的方法和方法的含义都是一模一样的 */@Testpublic void test4() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1. 获取Criteria对象 Criteria c = s.createCriteria(Customer.class); //设置分页条件 c.setFirstResult(2); c.setMaxResults(2); //2. 获取结果集 List list = c.list(); for (Object o : list) &#123; System.out.println(o); &#125; tx.commit();&#125; 5.2.2.4 排序查询123456789101112131415161718/** * 排序查询 */@Testpublic void test3() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1. 获取Criteria对象 Criteria c = s.createCriteria(Customer.class); //添加排序 c.addOrder(Order.desc(\"custId\")); //2. 获取结果集 List list = c.list(); for (Object o : list) &#123; System.out.println(o); &#125; tx.commit();&#125; 5.2.2.5 统计查询1234567891011121314151617/** * 统计（投影）查询 */@Testpublic void test5() &#123; Session s = HibernateUtil.getCurrentSession(); Transaction tx = s.beginTransaction(); //1. 获取Criteria对象 Criteria c = s.createCriteria(Customer.class); //设置聚合函数 //c.setProjection(Projections.rowCount()); c.setProjection(Projections.count(\"custId\")); //2. 获取结果集 Long count = (Long)c.uniqueResult(); System.out.println(count); tx.commit();&#125; 5.2.2.6 离线查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 离线查询 * 离线： * 它是和在线对应的。 * Criteria对象是一个在线对象，它是由一个可用的（活动的）Session对象获取的出来的。 * 当session失效时，就无法再获取该对象了。 * 有一个对象，它也可以用于设置条件，但是获取的时候并不需要Session对象。该对象就叫做离线对象： * DetachedCriteria对象 * 使用该对象进行的查询就叫做：离线查询 * * 如何获取该对象 * DetachedCriteria dCriteria = DetachedCriteria.forClass(要查询的实体类字节码); */@Testpublic void testServlet()&#123; //1.获取离线对象，不需要Session DetachedCriteria dc = DetachedCriteria.forClass(Customer.class); //2.封装查询条件 dc.add(Restrictions.eq(\"custLevel\", \"23\")); dc.add(Restrictions.like(\"custName\",\"%集%\")); List list = testService(dc); for(Object o : list)&#123; System.out.println(o); &#125;&#125;private List testService(DetachedCriteria dc) &#123; Session s = null; Transaction tx = null; try&#123; s = HibernateUtil.getCurrentSession(); tx = s.beginTransaction(); List list = testDao(dc); tx.commit(); return list; &#125;catch(Exception e)&#123; tx.rollback(); &#125; return null;&#125;private List testDao(DetachedCriteria dc) &#123; Session s = HibernateUtil.getCurrentSession(); //把离线对象转成在线对象 Criteria c = dc.getExecutableCriteria(s); return c.list();&#125; 5.2.3 QBC常用查询条件说明 短语 含义 Restrictions.eq 等于= Restrictions.allEq 使用Map,使用key/value进行多个等于的判断 Restrictions.gt 大于&gt; Restrictions.ge 大于等于&gt;= Restrictions.lt 小于&lt; Restrictions.le 小于等于&lt;= Restrictions.between 对应sql的between子句 Restrictions.like 对应sql的like子句 Restrictions.in 对应sql的in子句 Restrictions.and and 关系 Restrictions.or or关系 Restrictions.sqlRestriction Sql限定查询 Restrictions.asc() 根据传入的字段进行升序排序 Restrictions.desc() 根据传入的字段进行降序排序 运算类型 HQL运算符 QBC运算方法 type = Restrictions.eq() Restrictions.not(Restrictions.eq()) >= Restrictions.ge()","tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"https://wgy1993.gitee.io/tags/Hibernate/"},{"name":"ORM","slug":"ORM","permalink":"https://wgy1993.gitee.io/tags/ORM/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"}]},{"title":"Hibernate(一)","date":"2020-05-28T06:17:43.000Z","path":"archives/ef455019.html","text":"1. Hibernate和ORM的概念1.1 Hibrenate概述它是一个轻量级，企业级，开源的ORM持久层框架。是可以操作数据库的框架。 框架：它就是一个架构。 通常情况下，软件工程的持久层解决方案，一个为主一个为辅。两者并存（写SQL语句的和不写SQL语句的 轻量级：指的是使用时依赖的资源很少。（目前我们使用的阶段，只依赖log4j，c3p0连接池） 企业级：指的是在企业级应用中使用的比较多 开源的：开放源代码。 ORM的操作方式：建立对象关系映射，实现操作实体类就相当于操作数据库表。 Hibernate框架是当今主流的Java持久层框架之一，由于它具有简单易学、灵活性强、扩展性强等特点，能够大大地简化程序的代码量，提高工作效率，因此受到广大开发人员的喜爱。 Hibernate是一个开放源代码的ORM框架，它对JDBC进行了轻量级的对象封装，使得Java开发人员可以使用面向对象的编程思想来操作数据库。 1.2 ORMObject Relational Mapping 对象关系映射 就是建立实体类和数据库表的对应关系。 实现操作实体类对象就相当于操作数据库表 Object Relation Mapping 对象关系映射。 对象-关系映射（OBJECT/RELATIONALMAPPING，简称ORM），是随着面向对象的软件开发方法发展而产生的。用来把对象模型表示的对象映射到基于S Q L 的关系模型数据库结构中去。这样，我们在具体的操作实体对象的时候，就不需要再去和复杂的 SQ L 语句打交道，只需简单的操作实体对象的属性和方法[2] 。O R M 技术是在对象和关系之间提供了一条桥梁，前台的对象型数据和数据库中的关系型的数据通过这个桥梁来相互转化[1] 。 简单的说就是把我们程序中的实体类和数据库表建立起来对应关系。 1.3 为什么要学习Hibernate与其它操作数据库的技术相比，Hibernate具有以下几点优势： Hibernate对JDBC访问数据库的代码做了轻量级封装，大大简化了数据访问层繁琐的重复性代码，并且减少了内存消耗，加快了运行效率。 Hibernate是一个基于JDBC的主流持久化框架，是一个优秀的ORM实现，它很大程度的简化了DAO（Data Access Object，数据访问对象）层编码工作。 Hibernate的性能非常好，映射的灵活性很出色。它支持很多关系型数据库，从一对一到多对多的各种复杂关系。 可扩展性强，由于源代码的开源以及API的开放，当本身功能不够用时，可以自行编码进行扩展。 明确： ​ 操作实体类就相当于操作数据库表 使用传统的JDBC开发应用系统时，如果是小型应用系统，并不觉得有什么麻烦，但是对于大型应用系统的开发，使用JDBC就会显得力不从心。例如对几十、几百张包含几十个字段的表进行插入操作时，编写的SQL语句不但很长，而且繁琐，容易出错；在读取数据时，需要写多条getXxx语句从结果集中取出各个字段的信息，不但枯燥重复，并且工作量非常大。为了提高数据访问层的编程效率，Gavin King开发出了一个当今最流行的的ORM框架，它就是Hibernate框架。 所谓的ORM就是利用描述对象和数据库表之间映射的元数据，自动把Java应用程序中的对象，持久化到关系型数据库的表中。通过操作Java对象，就可以完成对数据库表的操作。可以把ORM理解为关系型数据和对象的一个纽带，开发人员只需要关注纽带一端映射的对象即可。ORM原理如图所示。 2. CRMCRM：客户关系管理系统 CRM（Customer Relationship Management）客户关系管理，是利用相应的信息技术以及互联网技术来协调企业与顾客间在销售、营销和服务上的交互，向客户提供创新式的个性化的客户交互和服务的过程。 其最终目标是将面向客户的各项信息和活动集成起来，组建一个以客户为中心的企业，实现对面向客户的活动的全面管理。 3. Hibernate快速入门3.1 需求介绍保存一个客户到数据库的客户表中。 3.2 搭建Hibernate开发环境3.2.1 第一步：拷贝必备的jar包到开发目录数据库驱动包，如下图： Hibernate/lib/required/*.jar，如下图： 日志记录的包，如下图: 3.2.2 第二步：创建数据库和实体类持久化类是应用程序中的业务实体类，这里的持久化是指类的对象能够被持久化保存到数据库中。Hibernate使用普通Java对象（Plain Old Java Object），即POJO的编程模式来进行持久化。POJO类中包含的是与数据库表相对应的各个属性，这些属性通过getter和setter方法来访问，对外部隐藏了内部的实现细节。下面就来编写Customer持久化类。 在项目src目录下，创建cn.wgy.domain包，并在包中创建实体类Customer（对应数据库表cst_customer），Customer类包含与cst_customer数据表字段对应的属性，以及相应的getXxx ()和setXxx ()方法。 1234567891011&#x2F;*创建客户表*&#x2F;CREATE TABLE &#96;cst_customer&#96; ( &#96;cust_id&#96; bigint(32) NOT NULL AUTO_INCREMENT COMMENT &#39;客户编号(主键)&#39;, &#96;cust_name&#96; varchar(32) NOT NULL COMMENT &#39;客户名称(公司名称)&#39;, &#96;cust_source&#96; varchar(32) DEFAULT NULL COMMENT &#39;客户信息来源&#39;, &#96;cust_industry&#96; varchar(32) DEFAULT NULL COMMENT &#39;客户所属行业&#39;, &#96;cust_level&#96; varchar(32) DEFAULT NULL COMMENT &#39;客户级别&#39;, &#96;cust_address&#96; varchar(128) DEFAULT NULL COMMENT &#39;客户联系地址&#39;, &#96;cust_phone&#96; varchar(64) DEFAULT NULL COMMENT &#39;客户联系电话&#39;, PRIMARY KEY (&#96;cust_id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;94 DEFAULT CHARSET&#x3D;utf8; 123456789101112131415/** * 客户实体类 * * @author wgy */public class Customer implements Serializable &#123; private Long custId; private String custName; private String custSource; private String custIndustry; private String custLevel; private String custAddress; private String custPhone; ...&#125; 3.2.3 第三步：编写映射配置文件（xml）实体类Customer目前还不具备持久化操作的能力，而Hibernate需要知道实体类Customer映射到数据库Hibernate中的哪个表，以及类中的哪个属性对应数据库表中的哪个字段，这些都需要在映射文件中配置。 在实体类Customer所在的包中，创建一个名称为Customer.hbm.xml的映射文件，在该文件中定义了实体类Customer的属性是如何映射到cst_customer表的列上的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!-- 在实体类所在的包下，创建一个xml文件。该文件建议名称为：实体类名称+.hbm+.xml --&gt;&lt;!-- 导入约束:dtd约束 位置：在Hibernate的核心jar包中名称为hibernate-mapping-3.0.dtd 明确该文件中的内容： 实体类和表的对应关系 实体类中属性和表的字段的对应关系--&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC &quot;-&#x2F;&#x2F;Hibernate&#x2F;Hibernate Mapping DTD 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.hibernate.org&#x2F;dtd&#x2F;hibernate-mapping-3.0.dtd&quot;&gt;&lt;!-- package属性用于设定包的名称，接下来该配置文件中凡是用到此包中的对象时都可以省略包名 --&gt;&lt;hibernate-mapping package&#x3D;&quot;com.wgy.domain&quot;&gt; &lt;!-- class标签 作用：建立实体类和表的对应关系 属性： name：指定实体类的名称 table：指定数据库表的名称 --&gt; &lt;class name&#x3D;&quot;Customer&quot; table&#x3D;&quot;cst_customer&quot;&gt; &lt;!-- id标签 作用：用于映射主键 属性： name：指定的是属性名称。也就是get&#x2F;set方法后面的部分，并且首字母要转小写。 column:指定的是数据库表的字段名称 --&gt; &lt;id name&#x3D;&quot;custId&quot; column&#x3D;&quot;cust_id&quot;&gt; &lt;!-- generator标签： 作用：配置主键的生成策略。 属性： class:指定生成方式的取值。 取值之一：native。使用本地数据库的自动增长能力。 mysql数据库的自动增长能力是让某一列自动+1。但是不是所有数据库都支持这种方式。 --&gt; &lt;generator class&#x3D;&quot;native&quot;&gt;&lt;&#x2F;generator&gt; &lt;&#x2F;id&gt; &lt;!-- property标签： 作用：映射其他字段 属性： name：指定属性的名称。和id标签的name属性含义一致 column：指定数据库表的字段名称 --&gt; &lt;property name&#x3D;&quot;custName&quot; column&#x3D;&quot;cust_name&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;custLevel&quot; column&#x3D;&quot;cust_level&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;custSource&quot; column&#x3D;&quot;cust_source&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;custIndustry&quot; column&#x3D;&quot;cust_industry&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;custAddress&quot; column&#x3D;&quot;cust_address&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;custPhone&quot; column&#x3D;&quot;cust_phone&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;class&gt;&lt;&#x2F;hibernate-mapping&gt; 3.2.4 第四步：编写主配置文件（hibernate.cfg.xml）Hibernate的映射文件反映了持久化类和数据库表的映射信息，而Hibernate的配置文件则主要用来配置数据库连接以及Hibernate运行时所需要的各个属性的值。在项目的src下创建一个名称为hibernate.cfg.xml的文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!-- 在类的根路径下创建名称为hibernate.cfg.xml的配置文件 --&gt;&lt;!-- 导入dtd约束： 位置：在核心jar包中的名称为hibernate-configuration-3.0.dtd中--&gt;&lt;!DOCTYPE hibernate-configuration PUBLIC &quot;-&#x2F;&#x2F;Hibernate&#x2F;Hibernate Configuration DTD 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;www.hibernate.org&#x2F;dtd&#x2F;hibernate-configuration-3.0.dtd&quot;&gt;&lt;hibernate-configuration&gt; &lt;!-- 配置SessionFactory SessionFactory的作用就是用于创建Session对象的。 Session对象就是hibernate中操作数据库的核心对象。 此处的配置不要求背，但是要求记住创建SessionFactory必须的三部分信息 第一部分： 连接数据库的信息 第二部分： hibernate的可选配置 第三部分： 映射文件的位置 --&gt; &lt;session-factory&gt; &lt;!-- 第一部分：连接数据库的信息 --&gt; &lt;property name&#x3D;&quot;hibernate.connection.driver_class&quot;&gt;com.mysql.jdbc.Driver&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.url&quot;&gt;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;hibernate01&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.username&quot;&gt;root&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;hibernate.connection.password&quot;&gt;root&lt;&#x2F;property&gt; &lt;!-- 数据库的方言 --&gt; &lt;property name&#x3D;&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.MySQLDialect&lt;&#x2F;property&gt; &lt;!-- 第二部分：hibernate的可选配置 --&gt; &lt;!-- 是否显示hibernate生成的SQL语句 --&gt; &lt;property name&#x3D;&quot;hibernate.show_sql&quot;&gt;true&lt;&#x2F;property&gt; &lt;!-- 是否使用格式化输出sql语句到控制台 --&gt; &lt;property name&#x3D;&quot;hibernate.format_sql&quot;&gt;false&lt;&#x2F;property&gt; &lt;!-- 是否让hibernate根据表结构的变化来生成DDL语句 DDL:数据定义语言 hibernate可以根据映射文件来为我们生成数据库的表结构。但是他不能生成数据库。 hbm2ddl.auto的取值 * none:不用Hibernate自动生成表. * create:每次都会创建一个新的表.(测试) * create-drop:每次都会创建一个新的表，执行程序结束后删除这个表.(测试) * update:如果数据库中有表，使用原来的表，如果没有表，创建一个新表.可以更新表结构。 * validate:只会使用原有的表.对映射关系进行校验. --&gt; &lt;property name&#x3D;&quot;hibernate.hbm2ddl.auto&quot;&gt;update&lt;&#x2F;property&gt; &lt;!-- 第三部分：映射配置文件的位置 --&gt; &lt;mapping resource&#x3D;&quot;com&#x2F;wgy&#x2F;domain&#x2F;Customer.hbm.xml&quot;&#x2F;&gt; &lt;&#x2F;session-factory&gt;&lt;&#x2F;hibernate-configuration&gt; 3.4 实现保存操作在项目中新建一个名称为cn.wgy.test的包，然后在包中建立一个名为HibernateDemo1Test.java的文件，该文件是用来测试的类文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * hibernate的入门案例 * 需求： * 实现保存一个客户到数据库 * * @author wgy */public class HibernateDemo1Test &#123; /** * 步骤分析 * 1、解析主配置文件 * 2、根据配置文件创建SessionFactory * 3、根据SessionFactory创建Session * 4、开启事务 * 5、执行操作（保存） * 6、提交事务 * 7、释放资源 */ @Test public void test1() &#123; Customer customer = new Customer(); customer.setCustName(\"测试1\"); // 1、解析主配置文件 Configuration configuration = new Configuration(); configuration.configure(); // 2、根据配置文件创建SessionFactory SessionFactory sessionFactory = configuration.buildSessionFactory(); // 3、根据SessionFactory创建Session Session session = sessionFactory.openSession(); // 4、开启事务 Transaction transaction = session.beginTransaction(); // 5、执行操作（保存） session.save(customer); // 6、提交事务 transaction.commit(); // 7、释放资源 session.close(); sessionFactory.close(); &#125;&#125; 3.5 入门案例的执行过程首先创建Configuration类的实例，并通过它来读取并解析配置文件hibernate.cfg.xml。然后创建SessionFactory读取解析映射文件信息，并将Configuration对象中的所有配置信息拷贝到SessionFactory内存中。接下来，打开Session，让SessionFactory提供连接，并开启一个事务，之后创建对象，向对象中添加数据，通过session.save()方法完成向数据库中保存数据的操作。最后提交事务，并关闭资源。 4. Hibernate中API介绍4.1 Configuration对象4.1.1 作用在使用Hibernate时，首先要创建Configuration实例，Configuration实例主要用于启动、加载、管理hibernate的配置文件信息。在启动Hibernate的过程中，Configuration实例首先确定Hibernate配置文件的位置，然后读取相关配置，最后创建一个唯一的SessionFactory实例。Configuration对象只存在于系统的初始化阶段，它将SessionFactory创建完成后，就完成了自己的使命。 Hibernate通常使用Configuration config = new Configuration().configure();的方式创建实例，此种方式默认会去src下读取 hibernate.cfg.xml配置文件。如果不想使用默认的hibernate.cfg.xml配置文件，而是使用指定目录下（或自定义）的配置文件，则需要向configure()方法中传递一个文件路径的参数，其代码写法如下： 1Configuration config &#x3D; new Configuration().configure(&quot;xml文件位置&quot;); 此种写法hibernate会去指定位置查找配置文件，例如，想要使用src下config包中的 hibernate.cfg.xml文件，只需将文件位置加入configure()中即可，其代码如下所示： 1Configuration config = new Configuration().configure(\"/config/hibernate.cfg.xml\"); 【加载映射文件】 Hibernate除了可以使用Configuration对象加载核心配置文件以外，还可以利用该对象加载映射文件。因为如何使用properties文件作为Hibernate的核心配置文件，其他的属性可以使用key=value的格式来设置，但是映射没有办法加载。这时这个对象就有了用武之地。可以在手动编写代码的时候去加载映射文件。 123Configuration configuration &#x3D; new Configuration().configure(&quot;xml文件位置&quot;);configuration.addResource(&quot;cn&#x2F;wgy&#x2F;domain&#x2F;Customer.hbm.xml&quot;); 4.1.2 常用方法1234567891011121314默认构造函数： 它只能加载类的根路径下，名称为hibernate.properties的配置文件。不能加载xmlconfigure()： 它用于加载类的根路径下，名称为hibernate.cfg.xml的配置文件。 configuration.configure();buildSessionFactory()： 根据配置文件，构建SessionFactory SessionFactory sessionFactory &#x3D; configuration.buildSessionFactory();addResource(String url); 指定映射文件的位置 configuration.addResource(&quot;com&#x2F;itheima&#x2F;domain&#x2F;Customer.hbm.xml&quot;);addClass(Class clazz); 指定实体类的字节码 configuration.addClass(Customer.class); 4.2 SessionFactory4.2.1 作用SessionFactory接口负责Hibernate的初始化和建立Session对象。它在Hibernate中起到一个缓冲区作用，Hibernate可以将自动生成的SQL语句、映射数据以及某些可重复利用的的数据放在这个缓冲区中。同时它还保存了对数据库配置的所有映射关系，维护了当前的二级缓存。 SessionFactory 实例是通过Configuration对象获取的，其获取方法如下所示。 1SessionFactory sessionFactory = config.buildSessionFactory(); 4.2.2 常用方法1openSession()：每次都是生成一个新的Session 4.2.3 细节该对象维护了很多信息： 连接数据库的信息 hibernate的基本配置 映射文件的位置，以及映射文件中的配置 一些预定义的SQL语句（这些语句都是通用的） 比如：全字段保存，根据id的全字段更新，根据id的全字段查询，根据id的删除等等。 hibernate的二级缓存（了解） 同时，它是一个线程安全的对象，所有由该工厂生产的Session都共享工厂中维护的数据。 4.2.4 使用原则由于SessionFactory维护了很多信息同时又是线程安全的，一般情况下，一个项目中只需要一个SessionFactory，只有当应用中存在多个数据源时，才为每个数据源建立一个SessionFactory实例。因此，不应该反复的创建和销毁。 原则：一个应用应该只有一个SessionFactory。在应用加载时创建，应用卸载时销毁。 4.2.5 在hibernate中使用数据源(连接池)SessionFactory内部还维护了一个连接池，如果我们需要使用第三方的连接池如C3P0，那么需要我们自己手动进行配置 配置C3P0步骤如下： 导入连接池的jar包 在hibernate主配置文件中配置 1234&lt;!-- 配置数据源的提供商 --&gt;&lt;property name&#x3D;&quot;hibernate.connection.provider_class&quot;&gt; org.hibernate.connection.C3P0ConnectionProvider&lt;&#x2F;property&gt; 4.3 Session4.3.1 作用Session 是应用程序与数据库之间交互操作的一个单线程对象，是 Hibernate 运作的中心，它的主要功能是为持久化对象提供创建、读取和删除的能力，所有持久化对象必须在session的管理下才可以进行持久化操作。 创建SessionFactory实例后，就可以通过它获取Session实例。获取Session实例有两种方式，一种是通过openSession()方法，另一种是通过getCurrentSession()方法。两种方法获取session的代码如下所示： 12345//采用openSession方法创建sessionSession session = sessionFactory.openSession();//采用getCurrentSession()方法创建sessionSession session = sessionFactory.getCurrentSession(); 以上两种获取session实例方式的主要区别是，采用openSession方法获取Session实例时，SessionFactory直接创建一个新的Session实例，并且在使用完成后需要调用close方法进行手动关闭。而getCurrentSession方法创建的Session实例会被绑定到当前线程中，它在提交或回滚操作时会自动关闭。 4.3.2 常用方法123456789save(Object entity); ：保存一个实体到数据库update(Object entity);：更新一个实体delete(Object entity);：删除一个实体get(Class clazz,Serializable id);：根据id查询一个实体。参数的含义：Class表示要查询的实体类字节码。Serializable就是查询的条件。beginTransaction();：开启事务，并返回事务对象 4.3.3 细节由于SessionFactory已经维护了很多数据，所以Session就维护较少的内容。 它是一个轻量级对象。并且：它不是线程安全的！！！！！！！ 它维护了hibernate的一级缓存。 它的反复创建销毁不会消耗太多资源。 4.3.4 使用原则每个线程都只有一个Session对象。 4.4 Transaction4.4.1 作用Transaction接口主要用于管理事务，它是Hibernate的数据库事务接口，且对底层的事务接口进行了封装。Transaction接口的事务对象是通过Session对象开启的，其开启方式如下所示。 1Transaction transaction &#x3D; session.beginTransaction(); 4.4.2 常用方法123commit()：提交事务rollback()：回滚事务 Session执行完数据库操作后，要使用Transaction接口的commit()方法进行事务提交，才能真正的将数据操作同步到数据库中。发生异常时，需要使用rollback()方法进行事务回滚，以避免数据发生错误。因此，在持久化操作后，必须调用Transaction接口的commit()方法和rollback()方法。如果没有开启事务，那么每个Session的操作，都相当于一个独立的操作。 123456789101112131415161718@Testpublic void testSave1() &#123; Customer c = new Customer(); c.setCustName(\"测试保存功能\"); Transaction tx = null; Session s = null; try &#123; s = HibernateUtil.openSession(); tx = s.beginTransaction(); //保存客户 s.save(c); tx.commit(); &#125; catch (Exception e) &#123; tx.rollback(); &#125; finally &#123; s.close(); &#125;&#125; 5. 抽取HibernateUtil工具类1234567891011121314151617181920212223242526272829/** * hibernate的工具类 * 用于生产一个Session对象 * * @author wgy */public class HibernateUtil &#123; private static SessionFactory factory; static&#123; try &#123; Configuration cfg = new Configuration(); cfg.configure(); factory = cfg.buildSessionFactory(); &#125; catch (Exception e) &#123; //e.printStackTrace(); throw new ExceptionInInitializerError(\"初始化SessionFactory失败\"); &#125; &#125; /** * 获取一个新的Session对象 * @return */ public static Session openSession()&#123; return factory.openSession(); &#125;&#125; 6. 案例：使用Hibernate实现CRUD6.1 保存操作12345678910111213141516@Testpublic void testAdd()&#123; Customer c = new Customer(); c.setCustName(\"测试保存功能\"); //1.使用工具类获取一个Session Session session = HibernateUtil.openSession(); //2.开启事务 //Transaction tx = session.beginTransaction(); //3.保存客户 session.save(c); //4.提交事务 //tx.commit(); //5.释放资源 session.close();&#125; 6.2 查询一个实体1234567891011121314@Testpublic void testFindOne()&#123; //1.使用工具类获取一个Session Session session = HibernateUtil.openSession(); //2.开启事务 Transaction tx = session.beginTransaction(); //3.查询id为2的客户 Customer c = session.get(Customer.class, 2L); System.out.println(c); //4.提交事务 tx.commit(); //5.释放资源 session.close();&#125; 6.3 修改操作1234567891011121314151617@Testpublic void testUpdate()&#123; //1.使用工具类获取一个Session Session session = HibernateUtil.openSession(); //2.开启事务 Transaction tx = session.beginTransaction(); //3.查询id为2的客户 Customer c = session.get(Customer.class, 2L); //修改客户的地址为：湖北省 c.setCustAddress(\"湖北省\"); //执行更新 session.update(c); //4.提交事务 tx.commit(); //5.释放资源 session.close();&#125; 6.4 删除操作123456789101112131415@Testpublic void testDelete()&#123; //1.使用工具类获取一个Session Session session = HibernateUtil.openSession(); //2.开启事务 Transaction tx = session.beginTransaction(); //3.查询id为1的客户 Customer c = session.get(Customer.class, 1L); //删除实体 session.delete(c);//delete from cst_customer where cust_id = ? //4.提交事务 tx.commit(); //5.释放资源 session.close();&#125; 6.5 实体查询get和load方法6.5.1 实体查询的概念所谓实体查询即OID查询，就是使用主键作为条件来查询一个实体。其中涉及的方法是Session对象get方法和load方法。 6.5.2 方法的说明get方法： 12345678910111213141516/** * 根据id查询一个实体 * @param entityType 指的是要查询的实体类字节码对象 * @param id 查询的条件，即主键的值。 * @return 返回的是实体类对象 */&lt;T&gt; T get(Class&lt;T&gt; entityType, Serializable id);@Testpublic void testGet()&#123; Session s = HibernateUtil.openSession(); Transaction tx = s.beginTransaction(); Customer c = s.get(Customer.class, 2L); System.out.println(\"get : \"+c.toString()); tx.commit();&#125; load方法： 12345678910111213141516/** * 根据id查询一个实体 * @param theClass 指的是要查询的实体类字节码 * @param id查询的条件，即主键的值。 * @return 返回的是实体类对象或者是实体类对象的代理对象 */&lt;T&gt; T load(Class&lt;T&gt; theClass, Serializable id);@Testpublic void testLoad()&#123; Session s = HibernateUtil.openSession(); Transaction tx = s.beginTransaction(); Customer c = s.load(Customer.class, 2L); System.out.println(\"load : \"+c.toString()); tx.commit();&#125; 6.5.3 get和load的区别 查询的时机不一样。 get的查询时机：每次调用get方法时，马上发起查询。 立即加载 load的查询时机：每次真正使用的时候，发起查询。 延迟加载 懒加载 惰性加载 返回的结果不一样。 get方法返回的对象是实体类类型 load方法返回的对象是实体类类型的代理对象 load方法默认情况下是延迟，可以通过配置的方式改为立即加载。 12&lt;!-- 由于load方法是hibernate的方法所以只有XML的方式：--&gt;&lt;class name=\"Customer\" table=\"cst_customer\" lazy=\"false\"&gt;","tags":[{"name":"Hibernate","slug":"Hibernate","permalink":"https://wgy1993.gitee.io/tags/Hibernate/"},{"name":"ORM","slug":"ORM","permalink":"https://wgy1993.gitee.io/tags/ORM/"},{"name":"框架","slug":"框架","permalink":"https://wgy1993.gitee.io/tags/%E6%A1%86%E6%9E%B6/"}]},{"title":"Sharding-JDBC分库分表","date":"2020-05-25T10:57:49.000Z","path":"archives/edfbc79e.html","text":"Sharding-JDBC分库分表1 概述1. 1 分库分表是什么小明是一家初创电商平台的开发人员，他负责卖家模块的功能开发，其中涉及了店铺、商品的相关业务，设计如下 数据库： 通过以下SQL能够获取到商品相关的店铺信息、地理区域信息： 12345SELECT p.*,r.[地理区域名称],s.[店铺名称],s.[信誉]FROM [商品信息] pLEFT JOIN [地理区域] r ON p.[产地] &#x3D; r.[地理区域编码]LEFT JOIN [店铺信息] s ON p.id &#x3D; s.[所属店铺]WHERE p.id &#x3D;? 形成类似以下列表展示： 随着公司业务快速发展，数据库中的数据量猛增，访问性能也变慢了，优化迫在眉睫。分析一下问题出现在哪儿呢？ 关系型数据库本身比较容易成为系统瓶颈，单机存储容量、连接数、处理能力都有限。当单表的数据量达到1000 W或 100 G以后，由于查询维度较多，即使添加从库、优化索引，做很多操作时性能仍下降严重。 方案 1 ： 通过提升服务器硬件能力来提高数据处理能力，比如增加存储容量 、CPU等，这种方案成本很高，并且如果瓶颈在MySQL本身那么提高硬件也是有很的。 方案 2 ： 把数据分散在不同的数据库中，使得单一数据库的数据量变小来缓解单一数据库的性能问题，从而达到提升数据库性能的目的，如下图：将电商数据库拆分为若干独立的数据库，并且对于大表也拆分为若干小表，通过这种数据库拆分的方法来解决数据库的性能问题。 分库分表就是为了解决由于数据量过大而导致数据库性能降低的问题，将原来独立的数据库拆分成若干数据库组成，将数据大表拆分成若干数据表组成，使得单一数据库、单一数据表的数据量变小，从而达到提升数据库性能的目的。 1. 2 分库分表的方式分库分表包括分库和分表两个部分，在生产中通常包括：垂直分库、水平分库、垂直分表、水平分表四种方式。 1. 2. 1 垂直分表下边通过一个商品查询的案例讲解垂直分表： 通常在商品列表中是不显示商品详情信息的，如下图： 用户在浏览商品列表时，只有对某商品感兴趣时才会查看该商品的详细描述。因此，商品信息中商品描述字段访问频次较低，且该字段存储占用空间较大，访问单个数据IO时间较长；商品信息中商品名称、商品图片、商品价格等其他字段数据访问频次较高。 由于这两种数据的特性不一样，因此他考虑将商品信息表拆分如下： 将访问频次低的商品描述信息单独存放在一张表中，访问频次较高的商品基本信息单独放在一张表中。 商品列表可采用以下sql： 12345SELECT p.*,r.[地理区域名称],s.[店铺名称],s.[信誉]FROM [商品信息] pLEFT JOIN [地理区域] r ON p.[产地] &#x3D; r.[地理区域编码]LEFT JOIN [店铺信息] s ON p.id &#x3D; s.[所属店铺]WHERE...ORDER BY...LIMIT... 需要获取商品描述时，再通过以下sql获取： 123SELECT *FROM [商品描述]WHERE [商品ID] &#x3D; ? 小明进行的这一步优化，就叫垂直分表。 垂直分表定义：将一个表按照字段分成多表，每个表存储其中一部分字段。 它带来的提升是： 为了避免IO争抢并减少锁表的几率，查看详情的用户与商品信息浏览互不影响 充分发挥热门数据的操作效率，商品信息的操作的高效率不会被商品描述的低效率所拖累。 一般来说，某业务实体中的各个数据项的访问频次是不一样的，部分数据项可能是占用存储空间比较大的BLOB或是TEXT。例如上例中的商品描述。所以，当表数据量很大时，可以将表按字段切开，将热门字段、冷门字段分开放置在不同库中，这些库可以放在不同的存储设备上，避免IO争抢。垂直切分带来的性能提升主要集中在热门数据的操作效率上，而且磁盘争用情况减少。 通常我们按以下原则进行垂直拆分: 把不常用的字段单独放在一张表; 把text，blob等大字段拆分出来放在附表中; 经常组合查询的列放在一张表中; 1. 2. 2 垂直分库通过垂直分表性能得到了一定程度的提升，但是还没有达到要求，并且磁盘空间也快不够了，因为数据还是始终限制在一台服务器，库内垂直分表只解决了单一表数据量过大的问题，但没有将表分布到不同的服务器上，因此每个表还是竞争同一个物理机的CPU、内存、网络IO、磁盘。 经过思考，他把原有的SELLER_DB(卖家库)，分为了PRODUCT_DB(商品库)和STORE_DB(店铺库)，并把这两个库分散到不同服务器，如下图： 由于商品信息与商品描述业务耦合度较高，因此一起被存放在PRODUCT_DB(商品库)；而店铺信息相对独立，因此单独被存放在STORE_DB(店铺库)。 小明进行的这一步优化，就叫垂直分库。 垂直分库是指按照业务将表进行分类，分布到不同的数据库上面，每个库可以放在不同的服务器上，它的核心理念是专库专用。 它带来的提升是： 解决业务层面的耦合，业务清晰 能对不同业务的数据进行分级管理、维护、监控、扩展等 高并发场景下，垂直分库一定程度的提升IO、数据库连接数、降低单机硬件资源的瓶颈 垂直分库通过将表按业务分类，然后分布在不同数据库，并且可以将这些数据库部署在不同服务器上，从而达到多个服务器共同分摊压力的效果，但是依然没有解决单表数据量过大的问题。 1. 2. 3 水平分库经过垂直分库后，数据库性能问题得到一定程度的解决，但是随着业务量的增长，PRODUCT_DB(商品库)单库存储数据已经超出预估。粗略估计，目前有 8 w店铺，每个店铺平均 150 个不同规格的商品，再算上增长，那商品数量得往 1500 w+上预估，并且PRODUCT_DB(商品库)属于访问非常频繁的资源，单台服务器已经无法支撑。此时该如何优化？ 再次分库？但是从业务角度分析，目前情况已经无法再次垂直分库。 尝试水平分库，将店铺ID为单数的和店铺ID为双数的商品信息分别放在两个库中。 也就是说，要操作某条数据，先分析这条数据所属的店铺ID。如果店铺ID为双数，将此操作映射至RRODUCT_DB 1 (商品库 1 )；如果店铺ID为单数，将操作映射至RRODUCT_DB 2 (商品库 2 )。此操作要访问数据库名称的表达式为RRODUCT_DB[店铺ID% 2 + 1 ] 。 小明进行的这一步优化，就叫水平分库。 水平分库是把同一个表的数据按一定规则拆到不同的数据库中，每个库可以放在不同的服务器上。 它带来的提升是： 解决了单库大数据，高并发的性能瓶颈。 提高了系统的稳定性及可用性。 当一个应用难以再细粒度的垂直切分，或切分后数据量行数巨大，存在单库读写、存储性能瓶颈，这时候就需要进行水平分库了，经过水平切分的优化，往往能解决单库存储量及性能瓶颈。但由于同一个表被分配在不同的数据 库，需要额外进行数据操作的路由工作，因此大大提升了系统复杂度。 1. 2. 4 水平分表按照水平分库的思路对他把PRODUCT_DB_X(商品库)内的表也可以进行水平拆分，其目的也是为解决单表数据量大的问题，如下图： 与水平分库的思路类似，不过这次操作的目标是表，商品信息及商品描述被分成了两套表。如果商品ID为双数，将此操作映射至商品信息 1 表；如果商品ID为单数，将操作映射至商品信息 2 表。此操作要访问表名称的表达式为商品信息[商品ID% 2 + 1 ] 。 小明进行的这一步优化，就叫水平分表。 水平分表是在同一个数据库内，把同一个表的数据按一定规则拆到多个表中。 它带来的提升是： 优化单一表数据量过大而产生的性能问题 避免IO争抢并减少锁表的几率 库内的水平分表，解决了单一表数据量过大的问题，分出来的小表中只包含一部分数据，从而使得单个表的数据量 变小，提高检索性能。 1. 2. 5 小结本章介绍了分库分表的各种方式，它们分别是垂直分表、垂直分库、水平分库和水平分表： 垂直分表：可以把一个宽表的字段按访问频次、是否是大字段的原则拆分为多个表，这样既能使业务清晰，还能提升部分性能。拆分后，尽量从业务角度避免联查，否则性能方面将得不偿失。 垂直分库：可以把多个表按业务耦合松紧归类，分别存放在不同的库，这些库可以分布在不同服务器，从而使访问压力被多服务器负载，大大提升性能，同时能提高整体架构的业务清晰度，不同的业务库可根据自身情况定制优化方案。但是它需要解决跨库带来的所有复杂问题。 水平分库：可以把一个表的数据(按数据行)分到多个不同的库，每个库只有这个表的部分数据，这些库可以分布在不同服务器，从而使访问压力被多服务器负载，大大提升性能。它不仅需要解决跨库带来的所有复杂问题，还要解决数据路由的问题(数据路由问题后边介绍)。 水平分表：可以把一个表的数据(按数据行)分到多个同一个数据库的多张表中，每个表只有这个表的部分数据，这样做能小幅提升性能，它仅仅作为水平分库的一个补充优化。 一般来说，在系统设计阶段就应该根据业务耦合松紧来确定垂直分库，垂直分表方案，在数据量及访问压力不是特别大的情况，首先考虑缓存、读写分离、索引技术等方案。若数据量极大，且持续增长，再考虑水平分库水平分表方案。 1. 3 分库分表带来的问题分库分表能有效的缓解了单机和单库带来的性能瓶颈和压力，突破网络IO、硬件资源、连接数的瓶颈，同时也带来了一些问题。 1. 3. 1 事务一致性问题由于分库分表把数据分布在不同库甚至不同服务器，不可避免会带来分布式事务问题。 1. 3. 2 跨节点关联查询在没有分库前，我们检索商品时可以通过以下SQL对店铺信息进行关联查询： 12345SELECT p.*,r.[地理区域名称],s.[店铺名称],s.[信誉]FROM [商品信息] pLEFT JOIN [地理区域] r ON p.[产地] &#x3D; r.[地理区域编码]LEFT JOIN [店铺信息] s ON p.id &#x3D; s.[所属店铺]WHERE...ORDER BY...LIMIT... 但垂直分库后[商品信息]和[店铺信息]不在一个数据库，甚至不在一台服务器，无法进行关联查询。可将原关联查询分为两次查询，第一次查询的结果集中找出关联数据id，然后根据id发起第二次请求得到关联数据，最后将获得到的数据进行拼装。 1. 3. 3 跨节点分页、排序函数跨节点多库进行查询时，limit分页、order by排序等问题，就变得比较复杂了。需要先在不同的分片节点中将数据进行排序并返回，然后将不同分片返回的结果集进行汇总和再次排序。 如，进行水平分库后的商品库，按ID倒序排序分页，取第一页： 以上流程是取第一页的数据，性能影响不大，但由于商品信息的分布在各数据库的数据可能是随机的，如果是取第 N页，需要将所有节点前N页数据都取出来合并，再进行整体的排序，操作效率可想而知。所以请求页数越大，系 统的性能也会越差。 在使用Max、Min、Sum、Count之类的函数进行计算的时候，与排序分页同理，也需要先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总和再次计算，最终将结果返回。 1. 3. 4 主键避重在分库分表环境中，由于表中数据同时存在不同数据库中，主键值平时使用的自增长将无用武之地，某个分区数据 库生成的ID无法保证全局唯一。因此需要单独设计全局主键，以避免跨库主键重复问题。 1. 3. 5 公共表实际的应用场景中，参数表、数据字典表等都是数据量较小，变动少，而且属于高频联合查询的依赖表。例子中地 理区域表也属于此类型。 可以将这类表在每个数据库都保存一份，所有对公共表的更新操作都同时发送到所有分库执行。 由于分库分表之后，数据被分散在不同的数据库、服务器。因此，对数据的操作也就无法通过常规方式完成，并且 它还带来了一系列的问题。好在，这些问题不是所有都需要我们在应用层面上解决，市面上有很多中间件可供我们选择，其中Sharding-JDBC使用流行度较高，我们来了解一下它。 1. 4 Sharding-JDBC介绍1. 4. 1 Sharding-JDBC介绍Sharding-JDBC是当当网研发的开源分布式数据库中间件，从 3. 0 开始Sharding-JDBC被包含在 Sharding-Sphere中，之后该项目进入进入Apache孵化器， 4. 0 版本之后的版本为Apache版本。 ShardingSphere是一套开源的分布式数据库中间件解决方案组成的生态圈，它由Sharding-JDBC、Sharding-Proxy和Sharding-Sidecar（计划中）这 3 款相互独立的产品组成。 他们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如Java同构、异构语言、容器、云原生等各种多样化的应用场景。 官方地址：https://shardingsphere.apache.org/document/current/cn/overview/ 咱们目前只需关注Sharding-JDBC，它定位为轻量级Java框架，在Java的JDBC层提供的额外服务。 它使用客户端直连数据库，以jar包形式提供服务，无需额外部署和依赖，可理解为增强版的JDBC驱动，完全兼容JDBC和各种ORM框架。 Sharding-JDBC的核心功能为数据分片和读写分离，通过Sharding-JDBC，应用可以透明的使用jdbc访问已经分库分表、读写分离的多个数据源，而不用关心数据源的数量以及数据如何分布。 适用于任何基于Java的ORM框架，如： Hibernate, Mybatis, Spring JDBC Template或直接使用JDBC。 基于任何第三方的数据库连接池，如：DBCP, C3P0 , BoneCP, Druid, HikariCP等。 支持任意实现JDBC规范的数据库。目前支持MySQL，Oracle，SQLServer和PostgreSQL。 上图展示了Sharding-Jdbc的工作方式，使用Sharding-Jdbc前需要人工对数据库进行分库分表，在应用程序中加入Sharding-Jdbc的Jar包，应用程序通过Sharding-Jdbc操作分库分表后的数据库和数据表，由于Sharding-Jdbc是对Jdbc驱动的增强，使用Sharding-Jdbc就像使用Jdbc驱动一样，在应用程序中是无需指定具体要操作的分库和分表的。 1. 4. 2 与jdbc性能对比 性能损耗测试：服务器资源充足、并发数相同，比较JDBC和Sharding-JDBC性能损耗，Sharding-JDBC相对JDBC损耗不超过 7 %。 性能对比测试：服务器资源使用到极限，相同的场景JDBC与Sharding-JDBC的吞吐量相当。 性能对比测试：服务器资源使用到极限，Sharding-JDBC采用分库分表后，Sharding-JDBC吞吐量较JDBC不分表有接近 2 倍的提升。 2 Sharding-JDBC快速入门2. 1 需求说明本章节使用Sharding-JDBC完成对订单表的水平分表，通过快速入门程序的开发，快速体验Sharding-JDBC的使用方法。 人工创建两张表，t_order_ 1 和t_order_ 2 ，这两张表是订单表拆分后的表，通过Sharding-Jdbc向订单表插入数据，按照一定的分片规则，主键为偶数的进入t_order_ 1 ，另一部分数据进入t_order_ 2 ，通过Sharding-Jdbc 查询数据，根据 SQL语句的内容从t_order_ 1 或t_order_ 2 查询数据。 2. 2 环境搭建2. 2. 1 环境说明 操作系统：Win 10 数据库：MySQL-5.7.25 JDK： 64位 jdk 1.8.0_201 应用框架：spring-boot- 2.1.3.RELEASE，Mybatis 3.5.0 Sharding-JDBC：sharding-jdbc-spring-boot-starter-4.0.0-RC1 2. 2. 2 创建数据库创建订单库order_db 1CREATE DATABASE &#96;order_db&#96; CHARACTER SET &#39;utf8&#39; COLLATE &#39;utf8_general_ci&#39;; 在order_db中创建t_order_ 1 、t_order_ 2 表 1234567891011121314151617DROP TABLE IF EXISTS &#96;t_order_1&#96;;CREATE TABLE &#96;t_order_1&#96; ( &#96;order_id&#96; bigint(20) NOT NULL COMMENT &#39;订单id&#39;, &#96;price&#96; decimal(10, 2) NOT NULL COMMENT &#39;订单价格&#39;, &#96;user_id&#96; bigint(20) NOT NULL COMMENT &#39;下单用户id&#39;, &#96;status&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT &#39;订单状态&#39;, PRIMARY KEY (&#96;order_id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic;DROP TABLE IF EXISTS &#96;t_order_2&#96;;CREATE TABLE &#96;t_order_2&#96; ( &#96;order_id&#96; bigint(20) NOT NULL COMMENT &#39;订单id&#39;, &#96;price&#96; decimal(10, 2) NOT NULL COMMENT &#39;订单价格&#39;, &#96;user_id&#96; bigint(20) NOT NULL COMMENT &#39;下单用户id&#39;, &#96;status&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT &#39;订单状态&#39;, PRIMARY KEY (&#96;order_id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic; 2. 2. 3 引入maven依赖引入 sharding-jdbc和SpringBoot整合的Jar包： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;&#x2F;groupId&gt; &lt;artifactId&gt;sharding‐jdbc‐spring‐boot‐starter&lt;&#x2F;artifactId&gt; &lt;version&gt;4.0.0‐RC1&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; 具体spring boot相关依赖及配置请参考资料中dbsharding/sharding-jdbc-simple工程，本指引只说明与Sharding-JDBC相关的内容。 2. 3 编写程序2. 3. 1 分片规则配置分片规则配置是sharding-jdbc进行对分库分表操作的重要依据，配置内容包括：数据源、主键生成策略、分片策略等。 在application.properties中配置 123456789101112131415161718192021222324252627282930313233343536373839404142server.port=56081spring.application.name = sharding‐jdbc‐simple‐demoserver.servlet.context‐path = /sharding‐jdbc‐simple‐demospring.http.encoding.enabled = truespring.http.encoding.charset = UTF‐8spring.http.encoding.force = truespring.main.allow‐bean‐definition‐overriding = truemybatis.configuration.map‐underscore‐to‐camel‐case = true# 以下是分片规则配置# 定义数据源spring.shardingsphere.datasource.names = m1spring.shardingsphere.datasource.m1.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m1.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m1.url = jdbc:mysql://localhost:3306/order_db?useUnicode=truespring.shardingsphere.datasource.m1.username = rootspring.shardingsphere.datasource.m1.password = root# 指定t_order表的数据分布情况，配置数据节点spring.shardingsphere.sharding.tables.t_order.actual‐data‐nodes = m1.t_order_$‐&gt;&#123;1..2&#125;# 指定t_order表的主键生成策略为SNOWFLAKEspring.shardingsphere.sharding.tables.t_order.key‐generator.column=order_idspring.shardingsphere.sharding.tables.t_order.key‐generator.type=SNOWFLAKE# 指定t_order表的分片策略，分片策略包括分片键和分片算法spring.shardingsphere.sharding.tables.t_order.table‐strategy.inline.sharding‐column = order_idspring.shardingsphere.sharding.tables.t_order.table‐strategy.inline.algorithm‐expression =t_order_$‐&gt;&#123;order_id % 2 + 1&#125; # 打开sql输出日志spring.shardingsphere.props.sql.show = trueswagger.enable = truelogging.level.root = infologging.level.org.springframework.web = infologging.level.com.itheima.dbsharding = debuglogging.level.druid.sql = debug 1 .首先定义数据源m 1 ，并对m 1 进行实际的参数配置。2 .指定t_order表的数据分布情况，他分布在m 1 .t_order_ 1 ，m 1 .t_order_ 23 .指定t_order表的主键生成策略为SNOWFLAKE，SNOWFLAKE是一种分布式自增算法，保证id全局唯一4 .定义t_order分片策略，order_id为偶数的数据落在t_order_ 1 ，为奇数的落在t_order_ 2 ，分表策略的表达式为t_order_$-&gt;{order_id % 2 + 1 } 2. 3. 2 数据操作123456789101112131415161718192021222324252627282930@Mapper@Componentpublic interface OrderDao &#123; /** * 新增订单 * @param price 订单价格 * @param userId 用户id * @param status 订单状态 * @return */ @Insert(\"insert into t_order(price,user_id,status) value(#&#123;price&#125;,#&#123;userId&#125;,#&#123;status&#125;)\") int insertOrder(@Param(\"price\") BigDecimal price, @Param(\"userId\")Long userId,@Param(\"status\")String status); /** * 根据id列表查询多个订单 * @param orderIds 订单id列表 * @return */ @Select(&#123;\"&lt;script&gt;\" + \"select \" + \" * \" + \" from t_order t\" + \" where t.order_id in \" + \"&lt;foreach collection='orderIds' item='id' open='(' separator=',' close=')'&gt;\" + \" #&#123;id&#125; \" + \"&lt;/foreach&gt;\"+ \"&lt;/script&gt;\"&#125;) List&lt;Map&gt; selectOrderbyIds(@Param(\"orderIds\")List&lt;Long&gt; orderIds);&#125; 2. 3. 3 测试编写单元测试： 1234567891011121314151617181920212223@RunWith(SpringRunner.class)@SpringBootTest(classes = &#123;ShardingJdbcSimpleDemoBootstrap.class&#125;)public class OrderDaoTest &#123; @Autowired private OrderDao orderDao; @Test public void testInsertOrder()&#123; for (int i = 0 ; i&lt;10; i++)&#123; orderDao.insertOrder(new BigDecimal((i+1)*5),1L,\"WAIT_PAY\"); &#125; &#125; @Test public void testSelectOrderbyIds()&#123; List&lt;Long&gt; ids = new ArrayList&lt;&gt;(); ids.add(373771636085620736L); ids.add(373771635804602369L); List&lt;Map&gt; maps = orderDao.selectOrderbyIds(ids); System.out.println(maps); &#125;&#125; 执行testInsertOrder： 通过日志可以发现order_id为奇数的被插入到t_order_ 2 表，为偶数的被插入到t_order_ 1 表，达到预期目标。 执行testSelectOrderbyIds： 通过日志可以发现，根据传入order_id的奇偶不同，sharding-jdbc分别去不同的表检索数据，达到预期目标。 2. 4 流程分析通过日志分析，Sharding-JDBC在拿到用户要执行的sql之后干了哪些事儿： （ 1 ）解析sql，获取片键值，在本例中是order_id （ 2 ）Sharding-JDBC通过规则配置 t_order_$-&gt;{order_id % 2 + 1 }，知道了当order_id为偶数时，应该往t_order_ 1 表插数据，为奇数时，往t_order_ 2 插数据。 （ 3 ）于是Sharding-JDBC根据order_id的值改写sql语句，改写后的SQL语句是真实所要执行的SQL语句。 （ 4 ）执行改写后的真实sql语句 （ 5 ）将所有真正执行sql的结果进行汇总合并，返回。 2. 5 其他集成方式Sharding-JDBC不仅可以与spring boot良好集成，它还支持其他配置方式，共支持以下四种集成方式。 Spring Boot Yaml 配置 定义application.yml，内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748server: port: 56081 servlet: context‐path: /sharding‐jdbc‐simple‐demospring: application: name: sharding‐jdbc‐simple‐demo http: encoding: enabled: true charset: utf‐8 force: true main: allow‐bean‐definition‐overriding: true shardingsphere: datasource: names: m1 m1: type: com.alibaba.druid.pool.DruidDataSource driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/order_db?useUnicode=true username: root password: mysql sharding: tables: t_order: actualDataNodes: m1.t_order_$‐&gt;&#123;1..2&#125; tableStrategy: inline: shardingColumn: order_id algorithmExpression: t_order_$‐&gt;&#123;order_id % 2 + 1&#125; keyGenerator: type: SNOWFLAKE column: order_id props: sql: show: truemybatis: configuration: map‐underscore‐to‐camel‐case: trueswagger: enable: truelogging: level: root: info org.springframework.web: info com.itheima.dbsharding: debug druid.sql: debug 如果使用application.yml则需要屏蔽原来的application.properties文件。 Java 配置 添加配置类： 12345678910111213141516171819202122232425262728293031323334353637383940 @Configurationpublic class ShardingJdbcConfig &#123; // 定义数据源 Map&lt;String, DataSource&gt; createDataSourceMap() &#123; DruidDataSource dataSource1 = new DruidDataSource(); dataSource1.setDriverClassName(\"com.mysql.jdbc.Driver\"); dataSource1.setUrl(\"jdbc:mysql://localhost:3306/order_db?useUnicode=true\"); dataSource1.setUsername(\"root\"); dataSource1.setPassword(\"root\"); Map&lt;String, DataSource&gt; result = new HashMap&lt;&gt;(); result.put(\"m1\", dataSource1); return result; &#125; // 定义主键生成策略 private static KeyGeneratorConfiguration getKeyGeneratorConfiguration() &#123; KeyGeneratorConfiguration result = new KeyGeneratorConfiguration(\"SNOWFLAKE\",\"order_id\"); return result; &#125; // 定义t_order表的分片策略 TableRuleConfiguration getOrderTableRuleConfiguration() &#123; TableRuleConfiguration result = new TableRuleConfiguration(\"t_order\",\"m1.t_order_$‐&gt;&#123;1..2&#125;\"); result.setTableShardingStrategyConfig(new InlineShardingStrategyConfiguration(\"order_id\", \"t_order_$‐&gt;&#123;order_id % 2 + 1&#125;\")); result.setKeyGeneratorConfig(getKeyGeneratorConfiguration()); return result; &#125; // 定义sharding‐Jdbc数据源 @Bean DataSource getShardingDataSource() throws SQLException &#123; ShardingRuleConfiguration shardingRuleConfig = new ShardingRuleConfiguration(); shardingRuleConfig.getTableRuleConfigs().add(getOrderTableRuleConfiguration()); //spring.shardingsphere.props.sql.show = true Properties properties = new Properties(); properties.put(\"sql.show\",\"true\"); return ShardingDataSourceFactory.createDataSource(createDataSourceMap(),shardingRuleConfig,properties); &#125;&#125; 由于采用了配置类所以需要屏蔽原来application.properties文件中spring.shardingsphere开头的配置信息。 还需要在SpringBoot启动类中屏蔽使用spring.shardingsphere配置项的类： 12@SpringBootApplication(exclude = &#123;SpringBootConfiguration.class&#125;)public class ShardingJdbcSimpleDemoBootstrap &#123;....&#125; Spring Boot properties配置 此方式同快速入门程序。 12345678910111213141516171819# 定义数据源spring.shardingsphere.datasource.names = m1spring.shardingsphere.datasource.m1.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m1.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m1.url = jdbc:mysql://localhost:3306/order_db?useUnicode=truespring.shardingsphere.datasource.m1.username = rootspring.shardingsphere.datasource.m1.password = root# 指定t_order表的主键生成策略为SNOWFLAKEspring.shardingsphere.sharding.tables.t_order.key‐generator.column=order_idspring.shardingsphere.sharding.tables.t_order.key‐generator.type=SNOWFLAKE# 指定t_order表的数据分布情况spring.shardingsphere.sharding.tables.t_order.actual‐data‐nodes = m1.t_order_$‐&gt;&#123;1..2&#125;# 指定t_order表的分表策略spring.shardingsphere.sharding.tables.t_order.table‐strategy.inline.sharding‐column = order_idspring.shardingsphere.sharding.tables.t_order.table‐strategy.inline.algorithm‐expression = t_order_$‐&gt;&#123;order_id % 2 + 1&#125; Spring命名空间配置 此方式使用xml方式配置，不推荐使用。 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version=\"1.0\" encoding=\"UTF‐8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema‐instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:sharding=\"http://shardingsphere.apache.org/schema/shardingsphere/sharding\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring‐beans.xsd http://shardingsphere.apache.org/schema/shardingsphere/sharding http://shardingsphere.apache.org/schema/shardingsphere/sharding/sharding.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring‐context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring‐tx.xsd\"&gt; &lt;context:annotation‐config /&gt; &lt;!‐‐定义多个数据源‐‐&gt; &lt;bean id=\"m1\" class=\"com.alibaba.druid.pool.DruidDataSource\" destroy‐method=\"close\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/order_db_1?useUnicode=true\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"root\" /&gt; &lt;/bean&gt; &lt;!‐‐定义分库策略‐‐&gt; &lt;sharding:inline‐strategy id=\"tableShardingStrategy\" sharding‐column=\"order_id\" algorithm‐expression=\"t_order_$‐&gt;&#123;order_id % 2 + 1&#125;\" /&gt; &lt;!‐‐定义主键生成策略‐‐&gt; &lt;sharding:key‐generator id=\"orderKeyGenerator\" type=\"SNOWFLAKE\" column=\"order_id\" /&gt; &lt;!‐‐定义sharding‐Jdbc数据源‐‐&gt; &lt;sharding:data‐source id=\"shardingDataSource\"&gt; &lt;sharding:sharding‐rule data‐source‐names=\"m1\"&gt; &lt;sharding:table‐rules&gt; &lt;sharding:table‐rule logic‐table=\"t_order\" table‐trategy‐ ref=\"tableShardingStrategy\" key‐generator‐ref=\"orderKeyGenerator\" /&gt; &lt;/sharding:table‐rules&gt; &lt;/sharding:sharding‐rule&gt; &lt;/sharding:data‐source&gt;&lt;/beans&gt; 3 Sharding-JDBC执行原理3. 1 基本概念在了解Sharding-JDBC的执行原理前，需要了解以下概念： 逻辑表 水平拆分的数据表的总称。例：订单数据表根据主键尾数拆分为 10 张表，分别是t_order_ 0 、t_order_ 1 到t_order_ 9 ，他们的逻辑表名为t_order。 真实表 在分片的数据库中真实存在的物理表。即上个示例中的t_order_ 0 到t_order_ 9 。 数据节点 数据分片的最小物理单元。由数据源名称和数据表组成，例：ds_ 0 .t_order_ 0 。 绑定表 指分片规则一致的主表和子表。例如：t_order表和t_order_item表，均按照order_id分片,绑定表之间的分区键完全相同，则此两张表互为绑定表关系。绑定表之间的多表关联查询不会出现笛卡尔积关联，关联查询效率将大大提升。举例说明，如果SQL为： 1SELECT i.* FROM t_order o JOIN t_order_item i ON o.order_id&#x3D;i.order_id WHERE o.order_id in (10,11); 在不配置绑定表关系时，假设分片键order_id将数值 10 路由至第 0 片，将数值 11 路由至第 1 片，那么路由后的SQL应该为 4 条，它们呈现为笛卡尔积： 1234SELECT i.* FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id&#x3D;i.order_id WHERE o.order_id in (10, 11);SELECT i.* FROM t_order_0 o JOIN t_order_item_1 i ON o.order_id&#x3D;i.order_id WHERE o.order_id in (10, 11);SELECT i.* FROM t_order_1 o JOIN t_order_item_0 i ON o.order_id&#x3D;i.order_id WHERE o.order_id in (10, 11);SELECT i.* FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id&#x3D;i.order_id WHERE o.order_id in (10, 11); 在配置绑定表关系后，路由的SQL应该为 2 条： 12SELECT i.* FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id&#x3D;i.order_id WHERE o.order_id in (10, 11);SELECT i.* FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id&#x3D;i.order_id WHERE o.order_id in (10, 11); 广播表 指所有的分片数据源中都存在的表，表结构和表中的数据在每个数据库中均完全一致。适用于数据量不大且需要与 海量数据的表进行关联查询的场景，例如：字典表。 分片键 用于分片的数据库字段，是将数据库(表)水平拆分的关键字段。例：将订单表中的订单主键的尾数取模分片，则订单主键为分片字段。 SQL中如果无分片字段，将执行全路由，性能较差。 除了对单分片字段的支持，Sharding-Jdbc也支持根据多个字段进行分片。 分片算法 通过分片算法将数据分片，支持通过=、BETWEEN和IN分片。分片算法需要应用方开发者自行实现，可实现的灵活度非常高。包括：精确分片算法 、范围分片算法 ，复合分片算法 等。例如：where order_id =? 将采用精确分片算法，where order_id in (?,?,?)将采用精确分片算法，where order_id BETWEEN? and? 将采用范围分片算法，复合分片算法用于分片键有多个复杂情况。 分片策略 包含分片键和分片算法，由于分片算法的独立性，将其独立抽离。真正可用于分片操作的是分片键 + 分片算法，也就是分片策略。内置的分片策略大致可分为尾数取模、哈希、范围、标签、时间等。由用户方配置的分片策略则更加灵活，常用的使用行表达式配置分片策略，它采用Groovy表达式表示，如: t_user_$-&gt;{u_id % 8 } 表示t_user表根据u_id模 8 ，而分成 8 张表，表名称为t_user_ 0 到t_user_ 7 。 自增主键生成策略 通过在客户端生成自增主键替换以数据库原生自增主键的方式，做到分布式主键无重复。 3. 2 SQL解析当Sharding-JDBC接受到一条SQL语句时，会陆续执行SQL解析 =&gt; 查询优化 =&gt; SQL路由 =&gt; SQL改写 =&gt; SQL执行 =&gt;结果归并，最终返回执行结果。 SQL解析过程分为词法解析和语法解析。 词法解析器用于将SQL拆解为不可再分的原子符号，称为Token。并根据不同数据库方言所提供的字典，将其归类为关键字，表达式，字面量和操作符。 再使用语法解析器将SQL转换为抽象语法树。 例如，以下SQL： 1SELECT id, name FROM t_user WHERE status &#x3D; &#39;ACTIVE&#39; AND age &gt; 18 解析之后的为抽象语法树见下图： 为了便于理解，抽象语法树中的关键字的Token用绿色表示，变量的Token用红色表示，灰色表示需要进一步拆分。 最后，通过对抽象语法树的遍历去提炼分片所需的上下文，并标记有可能需要SQL改写(后边介绍)的位置。 供分片使用的解析上下文包含查询选择项（Select Items）、表信息（Table）、分片条件（Sharding Condition）、自增主键信息（Auto increment Primary Key）、排序信息（Order By）、分组信息（Group By）以及分页信息（Limit、Rownum、Top）。 3. 3 SQL路由SQL路由就是把针对逻辑表的数据操作映射到对数据结点操作的过程。 根据解析上下文匹配数据库和表的分片策略，并生成路由路径。 对于携带分片键的SQL，根据分片键操作符不同可 以划分为单片路由(分片键的操作符是等号)、多片路由(分片键的操作符是IN)和范围路由(分片键的操作符是 BETWEEN)，不携带分片键的SQL则采用广播路由。根据分片键进行路由的场景可分为直接路由、标准路由、笛卡 尔路由等。 标准路由 标准路由是Sharding-Jdbc最为推荐使用的分片方式，它的适用范围是不包含关联查询或仅包含绑定表之间关联查询的SQL。 当分片运算符是等于号时，路由结果将落入单库（表），当分片运算符是BETWEEN或IN时，则路由结果不一定落入唯一的库（表），因此一条逻辑SQL最终可能被拆分为多条用于执行的真实SQL。 举例说明，如果按照order_id的奇数和偶数进行数据分片，一个单表查询的SQL如下： 1SELECT * FROM t_order WHERE order_id IN (1, 2); 那么路由的结果应为： 12SELECT * FROM t_order_ 0 WHERE order_id IN ( 1 , 2 );SELECT * FROM t_order_ 1 WHERE order_id IN ( 1 , 2 ); 绑定表的关联查询与单表查询复杂度和性能相当。举例说明，如果一个包含绑定表的关联查询的SQL如下： 1SELECT * FROM t_order o JOIN t_order_item i ON o.order_id&#x3D;i.order_id WHERE order_id IN (1, 2); 那么路由的结果应为： 12SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id&#x3D;i.order_id WHERE order_id IN (1,2);SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id&#x3D;i.order_id WHERE order_id IN (1,2); 可以看到，SQL拆分的数目与单表是一致的。 笛卡尔路由 笛卡尔路由是最复杂的情况，它无法根据绑定表的关系定位分片规则，因此非绑定表之间的关联查询需要拆解为笛 卡尔积组合执行。 如果上个示例中的SQL并未配置绑定表关系，那么路由的结果应为： 1234SELECT * FROM t_order_0 o JOIN t_order_item_0 i ON o.order_id&#x3D;i.order_id WHERE order_id IN (1,2);SELECT * FROM t_order_0 o JOIN t_order_item_1 i ON o.order_id&#x3D;i.order_id WHERE order_id IN (1,2);SELECT * FROM t_order_1 o JOIN t_order_item_0 i ON o.order_id&#x3D;i.order_id WHERE order_id IN (1,2);SELECT * FROM t_order_1 o JOIN t_order_item_1 i ON o.order_id&#x3D;i.order_id WHERE order_id IN (1,2); 笛卡尔路由查询性能较低，需谨慎使用。 全库表路由 对于不携带分片键的SQL，则采取广播路由的方式。根据SQL类型又可以划分为全库表路由、全库路由、全实例路 由、单播路由和阻断路由这 5 种类型。其中全库表路由用于处理对数据库中与其逻辑表相关的所有真实表的操作， 主要包括不带分片键的DQL(数据查询)和DML（数据操纵），以及DDL（数据定义）等。例如： 1SELECT * FROM t_order WHERE good_prority IN (1, 10); 则会遍历所有数据库中的所有表，逐一匹配逻辑表和真实表名，能够匹配得上则执行。路由后成为 1234SELECT * FROM t_order_ 0 WHERE good_prority IN ( 1 , 10 );SELECT * FROM t_order_ 1 WHERE good_prority IN ( 1 , 10 );SELECT * FROM t_order_ 2 WHERE good_prority IN ( 1 , 10 );SELECT * FROM t_order_ 3 WHERE good_prority IN ( 1 , 10 ); 3. 4 .SQL改写工程师面向逻辑表书写的SQL，并不能够直接在真实的数据库中执行，SQL改写用于将逻辑SQL改写为在真实数据 库中可以正确执行的SQL。 如一个简单的例子，若逻辑SQL为： 1SELECT order_id FROM t_order WHERE order_id&#x3D;1; 假设该SQL配置分片键order_id，并且order_id= 1 的情况，将路由至分片表 1 。那么改写之后的SQL应该为： 1SELECT order_id FROM t_order_1 WHERE order_id&#x3D;1; 再比如，Sharding-JDBC需要在结果归并时获取相应数据，但该数据并未能通过查询的SQL返回。 这种情况主要是针对GROUP BY和ORDER BY。结果归并时，需要根据GROUP BY和ORDER BY的字段项进行分组和排序，但如果原始SQL的选择项中若并未包含分组项或排序项，则需要对原始SQL进行改写。 先看一下原始SQL中带有结果归并所需信息的场景： 1SELECT order_id, user_id FROM t_order ORDER BY user_id; 由于使用user_id进行排序，在结果归并中需要能够获取到user_id的数据，而上面的SQL是能够获取到user_id数据的，因此无需补列。 如果选择项中不包含结果归并时所需的列，则需要进行补列，如以下SQL： 1SELECT order_id FROM t_order ORDER BY user_id; 由于原始SQL中并不包含需要在结果归并中需要获取的user_id，因此需要对SQL进行补列改写。补列之后的SQL是： 1SELECT order_id, user_id AS ORDER_BY_DERIVED_0 FROM t_order ORDER BY user_id; 3. 5 SQL执行Sharding-JDBC采用一套自动化的执行引擎，负责将路由和改写完成之后的真实SQL安全且高效发送到底层数据源执行。 它不是简单地将SQL通过JDBC直接发送至数据源执行；也并非直接将执行请求放入线程池去并发执行。它更关注平衡数据源连接创建以及内存占用所产生的消耗，以及最大限度地合理利用并发等问题。 执行引擎的目标是自动化的平衡资源控制与执行效率，他能在以下两种模式自适应切换： 内存限制模式 使用此模式的前提是，Sharding-JDBC对一次操作所耗费的数据库连接数量不做限制。 如果实际执行的SQL需要对某数据库实例中的 200 张表做操作，则对每张表创建一个新的数据库连接，并通过多线程的方式并发处理，以达成执行效率最大化。 连接限制模式 使用此模式的前提是，Sharding-JDBC严格控制对一次操作所耗费的数据库连接数量。 如果实际执行的SQL需要对某数据库实例中的 200 张表做操作，那么只会创建唯一的数据库连接，并对其 200 张表串行处理。 如果一次操作中的分片散落在不同的数据库，仍然采用多线程处理对不同库的操作，但每个库的每次操作仍然只创建一个唯一的数据库连接。 内存限制模式适用于OLAP操作，可以通过放宽对数据库连接的限制提升系统吞吐量； 连接限制模式适用于OLTP操作，OLTP通常带有分片键，会路由到单一的分片，因此严格控制数据库连接，以保证在线系统数据库资源能够被更多的应用所使用，是明智的选择。 3. 6 结果归并将从各个数据节点获取的多数据结果集，组合成为一个结果集并正确的返回至请求客户端，称为结果归并。 Sharding-JDBC支持的结果归并从功能上可分为遍历、排序、分组、分页和聚合 5 种类型，它们是组合而非互斥的关系。 归并引擎的整体结构划分如下图。 结果归并从结构划分可分为流式归并、内存归并和装饰者归并。流式归并和内存归并是互斥的，装饰者归并可以在 流式归并和内存归并之上做进一步的处理。 内存归并很容易理解，他是将所有分片结果集的数据都遍历并存储在内存中，再通过统一的分组、排序以及聚合等计算之后，再将其封装成为逐条访问的数据结果集返回。 流式归并是指每一次从数据库结果集中获取到的数据，都能够通过游标逐条获取的方式返回正确的单条数据，它与 数据库原生的返回结果集的方式最为契合。 下边举例说明排序归并的过程，如下图是一个通过分数进行排序的示例图，它采用流式归并方式。 图中展示了 3 张 表返回的数据结果集，每个数据结果集已经根据分数排序完毕，但是 3 个数据结果集之间是无序的。 将 3 个数据结果集的当前游标指向的数据值进行排序，并放入优先级队列，t_score_ 0 的第一个数据值最大，t_score_ 2 的第一个数据值次之，t_score_ 1 的第一个数据值最小，因此优先级队列根据t_score_ 0 ，t_score_ 2 和t_score_ 1 的方式排序队列。 下图则展现了进行next调用的时候，排序归并是如何进行的。 通过图中我们可以看到，当进行第一次next调用时，排在队列首位的t_score_ 0 将会被弹出队列，并且将当前游标指向的数据值（也就是 100 ）返回至查询客户端，并且将游标下移一位之后，重新放入优先级队列。 而优先级队列也会根据t_score_ 0 的当前数据结果集指向游标的数据值（这里是 90 ）进行排序，根据当前数值，t_score_ 0 排列在队列的最后一位。 之前队列中排名第二的t_score_ 2 的数据结果集则自动排在了队列首位。 在进行第二次next时，只需要将目前排列在队列首位的t_score_ 2 弹出队列，并且将其数据结果集游标指向的值返回至客户端，并下移游标，继续加入队列排队，以此类推。 当一个结果集中已经没有数据了，则无需再次加入队列。 可以看到，对于每个数据结果集中的数据有序，而多数据结果集整体无序的情况下，Sharding-JDBC无需将所有的数据都加载至内存即可排序。 它使用的是流式归并的方式，每次next仅获取唯一正确的一条数据，极大的节省了内存的消耗。 装饰者归并是对所有的结果集归并进行统一的功能增强，比如归并时需要聚合SUM前，在进行聚合计算前，都会通过内存归并或流式归并查询出结果集。因此，聚合归并是在之前介绍的归并类型之上追加的归并能力，即装饰者模式。 3. 7 总结通过以上内容介绍，相信大家已经了解到Sharding-JDBC基础概念、核心功能以及执行原理。 基础概念：逻辑表，真实表，数据节点，绑定表，广播表，分片键，分片算法，分片策略，主键生成策略 核心功能：数据分片，读写分离 执行流程：SQL解析 =&gt; 查询优化 =&gt; SQL路由 =&gt; SQL改写 =&gt; SQL执行 =&gt; 结果归并 接下来我们将通过一个个demo，来演示Sharding-JDBC实际使用方法。 4 水平分表前面已经介绍过，水平分表是在同一个数据库内，把同一个表的数据按一定规则拆到多个表中。在快速入门里，我 们已经对水平分库进行实现，这里不再重复介绍。 5 水平分库前面已经介绍过，水平分库是把同一个表的数据按一定规则拆到不同的数据库中，每个库可以放在不同的服务器上。接下来看一下如何使用Sharding-JDBC实现水平分库，咱们继续对快速入门中的例子进行完善。 ( 1 )将原有order_db库拆分为order_db_ 1 、order_db_ 2 ( 2 )分片规则修改 由于数据库拆分了两个，这里需要配置两个数据源。 分库需要配置分库的策略，和分表策略的意义类似，通过分库策略实现数据操作针对分库的数据库进行操作。 12345678910111213141516171819# 定义多个数据源spring.shardingsphere.datasource.names = m1,m2spring.shardingsphere.datasource.m1.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m1.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m1.url = jdbc:mysql://localhost:3306/order_db_1?useUnicode=truespring.shardingsphere.datasource.m1.username = rootspring.shardingsphere.datasource.m1.password = rootspring.shardingsphere.datasource.m2.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m2.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m2.url = jdbc:mysql://localhost:3306/order_db_2?useUnicode=truespring.shardingsphere.datasource.m2.username = rootspring.shardingsphere.datasource.m2.password = root...# 分库策略，以user_id为分片键，分片策略为user_id % 2 + 1，user_id为偶数操作m1数据源，否则操作m2。spring.shardingsphere.sharding.tables.t_order.database‐strategy.inline.sharding‐column = user_idspring.shardingsphere.sharding.tables.t_order.database‐strategy.inline.algorithm‐expression =m$‐&gt;&#123;user_id % 2 + 1&#125; 分库策略定义方式如下： 12345#分库策略，如何将一个逻辑表映射到多个数据源spring.shardingsphere.sharding.tables.&lt;逻辑表名称&gt;.database‐strategy.&lt;分片策略&gt;.&lt;分片策略属性名&gt;= #分片策略属性值#分表策略，如何将一个逻辑表映射为多个实际表spring.shardingsphere.sharding.tables.&lt;逻辑表名称&gt;.table‐strategy.&lt;分片策略&gt;.&lt;分片策略属性名&gt;= #分片策略属性值 Sharding-JDBC支持以下几种分片策略： 不管理分库还是分表，策略基本一样。 standard：标准分片策略，对应StandardShardingStrategy。提供对SQL语句中的=, IN和BETWEEN AND的分片操作支持。StandardShardingStrategy只支持单分片键，提供PreciseShardingAlgorithm和RangeShardingAlgorithm两个分片算法。PreciseShardingAlgorithm是必选的，用于处理=和IN的分片。RangeShardingAlgorithm是可选的，用于处理BETWEEN AND分片，如果不配置RangeShardingAlgorithm，SQL中的BETWEEN AND将按照全库路由处理。 complex：符合分片策略，对应ComplexShardingStrategy。复合分片策略。提供对SQL语句中的=, IN和BETWEEN AND的分片操作支持。ComplexShardingStrategy支持多分片键，由于多分片键之间的关系复杂，因此并未进行过多的封装，而是直接将分片键值组合以及分片操作符透传至分片算法，完全由应用开发者实现，提供最大的灵活度。 inline：行表达式分片策略，对应InlineShardingStrategy。使用Groovy的表达式，提供对SQL语句中的=和IN的分片操作支持，只支持单分片键。对于简单的分片算法，可以通过简单的配置使用，从而避免繁琐的Java代码开发，如: t_user_$-&gt;{u_id % 8 } 表示t_user表根据u_id模 8 ，而分成 8 张表，表名称为t_user_ 0 到t_user_ 7 。 hint：Hint分片策略，对应HintShardingStrategy。通过Hint而非SQL解析的方式分片的策略。对于分片字段非SQL决定，而由其他外置条件决定的场景，可使用SQL Hint灵活的注入分片字段。例：内部系统，按照员工登录主键分库，而数据库中并无此字段。SQL Hint支持通过Java API和SQL注释(待实现)两种方式使用 none：不分片策略，对应NoneShardingStrategy。不分片的策略。 目前例子中都使用inline分片策略，若对其他分片策略细节若感兴趣，请查阅官方文档：https://shardingsphere.apache.org ( 3 )插入测试 修改testInsertOrder方法，插入数据中包含不同的user_id 12345678910@Testpublic void testInsertOrder()&#123; for (int i = 0 ; i&lt;10; i++)&#123; orderDao.insertOrder(new BigDecimal((i+1)*5),1L,\"WAIT_PAY\"); &#125; for (int i = 0 ; i&lt;10; i++)&#123; orderDao.insertOrder(new BigDecimal((i+1)*10),2L,\"WAIT_PAY\"); &#125;&#125; 执行testInsertOrder: 通过日志可以看出，根据user_id的奇偶不同，数据分别落在了不同数据源，达到目标。 （ 4 ）查询测试 调用快速入门的查询接口进行测试： 1List&lt;Map&gt; selectOrderbyIds(@Param(\"orderIds\")List&lt;Long&gt; orderIds); 通过日志发现，sharding-jdbc将sql路由到m 1 和m 2 ： 问题分析： 由于查询语句中并没有使用分片键user_id，所以sharding-jdbc将广播路由到每个数据结点。 下边我们在sql中添加分片键进行查询。 在OrderDao中定义接口： 123456789101112@Select(&#123;\"&lt;script&gt;\", \" select\", \" * \", \" from t_order t \", \"where t.order_id in\", \"&lt;foreach collection='orderIds' item='id' open='(' separator=',' close=')'&gt;\", \"#&#123;id&#125;\", \"&lt;/foreach&gt;\", \" and t.user_id = #&#123;userId&#125; \", \"&lt;/script&gt;\"&#125;)List&lt;Map&gt; selectOrderbyUserAndIds(@Param(\"userId\") Integer userId,@Param(\"orderIds\")List&lt;Long&gt; orderIds); 编写测试方法： 123456789101112@Test^public void testSelectOrderbyUserAndIds()&#123; List&lt;Long&gt; orderIds = new ArrayList&lt;&gt;(); orderIds.add( 373422416644276224 L); orderIds.add( 373422415830581248 L); //查询条件中包括分库的键user_id int user_id = 1 ; List&lt;Map&gt; orders = orderDao.selectOrderbyUserAndIds(user_id,orderIds); JSONArray jsonOrders = new JSONArray(orders); System.out.println(jsonOrders);&#125; 执行testSelectOrderbyUserAndIds: 查询条件user_id为 1 ，根据分片策略m$-&gt;{user_id % 2 + 1 }计算得出m 2 ，此sharding-jdbc将sql路由到m 2 ，见上图日志。 6 垂直分库前面已经介绍过，垂直分库是指按照业务将表进行分类，分布到不同的数据库上面，每个库可以放在不同的服务器上，它的核心理念是专库专用。接下来看一下如何使用Sharding-JDBC实现垂直分库。 ( 1 )创建数据库 创建数据库user_db 1CREATE DATABASE &#96;user_db&#96; CHARACTER SET &#39;utf8&#39; COLLATE &#39;utf8_general_ci&#39;; 在user_db中创建t_user表 1234567DROP TABLE IF EXISTS &#96;t_user&#96;;CREATE TABLE &#96;t_user&#96; ( &#96;user_id&#96; bigint(20) NOT NULL COMMENT &#39;用户id&#39;, &#96;fullname&#96; varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT &#39;用户姓名&#39;, &#96;user_type&#96; char(1) DEFAULT NULL COMMENT &#39;用户类型&#39;, PRIMARY KEY (&#96;user_id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic; ( 2 )在Sharding-JDBC规则中修改 123456789101112131415# 新增m0数据源，对应user_dbspring.shardingsphere.datasource.names = m0,m1,m2...spring.shardingsphere.datasource.m0.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m0.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m0.url = jdbc:mysql://localhost:3306/user_db?useUnicode=truespring.shardingsphere.datasource.m0.username = rootspring.shardingsphere.datasource.m0.password = root....# t_user分表策略，固定分配至m0的t_user真实表spring.shardingsphere.sharding.tables.t_user.actual‐data‐nodes = m$‐&gt;&#123;0&#125;.t_userspring.shardingsphere.sharding.tables.t_user.table‐strategy.inline.sharding‐column = user_idspring.shardingsphere.sharding.tables.t_user.table‐strategy.inline.algorithm‐expression = t_user ( 3 )数据操作 新增UserDao: 123456789101112131415161718192021222324252627282930@Mapper@Componentpublic interface UserDao &#123; /** * 新增用户 * @param userId 用户id * @param fullname 用户姓名 * @return */ @Insert(\"insert into t_user(user_id, fullname) value(#&#123;userId&#125;,#&#123;fullname&#125;)\") int insertUser(@Param(\"userId\")Long userId,@Param(\"fullname\")String fullname); /** * 根据id列表查询多个用户 * @param userIds 用户id列表 * @return */ @Select(&#123;\"&lt;script&gt;\", \" select\", \" * \", \" from t_user t \", \" where t.user_id in\", \"&lt;foreach collection='userIds' item='id' open='(' separator=',' close=')'&gt;\", \"#&#123;id&#125;\", \"&lt;/foreach&gt;\", \"&lt;/script&gt;\" &#125;) List&lt;Map&gt; selectUserbyIds(@Param(\"userIds\")List&lt;Long&gt; userIds);&#125; ( 4 )测试 新增单元测试方法： 1234567891011121314151617@Testpublic void testInsertUser()&#123; for (int i = 0 ; i&lt;10; i++)&#123; Long id = i + 1L; userDao.insertUser(id,\"姓名\"+ id ); &#125;&#125;@Testpublic void testSelectUserbyIds()&#123; List&lt;Long&gt; userIds = new ArrayList&lt;&gt;(); userIds.add(1L); userIds.add(2L); List&lt;Map&gt; users = userDao.selectUserbyIds(userIds); System.out.println(users);&#125; 执行testInsertUser: 通过日志可以看出t_user表的数据被落在了m 0 数据源，达到目标。 执行testSelectUserbyIds: 通过日志可以看出t_user表的查询操作被落在了m 0 数据源，达到目标。 7 公共表公共表属于系统中数据量较小，变动少，而且属于高频联合查询的依赖表。参数表、数据字典表等属于此类型。可 以将这类表在每个数据库都保存一份，所有更新操作都同时发送到所有分库执行。接下来看一下如何使用Sharding-JDBC实现公共表。 ( 1 )创建数据库 分别在user_db、order_db_ 1 、order_db_ 2 中创建t_dict表： 1234567CREATE TABLE &#96;t_dict&#96; ( &#96;dict_id&#96; bigint(20) NOT NULL COMMENT &#39;字典id&#39;, &#96;type&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT &#39;字典类型&#39;, &#96;code&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT &#39;字典编码&#39;, &#96;value&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT &#39;字典值&#39;, PRIMARY KEY (&#96;dict_id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic; ( 2 )在Sharding-JDBC规则中修改 12# 指定t_dict为公共表spring.shardingsphere.sharding.broadcast‐tables=t_dict ( 3 )数据操作 新增DictDao： 12345678910111213141516171819202122@Mapper@Componentpublic interface DictDao &#123; /** * 新增字典 * @param type 字典类型 * @param code 字典编码 * @param value 字典值 * @return */ @Insert(\"insert into t_dict(dict_id,type,code,value) value(#&#123;dictId&#125;,#&#123;type&#125;,#&#123;code&#125;,#&#123;value&#125;)\") int insertDict(@Param(\"dictId\") Long dictId,@Param(\"type\") String type, @Param(\"code\")String code, @Param(\"value\")String value); /** * 删除字典 * @param dictId 字典id * @return */ @Delete(\"delete from t_dict where dict_id = #&#123;dictId&#125;\") int deleteDict(@Param(\"dictId\") Long dictId);&#125; ( 4 )字典操作测试 新增单元测试方法： 1234567891011@Testpublic void testInsertDict()&#123; dictDao.insertDict(1L,\"user_type\",\"0\",\"管理员\"); dictDao.insertDict(2L,\"user_type\",\"1\",\"操作员\"); &#125;@Testpublic void testDeleteDict()&#123; dictDao.deleteDict(1L); dictDao.deleteDict(2L); &#125; 执行testInsertDict： 通过日志可以看出，对t_dict的表的操作被广播至所有数据源。 测试删除字典，观察是否把所有数据源中该 公共表的记录删除。 （ 5 ）字典关联查询测试 字典表已在各各分库存在，各业务表即可和字典表关联查询。 定义用户关联查询dao： 在UserDao中定义： 12345678910111213141516/** * 根据id列表查询多个用户，关联查询字典表 * @param userIds 用户id列表 * @return */@Select(&#123;\"&lt;script&gt;\", \" select\", \" * \", \" from t_user t ,t_dict b\", \" where t.user_type = b.code and t.user_id in\", \"&lt;foreach collection='userIds' item='id' open='(' separator=',' close=')'&gt;\", \"#&#123;id&#125;\", \"&lt;/foreach&gt;\", \"&lt;/script&gt;\"&#125;)List&lt;Map&gt; selectUserInfobyIds(@Param(\"userIds\")List&lt;Long&gt; userIds); 定义测试方法： 12345678910@Test public void testSelectUserInfobyIds()&#123; List&lt;Long&gt; userIds = new ArrayList&lt;&gt;(); userIds.add(1L); userIds.add(2L); List&lt;Map&gt; users = userDao.selectUserInfobyIds(userIds); JSONArray jsonUsers = new JSONArray(users); System.out.println(jsonUsers);&#125; 执行测试方法，查看日志，成功关联查询字典表： 8 读写分离8. 1 理解读写分离面对日益增加的系统访问量，数据库的吞吐量面临着巨大瓶颈。 对于同一时刻有大量并发读操作和较少写操作类 型的应用系统来说，将数据库拆分为主库和从库，主库负责处理事务性的增删改操作，从库负责处理查询操作，能 够有效的避免由数据更新导致的行锁，使得整个系统的查询性能得到极大的改善。 通过一主多从的配置方式，可以将查询请求均匀的分散到多个数据副本，能够进一步的提升系统的处理能力。 使用 多主多从的方式，不但能够提升系统的吞吐量，还能够提升系统的可用性，可以达到在任何一个数据库宕机，甚至 磁盘物理损坏的情况下仍然不影响系统的正常运行。 读写分离的数据节点中的数据内容是一致的，而水平分片的每个数据节点的数据内容却并不相同。将水平分片和读 写分离联合使用，能够更加有效的提升系统的性能。 Sharding-JDBC读写分离则是根据SQL语义的分析，将读操作和写操作分别路由至主库与从库。它提供透明化读写分离，让使用方尽量像使用一个数据库一样使用主从数据库集群。 Sharding-JDBC提供一主多从的读写分离配置，可独立使用，也可配合分库分表使用，同一线程且同一数据库连接内，如有写入操作，以后的读操作均从主库读取，用于保证数据一致性。Sharding-JDBC不提供主从数据库的数据同步功能，需要采用其他机制支持。 接下来，咱们对上面例子中user_db进行读写分离实现。为了实现Sharding-JDBC的读写分离，首先，要进行mysql的主从同步配置。 8. 2 mysql主从同步(windows)一，新增mysql实例 复制原有mysql如：D:\\mysql- 5. 7. 25 (作为主库) - &gt; D:\\mysql- 5. 7. 25 - s 1 (作为从库)，并修改以下从库的my.ini： 1234567[mysqld]#设置3307端口port = 3307# 设置mysql的安装目录basedir=D:\\mysql‐5.7.25‐s1# 设置mysql数据库的数据的存放目录datadir=D:\\mysql‐5.7.25‐s1\\data 然后将从库安装为windows服务，注意配置文件位置： 1D:\\mysql‐5.7.25‐s1\\bin&gt;mysqld install mysqls1 ‐‐defaults‐file&#x3D;&quot;D:\\mysql‐5.7.25‐s1\\my.ini&quot; 由于从库是从主库复制过来的，因此里面的数据完全一致，可使用原来的账号、密码登录。 二，修改主、从库的配置文件(my.ini)，新增内容如下： 主库： 1234567891011[mysqld]#开启日志log‐bin = mysql‐bin#设置服务id，主从不能一致server‐id = 1#设置需要同步的数据库binlog‐do‐db=user_db#屏蔽系统库同步binlog‐ignore‐db=mysqlbinlog‐ignore‐db=information_schemabinlog‐ignore‐db=performance_schema 从库： 1234567891011[mysqld]#开启日志log‐bin = mysql‐bin#设置服务id，主从不能一致server‐id = 2#设置需要同步的数据库replicate_wild_do_table=user_db.%#屏蔽系统库同步replicate_wild_ignore_table=mysql.%replicate_wild_ignore_table=information_schema.%replicate_wild_ignore_table=performance_schema.% 重启主库和从库： 12net start [主库服务名]net start [从库服务名mysqls1] 请注意，主从MySQL下的数据(data)目录下有个文件auto.cnf，文件中定义了uuid，要保证主从数据库实例的uuid不一样，建议直接删除掉，重启服务后将会重新生成。 三，授权主从复制专用账号 12345678#切换至主库bin目录，登录主库mysql ‐h localhost ‐uroot ‐p#授权主备复制专用账号GRANT REPLICATION SLAVE ON *.* TO 'db_sync'@'%' IDENTIFIED BY 'db_sync';#刷新权限FLUSH PRIVILEGES;#确认位点 记录下文件名以及位点show master status; 四，设置从库向主库同步数据、并检查链路 12345678910111213141516171819#切换至从库bin目录，登录从库mysql ‐h localhost ‐P3307 ‐uroot ‐p#先停止同步 STOP SLAVE;#修改从库指向到主库，使用上一步记录的文件名以及位点CHANGE MASTER TO master_host = 'localhost', master_user = 'db_sync', master_password = 'db_sync', master_log_file = 'mysql‐bin.000002', master_log_pos = 154;#启动同步START SLAVE;#查看从库状态Slave_IO_Runing和Slave_SQL_Runing都为Yes说明同步成功，如果不为Yes，请检查error_log，然后排查相关异常。show slave status\\G#注意 如果之前此备库已有主库指向 需要先执行以下命令清空STOP SLAVE IO_THREAD FOR CHANNEL '';reset slave all; 最后测试在主库修改数据库，看从库是否能够同步成功。 8. 3 实现sharding-jdbc读写分离( 1 )在Sharding-JDBC规则中修改 123456789101112131415# 增加数据源s0，使用上面主从同步配置的从库。spring.shardingsphere.datasource.names = m0,m1,m2,s0...spring.shardingsphere.datasource.s0.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.s0.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.s0.url = jdbc:mysql://localhost:3307/user_db?useUnicode=truespring.shardingsphere.datasource.s0.username = rootspring.shardingsphere.datasource.s0.password = root....# 主库从库逻辑数据源定义 ds0为user_dbspring.shardingsphere.sharding.master‐slave‐rules.ds0.master‐data‐source‐name=m0spring.shardingsphere.sharding.master‐slave‐rules.ds0.slave‐data‐source‐names=s0# t_user分表策略，固定分配至ds0的t_user真实表spring.shardingsphere.sharding.tables.t_user.actual‐data‐nodes = ds0.t_user.... ( 2 )测试 执行testInsertUser单元测试： 通过日志可以看出，所有写操作落入m 0 数据源。 执行testSelectUserbyIds单元测试： 通过日志可以看出，所有写操作落入s 0 数据源，达到目标。 9 案例9. 1 需求描述电商平台商品列表展示，每个列表项中除了包含商品基本信息、商品描述信息之外，还包括了商品所属的店铺信息，如下： 本案例实现功能如下： 1 、添加商品 2 、商品分页查询 3 、商品统计 9. 2 数据库设计数据库设计如下，其中商品与店铺信息之间进行了垂直分库，分为了PRODUCT_DB(商品库)和STORE_DB(店铺库)；商品信息还进行了垂直分表，分为了商品基本信息(product_info)和商品描述信息(product_descript)，地理区域信息(region)作为公共表，冗余在两库中： 考虑到商品信息的数据增长性，对PRODUCT_DB(商品库)进行了水平分库，分片键使用店铺id，分片策略为店铺ID% 2 + 1 ，因此商品描述信息对所属店铺ID进行了冗余； 对商品基本信息(product_info)和商品描述信息(product_descript)进行水平分表，分片键使用商品id，分片策略为商品ID% 2 + 1 ,并将为这两个表设置为绑定表，避免笛卡尔积join； 为避免主键冲突，ID生成策略采用雪花算法来生成全局唯一ID，最终数据库设计为下图： 要求使用读写分离来提升性能，可用性。 9. 3 环境说明 操作系统：Win 10 数据库：MySQL- 5. 7. 25 JDK： 64 位 jdk 1. 8. 0 _ 201 应用框架：spring-boot-^2.^1.^3 .RELEASE，Mybatis^3.^5.^0 Sharding-JDBC：sharding-jdbc-spring-boot-starter- 4. 0. 0 - RC 1 9. 4 环境准备9. 4. 1 mysql主从同步(windows)参考读写分离章节，对以下库进行主从同步配置： 1234# 设置需要同步的数据库binlog‐do‐db=store_dbbinlog‐do‐db=product_db_1binlog‐do‐db=product_db_2 9. 4. 2 初始化数据库创建store_db数据库，并执行以下脚本创建表： 1234567891011121314151617181920212223242526DROP TABLE IF EXISTS &#96;region&#96;;CREATE TABLE &#96;region&#96; ( &#96;id&#96; bigint(20) NOT NULL COMMENT &#39;id&#39;, &#96;region_code&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;地理区域编码&#39;, &#96;region_name&#96; varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;地理区域名称&#39;, &#96;level&#96; tinyint(1) NULL DEFAULT NULL COMMENT &#39;地理区域级别(省、市、县)&#39;, &#96;parent_region_code&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;上级地理区域编码&#39;, PRIMARY KEY (&#96;id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic;INSERT INTO &#96;region&#96; VALUES (1, &#39;110000&#39;, &#39;北京&#39;, 0, NULL);INSERT INTO &#96;region&#96; VALUES (2, &#39;410000&#39;, &#39;河南省&#39;, 0, NULL);INSERT INTO &#96;region&#96; VALUES (3, &#39;110100&#39;, &#39;北京市&#39;, 1, &#39;110000&#39;);INSERT INTO &#96;region&#96; VALUES (4, &#39;410100&#39;, &#39;郑州市&#39;, 1, &#39;410000&#39;);DROP TABLE IF EXISTS &#96;store_info&#96;;CREATE TABLE &#96;store_info&#96; ( &#96;id&#96; bigint(20) NOT NULL COMMENT &#39;id&#39;, &#96;store_name&#96; varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;店铺名称&#39;, &#96;reputation&#96; int(11) NULL DEFAULT NULL COMMENT &#39;信誉等级&#39;, &#96;region_code&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;店铺所在地&#39;, PRIMARY KEY (&#96;id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic;INSERT INTO &#96;store_info&#96; VALUES (1, &#39;XX零食店&#39;, 4, &#39;110100&#39;);INSERT INTO &#96;store_info&#96; VALUES (2, &#39;XX饮品店&#39;, 3, &#39;410100&#39;); 创建product_db_ 1 、product_db_ 2 数据库，并分别对两库执行以下脚本创建表： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960DROP TABLE IF EXISTS &#96;product_descript_1&#96;;CREATE TABLE &#96;product_descript_1&#96; ( &#96;id&#96; bigint(20) NOT NULL COMMENT &#39;id&#39;, &#96;product_info_id&#96; bigint(20) NULL DEFAULT NULL COMMENT &#39;所属商品id&#39;, &#96;descript&#96; longtext CHARACTER SET utf8 COLLATE utf8_general_ci NULL COMMENT &#39;商品描述&#39;, &#96;store_info_id&#96; bigint(20) NULL DEFAULT NULL COMMENT &#39;所属店铺id&#39;, PRIMARY KEY (&#96;id&#96;) USING BTREE, INDEX &#96;FK_Reference_2&#96;(&#96;product_info_id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic;DROP TABLE IF EXISTS &#96;product_descript_2&#96;;CREATE TABLE &#96;product_descript_2&#96; ( &#96;id&#96; bigint(20) NOT NULL COMMENT &#39;id&#39;, &#96;product_info_id&#96; bigint(20) NULL DEFAULT NULL COMMENT &#39;所属商品id&#39;, &#96;descript&#96; longtext CHARACTER SET utf8 COLLATE utf8_general_ci NULL COMMENT &#39;商品描述&#39;, &#96;store_info_id&#96; bigint(20) NULL DEFAULT NULL COMMENT &#39;所属店铺id&#39;, PRIMARY KEY (&#96;id&#96;) USING BTREE, INDEX &#96;FK_Reference_2&#96;(&#96;product_info_id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic;DROP TABLE IF EXISTS &#96;product_info_1&#96;;CREATE TABLE &#96;product_info_1&#96; ( &#96;product_info_id&#96; bigint(20) NOT NULL COMMENT &#39;id&#39;, &#96;store_info_id&#96; bigint(20) NULL DEFAULT NULL COMMENT &#39;所属店铺id&#39;, &#96;product_name&#96; varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;商品名称&#39;, &#96;spec&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;规格&#39;, &#96;region_code&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;产地&#39;, &#96;price&#96; decimal(10, 0) NULL DEFAULT NULL COMMENT &#39;商品价格&#39;, &#96;image_url&#96; varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;商品图片&#39;, PRIMARY KEY (&#96;product_info_id&#96;) USING BTREE, INDEX &#96;FK_Reference_1&#96;(&#96;store_info_id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic;DROP TABLE IF EXISTS &#96;product_info_2&#96;;CREATE TABLE &#96;product_info_2&#96; ( &#96;product_info_id&#96; bigint(20) NOT NULL COMMENT &#39;id&#39;, &#96;store_info_id&#96; bigint(20) NULL DEFAULT NULL COMMENT &#39;所属店铺id&#39;, &#96;product_name&#96; varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;商品名称&#39;, &#96;spec&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;规格&#39;, &#96;region_code&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;产地&#39;, &#96;price&#96; decimal(10, 0) NULL DEFAULT NULL COMMENT &#39;商品价格&#39;, &#96;image_url&#96; varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;商品图片&#39;, PRIMARY KEY (&#96;product_info_id&#96;) USING BTREE, INDEX &#96;FK_Reference_1&#96;(&#96;store_info_id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic;DROP TABLE IF EXISTS &#96;region&#96;;CREATE TABLE &#96;region&#96; ( &#96;id&#96; bigint(20) NOT NULL COMMENT &#39;id&#39;, &#96;region_code&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;地理区域编码&#39;, &#96;region_name&#96; varchar(100) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;地理区域名称&#39;, &#96;level&#96; tinyint(1) NULL DEFAULT NULL COMMENT &#39;地理区域级别(省、市、县)&#39;, &#96;parent_region_code&#96; varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT &#39;上级地理区域编码&#39;, PRIMARY KEY (&#96;id&#96;) USING BTREE) ENGINE &#x3D; InnoDB CHARACTER SET &#x3D; utf8 COLLATE &#x3D; utf8_general_ci ROW_FORMAT &#x3D; Dynamic;INSERT INTO &#96;region&#96; VALUES (1, &#39;110000&#39;, &#39;北京&#39;, 0, NULL);INSERT INTO &#96;region&#96; VALUES (2, &#39;410000&#39;, &#39;河南省&#39;, 0, NULL);INSERT INTO &#96;region&#96; VALUES (3, &#39;110100&#39;, &#39;北京市&#39;, 1, &#39;110000&#39;);INSERT INTO &#96;region&#96; VALUES (4, &#39;410100&#39;, &#39;郑州市&#39;, 1, &#39;410000&#39;); 9. 5 实现步骤9. 5. 1 搭建maven工程（ 1 ）搭建工程maven工程shopping，导入资料中基础代码shopping，以dbsharding为总体父工程，并做好spring boot相关配置。 （ 2 ）引入maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding‐jdbc‐spring‐boot‐starter&lt;/artifactId&gt; &lt;version&gt;4.0.0‐RC1&lt;/version&gt; &lt;/dependency&gt; 9. 5. 2 分片配置既然是分库分表，那么就需要定义多个真实数据源，每一个数据库链接信息就是一个数据源定义，如： 12345spring.shardingsphere.datasource.m0.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m0.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m0.url = jdbc:mysql://localhost:3306/store_db?useUnicode=truespring.shardingsphere.datasource.m0.username = rootspring.shardingsphere.datasource.m0.password = root m0 ，就是这个真实数据源的名称，然后需要告诉Sharding-JDBC，咱们有哪些真实数据源，如： 1spring.shardingsphere.datasource.names = m0,m1,m2,s0,s1,s2 如果需要配置读写分离，还需要告诉Sharding-JDBC，这么多真实数据源，那几个是一套读写分离？也就是定义主从逻辑数据源： 12spring.shardingsphere.sharding.master‐slave‐rules.ds0.master‐data‐source‐name=m0spring.shardingsphere.sharding.master‐slave‐rules.ds0.slave‐data‐source‐names=s0 若我们已经对m0 和s0 做了mysql主从同步，那我们需要告诉Sharding-JDBC，m0 、s0 为一组主从同步数据源，其中m0 为主，s0 为从，并且定义名称为ds0 ，这个ds0 就是主从逻辑数据源。 最终配置如下，具体的分库分表策略参考注释内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 # 真实数据源定义 m为主库 s为从库spring.shardingsphere.datasource.names = m0,m1,m2,s0,s1,s2spring.shardingsphere.datasource.m0.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m0.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m0.url = jdbc:mysql://localhost:3306/store_db?useUnicode=truespring.shardingsphere.datasource.m0.username = rootspring.shardingsphere.datasource.m0.password = rootspring.shardingsphere.datasource.m1.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m1.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m1.url = jdbc:mysql://localhost:3306/product_db_1?useUnicode=truespring.shardingsphere.datasource.m1.username = rootspring.shardingsphere.datasource.m1.password = rootspring.shardingsphere.datasource.m2.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.m2.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.m2.url = jdbc:mysql://localhost:3306/product_db_2?useUnicode=truespring.shardingsphere.datasource.m2.username = rootspring.shardingsphere.datasource.m2.password = rootspring.shardingsphere.datasource.s0.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.s0.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.s0.url = jdbc:mysql://localhost:3307/store_db?useUnicode=truespring.shardingsphere.datasource.s0.username = rootspring.shardingsphere.datasource.s0.password = rootspring.shardingsphere.datasource.s1.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.s1.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.s1.url = jdbc:mysql://localhost:3307/product_db_1?useUnicode=truespring.shardingsphere.datasource.s1.username = rootspring.shardingsphere.datasource.s1.password = rootspring.shardingsphere.datasource.s2.type = com.alibaba.druid.pool.DruidDataSourcespring.shardingsphere.datasource.s2.driver‐class‐name = com.mysql.jdbc.Driverspring.shardingsphere.datasource.s2.url = jdbc:mysql://localhost:3307/product_db_2?useUnicode=truespring.shardingsphere.datasource.s2.username = rootspring.shardingsphere.datasource.s2.password = root# 主库从库逻辑数据源定义 ds0为store_db ds1为product_db_1 ds2为product_db_2spring.shardingsphere.sharding.master‐slave‐rules.ds0.master‐data‐source‐name=m0spring.shardingsphere.sharding.master‐slave‐rules.ds0.slave‐data‐source‐names=s0spring.shardingsphere.sharding.master‐slave‐rules.ds1.master‐data‐source‐name=m1spring.shardingsphere.sharding.master‐slave‐rules.ds1.slave‐data‐source‐names=s1spring.shardingsphere.sharding.master‐slave‐rules.ds2.master‐data‐source‐name=m2spring.shardingsphere.sharding.master‐slave‐rules.ds2.slave‐data‐source‐names=s2# 默认分库策略，以store_info_id为分片键，分片策略为store_info_id % 2 + 1，也就是store_info_id为双数的数据进入ds1，为单数的进入ds2spring.shardingsphere.sharding.default‐database‐strategy.inline.sharding‐column = store_info_idspring.shardingsphere.sharding.default‐database‐strategy.inline.algorithm‐expression = ds$‐&gt;&#123;store_info_id % 2 + 1&#125;# store_info分表策略，固定分配至ds0的store_info真实表，spring.shardingsphere.sharding.tables.store_info.actual‐data‐nodes = ds$‐&gt;&#123;0&#125;.store_infospring.shardingsphere.sharding.tables.store_info.table‐strategy.inline.sharding‐column = idspring.shardingsphere.sharding.tables.store_info.table‐strategy.inline.algorithm‐expression =store_info# product_info分表策略，分布在ds1,ds2的product_info_1 product_info_2表 ，分片策略为product_info_id% 2 + 1，product_info_id生成为雪花算法，为双数的数据进入product_info_1表，为单数的进入product_info_2表spring.shardingsphere.sharding.tables.product_info.actual‐data‐nodes = ds$‐&gt;&#123;1..2&#125;.product_info_$‐&gt;&#123;1..2&#125;spring.shardingsphere.sharding.tables.product_info.table‐strategy.inline.sharding‐column =product_info_idspring.shardingsphere.sharding.tables.product_info.table‐strategy.inline.algorithm‐expression =product_info_$‐&gt;&#123;product_info_id % 2 + 1&#125;spring.shardingsphere.sharding.tables.product_info.key‐generator.column=product_info_idspring.shardingsphere.sharding.tables.product_info.key‐generator.type=SNOWFLAKE# product_descript分表策略，分布在ds1,ds2的product_descript_1 product_descript_2表 ，分片策略为product_info_id % 2 + 1，id生成为雪花算法，product_info_id为双数的数据进入product_descript_1表，为单数的进入product_descript_2表spring.shardingsphere.sharding.tables.product_descript.actual‐data‐nodes = ds$‐&gt;&#123;1..2&#125;.product_descript_$‐&gt;&#123;1..2&#125;spring.shardingsphere.sharding.tables.product_descript.table‐strategy.inline.sharding‐column =product_info_idspring.shardingsphere.sharding.tables.product_descript.table‐strategy.inline.algorithm‐expression = product_descript_$‐&gt;&#123;product_info_id % 2 + 1&#125;spring.shardingsphere.sharding.tables.product_descript.key‐generator.column=idspring.shardingsphere.sharding.tables.product_descript.key‐generator.type=SNOWFLAKE# 设置product_info,product_descript为绑定表spring.shardingsphere.sharding.binding‐tables[0] = product_info,product_descript# 设置region为广播表(公共表)，每次更新操作会发送至所有数据源spring.shardingsphere.sharding.broadcast‐tables=region# 打开sql输出日志spring.shardingsphere.props.sql.show = true 9. 5. 3 添加商品实体类，参考基础工程： DAO实现 12345678910111213@Mapper@Componentpublic interface ProductDao &#123; //添加商品基本信息 @Insert(\"insert into product_info(store_info_id,product_name,spec,region_code,price) value(#&#123;storeInfoId&#125;,#&#123;productName&#125;,#&#123;spec&#125;,#&#123;regionCode&#125;,#&#123;price&#125;)\") @Options(useGeneratedKeys = true,keyProperty = \"productInfoId\",keyColumn = \"id\") int insertProductInfo(ProductInfo productInfo); //添加商品描述信息 @Insert(\"insert into product_descript(product_info_id,descript,store_info_id) value(#&#123;productInfoId&#125;,#&#123;descript&#125;,#&#123;storeInfoId&#125;)\") @Options(useGeneratedKeys = true,keyProperty = \"id\",keyColumn = \"id\") int insertProductDescript(ProductDescript productDescript);&#125; service实现，针对垂直分库的两个库，分别实现店铺服务、商品服务 1234567891011121314151617@Servicepublic class ProductServiceImpl implements ProductService &#123; @Autowired private ProductDao productDao; @Override @Transactional public void createProduct(ProductInfo product) &#123; ProductDescript productDescript = new ProductDescript(); productDescript.setDescript(product.getDescript()); productDao.insertProductInfo(product);//新增商品基本信息 productDescript.setProductInfoId(product.getProductInfoId()); productDescript.setStoreInfoId(product.getStoreInfoId()); //冗余店铺信息 productDao.insertProductDescript(productDescript);//新增商品描述信息 &#125;&#125; controller实现： 1234567891011121314/** * 卖家商品展示 */@RestControllerpublic class SellerController &#123; @Autowired private ProductService productService; @PostMapping(\"/products\") public String createProject(@RequestBody ProductInfo productInfo) &#123; productService.createProduct(productInfo); return \"创建成功!\"; &#125; 单元测试： 123456789101112131415161718192021@RunWith(SpringRunner.class)@SpringBootTest(classes = ShardingJdbcDemoBootstrap.class)public class ShardingTest &#123; @Autowired ProductService productService; @Test public void testCreateProduct()&#123; for(long i=1;i&lt;10;i++)&#123; //store_info_id,product_name,spec,region_code,price,image_url ProductInfo productInfo = new ProductInfo(); productInfo.setProductName(\"Java编程思想\"+i); productInfo.setDescript(\"Java编程思想是一本非常好的Java教程\"+i); productInfo.setRegionCode(\"110000\"); productInfo.setStoreInfoId(1); productInfo.setPrice(new BigDecimal(i)); productService.createProduct(productInfo); &#125; &#125; ... 这里使用了sharding-jdbc所提供的全局主键生成方式之一雪花算法，来生成全局业务唯一主键。 通过添加商品接口新增商品进行分库验证，store_info_id为偶数的数据在product_db_ 1 ,为奇数的数据在product_db_ 2 。 通过添加商品接口新增商品进行分表验证，product_id为偶数的数据在product_info_ 1 、product_descript_ 1 ，为奇数的数据在product_info_ 2 、product_descript_ 2 。 9. 5. 4 查询商品Dao实现： 在ProductDao中定义商品查询方法： 1234@Select(\"select i.*, d.descript, r.region_name placeOfOrigin \" + \"from product_info i join product_descript d on i.id = d.product_info_id \" + \"join region r on r.region_code = i.region_code order by i.id desc limit #&#123;start&#125;,#&#123;pageSize&#125;\")List&lt;ProductInfo&gt; selectProductList(@Param(\"start\")int start,@Param(\"pageSize\") int pageSize); Service实现： 在ProductServiceImpl定义商品查询方法： 12345@Overridepublic List&lt;ProductInfo&gt; queryProduct(int page,int pageSize) &#123; int start = (page‐1)*pageSize; return productDao.selectProductList(start,pageSize);&#125; Controller实现： 1234@GetMapping(value = \"/products/&#123;page&#125;/&#123;pageSize&#125;\")public List&lt;ProductInfo&gt; queryProduct(@PathVariable(\"page\")int page,@PathVariable(\"pageSize\")int pageSize)&#123; return productService.queryProduct(page,pageSize);&#125; 单元 测试： 12345@Testpublic void testSelectProductList()&#123; List&lt;ProductInfo&gt; productInfos = productService.queryProduct(1,10); System.out.println(productInfos);&#125; 通过查询商品列表接口，能够查询到所有分片的商品信息，关联的地理区域，店铺信息正确。 总结： 分页查询是业务中最常见的场景，Sharding-jdbc支持常用关系数据库的分页查询，不过Sharding-jdbc的分页功能比较容易让使用者误解，用户通常认为分页归并会占用大量内存。 在分布式的场景中，将LIMIT 10000000 , 10 改写为LIMIT 0 , 10000010 ，才能保证其数据的正确性。 用户非常容易产生ShardingSphere会将大量无意义的数据加载至内存中，造成内存溢出风险的错觉。 其实大部分情况都通过流式归并获取数据结果集，因此 ShardingSphere会通过结果集的next方法将无需取出的数据全部跳过，并不会将其存入内存。 但同时需要注意的是，由于排序的需要，大量的数据仍然需要传输到Sharding-Jdbc的内存空间。 因此，采用LIMIT这种方式分页，并非最佳实践。 由于LIMIT并不能通过索引查询数据，因此如果可以保证ID的连续性，通过ID进行分页是比较好的解决方案，例如： 1SELECT * FROM t_order WHERE id &gt; 100000 AND id &lt;&#x3D; 100010 ORDER BY id; 或通过记录上次查询结果的最后一条记录的ID进行下一页的查询，例如： 1SELECT * FROM t_order WHERE id &gt; 10000000 LIMIT 10; 排序功能是由Sharding-jdbc的排序归并来完成，由于在SQL中存在ORDER BY语句，因此每个数据结果集自身是有序的，因此只需要将数据结果集当前游标指向的数据值进行排序即可。 这相当于对多个有序的数组进行排序，归并排序是最适合此场景的排序算法。 9. 5. 5 统计商品本小节实现商品总数统计，商品分组统计 Dao实现，在ProductDao中定义： 1234567//总数统计@Select(\"select count(1) from product_info\")int selectCount();//分组统计@Select(\"select count(1) as num from product_info group by region_code having num&gt;1 ORDER BY region_code ASC\")List&lt;Map&gt; selectProductGroupList(); 单元测试： 1234567891011@Testpublic void testSelectCount()&#123; int i = productDao.selectCount(); System.out.println(i);&#125;@Testpublic void testSelectGroupList()&#123; List&lt;Map&gt; maps = productDao.selectProductGroupList(); System.out.println(maps);&#125; 总结： 分组统计 分组统计也是业务中常见的场景，分组功能的实现由Sharding-jdbc分组归并完成。分组归并的情况最为复杂，它分为流式分组归并和内存分组归并。 流式分组归并要求SQL的排序项与分组项的字段必须保持一致，否则只能通过内存归并才能保证其数据的正确性。 举例说明，假设根据科目分片，表结构中包含考生的姓名（为了简单起见，不考虑重名的情况）和分数。通过SQL获取每位考生的总分，可通过如下SQL： 1SELECT name, SUM(score) FROM t_score GROUP BY name ORDER BY name; 在分组项与排序项完全一致的情况下，取得的数据是连续的，分组所需的数据全数存在于各个数据结果集的当前游 标所指向的数据值，因此可以采用流式归并。如下图所示。 进行归并时，逻辑与排序归并类似。 下图展现了进行next调用的时候，流式分组归并是如何进行的。 通过图中我们可以看到，当进行第一次next调用时，排在队列首位的t_score_java将会被弹出队列，并且将分组值同为“Jetty”的其他结果集中的数据一同弹出队列。 在获取了所有的姓名为“Jetty”的同学的分数之后，进行累加操作，那么，在第一次next调用结束后，取出的结果集是“Jetty”的分数总和。 与此同时，所有的数据结果集中的游标都将下移至数据值“Jetty”的下一个不同的数据值，并且根据数据结果集当前游标指向的值进行重排序。 因此，包含名字顺着第二位的“John”的相关数据结果集则排在的队列的前列。 10 课程总结重点知识回顾： 为什么分库分表？分库分表就是为了解决由于数据量过大而导致数据库性能降低的问题，将原来独立的数据库拆分 成若干数据库组成 ，将数据大表拆分成若干数据表组成，使得单一数据库、单一数据表的数据量变小，从而达到提 升数据库性能的目的。 分库分表方式：垂直分表、垂直分库、水平分库、水平分表 分库分表带来问题：由于数据分散在多个数据库，服务器导致了事务一致性问题、跨节点join问题、跨节点分页、排序、函数，主键需要全局唯一，公共表。 Sharding-JDBC基础概念：逻辑表，真实表，数据节点，绑定表，广播表，分片键，分片算法，分片策略，主键生成策略 Sharding-JDBC核心功能：数据分片，读写分离 Sharding-JDBC执行流程：SQL解析 =&gt; 查询优化 =&gt; SQL路由 =&gt; SQL改写 =&gt; SQL执行 =&gt; 结果归并 最佳实践： 系统在设计之初就应该对业务数据的耦合松紧进行考量，从而进行垂直分库、垂直分表，使数据层架构清晰明了。 若非必要，无需进行水平切分，应先从缓存技术着手降低对数据库的访问压力。如果缓存使用过后，数据库访问量 还是非常大，可以考虑数据库读、写分离原则。若当前数据库压力依然大，且业务数据持续增长无法估量，最后可 考虑水平分库、分表，单表拆分数据控制在 1000 万以内。 附 SQL支持说明详细参考：https://shardingsphere.apache.org/document/current/cn/features/sharding/use-norms/sql/ 说明：以下为官方显示内容，具体是否适用以实际测试为准 。 支持的SQL SQL 必要条件 SELECT * FROM tbl_name SELECT * FROM tbl_name WHERE (col1 = ? or col2 = ?) and col3 = ? SELECT * FROM tbl_name WHERE col1 = ? ORDER BY col2 DESC LIMIT ? SELECT COUNT(*), SUM(col1), MIN(col1), MAX(col1), AVG(col1) FROM tbl_name WHERE col1= ? SELECT COUNT(col1) FROM tbl_name WHERE col2 = ? GROUP BY col1 ORDER BY col3 DESC LIMIT ?, ? INSERT INTO tbl_name (col1, col2,…) VALUES (?, ?, ….) INSERT INTO tbl_name VALUES (?, ?,….) INSERT INTO tbl_name (col1, col2, …) VALUES (?, ?, ….), (?, ?, ….) UPDATE tbl_name SET col1 = ? WHERE col2 = ? DELETE FROM tbl_name WHERE col1 = ? CREATE TABLE tbl_name (col1 int, …) ALTER TABLE tbl_name ADD col1 varchar(10) DROP TABLE tbl_name TRUNCATE TABLE tbl_name CREATE INDEX idx_name ON tbl_name DROP INDEX idx_name ON tbl_name DROP INDEX idx_name SELECT DISTINCT * FROM tbl_name WHERE col1 = ? SELECT COUNT(DISTINCT col1) FROM tbl_name 不支持的SQL SQL 不支持原因 INSERT INTO tbl_name (col1, col2, …) VALUES(1+2, ?, …) VALUES语句不支持运算表达式 INSERT INTO tbl_name (col1, col2, …) SELECT col1, col2, … FROM tbl_name WHERE col3 = ? INSERT .. SELECT SELECT COUNT(col1) as count_alias FROM tbl_name GROUP BY col1 HAVING count_alias &gt; ? HAVING SELECT * FROM tbl_name1 UNION SELECT * FROM tbl_name2 UNION SELECT * FROM tbl_name1 UNION ALL SELECT * FROM tbl_name2 UNION ALL SELECT * FROM ds.tbl_name1 包含schema SELECT SUM(DISTINCT col1), SUM(col1) FROM tbl_name 详见DISTINCT支持情况详细说明 DISTINCT支持情况详细说明支持的SQL SQL SELECT DISTINCT * FROM tbl_name WHERE col1 = ? SELECT DISTINCT col1 FROM tbl_name SELECT DISTINCT col1, col2, col3 FROM tbl_name SELECT DISTINCT col1 FROM tbl_name ORDER BY col1 SELECT DISTINCT col1 FROM tbl_name ORDER BY col2 SELECT DISTINCT(col1) FROM tbl_name SELECT AVG(DISTINCT col1) FROM tbl_name SELECT SUM(DISTINCT col1) FROM tbl_name SELECT COUNT(DISTINCT col1) FROM tbl_name SELECT COUNT(DISTINCT col1) FROM tbl_name GROUP BY col1 SELECT COUNT(DISTINCT col1 + col2) FROM tbl_name SELECT COUNT(DISTINCT col1), SUM(DISTINCT col1) FROM tbl_name SELECT COUNT(DISTINCT col1), col1 FROM tbl_name GROUP BY col1 SELECT col1, COUNT(DISTINCT col1) FROM tbl_name GROUP BY col1 不支持的SQL SQL 不支持原因 SELECT SUM(DISTINCT col1), SUM(col1) FROM tbl_name 同时使用普通聚合函数和DISTINCT聚合函数","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wgy1993.gitee.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://wgy1993.gitee.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MySQL高级教程(四)","date":"2020-05-21T07:21:30.000Z","path":"archives/e6ab56f8.html","text":"1. MySql中常用工具1.1 mysql该mysql不是指mysql服务，而是指mysql的客户端工具。 语法 ： 1mysql [options] [database] 1.1.1 连接选项12345678910参数 ： -u, --user&#x3D;name 指定用户名 -p, --password[&#x3D;name] 指定密码 -h, --host&#x3D;name 指定服务器IP或域名 -P, --port&#x3D;# 指定连接端口示例 ： mysql -h 127.0.0.1 -P 3306 -u root -p mysql -h127.0.0.1 -P3306 -uroot -p2143 1.1.2 执行选项1-e, --execute&#x3D;name 执行SQL语句并退出 此选项可以在Mysql客户端执行SQL语句，而不用连接到MySQL数据库再执行，对于一些批处理脚本，这种方式尤其方便。 12示例： mysql -uroot -p2143 db01 -e &quot;select * from tb_book&quot;; 1.2 mysqladminmysqladmin 是一个执行管理操作的客户端程序。可以用它来检查服务器的配置和当前状态、创建并删除数据库等。 可以通过 ： mysqladmin –help 指令查看帮助文档 1234示例 ： mysqladmin -uroot -p2143 create &#39;test01&#39;; mysqladmin -uroot -p2143 drop &#39;test01&#39;; mysqladmin -uroot -p2143 version; 1.3 mysqlbinlog由于服务器生成的二进制日志文件以二进制格式保存，所以如果想要检查这些文本的文本格式，就会使用到mysqlbinlog 日志管理工具。 语法 ： 123456789101112131415mysqlbinlog [options] log-files1 log-files2 ...选项： -d, --database&#x3D;name : 指定数据库名称，只列出指定的数据库相关操作。 -o, --offset&#x3D;# : 忽略掉日志中的前n行命令。 -r,--result-file&#x3D;name : 将输出的文本格式日志输出到指定文件。 -s, --short-form : 显示简单格式， 省略掉一些信息。 --start-datatime&#x3D;date1 --stop-datetime&#x3D;date2 : 指定日期间隔内的所有日志。 --start-position&#x3D;pos1 --stop-position&#x3D;pos2 : 指定位置间隔内的所有日志。 1.4 mysqldumpmysqldump 客户端工具用来备份数据库或在不同数据库之间进行数据迁移。备份内容包含创建表，及插入表的SQL语句。 语法 ： 12345mysqldump [options] db_name [tables]mysqldump [options] --database&#x2F;-B db1 [db2 db3...]mysqldump [options] --all-databases&#x2F;-A 1.4.1 连接选项12345参数 ： -u, --user&#x3D;name 指定用户名 -p, --password[&#x3D;name] 指定密码 -h, --host&#x3D;name 指定服务器IP或域名 -P, --port&#x3D;# 指定连接端口 1.4.2 输出内容选项12345678910参数： --add-drop-database 在每个数据库创建语句前加上 Drop database 语句 --add-drop-table 在每个表创建语句前加上 Drop table 语句 , 默认开启 ; 不开启 (--skip-add-drop-table) -n, --no-create-db 不包含数据库的创建语句 -t, --no-create-info 不包含数据表的创建语句 -d --no-data 不包含数据 -T, --tab&#x3D;name 自动生成两个文件：一个.sql文件，创建表结构的语句； 一个.txt文件，数据文件，相当于select into outfile 1234示例 ： mysqldump -uroot -p2143 db01 tb_book --add-drop-database --add-drop-table &gt; a mysqldump -uroot -p2143 -T &#x2F;tmp test city 1.5 mysqlimport/sourcemysqlimport 是客户端数据导入工具，用来导入mysqldump 加 -T 参数后导出的文本文件。 语法： 1mysqlimport [options] db_name textfile1 [textfile2...] 示例： 1mysqlimport -uroot -p2143 test &#x2F;tmp&#x2F;city.txt 如果需要导入sql文件,可以使用mysql中的source 指令 : 1source &#x2F;root&#x2F;tb_book.sql 1.6 mysqlshowmysqlshow 客户端对象查找工具，用来很快地查找存在哪些数据库、数据库中的表、表中的列或者索引。 语法： 1mysqlshow [options] [db_name [table_name [col_name]]] 参数： 123--count 显示数据库及表的统计信息（数据库，表 均可以不指定）-i 显示指定数据库或者指定表的状态信息 示例： 12345678#查询每个数据库的表的数量及表中记录的数量mysqlshow -uroot -p2143 --count#查询test库中每个表中的字段书，及行数mysqlshow -uroot -p2143 test --count#查询test库中book表的详细情况mysqlshow -uroot -p2143 test book --count 2. Mysql 日志在任何一种数据库中，都会有各种各样的日志，记录着数据库工作的方方面面，以帮助数据库管理员追踪数据库曾经发生过的各种事件。MySQL 也不例外，在 MySQL 中，有 4 种不同的日志，分别是错误日志、二进制日志（BINLOG 日志）、查询日志和慢查询日志，这些日志记录着数据库在不同方面的踪迹。 2.1 错误日志错误日志是 MySQL 中最重要的日志之一，它记录了当 mysqld 启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。当数据库出现任何故障导致无法正常使用时，可以首先查看此日志。 该日志是默认开启的 ， 默认存放目录为 mysql 的数据目录（var/lib/mysql）, 默认的日志文件名为 hostname.err（hostname是主机名）。 查看日志位置指令 ： 1show variables like &#39;log_error%&#39;; 查看日志内容 ： 1tail -f /var/lib/mysql/xaxh-server.err 2.2 二进制日志2.2.1 概述二进制日志（BINLOG）记录了所有的 DDL（数据定义语言）语句和 DML（数据操纵语言）语句，但是不包括数据查询语句。此日志对于灾难时的数据恢复起着极其重要的作用，MySQL的主从复制， 就是通过该binlog实现的。 二进制日志，默认情况下是没有开启的，需要到MySQL的配置文件中开启，并配置MySQL日志的格式。 配置文件位置 : /usr/my.cnf 日志存放位置 : 配置时，给定了文件名但是没有指定路径，日志默认写入Mysql的数据目录。 12345#配置开启binlog日志， 日志的文件前缀为 mysqlbin -----&gt; 生成的文件名如 : mysqlbin.000001,mysqlbin.000002log_bin&#x3D;mysqlbin#配置二进制日志的格式binlog_format&#x3D;STATEMENT 2.2.2 日志格式STATEMENT 该日志格式在日志文件中记录的都是SQL语句（statement），每一条对数据进行修改的SQL都会记录在日志文件中，通过Mysql提供的mysqlbinlog工具，可以清晰的查看到每条语句的文本。主从复制的时候，从库（slave）会将日志解析为原文本，并在从库重新执行一次。 ROW 该日志格式在日志文件中记录的是每一行的数据变更，而不是记录SQL语句。比如，执行SQL语句 ： update tb_book set status=’1’ , 如果是STATEMENT 日志格式，在日志中会记录一行SQL文件； 如果是ROW，由于是对全表进行更新，也就是每一行记录都会发生变更，ROW 格式的日志中会记录每一行的数据变更。 MIXED 这是目前MySQL默认的日志格式，即混合了STATEMENT 和 ROW两种格式。默认情况下采用STATEMENT，但是在一些特殊情况下采用ROW来进行记录。MIXED 格式能尽量利用两种模式的优点，而避开他们的缺点。 2.2.3 日志读取由于日志以二进制方式存储，不能直接读取，需要用mysqlbinlog工具来查看，语法如下 ： 1mysqlbinlog log-file； 查看STATEMENT格式日志 执行插入语句 ： 1insert into tb_book values(null,&#39;Lucene&#39;,&#39;2088-05-01&#39;,&#39;0&#39;); 查看日志文件 ： mysqlbin.index : 该文件是日志索引文件 ， 记录日志的文件名； mysqlbing.000001 ：日志文件 查看日志内容 ： 1mysqlbinlog mysqlbing.000001； 查看ROW格式日志 配置 : 12345#配置开启binlog日志， 日志的文件前缀为 mysqlbin -----&gt; 生成的文件名如 : mysqlbin.000001,mysqlbin.000002log_bin&#x3D;mysqlbin#配置二进制日志的格式binlog_format&#x3D;ROW 插入数据 : 1insert into tb_book values(null,&#39;SpringCloud实战&#39;,&#39;2088-05-05&#39;,&#39;0&#39;); 如果日志格式是 ROW , 直接查看数据 , 是查看不懂的 ; 可以在mysqlbinlog 后面加上参数 -vv 1mysqlbinlog -vv mysqlbin.000002 2.2.4 日志删除对于比较繁忙的系统，由于每天生成日志量大 ，这些日志如果长时间不清楚，将会占用大量的磁盘空间。下面我们将会讲解几种删除日志的常见方法 ： 方式一 通过 Reset Master 指令删除全部 binlog 日志，删除之后，日志编号，将从 xxxx.000001重新开始 。 查询之前 ，先查询下日志文件 ： 执行删除日志指令： 1Reset Master 执行之后， 查看日志文件 ： 方式二 执行指令 purge master logs to &#39;mysqlbin.******&#39; ，该命令将删除 ****** 编号之前的所有日志。 方式三 执行指令 purge master logs before &#39;yyyy-mm-dd hh24:mi:ss&#39; ，该命令将删除日志为 “yyyy-mm-dd hh24:mi:ss” 之前产生的所有日志 。 方式四 设置参数 –expire_logs_days=# ，此参数的含义是设置日志的过期天数， 过了指定的天数后日志将会被自动删除，这样将有利于减少DBA 管理日志的工作量。 配置如下 ： 2.3 查询日志查询日志中记录了客户端的所有操作语句，而二进制日志不包含查询数据的SQL语句。 默认情况下， 查询日志是未开启的。如果需要开启查询日志，可以设置以下配置 ： 12345#该选项用来开启查询日志 ， 可选值 ： 0 或者 1 ； 0 代表关闭， 1 代表开启 general_log&#x3D;1#设置日志的文件名 ， 如果没有指定， 默认的文件名为 host_name.log general_log_file&#x3D;file_name 在 mysql 的配置文件 /usr/my.cnf 中配置如下内容 ： 配置完毕之后，在数据库执行以下操作 ： 1234select * from tb_book;select * from tb_book where id &#x3D; 1;update tb_book set name &#x3D; &#39;lucene入门指南&#39; where id &#x3D; 5;select * from tb_book where id &lt; 8; 执行完毕之后， 再次来查询日志文件 ： 2.4 慢查询日志慢查询日志记录了所有执行时间超过参数 long_query_time 设置值并且扫描记录数不小于 min_examined_row_limit 的所有的SQL语句的日志。long_query_time 默认为 10 秒，最小为 0， 精度可以到微秒。 2.4.1 文件位置和格式慢查询日志默认是关闭的 。可以通过两个参数来控制慢查询日志 ： 12345678# 该参数用来控制慢查询日志是否开启， 可取值： 1 和 0 ， 1 代表开启， 0 代表关闭slow_query_log&#x3D;1 # 该参数用来指定慢查询日志的文件名slow_query_log_file&#x3D;slow_query.log# 该选项用来配置查询的时间限制， 超过这个时间将认为值慢查询， 将需要进行日志记录， 默认10slong_query_time&#x3D;10 2.4.2 日志的读取和错误日志、查询日志一样，慢查询日志记录的格式也是纯文本，可以被直接读取。 1） 查询long_query_time 的值。 2） 执行查询操作 1select id, title,price,num ,status from tb_item where id &#x3D; 1; 由于该语句执行时间很短，为0s ， 所以不会记录在慢查询日志中。 1select * from tb_item where title like &#39;%阿尔卡特 (OT-927) 炭黑 联通3G手机 双卡双待165454%&#39; ; 该SQL语句 ， 执行时长为 26.77s ，超过10s ， 所以会记录在慢查询日志文件中。 3） 查看慢查询日志文件 直接通过cat 指令查询该日志文件 ： 如果慢查询日志内容很多， 直接查看文件，比较麻烦， 这个时候可以借助于mysql自带的 mysqldumpslow 工具， 来对慢查询日志进行分类汇总。 3. Mysql复制3.1 复制概述复制是指将主数据库的DDL 和 DML 操作通过二进制日志传到从库服务器中，然后在从库上对这些日志重新执行（也叫重做），从而使得从库和主库的数据保持同步。 MySQL支持一台主库同时向多台从库进行复制， 从库同时也可以作为其他从服务器的主库，实现链状复制。 3.2 复制原理MySQL 的主从复制原理如下。 从上层来看，复制分成三步： Master 主库在事务提交时，会把数据变更作为时间 Events 记录在二进制日志文件 Binlog 中。 主库推送二进制日志文件 Binlog 中的日志事件到从库的中继日志 Relay Log 。 slave重做中继日志中的事件，将改变反映它自己的数据。 3.3 复制优势MySQL 复制的有点主要包含以下三个方面： 主库出现问题，可以快速切换到从库提供服务。 可以在从库上执行查询操作，从主库中更新，实现读写分离，降低主库的访问压力。 可以在从库中执行备份，以避免备份期间影响主库的服务。 3.4 搭建步骤3.4.1 master1） 在master 的配置文件（/usr/my.cnf）中，配置如下内容： 1234567891011121314151617181920212223242526#mysql 服务ID,保证整个集群环境中唯一server-id=1#mysql binlog 日志的存储路径和文件名log-bin=/var/lib/mysql/mysqlbin#错误日志,默认已经开启#log-err#mysql的安装目录#basedir#mysql的临时目录#tmpdir#mysql的数据存放目录#datadir#是否只读,1 代表只读, 0 代表读写read-only=0#忽略的数据, 指不需要同步的数据库binlog-ignore-db=mysql#指定同步的数据库#binlog-do-db=db01 2） 执行完毕之后，需要重启Mysql： 1service mysql restart ; 3） 创建同步数据的账户，并且进行授权操作： 123grant replication slave on *.* to &#39;root&#39;@&#39;192.168.192.131&#39; identified by &#39;root&#39;; flush privileges; 4） 查看master状态： 1show master status; 字段含义： 123File : 从哪个日志文件开始推送日志文件 Position ： 从哪个位置开始推送日志Binlog_Ignore_DB : 指定不需要同步的数据库 3.4.2 slave1） 在 slave 端配置文件中，配置如下内容： 12345#mysql服务端ID,唯一server-id=2#指定binlog日志log-bin=/var/lib/mysql/mysqlbin 2） 执行完毕之后，需要重启Mysql： 1service mysql restart； 3） 执行如下指令 ： 1change master to master_host= '192.168.192.130', master_user='root', master_password='root', master_log_file='mysqlbin.000001', master_log_pos=413; 指定当前从库对应的主库的IP地址，用户名，密码，从哪个日志文件开始的那个位置开始同步推送日志。 4） 开启同步操作 123start slave;show slave status; 5） 停止同步操作 1stop slave; 3.4.3 验证同步操作1） 在主库中创建数据库，创建表，并插入数据 ： 1234567891011121314create database db01;user db01;create table user( id int(11) not null auto_increment, name varchar(50) not null, sex varchar(1), primary key (id))engine&#x3D;innodb default charset&#x3D;utf8;insert into user(id,name,sex) values(null,&#39;Tom&#39;,&#39;1&#39;);insert into user(id,name,sex) values(null,&#39;Trigger&#39;,&#39;0&#39;);insert into user(id,name,sex) values(null,&#39;Dawn&#39;,&#39;1&#39;); 2） 在从库中查询数据，进行验证 ： 在从库中，可以查看到刚才创建的数据库： 在该数据库中，查询user表中的数据： 4. 综合案例4.1 需求分析在业务系统中，需要记录当前业务系统的访问日志，该访问日志包含：操作人，操作时间，访问类，访问方法，请求参数，请求结果，请求结果类型，请求时长等信息。记录详细的系统访问日志，主要便于对系统中的用户请求进行追踪，并且在系统 的管理后台可以查看到用户的访问记录。 记录系统中的日志信息，可以通过Spring 框架的AOP来实现。具体的请求处理流程，如下： 4.2 搭建案例环境4.2.1 数据库表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647CREATE DATABASE mysql_demo DEFAULT CHARACTER SET utf8mb4 ；CREATE TABLE &#96;brand&#96; ( &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT, &#96;name&#96; varchar(255) DEFAULT NULL COMMENT &#39;品牌名称&#39;, &#96;first_char&#96; varchar(1) DEFAULT NULL COMMENT &#39;品牌首字母&#39;, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;CREATE TABLE &#96;item&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT COMMENT &#39;商品id&#39;, &#96;title&#96; varchar(100) NOT NULL COMMENT &#39;商品标题&#39;, &#96;price&#96; double(10,2) NOT NULL COMMENT &#39;商品价格，单位为：元&#39;, &#96;num&#96; int(10) NOT NULL COMMENT &#39;库存数量&#39;, &#96;categoryid&#96; bigint(10) NOT NULL COMMENT &#39;所属类目，叶子类目&#39;, &#96;status&#96; varchar(1) DEFAULT NULL COMMENT &#39;商品状态，1-正常，2-下架，3-删除&#39;, &#96;sellerid&#96; varchar(50) DEFAULT NULL COMMENT &#39;商家ID&#39;, &#96;createtime&#96; datetime DEFAULT NULL COMMENT &#39;创建时间&#39;, &#96;updatetime&#96; datetime DEFAULT NULL COMMENT &#39;更新时间&#39;, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COMMENT&#x3D;&#39;商品表&#39;;CREATE TABLE &#96;user&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;username&#96; varchar(45) NOT NULL, &#96;password&#96; varchar(96) NOT NULL, &#96;name&#96; varchar(45) NOT NULL, &#96;birthday&#96; datetime DEFAULT NULL, &#96;sex&#96; char(1) DEFAULT NULL, &#96;email&#96; varchar(45) DEFAULT NULL, &#96;phone&#96; varchar(45) DEFAULT NULL, &#96;qq&#96; varchar(32) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;CREATE TABLE &#96;operation_log&#96; ( &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;ID&#39;, &#96;operate_class&#96; varchar(200) DEFAULT NULL COMMENT &#39;操作类&#39;, &#96;operate_method&#96; varchar(200) DEFAULT NULL COMMENT &#39;操作方法&#39;, &#96;return_class&#96; varchar(200) DEFAULT NULL COMMENT &#39;返回值类型&#39;, &#96;operate_user&#96; varchar(20) DEFAULT NULL COMMENT &#39;操作用户&#39;, &#96;operate_time&#96; varchar(20) DEFAULT NULL COMMENT &#39;操作时间&#39;, &#96;param_and_value&#96; varchar(500) DEFAULT NULL COMMENT &#39;请求参数名及参数值&#39;, &#96;cost_time&#96; bigint(20) DEFAULT NULL COMMENT &#39;执行方法耗时, 单位 ms&#39;, &#96;return_value&#96; varchar(200) DEFAULT NULL COMMENT &#39;返回值&#39;, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4; 4.2.2 pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.2.RELEASE&lt;/spring.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;mybatis.version&gt;3.4.5&lt;/mybatis.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- spring --&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.6.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.16&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-orm&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;$&#123;log4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;8080&lt;/port&gt; &lt;path&gt;/&lt;/path&gt; &lt;uriEncoding&gt;utf-8&lt;/uriEncoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 4.2.3 web.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\" version=\"2.5\"&gt; &lt;!-- 解决post乱码 --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; 4.2.4 db.properties1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://192.168.142.128:3306/mysql_demojdbc.username=rootjdbc.password=root 4.2.5 applicationContext.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 加载配置文件 --&gt; &lt;context:property-placeholder location=\"classpath:db.properties\"/&gt; &lt;!-- 配置 spring 创建容器时要扫描的包 --&gt; &lt;context:component-scan base-package=\"com.wgy\"&gt; &lt;context:exclude-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"&gt; &lt;/context:exclude-filter&gt; &lt;/context:component-scan&gt; &lt;!-- 配置 MyBatis 的 Session 工厂 --&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;property name=\"typeAliasesPackage\" value=\"com.wgy.domain\"/&gt; &lt;/bean&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"driverClass\" value=\"$&#123;jdbc.driver&#125;\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbc.url&#125;\"&gt;&lt;/property&gt; &lt;property name=\"user\" value=\"$&#123;jdbc.username&#125;\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置 Mapper 扫描器 --&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.wgy.mapper\"/&gt; &lt;/bean&gt; &lt;!-- 配置事务管理器 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;/bean&gt; &lt;!-- 配置事务的注解驱动 --&gt; &lt;tx:annotation-driven transaction-manager=\"transactionManager\"&gt;&lt;/tx:annotation-driven&gt;&lt;/beans&gt; 4.2.6 springmvc.xml123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;context:component-scan base-package=\"com.wgy.controller\"&gt;&lt;/context:component-scan&gt; &lt;mvc:annotation-driven&gt;&lt;/mvc:annotation-driven&gt; &lt;aop:aspectj-autoproxy /&gt;&lt;/beans&gt; 4.2.7 导入基础工程 4.3 通过AOP记录操作日志4.3.1 自定义注解通过自定义注解，来标示方法需不需要进行记录日志，如果该方法在访问时需要记录日志，则在该方法上标示该注解既可。 123456@Inherited@Documented@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface OperateLog &#123;&#125; 4.3.2 定义通知类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Component@Aspectpublic class OperateAdvice &#123; private static Logger log = Logger.getLogger(OperateAdvice.class); @Autowired private OperationLogService operationLogService; @Around(\"execution(* com.wgy.controller.*.*(..)) &amp;&amp; @annotation(operateLog)\") public Object insertLogAround(ProceedingJoinPoint pjp , OperateLog operateLog) throws Throwable&#123; System.out.println(\" ************************ 记录日志 [start] ****************************** \"); OperationLog op = new OperationLog(); DateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); op.setOperateTime(sdf.format(new Date())); op.setOperateUser(DataUtils.getRandStr(8)); op.setOperateClass(pjp.getTarget().getClass().getName()); op.setOperateMethod(pjp.getSignature().getName()); //获取方法调用时传递的参数 Object[] args = pjp.getArgs(); op.setParamAndValue(Arrays.toString(args)); long start_time = System.currentTimeMillis(); //放行 Object object = pjp.proceed(); long end_time = System.currentTimeMillis(); op.setCostTime(end_time - start_time); if(object != null)&#123; op.setReturnClass(object.getClass().getName()); op.setReturnValue(object.toString()); &#125;else&#123; op.setReturnClass(\"java.lang.Object\"); op.setParamAndValue(\"void\"); &#125; log.error(JsonUtils.obj2JsonString(op)); operationLogService.insert(op); System.out.println(\" ************************** 记录日志 [end] *************************** \"); return object; &#125;&#125; 4.3.3 方法上加注解在需要记录日志的方法上加上注解@OperateLog。 1234567891011@OperateLog@RequestMapping(\"/insert\")public Result insert(@RequestBody Brand brand)&#123; try &#123; brandService.insert(brand); return new Result(true,\"操作成功\"); &#125; catch (Exception e) &#123; e.printStackTrace(); return new Result(false,\"操作失败\"); &#125;&#125; 4.4 日志查询后端代码实现4.4.1 Mapper接口123456789public interface OperationLogMapper &#123; public void insert(OperationLog operationLog); public List&lt;OperationLog&gt; selectListByCondition(Map dataMap); public Long countByCondition(Map dataMap);&#125; 4.4.2 Mapper.xml 映射配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\" &gt;&lt;mapper namespace=\"com.wgy.mapper.OperationLogMapper\" &gt; &lt;insert id=\"insert\" parameterType=\"operationLog\"&gt; INSERT INTO operation_log(id,return_value,return_class,operate_user,operate_time,param_and_value, operate_class,operate_method,cost_time) VALUES(NULL,#&#123;returnValue&#125;,#&#123;returnClass&#125;,#&#123;operateUser&#125;,#&#123;operateTime&#125;,#&#123;paramAndValue&#125;, #&#123;operateClass&#125;,#&#123;operateMethod&#125;,#&#123;costTime&#125;) &lt;/insert&gt; &lt;select id=\"selectListByCondition\" parameterType=\"map\" resultType=\"operationLog\"&gt; select id , operate_class as operateClass , operate_method as operateMethod, return_class as returnClass, operate_user as operateUser, operate_time as operateTime, param_and_value as paramAndValue, cost_time as costTime, return_value as returnValue from operation_log &lt;include refid=\"oplog_where\"/&gt; limit #&#123;start&#125;,#&#123;size&#125; &lt;/select&gt; &lt;select id=\"countByCondition\" resultType=\"long\" parameterType=\"map\"&gt; select count(*) from operation_log &lt;include refid=\"oplog_where\"/&gt; &lt;/select&gt; &lt;sql id=\"oplog_where\"&gt; &lt;where&gt; &lt;if test=\"operateClass != null and operateClass != '' \"&gt; and operate_class = #&#123;operateClass&#125; &lt;/if&gt; &lt;if test=\"operateMethod != null and operateMethod != '' \"&gt; and operate_method = #&#123;operateMethod&#125; &lt;/if&gt; &lt;if test=\"returnClass != null and returnClass != '' \"&gt; and return_class = #&#123;returnClass&#125; &lt;/if&gt; &lt;if test=\"costTime != null\"&gt; and cost_time = #&#123;costTime&#125; &lt;/if&gt; &lt;/where&gt; &lt;/sql&gt;&lt;/mapper&gt; 4.4.3 Service123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Service@Transactionalpublic class OperationLogService &#123; //private static Logger logger = Logger.getLogger(OperationLogService.class); @Autowired private OperationLogMapper operationLogMapper; //插入数据 public void insert(OperationLog operationLog)&#123; operationLogMapper.insert(operationLog); &#125; //根据条件查询 public PageResult selectListByCondition(Map dataMap, Integer pageNum , Integer pageSize)&#123; if(paramMap ==null)&#123; paramMap = new HashMap(); &#125; paramMap.put(\"start\" , (pageNum-1)*rows); paramMap.put(\"rows\",rows); Object costTime = paramMap.get(\"costTime\"); if(costTime != null)&#123; if(\"\".equals(costTime.toString()))&#123; paramMap.put(\"costTime\",null); &#125;else&#123; paramMap.put(\"costTime\",new Long(paramMap.get(\"costTime\").toString())); &#125; &#125; System.out.println(dataMap); long countStart = System.currentTimeMillis(); Long count = operationLogMapper.countByCondition(dataMap); long countEnd = System.currentTimeMillis(); System.out.println(\"Count Cost Time : \" + (countEnd-countStart)+\" ms\"); List&lt;OperationLog&gt; list = operationLogMapper.selectListByCondition(dataMap); long queryEnd = System.currentTimeMillis(); System.out.println(\"Query Cost Time : \" + (queryEnd-countEnd)+\" ms\"); return new PageResult(count,list); &#125;&#125; 4.4.4 Controller1234567891011121314@RestController@RequestMapping(\"/operationLog\")public class OperationLogController &#123; @Autowired private OperationLogService operationLogService; @RequestMapping(\"/findList\") public PageResult findList(@RequestBody Map dataMap, Integer pageNum , Integer pageSize)&#123; PageResult page = operationLogService.selectListByCondition(dataMap, pageNum, pageSize); return page; &#125;&#125; 4.5 日志查询前端代码实现前端代码使用 BootStrap + AdminLTE 进行布局， 使用Vuejs 进行视图层展示。 4.5.1 js1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;script&gt; var vm = new Vue(&#123; el: '#app', data: &#123; dataList:[], searchEntity:&#123; operateClass:'', operateMethod:'', returnClass:'', costTime:'' &#125;, page: 1, //显示的是哪一页 pageSize: 10, //每一页显示的数据条数 total: 150, //记录总数 maxPage:8 //最大页数 &#125;, methods: &#123; pageHandler: function (page) &#123; this.page = page; this.search(); &#125;, search: function () &#123; var _this = this; this.showLoading(); axios.post('/operationLog/findList.do?pageNum=' + _this.page + \"&amp;pageSize=\" + _this.pageSize, _this.searchEntity).then(function (response) &#123; if (response) &#123; _this.dataList = response.data.dataList; _this.total = response.data.total; _this.hideLoading(); &#125; &#125;) &#125;, showLoading: function () &#123; $('#loadingModal').modal(&#123;backdrop: 'static', keyboard: false&#125;); &#125;, hideLoading: function () &#123; $('#loadingModal').modal('hide'); &#125;, &#125;, created:function()&#123; this.pageHandler(1); &#125; &#125;);&lt;/script&gt; 4.5.2 列表数据展示123456789101112131415&lt;tr v-for=\"item in dataList\"&gt; &lt;td&gt;&lt;input name=\"ids\" type=\"checkbox\"&gt;&lt;/td&gt; &lt;td&gt;&#123;&#123;item.id&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item.operateClass&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item.operateMethod&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item.returnClass&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item.returnValue&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item.operateUser&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item.operateTime&#125;&#125;&lt;/td&gt; &lt;td&gt;&#123;&#123;item.costTime&#125;&#125;&lt;/td&gt; &lt;td class=\"text-center\"&gt; &lt;button type=\"button\" class=\"btn bg-olive btn-xs\"&gt;详情&lt;/button&gt; &lt;button type=\"button\" class=\"btn bg-olive btn-xs\"&gt;删除&lt;/button&gt; &lt;/td&gt;&lt;/tr&gt; 4.5.3 分页插件12345&lt;div class=\"wrap\" id=\"wrap\"&gt; &lt;zpagenav v-bind:page=\"page\" v-bind:page-size=\"pageSize\" v-bind:total=\"total\" v-bind:max-page=\"maxPage\" v-on:pagehandler=\"pageHandler\"&gt; &lt;/zpagenav&gt;&lt;/div&gt; 4.6 联调测试可以通过postman来访问业务系统，再查看数据库中的日志信息，验证能不能将用户的访问日志记录下来。 4.7 分析性能问题系统中用户访问日志的数据量，随着时间的推移，这张表的数据量会越来越大，因此我们需要根据业务需求，来对日志查询模块的性能进行优化。 1） 分页查询优化 由于在进行日志查询时，是进行分页查询，那也就意味着，在查看时，至少需要查询两次： A. 查询符合条件的总记录数。–&gt; count 操作 B. 查询符合条件的列表数据。–&gt; 分页查询 limit 操作 通常来说，count() 都需要扫描大量的行（意味着需要访问大量的数据）才能获得精确的结果，因此是很难对该SQL进行优化操作的。如果需要对count进行优化，可以采用另外一种思路，可以增加汇总表，或者redis缓存来专门记录该表对应的记录数，这样的话，就可以很轻松的实现汇总数据的查询，而且效率很高，但是这种统计并不能保证百分之百的准确 。对于数据库的操作，“快速、精确、实现简单”，三者永远只能满足其二，必须舍掉其中一个。 2） 条件查询优化 针对于条件查询,需要对查询条件,及排序字段建立索引。 3） 读写分离 通过主从复制集群，来完成读写分离，使写操作走主节点， 而读操作，走从节点。 4） MySQL服务器优化 5） 应用优化 4.8 性能优化 - 分页4.8.1 优化count创建一张表用来记录日志表的总数据量： 123create table log_counter( logcount bigint not null)engine &#x3D; innodb default CHARSET &#x3D; utf8; 在每次插入数据之后，更新该表 ： 123&lt;update id=\"updateLogCounter\" &gt; update log_counter set logcount = logcount + 1&lt;/update&gt; 在进行分页查询时, 获取总记录数，从该表中查询既可。 123&lt;select id=\"countLogFromCounter\" resultType=\"long\"&gt; select logcount from log_counter limit 1&lt;/select&gt; 4.8.2 优化 limit在进行分页时，一般通过创建覆盖索引，能够比较好的提高性能。一个非常常见，而又非常头疼的分页场景就是 “limit 1000000,10” ，此时MySQL需要搜索出前1000010 条记录后，仅仅需要返回第 1000001 到 1000010 条记录，前1000000 记录会被抛弃，查询代价非常大。 当点击比较靠后的页码时，就会出现这个问题，查询效率非常慢。 优化SQL： 1select * from operation_log limit 3000000 , 10; 将上述SQL优化为 : 1select * from operation_log t , (select id from operation_log order by id limit 3000000,10) b where t.id &#x3D; b.id ; 123456789101112131415161718&lt;select id=\"selectListByCondition\" parameterType=\"map\" resultType=\"operationLog\"&gt; select id , operate_class as operateClass , operate_method as operateMethod, return_class as returnClass, operate_user as operateUser, operate_time as operateTime, param_and_value as paramAndValue, cost_time as costTime, return_value as returnValue from operation_log t, (select id from operation_log &lt;where&gt; &lt;include refid=\"oplog_where\"/&gt; &lt;/where&gt; order by id limit #&#123;start&#125;,#&#123;rows&#125;) b where t.id = b.id &lt;/select&gt; 4.9 性能优化 - 索引 当根据操作人进行查询时， 查询的效率很低，耗时比较长。原因就是因为在创建数据库表结构时，并没有针对于 操作人字段建立索引。 1CREATE INDEX idx_user_method_return_cost ON operation_log(operate_user,operate_method,return_class,cost_time); 同上 ， 为了查询效率高，我们也需要对 操作方法、返回值类型、操作耗时 等字段进行创建索引，以提高查询效率。 12345CREATE INDEX idx_optlog_method_return_cost ON operation_log(operate_method,return_class,cost_time);CREATE INDEX idx_optlog_return_cost ON operation_log(return_class,cost_time);CREATE INDEX idx_optlog_cost ON operation_log(cost_time); 4.10 性能优化 - 排序在查询数据时，如果业务需求中需要我们对结果内容进行了排序处理 , 这个时候,我们还需要对排序的字段建立适当的索引, 来提高排序的效率 。 4.11 性能优化 - 读写分离4.11.1 概述在Mysql主从复制的基础上，可以使用读写分离来降低单台Mysql节点的压力，从而来提高访问效率，读写分离的架构如下： 对于读写分离的实现，可以通过Spring AOP 来进行动态的切换数据源，进行操作 ： 4.11.2 实现方式db.properties 123456789jdbc.write.driver=com.mysql.jdbc.Driverjdbc.write.url=jdbc:mysql://192.168.142.128:3306/mysql_demojdbc.write.username=rootjdbc.write.password=rootjdbc.read.driver=com.mysql.jdbc.Driverjdbc.read.url=jdbc:mysql://192.168.142.129:3306/mysql_demojdbc.read.username=rootjdbc.read.password=root applicationContext-datasource.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 配置数据源 - Read --&gt; &lt;bean id=\"readDataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\" destroy-method=\"close\" lazy-init=\"true\"&gt; &lt;property name=\"driverClass\" value=\"$&#123;jdbc.read.driver&#125;\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbc.read.url&#125;\"&gt;&lt;/property&gt; &lt;property name=\"user\" value=\"$&#123;jdbc.read.username&#125;\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.read.password&#125;\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置数据源 - Write --&gt; &lt;bean id=\"writeDataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\" destroy-method=\"close\" lazy-init=\"true\"&gt; &lt;property name=\"driverClass\" value=\"$&#123;jdbc.write.driver&#125;\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"$&#123;jdbc.write.url&#125;\"&gt;&lt;/property&gt; &lt;property name=\"user\" value=\"$&#123;jdbc.write.username&#125;\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.write.password&#125;\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 配置动态分配的读写 数据源 --&gt; &lt;bean id=\"dataSource\" class=\"com.wgy.aop.datasource.ChooseDataSource\" lazy-init=\"true\"&gt; &lt;property name=\"targetDataSources\"&gt; &lt;map key-type=\"java.lang.String\" value-type=\"javax.sql.DataSource\"&gt; &lt;entry key=\"write\" value-ref=\"writeDataSource\"/&gt; &lt;entry key=\"read\" value-ref=\"readDataSource\"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=\"defaultTargetDataSource\" ref=\"writeDataSource\"/&gt; &lt;property name=\"methodType\"&gt; &lt;map key-type=\"java.lang.String\"&gt; &lt;entry key=\"read\" value=\",get,select,count,list,query,find\"/&gt; &lt;entry key=\"write\" value=\",add,create,update,delete,remove,insert\"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; ChooseDataSource 123456789101112131415161718192021222324252627public class ChooseDataSource extends AbstractRoutingDataSource &#123; public static Map&lt;String, List&lt;String&gt;&gt; METHOD_TYPE_MAP = new HashMap&lt;String, List&lt;String&gt;&gt;(); /** * 实现父类中的抽象方法，获取数据源名称 * @return */ protected Object determineCurrentLookupKey() &#123; return DataSourceHandler.getDataSource(); &#125; // 设置方法名前缀对应的数据源 public void setMethodType(Map&lt;String, String&gt; map) &#123; for (String key : map.keySet()) &#123; List&lt;String&gt; v = new ArrayList&lt;String&gt;(); String[] types = map.get(key).split(\",\"); for (String type : types) &#123; if (!StringUtils.isEmpty(type)) &#123; v.add(type); &#125; &#125; METHOD_TYPE_MAP.put(key, v); &#125; System.out.println(\"METHOD_TYPE_MAP : \"+METHOD_TYPE_MAP); &#125;&#125; DataSourceHandler 12345678910111213141516171819public class DataSourceHandler &#123; // 数据源名称 public static final ThreadLocal&lt;String&gt; holder = new ThreadLocal&lt;String&gt;(); /** * 在项目启动的时候将配置的读、写数据源加到holder中 */ public static void putDataSource(String datasource) &#123; holder.set(datasource); &#125; /** * 从holer中获取数据源字符串 */ public static String getDataSource() &#123; return holder.get(); &#125;&#125; DataSourceAspect 1234567891011121314151617181920212223242526272829303132333435@Aspect@Component@Order(-9999)@EnableAspectJAutoProxy(proxyTargetClass = true)public class DataSourceAspect &#123; protected Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 配置前置通知,使用在方法aspect()上注册的切入点 */ @Before(\"execution(* com.wgy.service.*.*(..))\") @Order(-9999) public void before(JoinPoint point) &#123; String className = point.getTarget().getClass().getName(); String method = point.getSignature().getName(); logger.info(className + \".\" + method + \"(\" + Arrays.asList(point.getArgs())+ \")\"); try &#123; for (String key : ChooseDataSource.METHOD_TYPE_MAP.keySet()) &#123; for (String type : ChooseDataSource.METHOD_TYPE_MAP.get(key)) &#123; if (method.startsWith(type)) &#123; System.out.println(\"key : \" + key); DataSourceHandler.putDataSource(key); break; &#125; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 通过 @Order(-9999) 注解来控制事务管理器, 与该通知类的加载顺序 , 需要让通知类 , 先加载 , 来判定使用哪个数据源 . 4.11.3 验证在主库和从库中，执行如下SQL语句，来查看是否读的时候， 从从库中读取 ； 写入操作的时候，是否写入到主库。 1show status like &#39;Innodb_rows_%&#39; ; 4.11.4 原理 4.12 性能优化 - 应用优化4.12.1 缓存可以在业务系统中使用redis来做缓存，缓存一些基础性的数据，来降低关系型数据库的压力，提高访问效率。 4.12.2 全文检索如果业务系统中的数据量比较大（达到千万级别），这个时候，如果再对数据库进行查询，特别是进行分页查询，速度将变得很慢（因为在分页时首先需要count求合计数），为了提高访问效率，这个时候，可以考虑加入Solr 或者 ElasticSearch全文检索服务，来提高访问效率。 4.13.3 非关系数据库也可以考虑将非核心（重要）数据，存在 MongoDB 中，这样可以提高插入以及查询的效率。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wgy1993.gitee.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://wgy1993.gitee.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MySQL高级教程(三)","date":"2020-05-20T04:09:18.000Z","path":"archives/1ce8b9f7.html","text":"1. 应用优化我们已经介绍了很多数据库的优化措施。但是在实际生产环境中，由于数据库本身的性能局限，就必须要对前台的应用进行一些优化，来降低数据库的访问压力。 1.1 使用连接池对于访问数据库来说，建立连接的代价是比较昂贵的，因为我们频繁的创建关闭连接，是比较耗费资源的，我们有必要建立数据库连接池，以提高访问的性能。 1.2 减少对MySQL的访问1.2.1 避免对数据进行重复检索在编写应用代码时，需要能够理清对数据库的访问逻辑。能够一次连接就获取到结果的，就不用两次连接，这样可以大大减少对数据库无用的重复请求。 比如 ，需要获取书籍的id 和name字段 ， 则查询如下： 1select id , name from tb_book; 之后，在业务逻辑中有需要获取到书籍状态信息， 则查询如下： 1select id , status from tb_book; 这样，就需要向数据库提交两次请求，数据库就要做两次查询操作。其实完全可以用一条SQL语句得到想要的结果。 1select id, name , status from tb_book; 1.2.2 增加cache层在应用中，我们可以在应用中增加缓存层来达到减轻数据库负担的目的。缓存层有很多种，也有很多实现方式，只要能达到降低数据库的负担又能满足应用需求就可以。 因此可以部分数据从数据库中抽取出来放到应用端以文本方式存储， 或者使用框架(Mybatis, Hibernate)提供的一级缓存/二级缓存，或者使用redis数据库来缓存数据 。 1.3 负载均衡负载均衡是应用中使用非常普遍的一种优化方法，它的机制就是利用某种均衡算法，将固定的负载量分布到不同的服务器上， 以此来降低单台服务器的负载，达到优化的效果。 1.3.1 利用MySQL复制分流查询通过MySQL的主从复制，实现读写分离，使增删改操作走主节点，查询操作走从节点，从而可以降低单台服务器的读写压力。 1.3.2 采用分布式数据库架构分布式数据库架构适合大数据量、负载高的情况，它有良好的拓展性和高可用性。通过在多台服务器之间分布数据，可以实现在多台服务器之间的负载均衡，提高访问效率。 2. Mysql中查询缓存优化2.1 概述开启Mysql的查询缓存，当执行完全相同的SQL语句的时候，服务器就会直接从缓存中读取结果，当数据被修改，之前的缓存会失效，修改比较频繁的表不适合做查询缓存。 2.2 操作流程 客户端发送一条查询给服务器； 服务器先会检查查询缓存，如果命中了缓存，则立即返回存储在缓存中的结果。否则进入下一阶段； 服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划； MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询； 将结果返回给客户端。 2.3 查询缓存配置1）查看当前的MySQL数据库是否支持查询缓存： 1SHOW VARIABLES LIKE &#39;have_query_cache&#39;; 2）查看当前MySQL是否开启了查询缓存 ： 1SHOW VARIABLES LIKE &#39;query_cache_type&#39;; 3）查看查询缓存的占用大小 ： 1SHOW VARIABLES LIKE &#39;query_cache_size&#39;; 4）查看查询缓存的状态变量： 1SHOW STATUS LIKE &#39;Qcache%&#39;; 各个变量的含义如下： 参数 含义 Qcache_free_blocks 查询缓存中的可用内存块数 Qcache_free_memory 查询缓存的可用内存量 Qcache_hits 查询缓存命中数 Qcache_inserts 添加到查询缓存的查询数 Qcache_lowmen_prunes 由于内存不足而从查询缓存中删除的查询数 Qcache_not_cached 非缓存查询的数量（由于 query_cache_type 设置而无法缓存或未缓存） Qcache_queries_in_cache 查询缓存中注册的查询数 Qcache_total_blocks 查询缓存中的块总数 2.4 开启查询缓存MySQL的查询缓存默认是关闭的，需要手动配置参数 query_cache_type ， 来开启查询缓存。query_cache_type 该参数的可取值有三个 ： 值 含义 OFF 或 0 查询缓存功能关闭 ON 或 1 查询缓存功能打开，SELECT的结果符合缓存条件即会缓存，否则，不予缓存，显式指定 SQL_NO_CACHE，不予缓存 DEMAND 或 2 查询缓存功能按需进行，显式指定 SQL_CACHE 的SELECT语句才会缓存；其它均不予缓存 在 /usr/my.cnf 配置中，增加以下配置 ： 配置完毕之后，重启服务既可生效 ； 然后就可以在命令行执行SQL语句进行验证 ，执行一条比较耗时的SQL语句，然后再多执行几次，查看后面几次的执行时间；获取通过查看查询缓存的缓存命中数，来判定是否走查询缓存。 2.5 查询缓存SELECT选项可以在SELECT语句中指定两个与查询缓存相关的选项 ： SQL_CACHE : 如果查询结果是可缓存的，并且 query_cache_type 系统变量的值为ON或 DEMAND ，则缓存查询结果 。 SQL_NO_CACHE : 服务器不使用查询缓存。它既不检查查询缓存，也不检查结果是否已缓存，也不缓存查询结果。 例子： 12SELECT SQL_CACHE id, name FROM customer;SELECT SQL_NO_CACHE id, name FROM customer; 2.6 查询缓存失效的情况1） SQL 语句不一致的情况， 要想命中查询缓存，查询的SQL语句必须一致。 12SQL1 : select count(*) from tb_item;SQL2 : Select count(*) from tb_item; 2） 当查询语句中有一些不确定的时，则不会缓存。如 ： now() , current_date() , curdate() , curtime() , rand() , uuid() , user() , database() 。 123SQL1 : select * from tb_item where updatetime &lt; now() limit 1;SQL2 : select user();SQL3 : select database(); 3） 不使用任何表查询语句。 1select &#39;A&#39;; 4） 查询 mysql， information_schema或 performance_schema 数据库中的表时，不会走查询缓存。 1select * from information_schema.engines; 5） 在存储的函数，触发器或事件的主体内执行的查询。 6） 如果表更改，则使用该表的所有高速缓存查询都将变为无效并从高速缓存中删除。这包括使用MERGE映射到已更改表的表的查询。一个表可以被许多类型的语句，如被改变 INSERT， UPDATE， DELETE， TRUNCATE TABLE， ALTER TABLE， DROP TABLE，或 DROP DATABASE 。 3. Mysql内存管理及优化3.1 内存优化原则1） 将尽量多的内存分配给MySQL做缓存，但要给操作系统和其他程序预留足够内存。 2） MyISAM 存储引擎的数据文件读取依赖于操作系统自身的IO缓存，因此，如果有MyISAM表，就要预留更多的内存给操作系统做IO缓存。 3） 排序区、连接区等缓存是分配给每个数据库会话（session）专用的，其默认值的设置要根据最大连接数合理分配，如果设置太大，不但浪费资源，而且在并发连接较高时会导致物理内存耗尽。 3.2 MyISAM 内存优化myisam存储引擎使用 key_buffer 缓存索引块，加速myisam索引的读写速度。对于myisam表的数据块，mysql没有特别的缓存机制，完全依赖于操作系统的IO缓存。 3.2.1 key_buffer_sizekey_buffer_size决定MyISAM索引块缓存区的大小，直接影响到MyISAM表的存取效率。可以在MySQL参数文件中设置key_buffer_size的值，对于一般MyISAM数据库，建议至少将1/4可用内存分配给key_buffer_size。 在/usr/my.cnf 中做如下配置： 1key_buffer_size&#x3D;512M 3.2.2 read_buffer_size如果需要经常顺序扫描myisam表，可以通过增大read_buffer_size的值来改善性能。但需要注意的是read_buffer_size是每个session独占的，如果默认值设置太大，就会造成内存浪费。 3.2.3 read_rnd_buffer_size对于需要做排序的myisam表的查询，如带有order by子句的sql，适当增加 read_rnd_buffer_size 的值，可以改善此类的sql性能。但需要注意的是 read_rnd_buffer_size 是每个session独占的，如果默认值设置太大，就会造成内存浪费。 3.3 InnoDB 内存优化innodb用一块内存区做IO缓存池，该缓存池不仅用来缓存innodb的索引块，而且也用来缓存innodb的数据块。 3.3.1 innodb_buffer_pool_size该变量决定了 innodb 存储引擎表数据和索引数据的最大缓存区大小。在保证操作系统及其他程序有足够内存可用的情况下，innodb_buffer_pool_size 的值越大，缓存命中率越高，访问InnoDB表需要的磁盘I/O 就越少，性能也就越高。 1innodb_buffer_pool_size&#x3D;512M 3.3.2 innodb_log_buffer_size决定了innodb重做日志缓存的大小，对于可能产生大量更新记录的大事务，增加innodb_log_buffer_size的大小，可以避免innodb在事务提交前就执行不必要的日志写入磁盘操作。 1innodb_log_buffer_size&#x3D;10M 4. Mysql并发参数调整从实现上来说，MySQL Server 是多线程结构，包括后台线程和客户服务线程。多线程可以有效利用服务器资源，提高数据库的并发性能。在Mysql中，控制并发连接和线程的主要参数包括 max_connections、back_log、thread_cache_size、table_open_cahce。 4.1 max_connections采用max_connections 控制允许连接到MySQL数据库的最大数量，默认值是 151。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接数已达到允许最大值而失败，这是可以考虑增大max_connections 的值。 Mysql 最大可支持的连接数，取决于很多因素，包括给定操作系统平台的线程库的质量、内存大小、每个连接的负荷、CPU的处理速度，期望的响应时间等。在Linux 平台下，性能好的服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。 4.2 back_logback_log 参数控制MySQL监听TCP端口时设置的积压请求栈大小。如果MySql的连接数达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源，将会报错。5.6.6 版本之前默认值为 50 ， 之后的版本默认为 50 + （max_connections / 5）， 但最大不超过900。 如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大back_log 的值。 4.3 table_open_cache该参数用来控制所有SQL语句执行线程可打开表缓存的数量， 而在执行SQL语句时，每一个SQL执行线程至少要打开 1 个表缓存。该参数的值应该根据设置的最大连接数 max_connections 以及每个连接执行关联查询中涉及的表的最大数量来设定 ： max_connections x N ； 4.4 thread_cache_size为了加快连接数据库的速度，MySQL 会缓存一定数量的客户服务线程以备重用，通过参数 thread_cache_size 可控制 MySQL 缓存客户服务线程的数量。 4.5 innodb_lock_wait_timeout该参数是用来设置InnoDB 事务等待行锁的时间，默认值是50ms ， 可以根据需要进行动态设置。对于需要快速反馈的业务系统来说，可以将行锁的等待时间调小，以避免事务长时间挂起； 对于后台运行的批量处理程序来说， 可以将行锁的等待时间调大， 以避免发生大的回滚操作。 5. Mysql锁问题5.1 锁概述锁是计算机协调多个进程或线程并发访问某一资源的机制（避免争抢）。 在数据库中，除传统的计算资源（如 CPU、RAM、I/O 等）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。 5.2 锁分类从对数据操作的粒度分 ： 1） 表锁：操作时，会锁定整个表。 2） 行锁：操作时，会锁定当前操作行。 从对数据操作的类型分： 1） 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响。 2） 写锁（排它锁）：当前操作没有完成之前，它会阻断其他写锁和读锁。 5.3 Mysql 锁相对其他数据库而言，MySQL的锁机制比较简单，其最显著的特点是不同的存储引擎支持不同的锁机制。下表中罗列出了各存储引擎对锁的支持情况： 存储引擎 表级锁 行级锁 页面锁 MyISAM 支持 不支持 不支持 InnoDB 支持 支持 不支持 MEMORY 支持 不支持 不支持 BDB 支持 不支持 支持 MySQL这3种锁的特性可大致归纳如下 ： 锁类型 特点 表级锁 偏向MyISAM 存储引擎，开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。 行级锁 偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 页面锁 开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 从上述特点可见，很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web 应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并查询的应用，如一些在线事务处理（OLTP）系统。 5.2 MyISAM 表锁MyISAM 存储引擎只支持表锁，这也是MySQL开始几个版本中唯一支持的锁类型。 5.2.1 如何加表锁MyISAM 在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT 等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用 LOCK TABLE 命令给 MyISAM 表显式加锁。 显示加表锁语法： 123加读锁 ： lock table table_name read;加写锁 ： lock table table_name write； 5.2.2 读锁案例准备环境 12345678910111213141516171819202122232425create database demo_03 default charset&#x3D;utf8mb4;use demo_03;CREATE TABLE &#96;tb_book&#96; ( &#96;id&#96; INT(11) auto_increment, &#96;name&#96; VARCHAR(50) DEFAULT NULL, &#96;publish_time&#96; DATE DEFAULT NULL, &#96;status&#96; CHAR(1) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;myisam DEFAULT CHARSET&#x3D;utf8 ;INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,&#39;java编程思想&#39;,&#39;2088-08-01&#39;,&#39;1&#39;);INSERT INTO tb_book (id, name, publish_time, status) VALUES(NULL,&#39;solr编程思想&#39;,&#39;2088-08-08&#39;,&#39;0&#39;);CREATE TABLE &#96;tb_user&#96; ( &#96;id&#96; INT(11) auto_increment, &#96;name&#96; VARCHAR(50) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;myisam DEFAULT CHARSET&#x3D;utf8 ;INSERT INTO tb_user (id, name) VALUES(NULL,&#39;令狐冲&#39;);INSERT INTO tb_user (id, name) VALUES(NULL,&#39;田伯光&#39;); 客户端 一 ： 1）获得tb_book 表的读锁 1lock table tb_book read; 2） 执行查询操作 1select * from tb_book; 可以正常执行 ， 查询出数据。 客户端 二 ： 1） 执行查询操作 1select * from tb_book; 客户端 一 ： 1）查询未锁定的表 1select name from tb_seller; 客户端 二 ： 1）查询未锁定的表 1select name from tb_seller; 可以正常查询出未锁定的表； 客户端 一 ： 1） 执行插入操作 1insert into tb_book values(null,&#39;Mysql高级&#39;,&#39;2088-01-01&#39;,&#39;1&#39;); 执行插入， 直接报错 ， 由于当前tb_book 获得的是读锁， 不能执行更新操作。 客户端 二 ： 1） 执行插入操作 1insert into tb_book values(null,&#39;Mysql高级&#39;,&#39;2088-01-01&#39;,&#39;1&#39;); 当在客户端一中释放锁指令 unlock tables 后 ， 客户端二中的 inesrt 语句 ， 立即执行 ； 5.2.3 写锁案例客户端 一 : 1）获得tb_book 表的写锁 1lock table tb_book write ; 2）执行查询操作 1select * from tb_book ; 查询操作执行成功； 3）执行更新操作 1update tb_book set name &#x3D; &#39;java编程思想（第二版）&#39; where id &#x3D; 1; 更新操作执行成功 ； 客户端 二 : 1）执行查询操作 1select * from tb_book ; 当在客户端一中释放锁指令 unlock tables 后 ， 客户端二中的 select 语句 ， 立即执行 ； 5.2.4 结论锁模式的相互兼容性如表中所示： 由上表可见： 1）对MyISAM 表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求； 2）对MyISAM 表的写操作，则会阻塞其他用户对同一表的读和写操作； 简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁，则既会阻塞读，又会阻塞写。 此外，MyISAM 的读写锁调度是写优先，这也是MyISAM不适合做写为主的表的存储引擎的原因。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。 5.2.5 查看锁的争用情况1show open tables； In_user : 表当前被查询使用的次数。如果该数为零，则表是打开的，但是当前没有被使用。 Name_locked：表名称是否被锁定。名称锁定用于取消表或对表进行重命名等操作。 1show status like &#39;Table_locks%&#39;; Table_locks_immediate ： 指的是能够立即获得表级锁的次数，每立即获取锁，值加1。 Table_locks_waited ： 指的是不能立即获取表级锁而需要等待的次数，每等待一次，该值加1，此值高说明存在着较为严重的表级锁争用情况。 5.3 InnoDB 行锁5.3.1 行锁介绍行锁特点 ：偏向InnoDB 存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。 InnoDB 与 MyISAM 的最大不同有两点：一是支持事务；二是采用了行级锁。 5.3.2 背景知识事务及其ACID属性 事务是由一组SQL语句组成的逻辑处理单元。 事务具有以下4个特性，简称为事务ACID属性。 ACID属性 含义 原子性（Atomicity） 事务是一个原子操作单元，其对数据的修改，要么全部成功，要么全部失败。 一致性（Consistent） 在事务开始和完成时，数据都必须保持一致状态。 隔离性（Isolation） 数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的 “独立” 环境下运行。 持久性（Durable） 事务完成之后，对于数据的修改是永久的。 并发事务处理带来的问题 问题 含义 丢失更新（Lost Update） 当两个或多个事务选择同一行，最初的事务修改的值，会被后面的事务修改的值覆盖。 脏读（Dirty Reads） 当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 不可重复读（Non-Repeatable Reads） 一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现和以前读出的数据不一致。 幻读（Phantom Reads） 一个事务按照相同的查询条件重新读取以前查询过的数据，却发现其他事务插入了满足其查询条件的新数据。 事务隔离级别 为了解决上述提到的事务并发问题，数据库提供一定的事务隔离机制来解决这个问题。数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使用事务在一定程度上“串行化” 进行，这显然与“并发” 是矛盾的。 数据库的隔离级别有4个，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏写、脏读、不可重复读、幻读这几类问题。 隔离级别 丢失更新 脏读 不可重复读 幻读 Read uncommitted × √ √ √ Read committed × × √ √ Repeatable read（默认） × × × √ Serializable × × × × 备注 ： √ 代表可能出现 ， × 代表不会出现 。 Mysql 的数据库的默认隔离级别为 Repeatable read ， 查看方式： 1show variables like &#39;tx_isolation&#39;; 5.3.3 InnoDB 的行锁模式InnoDB 实现了以下两种类型的行锁。 共享锁（S）：又称为读锁，简称S锁，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。 排他锁（X）：又称为写锁，简称X锁，排他锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行读取和修改。 对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)； 对于普通SELECT语句，InnoDB不会加任何锁； 可以通过以下语句显示给记录集加共享锁或排他锁 。 123共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE排他锁（X) ：SELECT * FROM table_name WHERE ... FOR UPDATE 5.3.4 案例准备工作123456789101112131415161718create table test_innodb_lock( id int(11), name varchar(16), sex varchar(1))engine &#x3D; innodb default charset&#x3D;utf8;insert into test_innodb_lock values(1,&#39;100&#39;,&#39;1&#39;);insert into test_innodb_lock values(3,&#39;3&#39;,&#39;1&#39;);insert into test_innodb_lock values(4,&#39;400&#39;,&#39;0&#39;);insert into test_innodb_lock values(5,&#39;500&#39;,&#39;1&#39;);insert into test_innodb_lock values(6,&#39;600&#39;,&#39;0&#39;);insert into test_innodb_lock values(7,&#39;700&#39;,&#39;0&#39;);insert into test_innodb_lock values(8,&#39;800&#39;,&#39;1&#39;);insert into test_innodb_lock values(9,&#39;900&#39;,&#39;1&#39;);insert into test_innodb_lock values(1,&#39;200&#39;,&#39;0&#39;);create index idx_test_innodb_lock_id on test_innodb_lock(id);create index idx_test_innodb_lock_name on test_innodb_lock(name); 5.3.5 行锁基本演示 Session-1 Session-2 关闭自动提交功能 关闭自动提交功能 可以正常的查询出全部的数据 可以正常的查询出全部的数据 查询id 为3的数据 ； 获取id为3的数据 ； 更新id为3的数据，但是不提交； 更新id为3 的数据， 出于等待状态 通过commit， 提交事务 解除阻塞，更新正常进行 以上， 操作的都是同一行的数据，接下来，演示不同行的数据 ： 更新id为3数据，正常的获取到行锁 ， 执行更新 ； 由于与Session-1 操作不是同一行，获取当前行锁，执行更新； 5.3.6 无索引行锁升级为表锁如果不通过索引条件检索数据，那么InnoDB将对表中的所有记录加锁，实际效果跟表锁一样。 查看当前表的索引 ： show index from test_innodb_lock ; Session-1 Session-2 关闭事务的自动提交 关闭事务的自动提交 执行更新语句 ： 执行更新语句， 但处于阻塞状态： 提交事务： 解除阻塞，执行更新成功 ： 执行提交操作 ： 由于 执行更新时 ， name字段本来为varchar类型， 我们是作为数组类型使用，存在类型转换，索引失效，最终行锁变为表锁 ； 5.3.7 间隙锁危害当我们用范围条件，而不是使用相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据进行加锁； 对于键值在条件范围内但并不存在的记录，叫做 “间隙（GAP）” ， InnoDB也会对这个 “间隙” 加锁，这种锁机制就是所谓的 间隙锁（Next-Key锁） 。 示例 ： Session-1 Session-2 关闭事务自动提交 关闭事务自动提交 根据id范围更新数据 插入id为2的记录， 出于阻塞状态 提交事务 ； 解除阻塞 ， 执行插入操作 ： 提交事务 ： 5.3.8 InnoDB 行锁争用情况1show status like &#39;innodb_row_lock%&#39;; 1234567891011Innodb_row_lock_current_waits: 当前正在等待锁定的数量Innodb_row_lock_time: 从系统启动到现在锁定总时间长度Innodb_row_lock_time_avg:每次等待所花平均时长Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间Innodb_row_lock_waits: 系统启动后到现在总共等待的次数当等待的次数很高，而且每次等待的时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。 5.3.9 总结InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面带来了性能损耗可能比表锁会更高一些，但是在整体并发处理能力方面要远远由于MyISAM的表锁的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势。 但是，InnoDB的行级锁同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。 优化建议： 尽可能让所有数据检索都能通过索引来完成，避免无索引行锁升级为表锁。 合理设计索引，尽量缩小锁的范围 尽可能减少索引条件，及索引范围，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可使用低级别事务隔离（但是需要业务层面满足需求） 6. 常用SQL技巧6.1 SQL执行顺序编写顺序 12345678910111213141516SELECT DISTINCT &lt;select list&gt;FROM &lt;left_table&gt; &lt;join_type&gt;JOIN &lt;right_table&gt; ON &lt;join_condition&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;HAVING &lt;having_condition&gt;ORDER BY &lt;order_by_condition&gt;LIMIT &lt;limit_params&gt; 执行顺序 1234567891011121314151617FROM &lt;left_table&gt;ON &lt;join_condition&gt;&lt;join_type&gt; JOIN &lt;right_table&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;HAVING &lt;having_condition&gt;SELECT DISTINCT &lt;select list&gt;ORDER BY &lt;order_by_condition&gt;LIMIT &lt;limit_params&gt; 6.2 正则表达式使用正则表达式（Regular Expression）是指一个用来描述或者匹配一系列符合某个句法规则的字符串的单个字符串。 符号 含义 ^ 在字符串开始处进行匹配 $ 在字符串末尾处进行匹配 . 匹配任意单个字符, 包括换行符 […] 匹配出括号内的任意字符 [^…] 匹配不出括号内的任意字符 a* 匹配零个或者多个a(包括空串) a+ 匹配一个或者多个a(不包括空串) a? 匹配零个或者一个a a1|a2 匹配a1或a2 a(m) 匹配m个a a(m,) 至少匹配m个a a(m,n) 匹配m个a 到 n个a a(,n) 匹配0到n个a (…) 将模式元素组成单一元素 12345select * from emp where name regexp &#39;^T&#39;;select * from emp where name regexp &#39;2$&#39;;select * from emp where name regexp &#39;[uvw]&#39;; 6.3 MySQL 常用函数数字函数 函数名称 作 用 ABS 求绝对值 SQRT 求二次方根 MOD 求余数 CEIL 和 CEILING 两个函数功能相同，都是返回不小于参数的最小整数，即向上取整 FLOOR 向下取整，返回值转化为一个BIGINT RAND 生成一个0~1之间的随机数，传入整数参数是，用来产生重复序列 ROUND 对所传参数进行四舍五入 SIGN 返回参数的符号 POW 和 POWER 两个函数的功能相同，都是所传参数的次方的结果值 SIN 求正弦值 ASIN 求反正弦值，与函数 SIN 互为反函数 COS 求余弦值 ACOS 求反余弦值，与函数 COS 互为反函数 TAN 求正切值 ATAN 求反正切值，与函数 TAN 互为反函数 COT 求余切值 字符串函数 函数名称 作 用 LENGTH 计算字符串长度函数，返回字符串的字节长度 CONCAT 合并字符串函数，返回结果为连接参数产生的字符串，参数可以使一个或多个 INSERT 替换字符串函数 LOWER 将字符串中的字母转换为小写 UPPER 将字符串中的字母转换为大写 LEFT 从左侧字截取符串，返回字符串左边的若干个字符 RIGHT 从右侧字截取符串，返回字符串右边的若干个字符 TRIM 删除字符串左右两侧的空格 REPLACE 字符串替换函数，返回替换后的新字符串 SUBSTRING 截取字符串，返回从指定位置开始的指定长度的字符换 REVERSE 字符串反转（逆序）函数，返回与原始字符串顺序相反的字符串 日期函数 函数名称 作 用 CURDATE 和 CURRENT_DATE 两个函数作用相同，返回当前系统的日期值 CURTIME 和 CURRENT_TIME 两个函数作用相同，返回当前系统的时间值 NOW 和 SYSDATE 两个函数作用相同，返回当前系统的日期和时间值 MONTH 获取指定日期中的月份 MONTHNAME 获取指定日期中的月份英文名称 DAYNAME 获取指定曰期对应的星期几的英文名称 DAYOFWEEK 获取指定日期对应的一周的索引位置值 WEEK 获取指定日期是一年中的第几周，返回值的范围是否为 0〜52 或 1〜53 DAYOFYEAR 获取指定曰期是一年中的第几天，返回值范围是1~366 DAYOFMONTH 获取指定日期是一个月中是第几天，返回值范围是1~31 YEAR 获取年份，返回值范围是 1970〜2069 TIME_TO_SEC 将时间参数转换为秒数 SEC_TO_TIME 将秒数转换为时间，与TIME_TO_SEC 互为反函数 DATE_ADD 和 ADDDATE 两个函数功能相同，都是向日期添加指定的时间间隔 DATE_SUB 和 SUBDATE 两个函数功能相同，都是向日期减去指定的时间间隔 ADDTIME 时间加法运算，在原始时间上添加指定的时间 SUBTIME 时间减法运算，在原始时间上减去指定的时间 DATEDIFF 获取两个日期之间间隔，返回参数 1 减去参数 2 的值 DATE_FORMAT 格式化指定的日期，根据参数返回指定格式的值 WEEKDAY 获取指定日期在一周内的对应的工作日索引 聚合函数 函数名称 作用 MAX 查询指定列的最大值 MIN 查询指定列的最小值 COUNT 统计查询结果的行数 SUM 求和，返回指定列的总和 AVG 求平均值，返回指定列数据的平均值","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wgy1993.gitee.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://wgy1993.gitee.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MySQL高级教程(二)","date":"2020-05-19T09:27:28.000Z","path":"archives/621b99dc.html","text":"1. Mysql的体系结构概览整个MySQL Server由以下组成 Connection Pool : 连接池组件 Management Services &amp; Utilities : 管理服务和工具组件 SQL Interface : SQL接口组件 Parser : 查询分析器组件 Optimizer : 优化器组件 Caches &amp; Buffers : 缓冲池组件 Pluggable Storage Engines : 存储引擎 File System : 文件系统 1.1 连接层最上层是一些客户端和链接服务，包含本地sock 通信和大多数基于客户端/服务端工具实现的类似于 TCP/IP的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。 1.2 服务层第二层架构主要完成大多数的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化，部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如 过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定表的查询的顺序，是否利用索引等， 最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存，如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。 1.3 引擎层存储引擎层， 存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API和存储引擎进行通信。不同的存储引擎具有不同的功能，这样我们可以根据自己的需要，来选取合适的存储引擎。 1.4 存储层数据存储层， 主要是将数据存储在文件系统之上，并完成与存储引擎的交互。 和其他数据库相比，MySQL有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎上，插件式的存储引擎架构，将查询处理和其他的系统任务以及数据的存储提取分离。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。 2. 存储引擎2.1 存储引擎概述和大多数的数据库不同, MySQL中有一个存储引擎的概念, 针对不同的存储需求可以选择最优的存储引擎。 存储引擎就是存储数据，建立索引，更新查询数据等等技术的实现方式 。存储引擎是基于表的，而不是基于库的。所以存储引擎也可被称为表类型。 Oracle，SqlServer等数据库只有一种存储引擎。MySQL提供了插件式的存储引擎架构。所以MySQL存在多种存储引擎，可以根据需要使用相应引擎，或者编写存储引擎。 MySQL5.0支持的存储引擎包含 ： InnoDB 、MyISAM 、BDB、MEMORY、MERGE、EXAMPLE、NDB Cluster、ARCHIVE、CSV、BLACKHOLE、FEDERATED等，其中InnoDB和BDB提供事务安全表，其他存储引擎是非事务安全表。 可以通过指定 show engines ， 来查询当前数据库支持的存储引擎 ： 创建新表时如果不指定存储引擎，那么系统就会使用默认的存储引擎，MySQL5.5之前的默认存储引擎是MyISAM，5.5之后就改为了InnoDB。 查看Mysql数据库默认的存储引擎 ， 指令 ： 1show variables like &#39;%storage_engine%&#39; ； 2.2 各种存储引擎特性下面重点介绍几种常用的存储引擎， 并对比各个存储引擎之间的区别， 如下表所示 ： 特点 InnoDB MyISAM MEMORY MERGE NDB 存储限制 64TB 有 有 没有 有 事务安全 支持 锁机制 行锁(适合高并发) 表锁 表锁 表锁 行锁 B树索引 支持 支持 支持 支持 支持 哈希索引 支持 全文索引 支持(5.6版本之后) 支持 集群索引 支持 数据索引 支持 支持 支持 索引缓存 支持 支持 支持 支持 支持 数据可压缩 支持 空间使用 高 低 N/A 低 低 内存使用 高 低 中等 低 高 批量插入速度 低 高 高 高 高 支持外键 支持 下面我们将重点介绍最长使用的两种存储引擎： InnoDB、MyISAM ， 另外两种 MEMORY、MERGE ， 了解即可。 2.2.1 InnoDBInnoDB存储引擎是Mysql的默认存储引擎。InnoDB存储引擎提供了具有提交、回滚、崩溃恢复能力的事务安全。但是对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保留数据和索引。 InnoDB存储引擎不同于其他存储引擎的特点 ： 2.2.1.1 事务控制12345create table goods_innodb( id int NOT NULL AUTO_INCREMENT, name varchar(20) NOT NULL, primary key(id))ENGINE&#x3D;innodb DEFAULT CHARSET&#x3D;utf8; 12345start transaction;insert into goods_innodb(id,name)values(null,&#39;Meta20&#39;);commit; 测试，发现在InnoDB中是存在事务的 ； 2.2.1.2 外键约束MySQL支持外键的存储引擎只有InnoDB ， 在创建外键的时候， 要求父表必须有对应的索引 ， 子表在创建外键的时候， 也会自动的创建对应的索引。 下面两张表中 ， country_innodb是父表 ， country_id为主键索引，city_innodb表是子表，country_id字段为外键，对应于country_innodb表的主键country_id 。 1234567891011121314151617181920create table country_innodb( country_id int NOT NULL AUTO_INCREMENT, country_name varchar(100) NOT NULL, primary key(country_id))ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;create table city_innodb( city_id int NOT NULL AUTO_INCREMENT, city_name varchar(50) NOT NULL, country_id int NOT NULL, primary key(city_id), key idx_fk_country_id(country_id), CONSTRAINT &#96;fk_city_country&#96; FOREIGN KEY(country_id) REFERENCES country_innodb(country_id) ON DELETE RESTRICT ON UPDATE CASCADE)ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;insert into country_innodb values(null,&#39;China&#39;),(null,&#39;America&#39;),(null,&#39;Japan&#39;);insert into city_innodb values(null,&#39;Xian&#39;,1),(null,&#39;NewYork&#39;,2),(null,&#39;BeiJing&#39;,1); 在创建索引时， 可以指定在删除、更新父表时，对子表进行的相应操作，包括 RESTRICT、CASCADE、SET NULL 和 NO ACTION。 RESTRICT和NO ACTION相同， 是指限制在子表有关联记录的情况下， 父表不能更新； CASCADE表示父表在更新或者删除时，更新或者删除子表对应的记录； SET NULL 则表示父表在更新或者删除的时候，子表的对应字段被SET NULL 。 针对上面创建的两个表， 子表的外键指定是ON DELETE RESTRICT ON UPDATE CASCADE 方式的， 那么在主表删除记录的时候， 如果子表有对应记录， 则不允许删除， 主表在更新记录的时候， 如果子表有对应记录， 则子表对应更新 。 表中数据如下图所示 ： 外键信息可以使用如下两种方式查看 ： 1show create table city_innodb ; 删除country_id为1 的country数据： 1delete from country_innodb where country_id &#x3D; 1; 更新主表country表的字段 country_id : 1update country_innodb set country_id &#x3D; 100 where country_id &#x3D; 1; 更新后， 子表的数据信息为 ： 2.2.1.3 存储方式InnoDB 存储表和索引有以下两种方式 ： 使用共享表空间存储， 这种方式创建的表的表结构保存在.frm文件中， 数据和索引保存在innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。 使用多表空间存储， 这种方式创建的表的表结构仍然存在 .frm 文件中，但是每个表的数据和索引单独保存在 .ibd 中。 2.2.2 MyISAMMyISAM 不支持事务、也不支持外键，其优势是访问的速度快，对事务的完整性没有要求或者以SELECT、INSERT为主的应用基本上都可以使用这个引擎来创建表 。有以下两个比较重要的特点： 2.2.2.1 不支持事务12345create table goods_myisam( id int NOT NULL AUTO_INCREMENT, name varchar(20) NOT NULL, primary key(id))ENGINE&#x3D;myisam DEFAULT CHARSET&#x3D;utf8; 通过测试，我们发现，在MyISAM存储引擎中，是没有事务控制的 ； 2.2.2.2 文件存储方式每个MyISAM在磁盘上存储成3个文件，其文件名都和表名相同，但拓展名分别是 ： .frm (存储表定义)； .MYD(MYData , 存储数据)； .MYI(MYIndex , 存储索引)； 2.2.3 MEMORYMemory存储引擎将表的数据存放在内存中。每个MEMORY表实际对应一个磁盘文件，格式是.frm ，该文件中只存储表的结构，而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。MEMORY 类型的表访问非常地快，因为他的数据是存放在内存中的，并且默认使用HASH索引 ， 但是服务一旦关闭，表中的数据就会丢失。 2.2.4 MERGEMERGE存储引擎是一组MyISAM表的组合，这些MyISAM表必须结构完全相同，MERGE表本身并没有存储数据，对MERGE类型的表可以进行查询、更新、删除操作，这些操作实际上是对内部的MyISAM表进行的。 对于MERGE类型表的插入操作，是通过INSERT_METHOD子句定义插入的表，可以有3个不同的值，使用FIRST 或 LAST 值使得插入操作被相应地作用在第一或者最后一个表上，不定义这个子句或者定义为NO，表示不能对这个MERGE表执行插入操作。 可以对MERGE表进行DROP操作，但是这个操作只是删除MERGE表的定义，对内部的表是没有任何影响的。 下面是一个创建和使用MERGE表的示例 ： 1）. 创建3个测试表 order_1990, order_1991, order_all , 其中order_all是前两个表的MERGE表 ： 1234567891011121314151617181920create table order_1990( order_id int , order_money double(10,2), order_address varchar(50), primary key (order_id))engine &#x3D; myisam default charset&#x3D;utf8;create table order_1991( order_id int , order_money double(10,2), order_address varchar(50), primary key (order_id))engine &#x3D; myisam default charset&#x3D;utf8;create table order_all( order_id int , order_money double(10,2), order_address varchar(50), primary key (order_id))engine &#x3D; merge union &#x3D; (order_1990,order_1991) INSERT_METHOD&#x3D;LAST default charset&#x3D;utf8; 2）. 分别向两张表中插入记录 12345insert into order_1990 values(1,100.0,&#39;北京&#39;);insert into order_1990 values(2,100.0,&#39;上海&#39;);insert into order_1991 values(10,200.0,&#39;北京&#39;);insert into order_1991 values(11,200.0,&#39;上海&#39;); 3）. 查询3张表中的数据。 order_1990中的数据 ： order_1991中的数据 ： order_all中的数据 ： 4）. 往order_all中插入一条记录 ，由于在MERGE表定义时，INSERT_METHOD 选择的是LAST，那么插入的数据会想最后一张表中插入。 1insert into order_all values(100,10000.0,&#39;西安&#39;)； 2.3 存储引擎的选择在选择存储引擎时，应该根据应用系统的特点选择合适的存储引擎。对于复杂的应用系统，还可以根据实际情况选择多种存储引擎进行组合。以下是几种常用的存储引擎的使用环境。 InnoDB : 是Mysql的默认存储引擎，用于事务处理应用程序，支持外键。如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，数据操作除了插入和查询意外，还包含很多的更新、删除操作，那么InnoDB存储引擎是比较合适的选择。InnoDB存储引擎除了有效的降低由于删除和更新导致的锁定， 还可以确保事务的完整提交和回滚，对于类似于计费系统或者财务系统等对数据准确性要求比较高的系统，InnoDB是最合适的选择。 MyISAM ：如果应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。 MEMORY：将所有数据保存在RAM中，在需要快速定位记录和其他类似数据环境下，可以提供几块的访问。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，其次是要确保表的数据可以恢复，数据库异常终止后表中的数据是可以恢复的。MEMORY表通常用于更新不太频繁的小表，用以快速得到访问结果。 MERGE：用于将一系列等同的MyISAM表以逻辑方式组合在一起，并作为一个对象引用他们。MERGE表的优点在于可以突破对单个MyISAM表的大小限制，并且通过将不同的表分布在多个磁盘上，可以有效的改善MERGE表的访问效率。这对于存储诸如数据仓储等VLDB环境十分合适。 3. 优化SQL步骤在应用的的开发过程中，由于初期数据量小，开发人员写 SQL 语句时更重视功能上的实现，但是当应用系统正式上线后，随着生产数据量的急剧增长，很多 SQL 语句开始逐渐显露出性能问题，对生产的影响也越来越大，此时这些有问题的 SQL 语句就成为整个系统性能的瓶颈，因此我们必须要对它们进行优化，本章将详细介绍在 MySQL 中优化 SQL 语句的方法。 当面对一个有 SQL 性能问题的数据库时，我们应该从何处入手来进行系统的分析，使得能够尽快定位问题 SQL 并尽快解决问题。 3.1 查看SQL执行频率MySQL 客户端连接成功后，通过 show [session|global] status 命令可以提供服务器状态信息。show [session|global] status 可以根据需要加上参数“session”或者“global”来显示 session 级（当前连接）的计结果和 global 级（自数据库上次启动至今）的统计结果。如果不写，默认使用参数是“session”。 下面的命令显示了当前 session 中所有统计参数的值： 1show status like &#39;Com_______&#39;; 1show status like &#39;Innodb_rows_%&#39;; Com_xxx 表示每个 xxx 语句执行的次数，我们通常比较关心的是以下几个统计参数。 参数 含义 Com_select 执行 select 操作的次数，一次查询只累加 1。 Com_insert 执行 INSERT 操作的次数，对于批量插入的 INSERT 操作，只累加一次。 Com_update 执行 UPDATE 操作的次数。 Com_delete 执行 DELETE 操作的次数。 Innodb_rows_read select 查询返回的行数。 Innodb_rows_inserted 执行 INSERT 操作插入的行数。 Innodb_rows_updated 执行 UPDATE 操作更新的行数。 Innodb_rows_deleted 执行 DELETE 操作删除的行数。 Connections 试图连接 MySQL 服务器的次数。 Uptime 服务器工作时间。 Slow_queries 慢查询的次数。 Com_*** : 这些参数对于所有存储引擎的表操作都会进行累计。 Innodb_*** : 这几个参数只是针对InnoDB 存储引擎的，累加的算法也略有不同。 3.2 定位低效率执行SQL可以通过以下两种方式定位执行效率较低的 SQL 语句。 慢查询日志 : 通过慢查询日志定位那些执行效率较低的 SQL 语句，用–log-slow-queries[=file_name]选项启动时，mysqld 写一个包含所有执行时间超过 long_query_time 秒的 SQL 语句的日志文件。 show processlist : 慢查询日志在查询结束以后才纪录，所以在应用反映执行效率出现问题的时候查询慢查询日志并不能定位问题，可以使用show processlist命令查看当前MySQL在进行的线程，包括线程的状态、是否锁表等，可以实时地查看 SQL 的执行情况，同时对一些锁表操作进行优化。 1234567891011121314151） id列，用户登录mysql时，系统分配的&quot;connection_id&quot;，可以使用函数connection_id()查看2） user列，显示当前用户。如果不是root，这个命令就只显示用户权限范围的sql语句3） host列，显示这个语句是从哪个ip的哪个端口上发的，可以用来跟踪出现问题语句的用户4） db列，显示这个进程目前连接的是哪个数据库5） command列，显示当前连接的执行的命令，一般取值为休眠（sleep），查询（query），连接（connect）等6） time列，显示这个状态持续的时间，单位是秒7） state列，显示使用当前连接的sql语句的状态，很重要的列。state描述的是语句执行中的某一个状态。一个sql语句，以查询为例，可能需要经过copying to tmp table、sorting result、sending data等状态才可以完成8） info列，显示这个sql语句，是判断问题语句的一个重要依据 3.3 explain分析执行计划通过以上步骤查询到效率低的 SQL 语句后，可以通过 EXPLAIN或者 DESC命令获取 MySQL如何执行 SELECT 语句的信息，包括在 SELECT 语句执行过程中表如何连接和连接的顺序。 查询SQL语句的执行计划 ： 1explain select * from tb_item where id &#x3D; 1; 1explain select * from tb_item where title &#x3D; &#39;阿尔卡特 (OT-979) 冰川白 联通3G手机3&#39;; 字段 含义 id select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。 select_type 表示 SELECT 的类型，常见的取值有 SIMPLE（简单表，即不使用表连接或者子查询）、PRIMARY（主查询，即外层的查询）、UNION（UNION 中的第二个或者后面的查询语句）、SUBQUERY（子查询中的第一个 SELECT）等 table 输出结果集的表 type 表示表的连接类型，性能由好到差的连接类型为( system —&gt; const —–&gt; eq_ref ——&gt; ref ——-&gt; ref_or_null—-&gt; index_merge —&gt; index_subquery —–&gt; range —–&gt; index ——&gt; all ) possible_keys 表示查询时，可能使用的索引 key 表示实际使用的索引 key_len 索引字段的长度 rows 扫描行的数量 extra 执行情况的说明和描述 3.3.1 环境准备 12345678910111213141516171819202122232425262728293031323334353637383940414243CREATE TABLE &#96;t_role&#96; ( &#96;id&#96; varchar(32) NOT NULL, &#96;role_name&#96; varchar(255) DEFAULT NULL, &#96;role_code&#96; varchar(255) DEFAULT NULL, &#96;description&#96; varchar(255) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;), UNIQUE KEY &#96;unique_role_name&#96; (&#96;role_name&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;CREATE TABLE &#96;t_user&#96; ( &#96;id&#96; varchar(32) NOT NULL, &#96;username&#96; varchar(45) NOT NULL, &#96;password&#96; varchar(96) NOT NULL, &#96;name&#96; varchar(45) NOT NULL, PRIMARY KEY (&#96;id&#96;), UNIQUE KEY &#96;unique_user_username&#96; (&#96;username&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;CREATE TABLE &#96;user_role&#96; ( &#96;id&#96; int(11) NOT NULL auto_increment , &#96;user_id&#96; varchar(32) DEFAULT NULL, &#96;role_id&#96; varchar(32) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;), KEY &#96;fk_ur_user_id&#96; (&#96;user_id&#96;), KEY &#96;fk_ur_role_id&#96; (&#96;role_id&#96;), CONSTRAINT &#96;fk_ur_role_id&#96; FOREIGN KEY (&#96;role_id&#96;) REFERENCES &#96;t_role&#96; (&#96;id&#96;) ON DELETE NO ACTION ON UPDATE NO ACTION, CONSTRAINT &#96;fk_ur_user_id&#96; FOREIGN KEY (&#96;user_id&#96;) REFERENCES &#96;t_user&#96; (&#96;id&#96;) ON DELETE NO ACTION ON UPDATE NO ACTION) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;insert into &#96;t_user&#96; (&#96;id&#96;, &#96;username&#96;, &#96;password&#96;, &#96;name&#96;) values(&#39;1&#39;,&#39;super&#39;,&#39;$2a$10$TJ4TmCdK.X4wv&#x2F;tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe&#39;,&#39;超级管理员&#39;);insert into &#96;t_user&#96; (&#96;id&#96;, &#96;username&#96;, &#96;password&#96;, &#96;name&#96;) values(&#39;2&#39;,&#39;admin&#39;,&#39;$2a$10$TJ4TmCdK.X4wv&#x2F;tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe&#39;,&#39;系统管理员&#39;);insert into &#96;t_user&#96; (&#96;id&#96;, &#96;username&#96;, &#96;password&#96;, &#96;name&#96;) values(&#39;3&#39;,&#39;itcast&#39;,&#39;$2a$10$8qmaHgUFUAmPR5pOuWhYWOr291WJYjHelUlYn07k5ELF8ZCrW0Cui&#39;,&#39;test02&#39;);insert into &#96;t_user&#96; (&#96;id&#96;, &#96;username&#96;, &#96;password&#96;, &#96;name&#96;) values(&#39;4&#39;,&#39;stu1&#39;,&#39;$2a$10$pLtt2KDAFpwTWLjNsmTEi.oU1yOZyIn9XkziK&#x2F;y&#x2F;spH5rftCpUMZa&#39;,&#39;学生1&#39;);insert into &#96;t_user&#96; (&#96;id&#96;, &#96;username&#96;, &#96;password&#96;, &#96;name&#96;) values(&#39;5&#39;,&#39;stu2&#39;,&#39;$2a$10$nxPKkYSez7uz2YQYUnwhR.z57km3yqKn3Hr&#x2F;p1FR6ZKgc18u.Tvqm&#39;,&#39;学生2&#39;);insert into &#96;t_user&#96; (&#96;id&#96;, &#96;username&#96;, &#96;password&#96;, &#96;name&#96;) values(&#39;6&#39;,&#39;t1&#39;,&#39;$2a$10$TJ4TmCdK.X4wv&#x2F;tCqHW14.w70U3CC33CeVncD3SLmyMXMknstqKRe&#39;,&#39;老师1&#39;);INSERT INTO &#96;t_role&#96; (&#96;id&#96;, &#96;role_name&#96;, &#96;role_code&#96;, &#96;description&#96;) VALUES(&#39;5&#39;,&#39;学生&#39;,&#39;student&#39;,&#39;学生&#39;);INSERT INTO &#96;t_role&#96; (&#96;id&#96;, &#96;role_name&#96;, &#96;role_code&#96;, &#96;description&#96;) VALUES(&#39;7&#39;,&#39;老师&#39;,&#39;teacher&#39;,&#39;老师&#39;);INSERT INTO &#96;t_role&#96; (&#96;id&#96;, &#96;role_name&#96;, &#96;role_code&#96;, &#96;description&#96;) VALUES(&#39;8&#39;,&#39;教学管理员&#39;,&#39;teachmanager&#39;,&#39;教学管理员&#39;);INSERT INTO &#96;t_role&#96; (&#96;id&#96;, &#96;role_name&#96;, &#96;role_code&#96;, &#96;description&#96;) VALUES(&#39;9&#39;,&#39;管理员&#39;,&#39;admin&#39;,&#39;管理员&#39;);INSERT INTO &#96;t_role&#96; (&#96;id&#96;, &#96;role_name&#96;, &#96;role_code&#96;, &#96;description&#96;) VALUES(&#39;10&#39;,&#39;超级管理员&#39;,&#39;super&#39;,&#39;超级管理员&#39;);INSERT INTO user_role(id,user_id,role_id) VALUES(NULL, &#39;1&#39;, &#39;5&#39;),(NULL, &#39;1&#39;, &#39;7&#39;),(NULL, &#39;2&#39;, &#39;8&#39;),(NULL, &#39;3&#39;, &#39;9&#39;),(NULL, &#39;4&#39;, &#39;8&#39;),(NULL, &#39;5&#39;, &#39;10&#39;) ; 3.3.2 explain 之 idid 字段是 select查询的序列号，是一组数字，表示的是查询中执行select子句或者是操作表的顺序。id 情况有三种 ： 1） id 相同表示加载表的顺序是从上到下。 1explain select * from t_role r, t_user u, user_role ur where r.id &#x3D; ur.role_id and u.id &#x3D; ur.user_id ; 2） id 不同id值越大，优先级越高，越先被执行。 1EXPLAIN SELECT * FROM t_role WHERE id &#x3D; (SELECT role_id FROM user_role WHERE user_id &#x3D; (SELECT id FROM t_user WHERE username &#x3D; &#39;stu1&#39;)) 3） id 有相同，也有不同，同时存在。id相同的可以认为是一组，从上往下顺序执行；在所有的组中，id的值越大，优先级越高，越先执行。 1EXPLAIN SELECT * FROM t_role r , (SELECT * FROM user_role ur WHERE ur.&#96;user_id&#96; &#x3D; &#39;2&#39;) a WHERE r.id &#x3D; a.role_id ; 3.3.3 explain 之 select_type 表示 SELECT 的类型，常见的取值，如下表所示： select_type 含义 SIMPLE 简单的select查询，查询中不包含子查询或者UNION PRIMARY 查询中若包含任何复杂的子查询，最外层查询标记为该标识 SUBQUERY 在SELECT 或 WHERE 列表中包含了子查询 DERIVED 在FROM 列表中包含的子查询，被标记为 DERIVED（衍生） MYSQL会递归执行这些子查询，把结果放在临时表中 UNION 若第二个SELECT出现在UNION之后，则标记为UNION ； 若UNION包含在FROM子句的子查询中，外层SELECT将被标记为 ： DERIVED UNION RESULT 从UNION表获取结果的SELECT 3.3.4 explain 之 table展示这一行的数据是关于哪一张表的 3.3.5 explain 之 typetype 显示的是访问类型，是较为重要的一个指标，可取值为： type 含义 NULL MySQL不访问任何表，索引，直接返回结果 system 表只有一行记录(等于系统表)，这是const类型的特例，一般不会出现 const 表示通过索引一次就找到了，const 用于比较primary key 或者 unique 索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL 就能将该查询转换为一个常亮。const于将 “主键” 或 “唯一” 索引的所有部分与常量值进行比较 eq_ref 类似ref，区别在于使用的是唯一索引，使用主键的关联查询，关联查询出的记录只有一条。常见于主键或唯一索引扫描 ref 非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，返回所有匹配某个单独值的所有行（多个） range 只检索给定返回的行，使用一个索引来选择行。 where 之后出现 between ， &lt; , &gt; , in 等操作。 index index 与 ALL的区别为 index 类型只是遍历了索引树， 通常比ALL 快， ALL 是遍历数据文件。 all 将遍历全表以找到匹配的行 结果值从最好到最坏以此是： 1234NULL &gt; system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALLsystem &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL 一般来说， 我们需要保证查询至少达到 range 级别， 最好达到ref 。 3.3.6 explain 之 key12345possible_keys : 显示可能应用在这张表的索引， 一个或多个。 key ： 实际使用的索引， 如果为NULL， 则没有使用索引。key_len : 表示索引中使用的字节数， 该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下， 长度越短越好 。 3.3.7 explain 之 rows扫描行的数量。 3.3.8 explain 之 extra其他的额外的执行计划信息，在该列展示 。 extra 含义 using filesort 说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取， 称为 “文件排序”, 效率低。 using temporary 使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于 order by 和 group by； 效率低 using index 表示相应的select操作使用了覆盖索引， 避免访问表的数据行， 效率不错。 3.4 show profile分析SQLMysql从5.0.37版本开始增加了对 show profiles 和 show profile 语句的支持。show profiles 能够在做SQL优化时帮助我们了解时间都耗费到哪里去了。 通过 have_profiling 参数，能够看到当前MySQL是否支持profile： 默认profiling是关闭的，可以通过set语句在Session级别开启profiling： 1set profiling&#x3D;1; &#x2F;&#x2F;开启profiling 开关； 通过profile，我们能够更清楚地了解SQL执行的过程。 首先，我们可以执行一系列的操作，如下图所示： 123456789show databases;use db01;show tables;select * from tb_item where id &lt; 5;select count(*) from tb_item; 执行完上述命令之后，再执行show profiles 指令， 来查看SQL语句执行的耗时： 通过show profile for query query_id 语句可以查看到该SQL执行过程中每个线程的状态和消耗的时间： 12TIP ： Sending data 状态表示MySQL线程开始访问数据行并把结果返回给客户端，而不仅仅是返回个客户端。由于在Sending data状态下，MySQL线程往往需要做大量的磁盘读取操作，所以经常是整各查询中耗时最长的状态。 在获取到最消耗时间的线程状态后，MySQL支持进一步选择all、cpu、block io 、context switch、page faults等明细类型类查看MySQL在使用什么资源上耗费了过高的时间。例如，选择查看CPU的耗费时间 ： 字段 含义 Status sql 语句执行的状态 Duration sql 执行过程中每一个步骤的耗时 CPU_user 当前用户占有的cpu CPU_system 系统占有的cpu 3.5 trace分析优化器执行计划MySQL5.6提供了对SQL的跟踪trace, 通过trace文件能够进一步了解为什么优化器选择A计划, 而不是选择B计划。 打开trace ， 设置格式为 JSON，并设置trace最大能够使用的内存大小，避免解析过程中因为默认内存过小而不能够完整展示。 12SET optimizer_trace&#x3D;&quot;enabled&#x3D;on&quot;,end_markers_in_json&#x3D;on;set optimizer_trace_max_mem_size&#x3D;1000000; 执行SQL语句 ： 1select * from tb_item where id &lt; 4; 最后， 检查information_schema.optimizer_trace就可以知道MySQL是如何执行SQL的 ： 1select * from information_schema.optimizer_trace\\G; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170*************************** 1. row ***************************QUERY: select * from tb_item where id &lt; 4TRACE: &#123; \"steps\": [ &#123; \"join_preparation\": &#123; \"select#\": 1, \"steps\": [ &#123; \"expanded_query\": \"/* select#1 */ select `tb_item`.`id` AS `id`,`tb_item`.`title` AS `title`,`tb_item`.`price` AS `price`,`tb_item`.`num` AS `num`,`tb_item`.`categoryid` AS `categoryid`,`tb_item`.`status` AS `status`,`tb_item`.`sellerid` AS `sellerid`,`tb_item`.`createtime` AS `createtime`,`tb_item`.`updatetime` AS `updatetime` from `tb_item` where (`tb_item`.`id` &lt; 4)\" &#125; ] /* steps */ &#125; /* join_preparation */ &#125;, &#123; \"join_optimization\": &#123; \"select#\": 1, \"steps\": [ &#123; \"condition_processing\": &#123; \"condition\": \"WHERE\", \"original_condition\": \"(`tb_item`.`id` &lt; 4)\", \"steps\": [ &#123; \"transformation\": \"equality_propagation\", \"resulting_condition\": \"(`tb_item`.`id` &lt; 4)\" &#125;, &#123; \"transformation\": \"constant_propagation\", \"resulting_condition\": \"(`tb_item`.`id` &lt; 4)\" &#125;, &#123; \"transformation\": \"trivial_condition_removal\", \"resulting_condition\": \"(`tb_item`.`id` &lt; 4)\" &#125; ] /* steps */ &#125; /* condition_processing */ &#125;, &#123; \"table_dependencies\": [ &#123; \"table\": \"`tb_item`\", \"row_may_be_null\": false, \"map_bit\": 0, \"depends_on_map_bits\": [ ] /* depends_on_map_bits */ &#125; ] /* table_dependencies */ &#125;, &#123; \"ref_optimizer_key_uses\": [ ] /* ref_optimizer_key_uses */ &#125;, &#123; \"rows_estimation\": [ &#123; \"table\": \"`tb_item`\", \"range_analysis\": &#123; \"table_scan\": &#123; \"rows\": 9816098, \"cost\": 2.04e6 &#125; /* table_scan */, \"potential_range_indices\": [ &#123; \"index\": \"PRIMARY\", \"usable\": true, \"key_parts\": [ \"id\" ] /* key_parts */ &#125; ] /* potential_range_indices */, \"setup_range_conditions\": [ ] /* setup_range_conditions */, \"group_index_range\": &#123; \"chosen\": false, \"cause\": \"not_group_by_or_distinct\" &#125; /* group_index_range */, \"analyzing_range_alternatives\": &#123; \"range_scan_alternatives\": [ &#123; \"index\": \"PRIMARY\", \"ranges\": [ \"id &lt; 4\" ] /* ranges */, \"index_dives_for_eq_ranges\": true, \"rowid_ordered\": true, \"using_mrr\": false, \"index_only\": false, \"rows\": 3, \"cost\": 1.6154, \"chosen\": true &#125; ] /* range_scan_alternatives */, \"analyzing_roworder_intersect\": &#123; \"usable\": false, \"cause\": \"too_few_roworder_scans\" &#125; /* analyzing_roworder_intersect */ &#125; /* analyzing_range_alternatives */, \"chosen_range_access_summary\": &#123; \"range_access_plan\": &#123; \"type\": \"range_scan\", \"index\": \"PRIMARY\", \"rows\": 3, \"ranges\": [ \"id &lt; 4\" ] /* ranges */ &#125; /* range_access_plan */, \"rows_for_plan\": 3, \"cost_for_plan\": 1.6154, \"chosen\": true &#125; /* chosen_range_access_summary */ &#125; /* range_analysis */ &#125; ] /* rows_estimation */ &#125;, &#123; \"considered_execution_plans\": [ &#123; \"plan_prefix\": [ ] /* plan_prefix */, \"table\": \"`tb_item`\", \"best_access_path\": &#123; \"considered_access_paths\": [ &#123; \"access_type\": \"range\", \"rows\": 3, \"cost\": 2.2154, \"chosen\": true &#125; ] /* considered_access_paths */ &#125; /* best_access_path */, \"cost_for_plan\": 2.2154, \"rows_for_plan\": 3, \"chosen\": true &#125; ] /* considered_execution_plans */ &#125;, &#123; \"attaching_conditions_to_tables\": &#123; \"original_condition\": \"(`tb_item`.`id` &lt; 4)\", \"attached_conditions_computation\": [ ] /* attached_conditions_computation */, \"attached_conditions_summary\": [ &#123; \"table\": \"`tb_item`\", \"attached\": \"(`tb_item`.`id` &lt; 4)\" &#125; ] /* attached_conditions_summary */ &#125; /* attaching_conditions_to_tables */ &#125;, &#123; \"refine_plan\": [ &#123; \"table\": \"`tb_item`\", \"access_type\": \"range\" &#125; ] /* refine_plan */ &#125; ] /* steps */ &#125; /* join_optimization */ &#125;, &#123; \"join_execution\": &#123; \"select#\": 1, \"steps\": [ ] /* steps */ &#125; /* join_execution */ &#125; ] /* steps */&#125; 4. 索引的使用索引是数据库优化最常用也是最重要的手段之一, 通过索引通常可以帮助用户解决大多数的MySQL的性能优化问题。 4.1 验证索引提升查询效率在我们准备的表结构tb_item 中， 一共存储了 300 万记录； 1). 根据ID查询 1select * from tb_item where id &#x3D; 1999\\G; 查询速度很快， 接近0s ， 主要的原因是因为id为主键， 有索引； 2). 根据 title 进行精确查询 1select * from tb_item where title &#x3D; &#39;iphoneX 移动3G 32G941&#39;\\G; 查看SQL语句的执行计划 ： 处理方案 ， 针对title字段， 创建索引 ： 1create index idx_item_title on tb_item(title); 索引创建完成之后，再次进行查询 ： 通过explain ， 查看执行计划，执行SQL时使用了刚才创建的索引 4.2 索引的使用4.2.1 准备环境12345678910111213141516171819202122232425create table &#96;tb_seller&#96; ( &#96;sellerid&#96; varchar (100), &#96;name&#96; varchar (100), &#96;nickname&#96; varchar (50), &#96;password&#96; varchar (60), &#96;status&#96; varchar (1), &#96;address&#96; varchar (100), &#96;createtime&#96; datetime, primary key(&#96;sellerid&#96;))engine&#x3D;innodb default charset&#x3D;utf8mb4; insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;alibaba&#39;,&#39;阿里巴巴&#39;,&#39;阿里小店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;1&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;baidu&#39;,&#39;百度科技有限公司&#39;,&#39;百度小店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;1&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;huawei&#39;,&#39;华为科技有限公司&#39;,&#39;华为小店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;0&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;itcast&#39;,&#39;传智播客教育科技有限公司&#39;,&#39;传智播客&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;1&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;itheima&#39;,&#39;黑马程序员&#39;,&#39;黑马程序员&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;0&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;luoji&#39;,&#39;罗技科技有限公司&#39;,&#39;罗技小店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;1&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;oppo&#39;,&#39;OPPO科技有限公司&#39;,&#39;OPPO官方旗舰店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;0&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;ourpalm&#39;,&#39;掌趣科技股份有限公司&#39;,&#39;掌趣小店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;1&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;qiandu&#39;,&#39;千度科技&#39;,&#39;千度小店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;2&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;sina&#39;,&#39;新浪科技有限公司&#39;,&#39;新浪官方旗舰店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;1&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;xiaomi&#39;,&#39;小米科技&#39;,&#39;小米官方旗舰店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;1&#39;,&#39;西安市&#39;,&#39;2088-01-01 12:00:00&#39;);insert into &#96;tb_seller&#96; (&#96;sellerid&#96;, &#96;name&#96;, &#96;nickname&#96;, &#96;password&#96;, &#96;status&#96;, &#96;address&#96;, &#96;createtime&#96;) values(&#39;yijia&#39;,&#39;宜家家居&#39;,&#39;宜家家居旗舰店&#39;,&#39;e10adc3949ba59abbe56e057f20f883e&#39;,&#39;1&#39;,&#39;北京市&#39;,&#39;2088-01-01 12:00:00&#39;);create index idx_seller_name_sta_addr on tb_seller(name,status,address); 4.2.2 避免索引失效1). 全值匹配 ，对索引中所有列都指定具体值。 改情况下，索引生效，执行效率高。 1explain select * from tb_seller where name&#x3D;&#39;小米科技&#39; and status&#x3D;&#39;1&#39; and address&#x3D;&#39;北京市&#39;\\G; 2). 最左前缀法则 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始，并且不跳过索引中的列。 匹配最左前缀法则，走索引： 违法最左前缀法则 ， 索引失效： 如果符合最左法则，但是出现跳跃某一列，只有最左列索引生效： 3). 范围查询右边的列，不能使用索引 。 根据前面的两个字段name ， status 查询是走索引的， 但是最后一个条件address 没有用到索引。 4). 不要在索引列上进行运算操作， 索引将失效。 5). 字符串不加单引号，造成索引失效。 由于，在查询是，没有对字符串加单引号，MySQL的查询优化器，会自动的进行类型转换，造成索引失效。 6). 尽量使用覆盖索引，避免select * 尽量使用覆盖索引（只访问索引的查询（索引列完全包含查询列）），减少select * 。 如果查询列，超出索引列，也会降低性能。 123456789TIP : using index ：使用覆盖索引的时候就会出现 using where：在查找使用索引的情况下，需要回表去查询所需的数据 using index condition：查找使用了索引，但是需要回表查询数据 using index ; using where：查找使用了索引，但是需要的数据都在索引列中能找到，所以不需要回表查询数据 7). 用or分割开的条件 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到。 示例，name字段是索引列 ， 而createtime不是索引列，中间是or进行连接是不走索引的 ： 1explain select * from tb_seller where name&#x3D;&#39;黑马程序员&#39; or createtime &#x3D; &#39;2088-01-01 12:00:00&#39;\\G; 8). 以%开头的Like模糊查询，索引失效。 如果仅仅是尾部模糊匹配，索引不会失效。如果是头部模糊匹配，索引失效。 解决方案 ： 通过覆盖索引来解决 9). 如果MySQL评估使用索引比全表更慢，则不使用索引。 10). is NULL ， is NOT NULL 有时索引失效。 11). in 走索引， not in 索引失效。 12). 单列索引和复合索引。 尽量使用复合索引，而少使用单列索引 。 创建复合索引 123456create index idx_name_sta_address on tb_seller(name, status, address);就相当于创建了三个索引 ： name name + status name + status + address 创建单列索引 123create index idx_seller_name on tb_seller(name);create index idx_seller_status on tb_seller(status);create index idx_seller_address on tb_seller(address); 数据库会选择一个最优的索引（辨识度最高索引）来使用，并不会使用全部索引 。 4.3 查看索引使用情况123show status like &#39;Handler_read%&#39;; show global status like &#39;Handler_read%&#39;; 1234567891011Handler_read_first：索引中第一条被读的次数。如果较高，表示服务器正执行大量全索引扫描（这个值越低越好）。Handler_read_key：如果索引正在工作，这个值代表一个行被索引值读的次数，如果值越低，表示索引得到的性能改善不高，因为索引不经常使用（这个值越高越好）。Handler_read_next ：按照键顺序读下一行的请求数。如果你用范围约束或如果执行索引扫描来查询索引列，该值增加。Handler_read_prev：按照键顺序读前一行的请求数。该读方法主要用于优化ORDER BY ... DESC。Handler_read_rnd ：根据固定位置读一行的请求数。如果你正执行大量查询并需要对结果进行排序该值较高。你可能使用了大量需要MySQL扫描整个表的查询或你的连接没有正确使用键。这个值较高，意味着运行效率低，应该建立索引来补救。Handler_read_rnd_next：在数据文件中读下一行的请求数。如果你正进行大量的表扫描，该值较高。通常说明你的表索引不正确或写入的查询没有利用索引。 5. SQL优化5.1 大批量插入数据环境准备 ： 12345678910111213141516CREATE TABLE &#96;tb_user_2&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;username&#96; varchar(45) NOT NULL, &#96;password&#96; varchar(96) NOT NULL, &#96;name&#96; varchar(45) NOT NULL, &#96;birthday&#96; datetime DEFAULT NULL, &#96;sex&#96; char(1) DEFAULT NULL, &#96;email&#96; varchar(45) DEFAULT NULL, &#96;phone&#96; varchar(45) DEFAULT NULL, &#96;qq&#96; varchar(32) DEFAULT NULL, &#96;status&#96; varchar(32) NOT NULL COMMENT &#39;用户状态&#39;, &#96;create_time&#96; datetime NOT NULL, &#96;update_time&#96; datetime DEFAULT NULL, PRIMARY KEY (&#96;id&#96;), UNIQUE KEY &#96;unique_user_username&#96; (&#96;username&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 ; 当使用load 命令导入数据的时候，适当的设置可以提高导入的效率。 对于 InnoDB 类型的表，有以下几种方式可以提高导入的效率： 1） 主键顺序插入 因为InnoDB类型的表是按照主键的顺序保存的，所以将导入的数据按照主键的顺序排列，可以有效的提高导入数据的效率。如果InnoDB表没有主键，那么系统会自动默认创建一个内部列作为主键，所以如果可以给表创建一个主键，将可以利用这点，来提高导入数据的效率。 123脚本文件介绍 : sql1.log ----&gt; 主键有序 sql2.log ----&gt; 主键无序 插入ID顺序排列数据： 插入ID无序排列数据： 2） 关闭唯一性校验 在导入数据前执行 SET UNIQUE_CHECKS=0，关闭唯一性校验，在导入结束后执行SET UNIQUE_CHECKS=1，恢复唯一性校验，可以提高导入的效率。 3） 手动提交事务 如果应用使用自动提交的方式，建议在导入前执行 SET AUTOCOMMIT=0，关闭自动提交，导入结束后再执行 SET AUTOCOMMIT=1，打开自动提交，也可以提高导入的效率。 5.2 优化insert语句当进行数据的insert操作的时候，可以考虑采用以下几种优化方案。 如果需要同时对一张表插入很多行数据时，应该尽量使用多个值表的insert语句，这种方式将大大的缩减客户端与数据库之间的连接、关闭等消耗。使得效率比分开执行的单个insert语句快。 示例， 原始方式为： 123insert into tb_test values(1,&#39;Tom&#39;);insert into tb_test values(2,&#39;Cat&#39;);insert into tb_test values(3,&#39;Jerry&#39;); 优化后的方案为 ： 1insert into tb_test values(1,&#39;Tom&#39;),(2,&#39;Cat&#39;)，(3,&#39;Jerry&#39;); 在事务中进行数据插入。 12345start transaction;insert into tb_test values(1,&#39;Tom&#39;);insert into tb_test values(2,&#39;Cat&#39;);insert into tb_test values(3,&#39;Jerry&#39;);commit; 数据有序插入 12345insert into tb_test values(4,&#39;Tim&#39;);insert into tb_test values(1,&#39;Tom&#39;);insert into tb_test values(3,&#39;Jerry&#39;);insert into tb_test values(5,&#39;Rose&#39;);insert into tb_test values(2,&#39;Cat&#39;); 优化后 12345insert into tb_test values(1,&#39;Tom&#39;);insert into tb_test values(2,&#39;Cat&#39;);insert into tb_test values(3,&#39;Jerry&#39;);insert into tb_test values(4,&#39;Tim&#39;);insert into tb_test values(5,&#39;Rose&#39;); 5.3 优化order by语句5.3.1 环境准备12345678910111213141516171819202122CREATE TABLE &#96;emp&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;name&#96; varchar(100) NOT NULL, &#96;age&#96; int(3) NOT NULL, &#96;salary&#96; int(11) DEFAULT NULL, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4;insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;1&#39;,&#39;Tom&#39;,&#39;25&#39;,&#39;2300&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;2&#39;,&#39;Jerry&#39;,&#39;30&#39;,&#39;3500&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;3&#39;,&#39;Luci&#39;,&#39;25&#39;,&#39;2800&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;4&#39;,&#39;Jay&#39;,&#39;36&#39;,&#39;3500&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;5&#39;,&#39;Tom2&#39;,&#39;21&#39;,&#39;2200&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;6&#39;,&#39;Jerry2&#39;,&#39;31&#39;,&#39;3300&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;7&#39;,&#39;Luci2&#39;,&#39;26&#39;,&#39;2700&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;8&#39;,&#39;Jay2&#39;,&#39;33&#39;,&#39;3500&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;9&#39;,&#39;Tom3&#39;,&#39;23&#39;,&#39;2400&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;10&#39;,&#39;Jerry3&#39;,&#39;32&#39;,&#39;3100&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;11&#39;,&#39;Luci3&#39;,&#39;26&#39;,&#39;2900&#39;);insert into &#96;emp&#96; (&#96;id&#96;, &#96;name&#96;, &#96;age&#96;, &#96;salary&#96;) values(&#39;12&#39;,&#39;Jay3&#39;,&#39;37&#39;,&#39;4500&#39;);create index idx_emp_age_salary on emp(age,salary); 5.3.2 两种排序方式1). 第一种是通过对返回数据进行排序，也就是通常说的 filesort 排序，所有不是通过索引直接返回排序结果的排序都叫 FileSort 排序。 2). 第二种通过有序索引顺序扫描直接返回有序数据，这种情况即为 using index，不需要额外排序，操作效率高。 多字段排序 了解了MySQL的排序方式，优化目标就清晰了：尽量减少额外的排序，通过索引直接返回有序数据。where 条件和Order by 使用相同的索引，并且Order By 的顺序和索引顺序相同， 并且Order by 的字段都是升序，或者都是降序。否则肯定需要额外的操作，这样就会出现FileSort。 5.3.3 Filesort 的优化通过创建合适的索引，能够减少 Filesort 的出现，但是在某些情况下，条件限制不能让Filesort消失，那就需要加快 Filesort的排序操作。对于Filesort ， MySQL 有两种排序算法： 1） 两次扫描算法 ：MySQL4.1 之前，使用该方式排序。首先根据条件取出排序字段和行指针信息，然后在排序区 sort buffer 中排序，如果sort buffer不够，则在临时表 temporary table 中存储排序结果。完成排序之后，再根据行指针回表读取记录，该操作可能会导致大量随机I/O操作。 2）一次扫描算法：一次性取出满足条件的所有字段，然后在排序区 sort buffer 中排序后直接输出结果集。排序时内存开销较大，但是排序效率比两次扫描算法要高。 MySQL 通过比较系统变量 max_length_for_sort_data 的大小和Query语句取出的字段总大小， 来判定是否那种排序算法，如果max_length_for_sort_data 更大，那么使用第二种优化之后的算法；否则使用第一种。 可以适当提高 sort_buffer_size 和 max_length_for_sort_data 系统变量，来增大排序区的大小，提高排序的效率。 5.4 优化group by 语句由于GROUP BY 实际上也同样会进行排序操作，而且与ORDER BY 相比，GROUP BY 主要只是多了排序之后的分组操作。当然，如果在分组的时候还使用了其他的一些聚合函数，那么还需要一些聚合函数的计算。所以，在GROUP BY 的实现过程中，与 ORDER BY 一样也可以利用到索引。 如果查询包含 group by 但是用户想要避免排序结果的消耗， 则可以执行order by null 禁止排序。如下 ： 123drop index idx_emp_age_salary on emp;explain select age,count(*) from emp group by age; 优化后 1explain select age,count(*) from emp group by age order by null; 从上面的例子可以看出，第一个SQL语句需要进行”filesort”，而第二个SQL由于order by null 不需要进行 “filesort”， 而上文提过Filesort往往非常耗费时间。 创建索引 ： 1create index idx_emp_age_salary on emp(age,salary)； 5.5 优化嵌套查询Mysql4.1版本之后，开始支持SQL的子查询。这个技术可以使用SELECT语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的SQL操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询是可以被更高效的连接（JOIN）替代。 示例 ，查找有角色的所有的用户信息 : 1explain select * from t_user where id in (select user_id from user_role ); 执行计划为 : 优化后 : 1explain select * from t_user u , user_role ur where u.id &#x3D; ur.user_id; 连接(Join)查询之所以更有效率一些 ，是因为MySQL不需要在内存中创建临时表来完成这个逻辑上需要两个步骤的查询工作。 5.6 优化OR条件对于包含OR的查询子句，如果要利用索引，则OR之间的每个条件列都必须用到索引 ， 而且不能使用到复合索引； 如果没有索引，则应该考虑增加索引。 获取 emp 表中的所有的索引 ： 示例 ： 1explain select * from emp where id &#x3D; 1 or age &#x3D; 30; 建议使用 union 替换 or ： 我们来比较下重要指标，发现主要差别是 type 和 ref 这两项 type 显示的是访问类型，是较为重要的一个指标，结果值从好到坏依次是： 1system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL UNION 语句的 type 值为 ref，OR 语句的 type 值为 range，可以看到这是一个很明显的差距 UNION 语句的 ref 值为 const，OR 语句的 type 值为 null，const 表示是常量值引用，非常快 这两项的差距就说明了 UNION 要优于 OR 。 5.7 优化分页查询一般分页查询时，通过创建覆盖索引能够比较好地提高性能。一个常见又非常头疼的问题就是 limit 2000000,10 ，此时需要MySQL排序前2000010 记录，仅仅返回2000000 - 2000010 的记录，其他记录丢弃，查询排序的代价非常大 。 5.7.1 优化思路一在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。 5.7.2 优化思路二该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询 。 5.8 使用SQL提示SQL提示，是优化数据库的一个重要手段，简单来说，就是在SQL语句中加入一些人为的提示来达到优化操作的目的。 5.8.1 USE INDEX在查询语句中表名的后面，添加 use index 来提供希望MySQL去参考的索引列表，就可以让MySQL不再考虑其他可用的索引。 1create index idx_seller_name on tb_seller(name); 5.8.2 IGNORE INDEX如果用户只是单纯的想让MySQL忽略一个或者多个索引，则可以使用 ignore index 作为 hint 。 1explain select * from tb_seller ignore index(idx_seller_name) where name &#x3D; &#39;小米科技&#39;; 5.8.3 FORCE INDEX为强制MySQL使用一个特定的索引，可在查询中使用 force index 作为hint 。 1create index idx_seller_address on tb_seller(address);","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wgy1993.gitee.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://wgy1993.gitee.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MySQL高级教程(一)","date":"2020-05-18T03:20:04.000Z","path":"archives/cd2a02be.html","text":"1. 索引1.1 索引概述MySQL官方对索引的定义为：索引（index）是帮助MySQL高效获取数据的数据结构（有序）。在数据之外，数据库系统还维护者满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据， 这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。 左边是数据表，一共有两列七条记录，最左边的是数据记录的物理地址（注意逻辑上相邻的记录在磁盘上也并不是一定物理相邻的）。为了加快Col2的查找，可以维护一个右边所示的二叉查找树，每个节点分别包含索引键值和一个指向对应数据记录物理地址的指针，这样就可以运用二叉查找快速获取到相应数据。 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。索引是数据库中用来提高性能的最常用的工具。 1.2 索引优势劣势优势： 类似于书籍的目录索引，提高数据检索的效率，降低数据库的IO成本。 通过索引列对数据进行排序，降低数据排序的成本，降低CPU的消耗。 劣势： 实际上索引也是一张表，该表中保存了主键与索引字段，并指向实体类的记录，所以索引列也是要占用空间的。 虽然索引大大提高了查询效率，同时却也降低更新表的速度，如对表进行INSERT、UPDATE、DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存一下索引文件每次更新添加了索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。 1.3 索引结构索引是在MySQL的存储引擎层中实现的，而不是在服务器层实现的。所以每种存储引擎的索引都不一定完全相同，也不是所有的存储引擎都支持所有的索引类型的。MySQL目前提供了以下4种索引： BTREE 索引 ： 最常见的索引类型，大部分索引都支持 B 树索引。 HASH 索引：只有Memory引擎支持 ， 使用场景简单 。 R-tree 索引（空间索引）：空间索引是MyISAM引擎的一个特殊索引类型，主要用于地理空间数据类型，通常使用较少，不做特别介绍。 Full-text （全文索引） ：全文索引也是MyISAM的一个特殊索引类型，主要用于全文索引，InnoDB从Mysql5.6版本开始支持全文索引。 MyISAM、InnoDB、Memory三种存储引擎对各种索引类型的支持 索引 InnoDB引擎 MyISAM引擎 Memory引擎 BTREE索引 支持 支持 支持 HASH 索引 不支持 不支持 支持 R-tree 索引 不支持 支持 不支持 Full-text 5.6版本之后支持 支持 不支持 我们平常所说的索引，如果没有特别指明，都是指B+树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引、复合索引、前缀索引、唯一索引默认都是使用 B+tree 索引，统称为索引。 1.3.1 BTREE 结构BTree又叫多路平衡搜索树，一颗m叉的BTree特性如下： 树中每个节点最多包含m个孩子。 除根节点与叶子节点外，每个节点至少有[ceil(m/2)]个孩子。 若根节点不是叶子节点，则至少有两个孩子。 所有的叶子节点都在同一层。 每个非叶子节点由n个key与n+1个指针组成，其中[ceil(m/2)-1] &lt;= n &lt;= m-1 以5叉BTree为例，key的数量：公式推导[ceil(m/2)-1] &lt;= n &lt;= m-1。所以 2 &lt;= n &lt;=4 。当n&gt;4时，中间节点分裂到父节点，两边节点分裂。 插入 C N G A H E K Q M F W L T Z D P R X Y S 数据为例。 演变过程如下： 1). 插入前4个字母 C N G A 2). 插入H，n&gt;4，中间元素G字母向上分裂到新的节点 3). 插入E，K，Q不需要分裂 4). 插入M，中间元素M字母向上分裂到父节点G 5). 插入F，W，L，T不需要分裂 6). 插入Z，中间元素T向上分裂到父节点中 7). 插入D，中间元素D向上分裂到父节点中。然后插入P，R，X，Y不需要分裂 8). 最后插入S，NPQR节点n&gt;5，中间节点Q向上分裂，但分裂后父节点DGMT的n&gt;5，中间节点M向上分裂 到此，该BTREE树就已经构建完成了， BTREE树 和 二叉树 相比， 查询数据的效率更高， 因为对于相同的数据量来说，BTREE的层级结构比二叉树小，因此搜索速度快。 1.3.2 B+TREE 结构B+Tree为BTree的变种，B+Tree与BTree的区别为： n叉B+Tree最多含有n个key，而BTree最多含有n-1个key。 B+Tree的叶子节点保存所有的key信息，依key大小顺序排列。 所有的非叶子节点都可以看作是key的索引部分。 由于B+Tree只有叶子节点保存key信息，查询任何key都要从root走到叶子。所以B+Tree的查询效率更加稳定。 1.3.3 MySQL中的B+TreeMySql索引数据结构对经典的B+Tree进行了优化。在原B+Tree的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的B+Tree，提高区间访问的性能。 MySQL中的 B+Tree 索引结构示意图: 1.4 索引分类 单值索引 ：即一个索引只包含单个列，一个表可以有多个单列索引 唯一索引 ：索引列的值必须唯一，但允许有空值 复合索引 ：即一个索引包含多个列 1.5 索引语法索引在创建表的时候，可以同时创建， 也可以随时增加新的索引。 准备环境: 123456789101112131415161718192021222324252627create database demo_01 default charset&#x3D;utf8mb4;use demo_01;CREATE TABLE &#96;city&#96; ( &#96;city_id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;city_name&#96; varchar(50) NOT NULL, &#96;country_id&#96; int(11) NOT NULL, PRIMARY KEY (&#96;city_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;CREATE TABLE &#96;country&#96; ( &#96;country_id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;country_name&#96; varchar(100) NOT NULL, PRIMARY KEY (&#96;country_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8;insert into &#96;city&#96; (&#96;city_id&#96;, &#96;city_name&#96;, &#96;country_id&#96;) values(1,&#39;西安&#39;,1);insert into &#96;city&#96; (&#96;city_id&#96;, &#96;city_name&#96;, &#96;country_id&#96;) values(2,&#39;NewYork&#39;,2);insert into &#96;city&#96; (&#96;city_id&#96;, &#96;city_name&#96;, &#96;country_id&#96;) values(3,&#39;北京&#39;,1);insert into &#96;city&#96; (&#96;city_id&#96;, &#96;city_name&#96;, &#96;country_id&#96;) values(4,&#39;上海&#39;,1);insert into &#96;country&#96; (&#96;country_id&#96;, &#96;country_name&#96;) values(1,&#39;China&#39;);insert into &#96;country&#96; (&#96;country_id&#96;, &#96;country_name&#96;) values(2,&#39;America&#39;);insert into &#96;country&#96; (&#96;country_id&#96;, &#96;country_name&#96;) values(3,&#39;Japan&#39;);insert into &#96;country&#96; (&#96;country_id&#96;, &#96;country_name&#96;) values(4,&#39;UK&#39;); 1.5.1 创建索引语法 ： 12345CREATE [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name [USING index_type]ON tbl_name(index_col_name,...)index_col_name : column_name[(length)][ASC | DESC] 示例 ： 为city表中的city_name字段创建索引 ； 1.5.2 查看索引语法： 1show index from table_name; 示例： 查看city表中的索引信息； 1.5.3 删除索引语法 ： 1DROP INDEX index_name ON tbl_name; 示例 ： 想要删除city表上的索引idx_city_name，可以操作如下： 1.5.4 ALTER命令1234567891011121314151). alter table tb_name add primary key(column_list); 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL 2). alter table tb_name add unique index_name(column_list); 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次） 3). alter table tb_name add index index_name(column_list); 添加普通索引， 索引值可以出现多次。 4). alter table tb_name add fulltext index_name(column_list); 该语句指定了索引为FULLTEXT， 用于全文索引 1.6 索引设计原则索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率，更高效的使用索引。 对查询频次较高，且数据量比较大的表建立索引。 索引字段的选择，最佳候选列应当从where子句的条件中提取，如果where子句中的组合比较多，那么应当挑选最常用、过滤效果最好的列的组合。 使用唯一索引，区分度越高，使用索引的效率越高。 索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价自然也就水涨船高。对于插入、更新、删除等DML操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低DML操作的效率，增加相应操作的时间消耗。另外索引过多的话，MySQL也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但无疑提高了选择的代价。 使用短索引，索引创建之后也是使用硬盘来存储的，因此提升索引访问的I/O效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升MySQL访问索引的I/O效率。 利用最左前缀，N个列组合而成的组合索引，那么相当于是创建了N个索引，如果查询时where子句中使用了组成该索引的前几个字段，那么这条查询SQL可以利用组合索引来提升查询效率。 12345678创建复合索引: CREATE INDEX idx_name_email_status ON tb_seller(NAME,email,STATUS);就相当于 对name 创建索引 ; 对name , email 创建了索引 ; 对name , email, status 创建了索引 ; 2. 视图2.1 视图概述视图（View）是一种虚拟存在的表。视图并不在数据库中实际存在，行和列数据来自定义视图的查询中使用的表，并且是在使用视图时动态生成的。通俗的讲，视图就是一条SELECT语句执行后返回的结果集。所以我们在创建视图的时候，主要的工作就落在创建这条SQL查询语句上。 视图相对于普通的表的优势主要包括以下几项。 简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。 数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。 2.2 创建或者修改视图创建视图的语法为： 1234567CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;]VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION] 修改视图的语法为： 1234567ALTER [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;]VIEW view_name [(column_list)]AS select_statement[WITH [CASCADED | LOCAL] CHECK OPTION] 12345选项 : WITH [CASCADED | LOCAL] CHECK OPTION 决定了是否允许更新数据使记录不再满足视图的条件。 LOCAL ： 只要满足本视图的条件就可以更新。 CASCADED ： 必须满足所有针对该视图的所有视图的条件才可以更新。 默认值. 示例： 创建city_country_view视图 , 执行如下SQL : 123create or replace view city_country_view as select t.*,c.country_name from country c , city t where c.country_id &#x3D; t.country_id; 查询视图 : 2.3 查看视图从 MySQL 5.1 版本开始，使用 SHOW TABLES 命令的时候不仅显示表的名字，同时也会显示视图的名字，而不存在单独显示视图的 SHOW VIEWS 命令。 同样，在使用 SHOW TABLE STATUS 命令的时候，不但可以显示表的信息，同时也可以显示视图的信息。 如果需要查询某个视图的定义，可以使用 SHOW CREATE VIEW 命令进行查看 ： 2.4 删除视图语法 : 1DROP VIEW [IF EXISTS] view_name [, view_name] ...[RESTRICT | CASCADE] 示例： 删除视图city_country_view : 1DROP VIEW city_country_view; 3. 存储过程和函数3.1 存储过程和函数概述存储过程和函数是事先经过编译并存储在数据库中的一段 SQL 语句的集合，调用存储过程和函数可以简化应用开发人员的很多工作，减少数据在数据库和应用服务器之间的传输，对于提高数据处理的效率是有好处的。 存储过程和函数的区别在于函数必须有返回值，而存储过程没有。 函数 ： 是一个有返回值的过程 ； 过程 ： 是一个没有返回值的函数 ； 3.2 创建存储过程1234CREATE PROCEDURE procedure_name ([proc_parameter[,...]])begin -- SQL语句end; 示例 ： 12345678delimiter $create procedure pro_test1()begin select &#39;Hello Mysql&#39; ;end$delimiter ; 知识小贴士 DELIMITER 该关键字用来声明SQL语句的分隔符 , 告诉 MySQL 解释器，该段命令是否已经结束了，mysql是否可以执行了。默认情况下，delimiter是分号;。在命令行客户端中，如果有一行命令以分号结束，那么回车后，mysql将会执行该命令。 3.3 调用存储过程1call procedure_name() ; 3.4 查看存储过程12345678-- 查询db_name数据库中的所有的存储过程select name from mysql.proc where db&#x3D;&#39;db_name&#39;;-- 查询存储过程的状态信息show procedure status;-- 查询某个存储过程的定义show create procedure test.pro_test1 \\G; 3.5 删除存储过程1DROP PROCEDURE [IF EXISTS] sp_name ; 3.6 语法存储过程是可以编程的，意味着可以使用变量，表达式，控制结构 ， 来完成比较复杂的功能。 3.6.1 变量 DECLARE 通过 DECLARE 可以定义一个局部变量，该变量的作用范围只能在 BEGIN…END 块中。 1DECLARE var_name[,...] type [DEFAULT value] 示例 : 123456789delimiter $create procedure pro_test2() begin declare num int default 5; select num + 10; end$delimiter ; SET 直接赋值使用 SET，可以赋常量或者赋表达式，具体语法如下： 1SET var_name &#x3D; expr [, var_name &#x3D; expr] ... 示例 : 12345678910DELIMITER $ CREATE PROCEDURE pro_test3()BEGIN DECLARE NAME VARCHAR(20); SET NAME &#x3D; &#39;MYSQL&#39;; SELECT NAME ;END$ DELIMITER ; 也可以通过select … into 方式进行赋值操作 : 12345678910DELIMITER $CREATE PROCEDURE pro_test5()BEGIN declare countnum int; select count(*) into countnum from city; select countnum;END$DELIMITER ; 3.6.2 if条件判断语法结构 : 1234567if search_condition then statement_list [elseif search_condition then statement_list] ... [else statement_list] end if; 需求： 1234567根据定义的身高变量，判定当前身高的所属的身材类型 180 及以上 ----------&gt; 身材高挑 170 - 180 ---------&gt; 标准身材 170 以下 ----------&gt; 一般身材 示例 : 12345678910111213141516171819delimiter $create procedure pro_test6()begin declare height int default 175; declare description varchar(50); if height &gt;&#x3D; 180 then set description &#x3D; &#39;身材高挑&#39;; elseif height &gt;&#x3D; 170 and height &lt; 180 then set description &#x3D; &#39;中等身材&#39;; else set description &#x3D; &#39;一般身材&#39;; end if; select description ;end$delimiter ; 调用结果为 : 3.6.3 传递参数语法格式 : 1234567create procedure procedure_name([in&#x2F;out&#x2F;inout] 参数名 参数类型)...IN : 该参数可以作为输入，也就是需要调用方传入值 , 默认OUT: 该参数作为输出，也就是该参数可以作为返回值INOUT: 既可以作为输入参数，也可以作为输出参数 3.6.3.1 IN - 输入需求 : 1根据定义的身高变量，判定当前身高的所属的身材类型 示例 : 12345678910111213141516delimiter $create procedure pro_test5(in height int)begin declare description varchar(50) default &#39;&#39;; if height &gt;&#x3D; 180 then set description&#x3D;&#39;身材高挑&#39;; elseif height &gt;&#x3D; 170 and height &lt; 180 then set description&#x3D;&#39;标准身材&#39;; else set description&#x3D;&#39;一般身材&#39;; end if; select concat(&#39;身高&#39;, height , &#39;对应的身材类型为:&#39;,description);end$delimiter ; 3.6.3.2 OUT-输出 需求 : 1根据传入的身高变量，获取当前身高的所属的身材类型 示例: 12345678910create procedure pro_test5(in height int , out description varchar(100))begin if height &gt;&#x3D; 180 then set description&#x3D;&#39;身材高挑&#39;; elseif height &gt;&#x3D; 170 and height &lt; 180 then set description&#x3D;&#39;标准身材&#39;; else set description&#x3D;&#39;一般身材&#39;; end if;end$ 调用: 123call pro_test5(168, @description)$select @description$ 小知识 @description : 这种变量要在变量名称前面加上“@”符号，叫做用户会话变量，代表整个会话过程他都是有作用的，这个类似于全局变量一样。 @@global.sort_buffer_size : 这种在变量前加上 “@@” 符号, 叫做系统变量 3.6.4 case结构语法结构 : 1234567891011121314151617181920212223方式一 : CASE case_value WHEN when_value THEN statement_list [WHEN when_value THEN statement_list] ... [ELSE statement_list] END CASE;方式二 : CASE WHEN search_condition THEN statement_list [WHEN search_condition THEN statement_list] ... [ELSE statement_list] END CASE; 需求: 1给定一个月份, 然后计算出所在的季度 示例 : 123456789101112131415161718192021delimiter $create procedure pro_test9(month int)begin declare result varchar(20); case when month &gt;&#x3D; 1 and month &lt;&#x3D;3 then set result &#x3D; &#39;第一季度&#39;; when month &gt;&#x3D; 4 and month &lt;&#x3D;6 then set result &#x3D; &#39;第二季度&#39;; when month &gt;&#x3D; 7 and month &lt;&#x3D;9 then set result &#x3D; &#39;第三季度&#39;; when month &gt;&#x3D; 10 and month &lt;&#x3D;12 then set result &#x3D; &#39;第四季度&#39;; end case; select concat(&#39;您输入的月份为 :&#39;, month , &#39; , 该月份为 : &#39; , result) as content ; end$delimiter ; 3.6.5 while循环语法结构: 12345while search_condition do statement_list end while; 需求: 1计算从1加到n的值 示例 : 1234567891011121314delimiter $create procedure pro_test8(n int)begin declare total int default 0; declare num int default 1; while num&lt;&#x3D;n do set total &#x3D; total + num; set num &#x3D; num + 1; end while; select total;end$delimiter ; 3.6.6 repeat结构有条件的循环控制语句, 当满足条件的时候退出循环 。while 是满足条件才执行，repeat 是满足条件就退出循环。 语法结构 : 1234567REPEAT statement_list UNTIL search_conditionEND REPEAT; 需求: 1计算从1加到n的值 示例 : 1234567891011121314151617delimiter $create procedure pro_test10(n int)begin declare total int default 0; repeat set total &#x3D; total + n; set n &#x3D; n - 1; until n&#x3D;0 end repeat; select total ; end$delimiter ; 3.6.7 loop语句LOOP 实现简单的循环，退出循环的条件需要使用其他的语句定义，通常可以使用 LEAVE 语句实现，具体语法如下： 12345[begin_label:] LOOP statement_listEND LOOP [end_label] 如果不在 statement_list 中增加退出循环的语句，那么 LOOP 语句可以用来实现简单的死循环。 3.6.8 leave语句用来从标注的流程构造中退出，通常和 BEGIN … END 或者循环一起使用。下面是一个使用 LOOP 和 LEAVE 的简单例子 , 退出循环： 123456789101112131415161718192021delimiter $CREATE PROCEDURE pro_test11(n int)BEGIN declare total int default 0; ins: LOOP IF n &lt;&#x3D; 0 then leave ins; END IF; set total &#x3D; total + n; set n &#x3D; n - 1; END LOOP ins; select total;END$delimiter ; 3.6.9 游标/光标游标是用来存储查询结果集的数据类型 , 在存储过程和函数中可以使用光标对结果集进行循环的处理。光标的使用包括光标的声明、OPEN、FETCH 和 CLOSE，其语法分别如下。 声明光标： 1DECLARE cursor_name CURSOR FOR select_statement ; OPEN 光标： 1OPEN cursor_name ; FETCH 光标： 1FETCH cursor_name INTO var_name [, var_name] ... CLOSE 光标： 1CLOSE cursor_name ; 示例 : 初始化脚本: 123456789create table emp( id int(11) not null auto_increment , name varchar(50) not null comment &#39;姓名&#39;, age int(11) comment &#39;年龄&#39;, salary int(11) comment &#39;薪水&#39;, primary key(&#96;id&#96;))engine&#x3D;innodb default charset&#x3D;utf8 ;insert into emp(id,name,age,salary) values(null,&#39;金毛狮王&#39;,55,3800),(null,&#39;白眉鹰王&#39;,60,4000),(null,&#39;青翼蝠王&#39;,38,2800),(null,&#39;紫衫龙王&#39;,42,1800); 12345678910111213141516171819202122232425262728-- 查询emp表中数据, 并逐行获取进行展示create procedure pro_test11()begin declare e_id int(11); declare e_name varchar(50); declare e_age int(11); declare e_salary int(11); declare emp_result cursor for select * from emp; open emp_result; fetch emp_result into e_id,e_name,e_age,e_salary; select concat(&#39;id&#x3D;&#39;,e_id , &#39;, name&#x3D;&#39;,e_name, &#39;, age&#x3D;&#39;, e_age, &#39;, 薪资为: &#39;,e_salary); fetch emp_result into e_id,e_name,e_age,e_salary; select concat(&#39;id&#x3D;&#39;,e_id , &#39;, name&#x3D;&#39;,e_name, &#39;, age&#x3D;&#39;, e_age, &#39;, 薪资为: &#39;,e_salary); fetch emp_result into e_id,e_name,e_age,e_salary; select concat(&#39;id&#x3D;&#39;,e_id , &#39;, name&#x3D;&#39;,e_name, &#39;, age&#x3D;&#39;, e_age, &#39;, 薪资为: &#39;,e_salary); fetch emp_result into e_id,e_name,e_age,e_salary; select concat(&#39;id&#x3D;&#39;,e_id , &#39;, name&#x3D;&#39;,e_name, &#39;, age&#x3D;&#39;, e_age, &#39;, 薪资为: &#39;,e_salary); fetch emp_result into e_id,e_name,e_age,e_salary; select concat(&#39;id&#x3D;&#39;,e_id , &#39;, name&#x3D;&#39;,e_name, &#39;, age&#x3D;&#39;, e_age, &#39;, 薪资为: &#39;,e_salary); close emp_result;end$ 通过循环结构 , 获取游标中的数据 : 12345678910111213141516171819202122232425DELIMITER $create procedure pro_test12()begin DECLARE id int(11); DECLARE name varchar(50); DECLARE age int(11); DECLARE salary int(11); DECLARE has_data int default 1; DECLARE emp_result CURSOR FOR select * from emp; DECLARE EXIT HANDLER FOR NOT FOUND set has_data &#x3D; 0; open emp_result; repeat fetch emp_result into id , name , age , salary; select concat(&#39;id为&#39;,id, &#39;, name 为&#39; ,name , &#39;, age为 &#39; ,age , &#39;, 薪水为: &#39;, salary); until has_data &#x3D; 0 end repeat; close emp_result;end$DELIMITER ; 3.7 存储函数语法结构: 12345CREATE FUNCTION function_name([param type ... ]) RETURNS type BEGIN ...END; 案例 : 定义一个存储过程, 请求满足条件的总记录数 ; 12345678910111213delimiter $create function count_city(countryId int)returns intbegin declare cnum int ; select count(*) into cnum from city where country_id &#x3D; countryId; return cnum;end$delimiter ; 调用: 123select count_city(1);select count_city(2); 4. 触发器4.1 介绍触发器是与表有关的数据库对象，指在 insert/update/delete 之前或之后，触发并执行触发器中定义的SQL语句集合。触发器的这种特性可以协助应用在数据库端确保数据的完整性 , 日志记录 , 数据校验等操作 。 使用别名 OLD 和 NEW 来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。 触发器类型 NEW 和 OLD的使用 INSERT 型触发器 NEW 表示将要或者已经新增的数据 UPDATE 型触发器 OLD 表示修改之前的数据 , NEW 表示将要或已经修改后的数据 DELETE 型触发器 OLD 表示将要或者已经删除的数据 4.2 创建触发器语法结构 : 12345678910111213create trigger trigger_name before&#x2F;after insert&#x2F;update&#x2F;deleteon tbl_name [ for each row ] -- 行级触发器begin trigger_stmt ;end; 示例 : 需求 1通过触发器记录 emp 表的数据变更日志 , 包含增加, 修改 , 删除 ; 首先创建一张日志表 : 12345678create table emp_logs( id int(11) not null auto_increment, operation varchar(20) not null comment &#39;操作类型, insert&#x2F;update&#x2F;delete&#39;, operate_time datetime not null comment &#39;操作时间&#39;, operate_id int(11) not null comment &#39;操作表的ID&#39;, operate_params varchar(500) comment &#39;操作参数&#39;, primary key(&#96;id&#96;))engine&#x3D;innodb default charset&#x3D;utf8; 创建 insert 型触发器，完成插入数据时的日志记录 : 1234567891011DELIMITER $create trigger emp_logs_insert_triggerafter insert on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,&#39;insert&#39;,now(),new.id,concat(&#39;插入后(id:&#39;,new.id,&#39;, name:&#39;,new.name,&#39;, age:&#39;,new.age,&#39;, salary:&#39;,new.salary,&#39;)&#39;)); end $DELIMITER ; 创建 update 型触发器，完成更新数据时的日志记录 : 1234567891011DELIMITER $create trigger emp_logs_update_triggerafter update on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,&#39;update&#39;,now(),new.id,concat(&#39;修改前(id:&#39;,old.id,&#39;, name:&#39;,old.name,&#39;, age:&#39;,old.age,&#39;, salary:&#39;,old.salary,&#39;) , 修改后(id&#39;,new.id, &#39;name:&#39;,new.name,&#39;, age:&#39;,new.age,&#39;, salary:&#39;,new.salary,&#39;)&#39;)); end $DELIMITER ; 创建delete 行的触发器 , 完成删除数据时的日志记录 : 1234567891011DELIMITER $create trigger emp_logs_delete_triggerafter delete on emp for each row begin insert into emp_logs (id,operation,operate_time,operate_id,operate_params) values(null,&#39;delete&#39;,now(),old.id,concat(&#39;删除前(id:&#39;,old.id,&#39;, name:&#39;,old.name,&#39;, age:&#39;,old.age,&#39;, salary:&#39;,old.salary,&#39;)&#39;)); end $DELIMITER ; 测试： 123456insert into emp(id,name,age,salary) values(null, &#39;光明左使&#39;,30,3500);insert into emp(id,name,age,salary) values(null, &#39;光明右使&#39;,33,3200);update emp set age &#x3D; 39 where id &#x3D; 3;delete from emp where id &#x3D; 5; 4.3 删除触发器语法结构 : 1drop trigger [schema_name.]trigger_name 如果没有指定 schema_name，默认为当前数据库 。 4.4 查看触发器可以通过执行 SHOW TRIGGERS 命令查看触发器的状态、语法等信息。 语法结构 ： 1show triggers;","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wgy1993.gitee.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://wgy1993.gitee.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"使用hexo+github搭建免费个人博客","date":"2020-05-15T11:40:37.000Z","path":"archives/52429b60.html","text":"前言使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的； 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行； 博客内容可以轻松打包、转移、发布到其它平台 准备工作在开始一切之前，你必须已经： 有一个github账号，没有的话去注册一个； 安装了node.js、npm，并了解相关基础知识； 安装了git for windows（或者其它git客户端） 搭建github博客创建仓库新建一个名为你的用户名.github.io的仓库，比如说，如果你的github用户名是test，那么你就新建test.github.io的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是 http://test.github.io 了，是不是很方便？ 由此可见，每一个github账户最多只能创建一个这样可以直接使用域名访问的仓库。 几个注意的地方： 注册的邮箱一定要验证，否则不会成功； 仓库名字必须是：username.github.io，其中username是你的用户名； 仓库创建成功不会立即生效，需要过一段时间，大概10-30分钟，或者更久，我的等了半个小时才生效； 创建成功后，默认会在你这个仓库里生成一些示例页面，以后你的网站所有代码都是放在这个仓库里啦。 配置SSH key为什么要配置这个呢？因为你提交代码肯定要拥有你的github权限才可以，但是直接使用用户名和密码太不安全了，所以我们使用ssh key来解决本地和服务器的连接问题。 1$ cd ~/. ssh #检查本机已存在的ssh密钥 如果提示：No such file or directory 说明你是第一次使用git。 1ssh-keygen -t rsa -C &quot;邮件地址&quot; 然后连续3次回车，最终会生成一个文件在用户目录下，打开用户目录，找到.ssh\\id_rsa.pub文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key： 将刚复制的内容粘贴到key那里，title随便填，保存。 测试是否成功1$ ssh -T git@github.com # 注意邮箱地址不用改 如果提示Are you sure you want to continue connecting (yes/no)?，输入yes，然后会看到： Hi wgy1993! You’ve successfully authenticated, but GitHub does not provide shell access. 看到这个信息说明SSH已配置成功！ 此时你还需要配置： 12$ git config --global user.name &quot;yourname&quot;&#x2F;&#x2F; 你的github用户名，非昵称$ git config --global user.email &quot;xxx@163.com&quot;&#x2F;&#x2F; 填写你的github注册邮箱 具体这个配置是干嘛的我没仔细深究。 使用hexo写博客hexo简介Hexo是一个简单、快速、强大的基于 Github Pages 的博客发布工具，支持Markdown格式，有众多优秀插件和主题。 官网： http://hexo.iogithub: https://github.com/hexojs/hexo 原理由于github pages存放的都是静态文件，博客存放的不只是文章内容，还有文章列表、分类、标签、翻页等动态内容，假如每次写完一篇文章都要手动更新博文目录和相关链接信息，相信谁都会疯掉，所以hexo所做的就是将这些md文件都放在本地，每次写完文章后调用写好的命令来批量完成相关页面的生成，然后再将有改动的页面提交到github。 注意事项安装之前先来说几个注意事项： 很多命令既可以用Windows的cmd来完成，也可以使用git bash来完成，但是部分命令会有一些问题，为避免不必要的问题，建议全部使用git bash来执行； hexo不同版本差别比较大，网上很多文章的配置信息都是基于2.x的，所以注意不要被误导； hexo有2种_config.yml文件，一个是根目录下的全局的_config.yml，一个是各个theme下的； 安装1$ npm install -g hexo 初始化在电脑的某个地方新建一个名为hexo的文件夹（名字可以随便取），比如我的是E:\\MyGitWorkSpace\\hexo，由于这个文件夹将来就作为你存放代码的地方，所以最好不要随便放。 12$ cd /e/MyGitWorkSpace/hexo/$ hexo init hexo会自动下载一些文件到这个目录，包括node_modules，目录结构如下图： 12$ hexo g # 生成$ hexo s # 启动服务 执行以上命令之后，hexo就会在public文件夹生成相关html文件，这些文件将来都是要提交到github去的： hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容，很多人会碰到浏览器一直在转圈但是就是加载不出来的问题，一般情况下是因为端口占用的缘故，因为4000这个端口太常见了 浏览目录 修改主题既然默认主题很丑，那我们别的不做，首先来替换一个好看点的主题。这是 官方主题。 个人比较喜欢的2个主题：hexo-theme-jekyll 和 hexo-theme-yilia。 首先下载这个主题： 12$ cd /f/Workspaces/hexo/$ git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 下载后的主题都在这里： 修改_config.yml中的theme: landscape改为theme: yilia，然后重新执行hexo g来重新生成。 如果出现一些莫名其妙的问题，可以先执行hexo clean来清理一下public的内容，然后再来重新生成和发布。 上传之前在上传代码到github之前，一定要记得先把你以前所有代码下载下来（虽然github有版本管理，但备份一下总是好的），因为从hexo提交代码时会把你以前的所有代码都删掉。 上传到github如果你一切都配置好了，发布上传很容易，一句hexo d就搞定，当然关键还是你要把所有东西配置好。 首先，ssh key肯定要配置好。 其次，配置_config.yml中有关deploy的部分： 正确写法： 1234deploy: type: git repository: git@github.com:wgy1993&#x2F;wgy1993.github.io.git branch: master 错误写法： 1234deploy: type: github repository: https:&#x2F;&#x2F;github.com&#x2F;wgy1993&#x2F;wgy1993.github.io.git branch: master 后面一种写法是hexo2.x的写法，现在已经不行了，无论是哪种写法，此时直接执行hexo d的话一般会报如下错误： 1Deployer not found: github 或者 Deployer not found: git 原因是还需要安装一个插件： 1npm install hexo-deployer-git --save 其它命令不确定，部署这个命令一定要用git bash，否则会提示Permission denied (publickey). 打开你的git bash，输入hexo d就会将本次有改动的代码全部提交，没有改动的不会： 保留README.md等文件提交之后网页上一看，发现以前其它代码都没了，此时不要慌，一些非md文件可以把他们放到source文件夹下，这里的所有文件都会原样复制（除了md文件）到public目录的： 由于hexo默认会把所有md文件都转换成html，包括README.md，所有需要每次生成之后、上传之前，手动将README.md复制到public目录，并删除README.html。 常用hexo命令常见命令 1234567hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&#39;ctrl + c&#39;关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 缩写： 1234hexo n &#x3D;&#x3D; hexo newhexo g &#x3D;&#x3D; hexo generatehexo s &#x3D;&#x3D; hexo serverhexo d &#x3D;&#x3D; hexo deploy 组合命令： 12hexo s -g #生成并本地预览hexo d -g #生成并上传 _config.yml这里面都是一些全局配置，每个参数的意思都比较简单明了，所以就不作详细介绍了。 需要特别注意的地方是，冒号后面必须有一个空格，否则可能会出问题。 写博客定位到我们的hexo根目录，执行命令： 1hexo new &#39;my-first-blog&#39; hexo会帮我们在_posts下生成相关md文件： 我们只需要打开这个文件就可以开始写博客了，默认生成如下内容： 当然你也可以直接自己新建md文件，用这个命令的好处是帮我们自动生成了时间。 一般完整格式如下： 123456789---title: postName #文章页面上的显示名称，一般是中文date: 2013-12-02 15:30:16 #文章生成时间，一般不改，当然也可以任意修改categories: 默认分类 #分类tags: [tag1,tag2,tag3] #文章标签，可空，多标签请用格式，注意:后面有个空格description: 附加一段文章摘要，字数最好在140字以内，会出现在meta的description里面---以下是正文 那么hexo new page &#39;postName&#39;命令和hexo new &#39;postName&#39;有什么区别呢？ 1hexo new page &quot;my-second-blog&quot; 生成如下： 最终部署时生成：hexo\\public\\my-second-blog\\index.html，但是它不会作为文章出现在博文目录。 如何让博文列表不显示全部内容默认情况下，生成的博文目录会显示全部的文章内容，如何设置文章摘要的长度呢？ 答案是在合适的位置加上``即可，例如： 12345678910111213# 前言使用github pages服务搭建博客的好处有：1. 全是静态文件，访问速度快；2. 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；3. 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；&lt;!--more--&gt;4. 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；5. 博客内容可以轻松打包、转移、发布到其它平台；6. 等等； 最终效果：","tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://wgy1993.gitee.io/tags/Hexo/"},{"name":"GitHub","slug":"GitHub","permalink":"https://wgy1993.gitee.io/tags/GitHub/"}]},{"title":"Git","date":"2020-04-22T10:07:41.000Z","path":"archives/69c3279c.html","text":"1. Git 简介1.1 版本控制版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 本地版本控制系统：采用某种简单的数据库来记录文件的历次更新差异（RCS）； 集中化的版本控制系统（ CVCS）：有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新 分布式版本控制系统（DVCS）：客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。 Git 是一个分布式版本控制管理软件。 1.2 Git特点 直接记录快照，而非差异比较。Git 和其它版本控制系统的主要差别在于 Git 对待数据的方法: 其它大部分系统以文件变更列表的方式存储信息。这类系统将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。 Git 更像是把数据看作是对小型文件系统的一组快照。每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。Git 对待数据更像是一个 快照流。 近乎所有操作都是本地执行 Git保证完整性 2. Git 使用交互流程 3. 安装和配置 Git 环境下载地址：https://git-scm.com/ 环境配置：Git安装bin目录加入系统patch中 4. 初始化配置12345678# 设置用户名git config --global user.name \"你的名字\"# 配置用户邮箱git config --global user.email \"你的常用邮箱\"# 设置 gitk 图形查看工具中文显示默认编码（防止乱码）git config --global gui.encoding utf-8# 查看配置列表项git config --list 5. 基本使用12345678910111213141516git init 初始化一个 Git 仓库git status 查看当前工作区、暂存区、本地仓库的状态git add 添加文件到暂存区git commit 文件提交暂存区到仓库区 示例：git commit -m &quot;日志说明&quot; --author&#x3D;&quot;操作者姓名 &lt;邮箱&gt;&quot; 执行 git commit 的时候，Git 会要求具有用户名和邮箱的参数选项,可以通过 git config 命令配置一下用户名和邮箱git log 查看提交历史gitk 图形查看提交历史工具 总结：操作 Git 的基本工作流程就是先修改文件，然后执行 git add 命令。git add 命令会把文件加入到暂存区，接着就可以执行 git commit 命令，将文件存入文档库，从而形成一次历史记录 5.1 添加/删除文件1234567891011121314151617# 添加指定文件到暂存区git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录git add [dir]# 添加当前目录的所有文件到暂存区git add .# 删除工作区文件，并且将这次删除放入暂存区git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区git rm --cached [file]# 改名文件，并且将这个改名放入暂存区git mv [file-original] [file-renamed] 5.2 代码提交123456789101112131415# 提交暂存区到仓库区git commit -m [message]# 提交暂存区的指定文件到仓库区git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区git commit -a# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化git commit --amend [file1] [file2] ... 5.3 回退撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区git checkout [commit] [file]# 恢复暂存区的所有文件到工作区git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset [file]# 重置暂存区与工作区，与上一次commit保持一致git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支git revert [commit]# 暂时将未提交的变化移除，稍后再移入git stashgit stash pop 5.4 分支管理默认在 git 的仓库中，会有个分支的原点：master 1234567891011121314151617181920# 列出所有本地分支git branch# 基于当前分支新建一个分支，但依然停留在当前分支git branch [branch-name]# 基于当前分支新建一个分支，并切换到该分支git checkout -b [branch]# 切换到指定分支，并更新工作区git checkout [branch-name]# 切换到上一个分支，交替和上一个分支进行切换git checkout -# 合并指定分支到当前分支git merge [branch]# 删除分支git branch -d [branch-name] 5.5 远程操作1234567891011121314151617181920# 下载一个远程仓库git clone [url]# 显示所有远程仓库git remote -v# 显示某个远程仓库的信息git remote show [remote]# 增加一个新的远程仓库，并命名git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并git pull [remote] [branch]# 上传本地指定分支到远程仓库git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突git push [remote] --force 本地已有仓库，需要提交到线上： 如果是 git init 出来的仓库，进行 push 提交的时候就不知道要往哪里 push。 所以，这里通过 remote 相关命令进行设置： 123456# 查看所有的远程仓库信息git remote show# 根据别名查看指定的远程仓库信息git remote show 远程仓库地址别名# 添加远程仓库信息git remote add 别名 远程仓库地址 通过上面的 git remote add 添加完远程仓库地址信息之后，还不能直接 git push，必须在每一次push 的时候加上 git push 仓库地址别名 master 就可以提交了。 如果想要省略 git push 后面需要指定的 仓库地址别名 master 可以通过下面的命令修改： 1git push --set-upstream wgy master 这样就可以直接使用 git pish 进行提交而不需要指定 wgy master 了 6. Git 工作流程：分支策略6.1 Git Flow 6.2 Github Collabrators这种方式公司团队项目使用居多 6.3 Github Flow这种方式开源项目使用居多 fork clone 到你的本地 在clone下来的项目中拉出一个新的分支 修改的时候最好是基于 master 拉出一个修改的分支，例如这个分支是用来添加某个功能的 在新分支上开发或者修改完成之后，提交到本地仓库，然后 push 推到自己的账户中 fork 过来的仓库 最后，在 Github 上你 fork 过来的仓库界面中找到 New Pull Request 发起提交请求 对方就会在仓库的 Pull Requests 中收到你发起的提交请求 然后双方就可以使用社会化交流方式进行沟通协作 例如 Code Review 代码审查 最后对方审查通过没有问题之后，选择 Merge Request 到此，一个完整的 Github 工作流结束 这种方式开源项目更多一些（大家都不认识） 7. Github Github 就是程序员的新浪微博它可以让你使用社交化的方式进行编程协作、 - 点赞 - 评论 - 转发 - etc.主要作用：可以免费在线托管你的仓库可以实现多人协作提供了一个可视化界面（Web Page）让你能直观清晰的了解你的项目源代码 7.1 基本使用 注册 登陆 创建远程仓库 通过 git clone 命令下载远程仓库到本地 git clone 会自动帮你把远程仓库下载到本地，不需要再去 git init 了 通过 clone 下来的仓库，git 有一个远程仓库地址列表，git 默认会把你 clone 的地址起一个别名：origin 然后你执行 push 的时候实际上就是将本地的版本提交到 origin 上 在本地进行操作，通过 git commit 形成历史记录 通过 git push 将本地仓库中的历史记录提交到远程仓库 7.2 Github PagesGithub Pages 提供了一个免费在线托管静态资源的服务，叫做：Github Pages。 使用方法如下： 在个人的 Github 账户中创建一个仓库 仓库名称必须为 个人账户名称.github.io 往该仓库根目录中提交一个 index.html 文件 然后就可以在地址栏输入 个人账户名称.github.io 地址，就可以看到 index.html 网页内容了 注意：上面创建的仓库名称必须是 个人账户名称.github.io ，否则无法访问","tags":[{"name":"项目管理","slug":"项目管理","permalink":"https://wgy1993.gitee.io/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"},{"name":"Git","slug":"Git","permalink":"https://wgy1993.gitee.io/tags/Git/"}]},{"title":"Node.js(二)","date":"2020-04-21T07:29:47.000Z","path":"archives/543e3a79.html","text":"1. Web开发概述传统的动态网站开发需要应用软件 PHP ： Apache + php模块 java ：Tomcat 、Weblogic Node.js : 不需要应用软件（可以自己实现） Node.js服务器模型与php服务器模型的区别 2. Node.js实现静态网站功能2.1 使用http模块初步实现服务器功能12345678910111213141516171819/* 初步实现服务器功能*/const http = require('http');// 创建服务器实例对象let server = http.createServer();// 绑定请求事件server.on('request', (req, res) =&gt; &#123; res.end('hello');&#125;);// 监听端口server.listen(3000);-----------------------------http.createServer((req, res) =&gt; &#123; res.end('ok');&#125;).listen(3000, '127.0.0.1', () =&gt; &#123; console.log('running...');&#125;); 2.2 实现静态服务器功能2.2.1 处理请求路径的分发123456789101112131415161718192021222324/* 处理请求路径的分发 1、req对象是Class: http.IncomingMessage的实例对象 2、res对象是Class: http.ServerResponse的实例对象*/const http = require('http');http.createServer((req, res) =&gt; &#123; // req.url可以获取URL中的路径（端口之后部分） // res.end(req.url); if (req.url.startsWith('/index')) &#123; // write向客户端响应内容,可以写多次 res.write('hello'); res.write('hi'); res.write('nihao'); // end方法用来完成响应，只能执行一次 res.end(); &#125; else if (req.url.startsWith('/about')) &#123; res.end('about'); &#125; else &#123; res.end('no content'); &#125;&#125;).listen(3000, '192.168.0.106', () =&gt; &#123; console.log('running...');&#125;); 2.2.2 响应完整的页面信息123456789101112131415161718192021222324252627282930313233343536/* 响应完整的页面信息*/const http = require('http');const path = require('path');const fs = require('fs');// 根据路径读取文件的内容，并且响应到浏览器端let readFile = (url, res) =&gt; &#123; fs.readFile(path.join(__dirname, 'www', url), 'utf8', (err, fileContent) =&gt; &#123; if (err) &#123; res.end('server error'); &#125; else &#123; res.end(fileContent); &#125; &#125;);&#125;http.createServer((req, res) =&gt; &#123; // 处理路径的分发 if (req.url.startsWith('/index')) &#123; readFile('index.html', res); &#125; else if (req.url.startsWith('/about')) &#123; readFile('about.html', res); &#125; else if (req.url.startsWith('/list')) &#123; readFile('list.html', res); &#125; else &#123; // 设置相应类型和编码 res.writeHead(200, &#123; 'Content-Type': 'text/plain; charset=utf8' &#125;); res.end('页面被狗狗叼走了'); &#125;&#125;).listen(3000, '192.168.0.106', () =&gt; &#123; console.log('running...');&#125;); 2.2.3 单独封装方法1234567891011121314151617181920212223242526272829303132333435363738394041424344const path = require('path');const fs = require('fs');const mime = require('./mime.json');exports.staticServer = (req, res, root) =&gt; &#123; fs.readFile(path.join(root, req.url), (err, fileContent) =&gt; &#123; if (err) &#123; // 没有找到对应文件 res.writeHead(404, &#123; 'Content-Type': 'text/plain; charset=utf8' &#125;); res.end('页面被狗狗叼走了'); &#125; else &#123; let dtype = 'text/html'; // 获取请求文件的后缀 let ext = path.extname(req.url); // 如果请求的文件后缀合理，就获取到标准的响应格式 if (mime[ext]) &#123; dtype = mime[ext]; &#125; // 如果响应的内容是文本，就设置成utf8编码 if (dtype.startsWith('text')) &#123; dtype += '; charset=utf8' &#125; // 设置响应头信息 res.writeHead(200, &#123; 'Content-Type': dtype &#125;); res.end(fileContent); &#125; &#125;);&#125;--------------------------------------------------------------const http = require('http');const ss = require('./06.js');const path = require('path');http.createServer((req, res) =&gt; &#123; // ss.staticServer(req,res,path.join(__dirname,'www')); ss.staticServer(req, res, path.join('C:\\\\Users\\\\www\\\\Desktop', 'test'));&#125;).listen(3000, () =&gt; &#123; console.log('running....');&#125;); 3. 参数传递与获取3.1 url核心模块1234567891011121314151617181920212223const url = require('url');// 1. parse方法的作用就是把URL字符串转化为对象let str = 'http://www.baidu.com/abc/qqq?flag=123&amp;keyword=java';let ret = url.parse(str, true);console.log(ret.query.keyword); -&gt;java// 2. format的作用就是把对象转化为标准的URL字符串let obj = &#123; protocol: 'http:', slashes: true, auth: null, host: 'www.baidu.com', port: null, hostname: 'www.baidu.com', hash: null, search: '?flag=123&amp;keyword=java', query: 'flag=123&amp;keyword=java', pathname: '/abc/qqq', path: '/abc/qqq?flag=123&amp;keyword=java', href: 'http://www.baidu.com/abc/qqq?flag=123&amp;keyword=java'&#125;;let ret1 = url.format(obj);console.log(ret1); -&gt;http://www.baidu.com/abc/qqq?flag=123&amp;keyword=java 3.2 get参数获取12345678910const http = require('http');const path = require('path');const url = require('url');http.createServer((req, res) =&gt; &#123; let obj = url.parse(req.url, true); res.end(obj.query.username + '=========' + obj.query.password);&#125;).listen(3000, () =&gt; &#123; console.log('running....');&#125;); 3.3 post参数获取1234567891011121314151617181920212223242526272829303132333435const querystring = require('querystring');const http = require('http');// parse方法的作用就是把字符串转成对象let param = 'username=lisi&amp;password=123';// let param = 'foo=bar&amp;abc=xyz&amp;abc=123';let obj = querystring.parse(param);console.log(obj); -&gt;[Object: null prototype] &#123; username: 'lisi', password: '123' &#125;// stringify的作用就是把对象转成字符串let obj1 = &#123; flag: '123', abc: ['hello', 'hi']&#125;;let str1 = querystring.stringify(obj1);console.log(str1); -&gt;flag=123&amp;abc=hello&amp;abc=hi----------------------------------------------------- http.createServer((req, res) =&gt; &#123; if (req.url.startsWith('/login')) &#123; let pdata = ''; req.on('data', (chunk) =&gt; &#123; // 每次获取一部分数据 pdata += chunk; &#125;); req.on('end', () =&gt; &#123; // 这里才能得到完整的数据 console.log(pdata); let obj = querystring.parse(pdata); res.end(obj.username + '-----' + obj.password); &#125;); &#125;&#125;).listen(3000, () =&gt; &#123; console.log('running...');&#125;); 3.4 案例：登录验证功能1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=\"http://localhost:3000/login\" method=\"post\"&gt; 用户名：&lt;input type=\"text\" name=\"username\"&gt;&lt;br&gt; 密码：&lt;input type=\"password\" name=\"password\"&gt;&lt;br&gt; &lt;input type=\"submit\" value=\"提交\"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445/* 登录验证功能*/const http = require('http');const url = require('url');const querystring = require('querystring');const ss = require('./06.js');http.createServer((req, res) =&gt; &#123; // 启动静态资源服务 if (req.url.startsWith('/www')) &#123; ss.staticServer(req, res, __dirname); &#125; console.log(req.url); // 动态资源 if (req.url.startsWith('/login')) &#123; // get请求 if (req.method == 'GET') &#123; let param = url.parse(req.url, true).query; if (param.username == 'admin' &amp;&amp; param.password == '123') &#123; res.end('get success'); &#125; else &#123; res.end('get failure'); &#125; &#125; // post请求 if (req.method == 'POST') &#123; let pdata = ''; req.on('data', (chunk) =&gt; &#123; pdata += chunk; &#125;); req.on('end', () =&gt; &#123; let obj = querystring.parse(pdata); if (obj.username == 'admin' &amp;&amp; obj.password == '123') &#123; res.end('post success'); &#125; else &#123; res.end('post failure'); &#125; &#125;); &#125; &#125;&#125;).listen(3000, () =&gt; &#123; console.log('running....');&#125;); 4. 动态网站开发创建服务器实现动态网站效果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* 动态网站开发 成绩查询功能*/const http = require('http');const path = require('path');const fs = require('fs');const querystring = require('querystring');const scoreData = require('./scores.json');http.createServer((req, res) =&gt; &#123; // 路由（请求路径+请求方式） // 查询成绩的入口地址 /query if (req.url.startsWith('/query') &amp;&amp; req.method == 'GET') &#123; fs.readFile(path.join(__dirname, 'view', 'index.tpl'), 'utf8', (err, content) =&gt; &#123; if (err) &#123; res.writeHead(500, &#123; 'Content-Type': 'text/plain; charset=utf8' &#125;); res.end('服务器错误，请与管理员联系'); &#125; res.end(content); &#125;); &#125; else if (req.url.startsWith('/score') &amp;&amp; req.method == 'POST') &#123; // 获取成绩的结果 /score let pdata = ''; req.on('data', (chunk) =&gt; &#123; pdata += chunk; &#125;); req.on('end', () =&gt; &#123; let obj = querystring.parse(pdata); let result = scoreData[obj.code]; fs.readFile(path.join(__dirname, 'view', 'result.tpl'), 'utf8', (err, content) =&gt; &#123; if (err) &#123; res.writeHead(500, &#123; 'Content-Type': 'text/plain; charset=utf8' &#125;); res.end('服务器错误，请与管理员联系'); &#125; // 返回内容之前要进行数据渲染 content = content.replace('$$chinese$$', result.chinese); content = content.replace('$$math$$', result.math); content = content.replace('$$english$$', result.english); content = content.replace('$$summary$$', result.summary); res.end(content); &#125;); &#125;); &#125;&#125;).listen(3000, () =&gt; &#123; console.log('running....');&#125;); 1234567891011121314151617181920&#123; \"no123\": &#123; \"chinese\": \"100\", \"math\": \"140\", \"english\": \"149\", \"summary\": \"289\" &#125;, \"no124\": &#123; \"chinese\": \"120\", \"math\": \"120\", \"english\": \"119\", \"summary\": \"239\" &#125;, \"no125\": &#123; \"chinese\": \"130\", \"math\": \"110\", \"english\": \"139\", \"summary\": \"269\" &#125;&#125; 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;查询成绩&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=\"http://localhost:3000/score\" method=\"post\"&gt; 请输入考号：&lt;input type=\"text\" name=\"code\"&gt; &lt;input type=\"submit\" value=\"查询\"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;成绩结果&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div&gt; &lt;ul&gt; &lt;li&gt;语文：$$chinese$$&lt;/li&gt; &lt;li&gt;数学：$$math$$&lt;/li&gt; &lt;li&gt;外语：$$english$$&lt;/li&gt; &lt;li&gt;综合：$$summary$$&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 5. 模板引擎模板引擎是为了使用户界面与业务数据（内容）分离而产生的，它可以生成一个标准的HTML结构。 模板+数据—&gt;静态页面片段 流行的模板引擎: 5.1 artTemplate5.1.1 特性 性能卓越，执行速度通常是 Mustache 与 tmpl 的 20 多倍（性能测试） 支持运行时调试，可精确定位异常模板所在语句（演示） 对 NodeJS Express 友好支持 安全，默认对输出进行转义、在沙箱中运行编译后的代码（Node版本可以安全执行用户上传的模板） 支持include语句 可在浏览器端实现按路径加载模板（详情） 支持预编译，可将模板转换成为非常精简的 js 文件 模板语句简洁，无需前缀引用数据，有简洁版本与原生语法版本可选 支持所有流行的浏览器 5.1.2 安装1234567npm install art-template&#x2F;&#x2F; 使用var template &#x3D; require(&#39;art-template&#39;);var data &#x3D; &#123;list: [&quot;aui&quot;, &quot;test&quot;]&#125;;var html &#x3D; template(__dirname + &#39;&#x2F;index&#x2F;main&#39;, data); 5.1.3 快速上手5.1.3.1 编写模板使用一个type=&quot;text/html&quot;的script标签存放模板： 12345678&lt;script id&#x3D;&quot;test&quot; type&#x3D;&quot;text&#x2F;html&quot;&gt; &lt;h1&gt;&#123; &#123;title&#125; &#125;&lt;&#x2F;h1&gt; &lt;ul&gt; &#123; &#123;each list as value i&#125; &#125; &lt;li&gt;索引 &#123; &#123;i + 1&#125; &#125; ：&#123; &#123;value&#125; &#125;&lt;&#x2F;li&gt; &#123; &#123;&#x2F;each&#125; &#125; &lt;&#x2F;ul&gt;&lt;&#x2F;script&gt; 5.1.3.2 渲染模板12345678910var data &#x3D; &#123; title: &#39;标签&#39;, list: [&#39;文艺&#39;, &#39;博客&#39;, &#39;摄影&#39;, &#39;电影&#39;, &#39;民谣&#39;, &#39;旅行&#39;, &#39;吉他&#39;]&#125;;var html &#x3D; template(&#39;test&#39;, data);document.getElementById(&#39;content&#39;).innerHTML &#x3D; html;template的作用就是把模板和数据拼接到一块生成一个静态HTML片段，实际就是该方法的返回值 参数一：模板id 参数二：用来渲染的数据 5.1.4 模板语法有两个版本的模板语法可以选择。 5.1.4.1 简洁语法推荐使用，语法简单实用，利于读写。 1234567&#123; &#123;if admin&#125; &#125; &#123; &#123;include &#39;admin_content&#39;&#125; &#125; &#123; &#123;each list&#125; &#125; &lt;div&gt;&#123; &#123;$index&#125; &#125;. &#123; &#123;$value.user&#125; &#125;&lt;&#x2F;div&gt; &#123;&#123;&#x2F;each&#125;&#125;&#123; &#123;&#x2F;if&#125; &#125; 5.1.4.2 原生语法1234567&lt; % if (admin)&#123; % &gt; &lt; %include(&#39;admin_content&#39;)% &gt; &lt; %for (var i&#x3D;0;i&lt;list.length;i++) &#123; % &gt; &lt;div&gt;&lt; % &#x3D;i % &gt;. &lt; %&#x3D;list[i].user % &gt;&lt;&#x2F;div&gt; &lt; % &#125; % &gt;&lt; % &#125; % &gt; 5.1.5 方法5.1.5.1 template(id, data)根据 id 渲染模板。内部会根据document.getElementById(id)查找模板。 如果没有 data 参数，那么将返回一渲染函数。 123&#123; &#123;if user&#125; &#125; &lt;h2&gt;&#123; &#123;user.name&#125; &#125;&lt;&#x2F;h2&gt;&#123; &#123;&#x2F;if&#125; &#125; 1234567let template = require('art-template');let html = template(__dirname + '/mytpl.art', &#123; user: &#123; name: 'lisi' &#125;&#125;);console.log(html); 5.1.5.2 template.compile(source, options)将返回一个渲染函数。 123456let tpl = '&lt;ul&gt;&#123; &#123;each list as value&#125; &#125;&lt;li&gt;&#123; &#123;value&#125; &#125;&lt;/li&gt;&#123; &#123;/each&#125; &#125;&lt;/ul&gt;';let render = template.compile(tpl);let ret = render(&#123; list: ['apple', 'orange', 'banana']&#125;);console.log(ret); 5.1.5.3 template.render(source, options)将返回渲染结果。 12345let tpl = '&lt;ul&gt;&#123; &#123;each list&#125; &#125;&lt;li&gt;&#123; &#123;$index&#125; &#125;-------------&#123; &#123;$value&#125; &#125;&lt;/li&gt;&#123; &#123;/each&#125; &#125;&lt;/ul&gt;';let ret = template.render(tpl, &#123; list: ['apple', 'orange', 'banana', 'pineapple']&#125;);console.log(ret); 5.1.5.4 template.helper(name, callback)添加辅助方法。 5.1.5.5 template.config(name, value)更改引擎的默认配置。 字段 类型 默认值 说明 openTag String &#39;{ {&#39; 逻辑语法开始标签 closeTag String &quot;} }&quot; 逻辑语法结束标签 escape Boolean true 是否编码输出 HTML 字符 cache Boolean true 是否开启缓存（依赖 options 的 filename 字段） compress Boolean false 是否压缩 HTML 多余空白字符 6. Express基于Node.js平台，快速、开放、极简的 Web 开发框架 6.1 Express之HelloWorld12345678910111213141516var express = require('express');var app = express();// 绑定路由app.get('/', function (req, res) &#123; // 响应请求 res.send('Hello World!');&#125;);var server = app.listen(3000, \"localhost\", function () &#123; // 监听的域名或者IP var host = server.address().address; // 监听的端口 var port = server.address().port; console.log(\"Example app listening at http:// % s:% s\", host, port);&#125;); 6.2 初步实现服务器功能1234567891011121314151617181920const express = require('express');const app = express();----------------------------------- // 上面两行与下面一行代码等效const app = require('express')();app.get('/', (req, res) =&gt; &#123; res.send('ok');&#125;).listen(3000, () =&gt; &#123; console.log('running...');&#125;);----------------------------------let server = app.get('/',(req,res)=&gt;&#123; res.send('abc');&#125;);server.listen(3000,()=&gt;&#123; console.log('running...');&#125;); 6.3 托管静态文件1234567891011121314151617const express = require('express');const app = express();// 实现静态资源服务// use方法的第一个参数可以指定一个虚拟路径let server = app.use('/abc', express.static('public'));app.use('/nihao', express.static('hello'));server.listen(3000, () =&gt; &#123; console.log('running...');&#125;);----------------------------------- app.use('/abc',express.static('public'));app.use('/nihao',express.static('hello'));app.listen(3000,()=&gt;&#123; console.log('running...');&#125;); 6.4 路由根据请求路径和请求方式进行路径分发处理 1234567http的常用请求方式： post 添加 get 查询 put 更新 delete 删除 restful api (一种URL的格式) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647const express = require('express');const app = express();const router = require('./myrouter.js');// 直接使用use分发可以处理所有的路由请求app.use((req, res) =&gt; &#123; res.send('ok');&#125;);------------------------------// all方法绑定的路由与请求方式无关app.all('/abc', (req, res) =&gt; &#123; res.end('test router');&#125;);------------------------------// 基本的路由处理app.get('/', (req, res) =&gt; &#123; res.send('get data');&#125;);app.post('/', (req, res) =&gt; &#123; res.send('post data');&#125;);app.put('/', (req, res) =&gt; &#123; res.send('put data');&#125;);app.delete('/', (req, res) =&gt; &#123; res.send('delete data');&#125;);----------------------------// route方法可以指定特定的请求方式app.route('/hello') .get((req, res) =&gt; &#123; res.send('get data'); &#125;).post((req, res) =&gt; &#123; res.send('post data');&#125;);app.use('/admin', router);app.listen(3000, () =&gt; &#123; console.log('running...');&#125;); myrouter.js 12345678910111213141516const express = require('express');const router = express.Router();router.get('/hi', (req, res) =&gt; &#123; res.send('hi router');&#125;);router.get('/hello', (req, res) =&gt; &#123; res.send('hello router');&#125;);router.post('/abc', (req, res) =&gt; &#123; res.send('abc router');&#125;);module.exports = router; 6.5 中间件就是处理过程中的一个环节（本质上就是一个函数） 1234567891011121314151617181920212223242526272829303132const express = require('express');const app = express();let total = 0;app.use((req, res, next) =&gt; &#123; console.log('有人访问'); // next方法的作用就是把请求传递到下一个中间件 next()&#125;);app.use('/user', (req, res, next) =&gt; &#123; // 记录访问时间 console.log(Date.now()); // next方法的作用就是把请求传递到下一个中间件 next()&#125;);app.use('/user', (req, res, next) =&gt; &#123; // 记录访问日志 console.log('访问了/user'); next()&#125;);app.use('/user', (req, res) =&gt; &#123; total++; console.log(total); res.send('result');&#125;);app.listen(3000, () =&gt; &#123; console.log('running...');&#125;); 6.5.1 中间件的挂载方式和执行流程1234567891011121314151617181920212223242526272829303132333435363738const express = require('express');const app = express();app.get('/abc', (req, res, next) =&gt; &#123; console.log(1); // next(); 走2 // 跳转到下一个路由3，不走2 next('route');&#125;, (req, res) =&gt; &#123; console.log(2); res.send('abc');&#125;);app.get('/abc', (req, res) =&gt; &#123; console.log(3); res.send('hello');&#125;);--------------------------------var cb0 = function (req, res, next) &#123; console.log('CB0'); next();&#125;var cb1 = function (req, res, next) &#123; console.log('CB1'); next();&#125;var cb2 = function (req, res) &#123; res.send('Hello from C!');&#125;app.get('/example', [cb0, cb1, cb2]);app.listen(3000, () =&gt; &#123; console.log('running...');&#125;); 6.6 参数处理12345678910111213141516171819202122232425262728293031323334353637383940const express = require('express');const app = express();const bodyParser = require('body-parser')// 挂载内置中间件app.use(express.static('public'));// 挂载参数处理中间件（post）app.use(bodyParser.urlencoded(&#123;extended: false&#125;));// 处理json格式的参数app.use(bodyParser.json());// 处理get提交参数app.get('/login', (req, res) =&gt; &#123; let data = req.query; console.log(data); res.send('get data');&#125;);// 处理post提交参数app.post('/login', (req, res) =&gt; &#123; let data = req.body; // console.log(data); if (data.username == 'admin' &amp;&amp; data.password == '123') &#123; res.send('success'); &#125; else &#123; res.send('failure'); &#125;&#125;);app.put('/login', (req, res) =&gt; &#123; res.end('put data');&#125;);app.delete('/login', (req, res) =&gt; &#123; res.end('delete data');&#125;);app.listen(3000, () =&gt; &#123; console.log('running...');&#125;); 6.7 模板引擎整合12345678910111213141516171819202122232425const express = require('express');const path = require('path');const template = require('art-template');const app = express();// 设置模板的路径app.set('views', path.join(__dirname, 'views'));// 设置模板引擎app.set('view engine', 'art');// 使express兼容art-template模板引擎app.engine('art', require('express-art-template'));app.get('/list', (req, res) =&gt; &#123; let data = &#123; title: '水果', list: ['apple', 'orange', 'banana'] &#125; // 参数一：模板名称；参数二：渲染模板的数据 res.render('list', data);&#125;);app.listen(3000, () =&gt; &#123; console.log('running...');&#125;); list.art 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html lang&#x3D;&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt; &lt;title&gt;模板&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt; &lt;div&gt;&#123; &#123;title&#125; &#125;&lt;&#x2F;div&gt; &lt;div&gt; &lt;ul&gt; &#123; &#123;each list&#125; &#125; &lt;li&gt;&#123; &#123;$value&#125; &#125;&lt;&#x2F;li&gt; &#123; &#123;&#x2F;each&#125; &#125; &lt;&#x2F;ul&gt; &lt;&#x2F;div&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; 7. Mysql API7.1 封装操作数据库的通用api123456789101112131415161718192021const mysql = require('mysql');exports.base = (sql, data, callback) =&gt; &#123; // 创建数据库连接 const connection = mysql.createConnection(&#123; host: 'localhost', // 数据库所在的服务器的域名或者IP地址 user: 'root', // 登录数据库的账号 password: 'root', // 登录数据库的密码 database: 'book' // 数据库名称 &#125;); // 执行连接操作 connection.connect(); // 操作数据库(数据库操作也是异步的) connection.query(sql, data, function (error, results, fields) &#123; if (error) throw error; callback(results); &#125;); // 关闭数据库 connection.end();&#125;; 7.2 测试通用api12345678910111213141516171819202122232425262728293031323334353637const db = require('./db.js');// 插入操作let sql = 'insert into book set ?';let data = &#123; name: '笑傲江湖', author: '金庸', category: '文学', description: '武侠小说'&#125;db.base(sql, data, (result) =&gt; &#123; console.log(result);&#125;);--------------------------------------- // 更新操作let sql = 'update book set name=?,author=?,category=?,description=? where id=?';let data = ['天龙八部', '金庸', '文学', '武侠小说', 11];db.base(sql, data, (result) =&gt; &#123; console.log(result);&#125;);--------------------------------------- // 删除操作let sql = 'delete from book where id = ?';let data = [11];db.base(sql, data, (result) =&gt; &#123; console.log(result);&#125;);--------------------------------------- // 查询操作let sql = 'select * from book where id = ?';let data = [8];db.base(sql, data, (result) =&gt; &#123; console.log(result[0].name);&#125;); 8. 后台接口开发8.1 json接口1234567// 指定api路径 allBooks (json接口)app.get('/allBooks', (req, res) =&gt; &#123; let sql = 'select * from book'; db.base(sql, null, (result) =&gt; &#123; res.json(result); &#125;);&#125;); 8.2 jsonp接口123456789// 修改jsonp回调函数传递参数的keyapp.set('jsonp callback name', 'cb');// 指定api路径 allBooks （jsonp接口）app.get('/allBooks', (req, res) =&gt; &#123; let sql = 'select * from book'; db.base(sql, null, (result) =&gt; &#123; res.jsonp(result); &#125;);&#125;); 8.3 restful接口1234567891011121314151617181920212223242526272829303132333435363738394041/* restful api 是从URL的格式来表述的 get http://localhost:3000/books get http://localhost:3000/books/book post http://localhost:3000/books/book get http://localhost:3000/books/book/1 put http://localhost:3000/books/book delete http://localhost:3000/books/book/2 传统的URL风格 http://localhost:3000/ http://localhost:3000/toAddBook http://localhost:3000/addBook http://localhost:3000/toEditBook?id=1 http://localhost:3000/editBook http://localhost:3000/deleteBook?id=2*/const express = require('express');const db = require('./db.js');const app = express();app.get('/books', (req, res) =&gt; &#123; let sql = 'select * from book'; db.base(sql, null, (result) =&gt; &#123; res.json(result); &#125;);&#125;);// http://localhost:3000/books/book/1app.get('/books/book/:id', (req, res) =&gt; &#123; let id = req.params.id; let sql = 'select * from book where id=?'; let data = [id]; db.base(sql, data, (result) =&gt; &#123; res.json(result[0]); &#125;);&#125;);app.listen(3000, () =&gt; &#123; console.log('running...');&#125;); 9. 从服务器主动发送请求12345678910111213141516171819202122232425262728/* 从服务器主动发送请求 http.request(options[, callback])*/const http = require('http');const path = require('path');const fs = require('fs');let options = &#123; hostname: 'www.baidu.com', port: 80&#125;let req = http.request(options, (res) =&gt; &#123; let info = ''; res.on('data', (chunk) =&gt; &#123; info += chunk; &#125;); res.on('end', () =&gt; &#123; fs.writeFile(path.join(__dirname, 'baidu.html'), info, (err) =&gt; &#123; console.log('已经获取到百度主页的内容'); &#125;); &#125;);&#125;);req.end(); 9.1 查询数据123456789101112131415161718192021const http = require('http');let options = &#123; protocol: 'http:', hostname: 'localhost', port: 3000, path: '/books'&#125;let req = http.request(options, (res) =&gt; &#123; let info = ''; res.on('data', (chunk) =&gt; &#123; info += chunk; &#125;); res.on('end', () =&gt; &#123; console.log(info); &#125;);&#125;);req.end(); 9.2 添加数据123456789101112131415161718192021222324252627282930313233const http = require('http');const querystring = require('querystring');let options = &#123; protocol: 'http:', hostname: 'localhost', port: 3000, path: '/books/book', method: 'post', headers: &#123; 'Content-Type': 'application/x-www-form-urlencoded', &#125;&#125;let req = http.request(options, (res) =&gt; &#123; let info = ''; res.on('data', (chunk) =&gt; &#123; info += chunk; &#125;); res.on('end', () =&gt; &#123; console.log(info); &#125;);&#125;);let data = querystring.stringify(&#123; name: 'adafa', author: 'asdfa', category: 'afdasf', description: 'adsfa'&#125;);req.write(data);req.end(); 9.3 根据id去查询数据1234567891011121314151617181920212223const http = require('http');const querystring = require('querystring');let options = &#123; protocol: 'http:', hostname: 'localhost', port: 3000, path: '/books/book/2', method: 'get'&#125;let req = http.request(options, (res) =&gt; &#123; let info = ''; res.on('data', (chunk) =&gt; &#123; info += chunk; &#125;); res.on('end', () =&gt; &#123; console.log(info); &#125;);&#125;);req.end(); 9.4 编辑数据12345678910111213141516171819202122232425262728293031323334const http = require('http');const querystring = require('querystring');let options = &#123; protocol: 'http:', hostname: 'localhost', port: 3000, path: '/books/book', method: 'put', headers: &#123; 'Content-Type': 'application/x-www-form-urlencoded', &#125;&#125;let req = http.request(options, (res) =&gt; &#123; let info = ''; res.on('data', (chunk) =&gt; &#123; info += chunk; &#125;); res.on('end', () =&gt; &#123; console.log(info); &#125;);&#125;);let data = querystring.stringify(&#123; id: 34, name: 'ddddddddd', author: 'sssssssss', category: 'eeeeeeeee', description: 'gggggggg'&#125;);req.write(data);req.end(); 9.5 删除数据1234567891011121314151617181920212223const http = require('http');const querystring = require('querystring');let options = &#123; protocol: 'http:', hostname: 'localhost', port: 3000, path: '/books/book/34', method: 'delete'&#125;let req = http.request(options, (res) =&gt; &#123; let info = ''; res.on('data', (chunk) =&gt; &#123; info += chunk; &#125;); res.on('end', () =&gt; &#123; console.log(info); &#125;);&#125;);req.end(); 10. 调用第三方接口123456789101112131415161718192021222324const http = require('http');const querystring = require('querystring');let cityCode = '101010300';let options = &#123; protocol: 'http:', hostname: 'www.weather.com.cn', port: 80, path: '/data/sk/' + cityCode + '.html', method: 'get'&#125;let req = http.request(options, (res) =&gt; &#123; let info = ''; res.on('data', (chunk) =&gt; &#123; info += chunk; &#125;); res.on('end', () =&gt; &#123; console.log(info); &#125;);&#125;);req.end();","tags":[{"name":"前端技术","slug":"前端技术","permalink":"https://wgy1993.gitee.io/tags/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"},{"name":"Node.js","slug":"Node-js","permalink":"https://wgy1993.gitee.io/tags/Node-js/"},{"name":"artTemplate","slug":"artTemplate","permalink":"https://wgy1993.gitee.io/tags/artTemplate/"},{"name":"Express","slug":"Express","permalink":"https://wgy1993.gitee.io/tags/Express/"}]},{"title":"less","date":"2020-04-19T13:43:34.000Z","path":"archives/d3b694df.html","text":"1. CSS预处理器CSS 预处理器是一种语言，用来为 CSS 增加一些编程的的特性，无需考虑浏览器的兼容性问题，并且你可以在CSS 中使用变量、简单的程序逻辑、函数等等在编程语言中的一些基本技巧，可以让你的 CSS 更简洁，适应性更强，代码更直观等诸多好处。 常见的CSS预处理器有：LESS、SASS、Stylus等 2. LESSLESS 是动态的样式表语言，通过简洁明了的语法定义，使编写 CSS 的工作变得非常简单，本质上，LESS 包含一套自定义的语法及一个解析器。 2.1 安装1、安装Nodejs环境 Node Package Manager (验证 node -v npm -v) 2、打开控制台（cmd），执行npm install -g less (验证 lessc -v) 3、命令行编译 lessc path/xxx.less path/xxx.css 2.2 编译浏览器只能识别CSS，LESS只是用来提升CSS可维护性的一个工具，所最终需要将LESS编译成CSS，然而通过命令行编译效率比较低下，一般都会借助于编辑器来完成编译，以sublime_text为例，sublime_text默认并不支持LESS的编译操作，需要安装插件实现。 1、执行npm install -g less-plugin-clean-css（使用sublime_text才用） 2、ctrl+shit+p打开命令面板 3、输入install package然后回车 4、安装 LESS、lessc、Less2Css三个插件 5、alt+s快捷键即可实现编译 2.3 语法2.3.1 变量格式：@变量名: 值，定义完成后可以重复使用 2.3.2 混合我们可以像使用函数一样来使用CSS 2.3.3 嵌套嵌套可以非常方便的管理我们的CSS层级关系","tags":[{"name":"前端技术","slug":"前端技术","permalink":"https://wgy1993.gitee.io/tags/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"},{"name":"less","slug":"less","permalink":"https://wgy1993.gitee.io/tags/less/"},{"name":"预处理器","slug":"预处理器","permalink":"https://wgy1993.gitee.io/tags/%E9%A2%84%E5%A4%84%E7%90%86%E5%99%A8/"}]},{"title":"ECMAScript6","date":"2020-04-19T10:22:50.000Z","path":"archives/fa865a02.html","text":"1. ECMAScript 6ECMAScript 6.0（以下简称ES6）是JavaScript语言的下一代标准，已经在2015年6月正式发布了。 Ecmascript 是 JavaScript 语言的标注规范。 JavaScript 是 Ecmascript 规范的具体实现，具体实现取决于各大浏览器厂商的支持进度。 Ecmascript 6 也被称作 Ecmascript 2015。 各大浏览器厂商对于最新的 Ecmascript 6 标准支持可以参照：http://kangax.github.io/compat-table/es6/。 对于不支持 ES6 的环境，可以使用一些编译转码工具做转换处理再使用，例如 babel。 1.1 变量声明let与constlet： let 类似于 var，用来声明变量 通过 let 声明的变量不同于 var，只在 let 命令所在的代码块内有效（块级作用域） let 声明的变量不存在变量提升 let不允许在相同作用域内，重复声明同一个变量 12345678910111213141516171819202122232425262728293031323334353637// 1. let声明的变量不存在预解析console.log(flag);// var flag = 123; -&gt;打印123let flag = 456; // 打印undefined------------------------// 2. let声明的变量不允许重复（在同一个作用域内）let flag = 123;// let flag = 456;-&gt;已声明，不允许重复--------------------------// 3. ES6引入了块级作用域// 块内部定义的变量，在外部是不可以访问的if (true) &#123; // var flag = 123;-&gt;打印123 let flag = 123;&#125;&#123; // 这里是块级作用域 let flag = 111; console.log(flag);// 打印111&#125;console.log(flag);// 打印undefinedfor (let i = 0; i &lt; 3; i++) &#123; // for循环括号中声明的变量只能在循环体中使用 console.log(i);&#125;console.log(i);// 打印undefined--------------------------------------- // 4. 在块级作用域内部，变量只能先声明再使用if (true) &#123; console.log(flag);// 打印undefined let flag = 123;&#125; const： const声明一个只读的常量。一旦声明，常量的值就不能改变 const 声明必须初始化 const的作用域与let命令相同：只在声明所在的块级作用域内有效 const命令声明的常量也是不提升，必须先声明后使用 const声明的常量，也与let一样不可重复声明 12345678// const用来声明常量// 1. const声明的常量不允许重新赋值const n = 1;// n = 2;--------------------------------// 2. const声明的常量必须初始化// const abc; 1.2 变量的解构赋值ES6 允许按照一定模式，从数组和对象中提取值，对变量进行赋值，这被称为解构（Destructuring）。 1.2.1 数组解构1234let [a, b, c] = [1, 2, 3]; -&gt;1 2 3let [a, b, c] = [, 123,]; -&gt;undefined 123 undefinedlet [a = 111, b, c] = [, 123,]; -&gt;111 123 undefinedconsole.log(a, b, c); 1.2.2 对象解构123456789101112131415// 1. 赋值与顺序无关let &#123;foo, bar&#125; = &#123;foo: 'hello', bar: 'hi'&#125;; -&gt;hello hilet &#123;foo, bar&#125; = &#123;bar: 'hi', foo: 'hello'&#125;; -&gt;hello hiconsole.log(foo, bar);-------------------------------------// 2. 对象属性别名(如果有了别名，那么原来的名字就无效了)let &#123;foo: abc, bar&#125; = &#123;bar: 'hi', foo: 'nihao'&#125;; -&gt;nihao hi// console.log(foo, bar); -&gt;foo未定义console.log(abc, bar);--------------------------------------// 3. 对象的解构赋值指定默认值let &#123;foo: abc = 'hello', bar&#125; = &#123;bar: 'hi'&#125;; -&gt;hello hiconsole.log(abc, bar); 1.2.3 函数参数解构12345function f (p1, &#123; p2 = 'aa', p3 = 'bb' &#125;) &#123; console.log(p1, p2, p3)&#125;f('p1', &#123; p2: 'p2' &#125;) -&gt;p1 p2 bb 1.2.4 字符串解构12345678let [a, b, c, d, e, length] = \"hello\";console.log(a, b, c, d, e); -&gt;h e l l oconsole.log(length); -&gt;undefinedconsole.log(\"hello\".length); -&gt;5let &#123;length&#125; = \"hi\";console.log(length); -&gt;2 1.3 字符串实用方法： 123456789includes(String)：返回布尔值，判断字符串中是否包含指定的字串。参数一：匹配的字串；参数二：从第几个开始匹配startsWith(String)：返回布尔值，判断字符串是否以特定的字串开始。endsWith(String)：返回布尔值，判断字符串是否以特定的字串结束。console.log('hello world'.includes('world',7)); -&gt;falselet url = 'admin/index.php';console.log(url.startsWith('admin')); -&gt;trueconsole.log(url.endsWith('phph')); -&gt;false 模板字符串： 12345678910111213141516171819202122232425// 普通拼接let obj = &#123; username: 'lisi', age: '12', gender: 'male'&#125;let tag = '&lt;div&gt;&lt;span&gt;' + obj.username + '&lt;/span&gt;&lt;span&gt;' + obj.age + '&lt;/span&gt;&lt;span&gt;' + obj.gender + '&lt;/span&gt;&lt;/div&gt;';console.log(tag);------------------------------------------------------------- // 反引号表示模板，模板中的内容可以有格式，通过$&#123;&#125;方式填充数据let fn = function (info) &#123; return info;&#125;let tpl = ` &lt;div&gt; &lt;span&gt;$&#123;obj.username&#125;&lt;/span&gt; &lt;span&gt;$&#123;obj.age&#125;&lt;/span&gt; &lt;span&gt;$&#123;obj.gender&#125;&lt;/span&gt; &lt;span&gt;$&#123;1 + 1&#125;&lt;/span&gt; &lt;span&gt;$&#123;fn('nihao')&#125;&lt;/span&gt; &lt;/div&gt;`;console.log(tpl); 模板字符串（template string）是增强版的字符串，用反引号（`）标识 它可以当作普通字符串使用，也可以用来定义多行字符串，或者在字符串中嵌入变量 如果使用模板字符串表示多行字符串，所有的空格和缩进都会被保留在输出之中 模板字符串中嵌入变量，需要将变量名写在 ${} 之中 大括号内部可以放入任意的JavaScript表达式，可以进行运算，以及引用对象属性 大括号内部还可以调用函数 1.4 数组方法： 123456789Array.from() 将一个伪数组转为一个真正的数组 实际应用中，常见的类似数组的对象是DOM操作返回的NodeList集合， 以及函数内部的arguments对象。Array.from都可以将它们转为真正的数组。Array.of() Array.of方法用于将一组值，转换为数组 这个方法的主要目的，是弥补数组构造函数Array()的不足。 因为参数个数的不同，会导致Array()的行为有差异。find() 查找数组中某个元素findIndex() 查找数组中某个元素的索引下标includes() 返回一个布尔值，表示某个数组是否包含给定的值，与字符串的includes方法类似 实例方法： ES6提供三个新的方法——entries()，keys()和values()——用于遍历数组.可以用 for...of 循环进行遍历，唯一的区别是 keys() 是对键名的遍历、values() 是对键值的遍历，entries() 是对键值对的遍历。 1234567891011121314151617181920212223242526272829303132333435const arr = [ &#123;foo: 'a', name: 'aaa'&#125;, &#123;foo: 'b', name: 'bbb'&#125;, &#123;foo: 'c', name: 'ccc'&#125;,];var item = arr.find(function (item, index) &#123; return item.foo === 'b'&#125;);var item = arr.find(i =&gt; i.foo === 'b');var index = arr.findIndex(function (item, index) &#123; return item.foo === 'b'&#125;);------------------------------------------------------------------let arr = ['a', 'b', 'c'];for (let index of arr.keys()) &#123; console.log(index)&#125;// 不支持for (let item of arr.values()) &#123; console.log(item)&#125;// 遍历数组for (let [index, item] of arr.entries()) &#123; console.log(index, item)&#125;// 是否包含某个元素console.log(arr.includes('d')); 1.5 函数的扩展1.5.1 参数默认值1234567891011121314151617181920function foo(param) &#123; let p = param || 'hello'; console.log(p); -&gt;hello&#125;foo('hi'); -&gt;hifunction foo(param = 'nihao') &#123; console.log(param); -&gt;nihao&#125;foo('hello kitty'); -&gt;hello kitty------------------------------------------function foo(uname = 'lisi', age = 12) &#123; console.log(uname, age); -&gt;lisi 12&#125;foo('zhangsan', 13); -&gt;zhangsan 13foo(); -&gt;lisi 12 1.5.2 参数解构赋值123456function foo(&#123;uname &#x3D; &#39;lisi&#39;, age &#x3D; 13&#125; &#x3D; &#123;&#125;) &#123; console.log(uname, age);&#125;foo(); -&gt;lisi 13foo(&#123;uname: &#39;zhangsan&#39;, age: 15&#125;); -&gt;zhangsan 15 1.5.3 rest 参数（剩余参数）1234567function foo(a, b, ...param) &#123; console.log(a); console.log(b); console.log(param);&#125;foo(1, 2, 3, 4, 5); -&gt;1 2 [3,4,5] 1.5.4 扩展运算符…12345678910111213function foo(a, b, c, d, e, f, g) &#123; console.log(a + b + c + d + e + f + g);&#125;let arr = [1, 2, 3, 4, 5, 6, 7];foo(...arr);----------------------------------------------- // 合并数组let arr1 = [1, 2, 3];let arr2 = [4, 5, 6];let arr3 = [...arr1, ...arr2];console.log(arr3); -&gt;[1,2,3,4,5,6] 1.5.5 箭头函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455var f = v =&gt; v上面的箭头函数等同于：var f = function(v) &#123; return v&#125;-------------------------------------------// 多个参数必须用小括号包住let foo = (a, b) =&gt; &#123; let c = 1; console.log(a + b + c);&#125;;foo(1, 2);let arr = [123, 456, 789];arr.forEach(function (element, index) &#123; console.log(element, index);&#125;);arr.forEach((element, index) =&gt; &#123; console.log(element, index);&#125;);--------------------------------------------------// 箭头函数的注意事项：// 1、箭头函数中this取决于函数的定义，而不是调用function foo() &#123; // 使用call调用foo时，这里的this其实就是call的第一个参数 // console.log(this); setTimeout(() =&gt; &#123; console.log(this.num); &#125;, 100);&#125;foo.call(&#123;num: 1&#125;);--------------------------------------------------// 2、箭头函数不可以newlet foo = () =&gt; &#123; this.num = 123;&#125;;// new foo();--------------------------------------------------// 3、箭头函数不可以使用arguments获取参数列表，可以使用rest参数代替let foo = (a, b) =&gt; &#123; // console.log(a,b); console.log(arguments);//这种方式获取不到实参列表&#125;foo(123, 456);let foo = (...param) =&gt; &#123; console.log(param);&#125;foo(123, 456); 1.6 类与继承123456789101112131415161718192021222324252627282930313233343536373839class Animal &#123; // 静态方法(静态方法只能通过类名调用，不可以使用实例对象调用) static showInfo() &#123; console.log('hi'); &#125; // 构造函数 constructor(name) &#123; this.name = name; &#125; showName() &#123; console.log(this.name); &#125;&#125;let a = new Animal('spike');a.showName();// a.showInfo();Animal.showInfo();------------------------------------------------------------// 类的继承extendsclass Dog extends Animal &#123; constructor(name, color) &#123; super(name);//super用来调用父类 this.color = color; &#125; showColor() &#123; console.log(this.color); &#125;&#125;let d = new Dog('doudou', 'yellow');d.showName();d.showColor();// d.showInfo();Dog.showInfo(); 1.7 对象属性的简洁表示法： 123456789101112131415161718192021var foo = 'bar';var baz = &#123;foo&#125;;baz &#123;foo: \"bar\"&#125;等同于var baz = &#123;foo: foo&#125;----------------------------------------------------------- 除了属性简写，方法也可以简写:var o = &#123; method() &#123; return \"Hello!\" &#125;&#125;等同于var o = &#123; method: function() &#123; return \"Hello!\" &#125;&#125; 2. Babel第一：在项目根目录下创建一个 .babelrc 文件，写入以下内容： 1234&#123; \"presets\": [ ]&#125; 第二：安装对应的转码规则： 1234567891011# ES2015转码规则npm install --save-dev babel-preset-es2015# react转码规则npm install --save-dev babel-preset-react# ES7不同阶段语法提案的转码规则（共有4个阶段），选装一个npm install --save-dev babel-preset-stage-0npm install --save-dev babel-preset-stage-1npm install --save-dev babel-preset-stage-2npm install --save-dev babel-preset-stage-3 第三：将 .babelrc 文件中修改为以下内容： 12345&#123; \"presets\": [ \"es2015\" ]&#125; 第四步（从第四步开始，前三部必不可少）： babel-cli：命令行转码 babel-node：babel-cli工具自带一个babel-node命令，提供一个支持ES6的REPL环境 babel-register：实时转码，所以只适合在开发环境使用 babel-core：如果某些代码需要调用Babel的API进行转码，就要使用babel-core模块 babel-cli： 一种使用方式就是全局安装：npm install -g babel-cli（可以通过 npm root -g 查看全局包安装目录），只要全局安装了 babel-cli，则会在命令行中多出一个命令：babel。 这里如果使用全局安装的 babel-cli 进行转码是没有问题的，但是问题是如果一旦项目给了别人，别人不知道你使用了这个转码工具，所以解决方式就是将 babel-cli 安装到本地项目中： 1npm install --save-dev babel-cli 这种第三方命令行工具如果安装到本地项目，会在 node_modules 中生成一个目录：.bin，然后第三方命令行工具会将对应的可执行文件放到该目录中。这样的话，就可以直接在本地项目中使用该第三方命令行工具了。 对于如何使用，则可以通过配置 package.json 文件中的 scripts 字段来配置使用： 123456789101112&#123; \"name\": \"babel-demo\", \"scripts\": &#123; \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\", \"build\": \"babel demo1.js\" &#125;, \"devDependencies\": &#123; \"babel-cli\": \"^6.22.2\", \"babel-preset-es2015\": \"^6.22.0\", \"babel-preset-react\": \"^6.22.0\" &#125;&#125; babel-register（适合开发阶段，实时编码转换）: 第一：安装 babel-register 1npm install --save-dev babel-register 第二：添加一个傀儡文件(main.js)： 12require('babel-register')require('你的核心功能代码入口文件模块') 第三：使用 node 执行 main.js，而不是你的入口文件。","tags":[{"name":"ECMAScript6","slug":"ECMAScript6","permalink":"https://wgy1993.gitee.io/tags/ECMAScript6/"},{"name":"JavaScript","slug":"JavaScript","permalink":"https://wgy1993.gitee.io/tags/JavaScript/"},{"name":"前端技术","slug":"前端技术","permalink":"https://wgy1993.gitee.io/tags/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"}]},{"title":"Node.js(一)","date":"2020-04-19T07:08:49.000Z","path":"archives/fb0fa11b.html","text":"1. 初识Node.jsNode.js 是一种建立在Google Chrome’s v8 engine上的 non-blocking (非阻塞）, event-driven （基于事件的）I/O平台。 Node.js平台使用的开发语言是JavaScript，平台提供了操作系统低层的API，方便做服务器端编程，具体包括文件操作、进程操作、通信操作等系统模块 2. Node.js可以用来做什么？ 具有复杂逻辑的动态网站 WebSocket服务器 命令行工具 带有图形界面的本地应用程序 …… 3. Node.js开发环境准备1、普通安装方式官方网站 2、多版本安装方式 12345678910112.1 卸载已有的Node.js2.2 下载[nvm](https:&#x2F;&#x2F;github.com&#x2F;coreybutler&#x2F;nvm-windows)2.3 在C盘创建目录dev2.4 在dev目中中创建两个子目录nvm和nodejs2.5 并且把nvm包解压进去nvm目录中2.6 在install.cmd文件上面右键选择【以管理员身份运行】2.7 打开的cmd窗口直接回车会生成一个settings.txt文件，修改文件中配置信息2.8 配置nvm和Node.js环境变量 2.8.1 NVM_HOME:C:\\dev\\nvm 2.8.2 NVM_SYMLINK:C:\\dev\\nodejs2.9 把配置好的两个环境变量加到Path中%NVM_HOME%;%NVM_SYMLINK%; 4. nvm常用的命令 nvm list 查看当前安装的Node.js所有版本 nvm install 版本号 安装指定版本的Node.js nvm uninstall 版本号 卸载指定版本的Node.js nvm use 版本号 选择指定版本的Node.js 5. Node.js之HelloWorld5.1 命令行方式REPLREPL read-eval-print-loop 读取代码-执行-打印结果-循环这个过程 在REPL环境中，_表示最后一次执行结果; .exit 可以退出REPL环境 5.2 运行文件方式123456789101112131415/* 初识Node.js*/console.log('hello tom and jerry');function sum(n) &#123; var result = 0; for (var i = 0; i &lt;= n; i++) &#123; result += i; &#125; return result;&#125;var ret = sum(100);console.log(ret); 6. 全局对象概览1234567891011121314151617181920212223242526/* 全局成员概述*/// 1. 包含文件名称的全路径console.log(__filename);// 2. 文件的路径（不包含文件名称）console.log(__dirname);// 3. 定时函数，用法与浏览器中的定时函数类似var timer = setTimeout(function () &#123; console.log(123);&#125;, 1000);setTimeout(function () &#123; clearTimeout(timer);&#125;, 2000);// 4. 在Node.js中没有window对象，但是有一个类似的对象global，访问全局成员的时候可以省略globalglobal.console.log(123456);// 5. argv是一个数组，默认情况下，前两项数据分别是：Node.js环境的路径、当前执行的js文件的全路径// 从第三个参数开始表示命令行参数console.log(process.argv);// 6. 打印当前系统的架构（64位或者32位）console.log(process.arch); 7. 模块化开发7.1 模块化规范与实现12345678910111213141516传统非模块化开发有如下的缺点：1、命名冲突2、文件依赖前端标准的模块化规范：1、AMD - requirejs2、CMD - seajs服务器端的模块化规范：1、CommonJS - Node.js模块化相关的规则：1、如何定义模块：一个js文件就是一个模块，模块内部的成员都是相互独立2、模块成员的导出和引入模块成员的导出最终以module.exports为准 7.2 模块导出与引入7.2.1 导出方式一：03.js 123456var sum = function(a,b)&#123; return parseInt(a) + parseInt(b);&#125;// 导出模块成员exports.sum = sum; 04.js 12345//引入模块var module = require('./03.js');var ret = module.sum(12,13);console.log(ret); 7.2.2 导出方式二：03.js 123456var sum = function(a,b)&#123; return parseInt(a) + parseInt(b);&#125;// 导出模块成员module.exports = sum; 04.js 12345//引入模块var module = require('./03.js');var ret = module(12,13);console.log(ret); 7.2.3 两种导出方式区别如果要导出单个的成员或者比较少的成员，一般我们使用exports导出；如果要导出的成员比较多，一般我们使用module.exports的方式。 这两种方式不能同时使用，exports与module的关系：module.exports = exports = {}; 7.3 模块导出机制分析05.js 1234567891011121314151617181920212223242526/* 四则运算-成员导出方式分析*/var sum = function (a, b) &#123; return parseInt(a) + parseInt(b);&#125;var subtract = function (a, b) &#123; return parseInt(a) - parseInt(b);&#125;var multiply = function (a, b) &#123; return parseInt(a) * parseInt(b);&#125;var divide = function (a, b) &#123; return parseInt(a) / parseInt(b);&#125;// 导出成员// exports.sum = sum;// exports.subtract = subtract;module.exports = &#123; sum: sum, subtract: subtract, multiply: multiply, divide: divide&#125; 06.js 12345678/* 测试导出*/var m = require('./05.js');var ret = m.sum(1, 2);var ret1 = m.subtract(1, 2);console.log(ret, ret1); 7.4 模块成员导出：global07.js 12var flag = 123;global.flag = flag; 08.js 12require('./07.js');console.log(global.flag); 7.5 模块加载规则已经加载的模块会缓存 模块文件的后缀3种情况：.js .json .node 上述三种模块的加载优先级(不加文件后缀时的优先级)：.js -&gt; .json -&gt; .node data.json 1234&#123; \"username\":\"张三\", \"age\":\"12\"&#125; data.js 123exports.showInfo = function()&#123; console.log('nihao');&#125; 08.js 12var m = require('./data');var ret = m.showInfo(); 7.6 模块分类 自定义模块 系统核心模块 fs 文件操作 http 网络操作 path 路径操作 querystring 查询参数解析 url url解析 …… 8. Buffer基本操作 Buffer对象是Node处理二进制数据的一个接口。它是Node原生提供的全局对象，可以直接使用，不需要require(‘buffer’)。Buffer本质上就是字节数组。 8.1 实例化 Buffer.from(array) Buffer.from(string) Buffer.alloc(size) 123456789let buf = new Buffer(5);//不推荐let buf = Buffer.alloc(5);console.log(buf); -&gt;&lt;Buffer 00 00 00 00 00&gt;let buf = Buffer.from('hello', 'utf8');console.log(buf); -&gt;&lt;Buffer 68 65 6c 6c 6f&gt;let buf = Buffer.from([0x62, 0x75, 0x66, 0x66, 0x65, 0x72]);console.log(buf.toString()); -&gt;buffer 8.2 功能方法 Buffer.isEncoding() 判断是否支持该编码 Buffer.isBuffer() 判断是否为Buffer Buffer.byteLength() 返回指定编码的字节长度，默认utf8 Buffer.concat() 将一组Buffer对象合并为一个Buffer对象 123456789101112131415161718192021console.log(Buffer.isEncoding('utf8')); -&gt;trueconsole.log(Buffer.isEncoding('gbk')); -&gt;falselet buf = Buffer.from('hello');console.log(Buffer.isBuffer(buf)); -&gt;trueconsole.log(Buffer.isBuffer(&#123;&#125;)); -&gt;falselet buf = Buffer.from('中国', 'ascii');console.log(Buffer.byteLength(buf)); -&gt;2console.log(buf.toString());let buf1 = Buffer.alloc(3);let buf2 = Buffer.alloc(5);let buf3 = Buffer.concat([buf1, buf2]);console.log(Buffer.byteLength(buf3)); -&gt;8let buf1 = Buffer.from('tom');let buf2 = Buffer.from('jerry');let buf3 = Buffer.concat([buf1, buf2]);console.log(Buffer.byteLength(buf3)); -&gt;8console.log(buf3.toString()); -&gt;tomjerry 8.3 实例方法 write() 向buffer对象中写入内容 slice() 截取新的buffer对象 toString() 把buf对象转成字符串 toJson() 把buf对象转成json形式的字符串 1234567891011121314let buf = Buffer.alloc(5);buf.write('hello', 2, 2);console.log(buf); -&gt;&lt;Buffer 00 00 68 65 00&gt; 空格空格he空格let buf = Buffer.from('hello');let buf1 = buf.slice(2, 3);console.log(buf === buf1);//falseconsole.log(buf1.toString()); -&gt;l// toJSON方法不需要显式调用，当JSON.stringify方法调用的时候会自动调用toJSON方法const buf = Buffer.from([0x1, 0x2, 0x3, 0x4, 0x5]); -&gt;&#123;\"type\":\"Buffer\",\"data\":[1,2,3,4,5]&#125;const buf = Buffer.from('hello'); -&gt;&#123;\"type\":\"Buffer\",\"data\":[104,101,108,108,111]&#125;const json = JSON.stringify(buf);console.log(json); 9. 核心模块API9.1 路径操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859//引入path模块const path = require('path');// 1. 获取路径的最后一部分console.log(path.basename('/foo/bar/baz/asdf/quux.html')); -&gt;quux.htmlconsole.log(path.basename('/foo/bar/baz/asdf/quux.html', '.html')); -&gt;quux// 2. 获取路径console.log(__dirname);-&gt;D:\\02-codeconsole.log(path.dirname('/abc/qqq/www/abc')); -&gt;/abc/qqq/www// 3. 获取扩展名称console.log(path.extname('index.html')); -&gt;.html// 4. 路径的格式化处理// path.format() obj-&gt;string// path.parse() string-&gt;objlet obj = path.parse(__filename);console.log(obj.base);/*&#123; root: 'D:\\\\', 文件的跟路径 dir: 'D:\\\\02-code',文件的全路径 base: '02.js',文件的名称 ext: '.js',扩展名 name: '02' 文件名称&#125;*/let objpath = &#123; root: 'd:\\\\', dir: 'd:\\\\qqq\\\\www', base: 'abc.txt', ext: '.txt', name: 'abc'&#125;;let strpath = path.format(objpath);console.log(strpath); -&gt;d:\\qqq\\www\\abc.txt// 5. 判断是否为绝对路径console.log(path.isAbsolute('/foo/bar')); -&gt;trueconsole.log(path.isAbsolute('C:/foo/..')); -&gt;true// 6. 拼接路径（..表示上层路径；.表示当前路径）,在连接路径的时候会格式化路径console.log(path.join('/foo', 'bar', 'baz/asdf', 'quux', '../../')); -&gt;\\foo\\bar\\baz\\// 7. 规范化路径console.log(path.normalize('/foo/bar//baz/asdf/quux/..')); -&gt;\\foo\\bar\\baz\\asdfconsole.log(path.normalize('C:\\\\temp\\\\\\\\foo\\\\bar\\\\..\\\\')); -&gt;C:\\temp\\foo\\// 8. 计算相对路径console.log(path.relative('/data/orandea/test/aaa', '/data/orandea/impl/bbb')); -&gt;..\\..\\impl\\bbbconsole.log(path.relative('C:\\\\orandea\\\\test\\\\aaa', 'C:\\\\orandea\\\\impl\\\\bbb')); -&gt;..\\..\\impl\\bbb// 9. 解析路径console.log(path.resolve('wwwroot', 'static_files/png/', '../gif/image.gif')); -&gt;D:\\02-code\\wwwroot\\static_files\\gif\\image.gif// 10. 两个特殊属性console.log(path.delimiter);//环境变量分隔符(windows中使用; linux中使用:) -&gt;;console.log(path.sep);//表示路径分隔符（windows是\\ Linux是/）-&gt;\\ 9.2 文件操作9.2.1 文件信息获取123456789101112131415161718192021222324252627const fs = require('fs');console.log(1);fs.stat('./data.txt', (err, stat) =&gt; &#123; // 一般回调函数的第一个参数是错误对象，如果err为null,表示没有错误，否则表示报错了 if (err) return; if (stat.isFile()) &#123; console.log('文件'); &#125; else if (stat.isDirectory()) &#123; console.log('目录'); &#125; console.log(stat); /* atime 文件访问时间 ctime 文件的状态信息发生变化的时间（比如文件的权限） mtime 文件数据发生变化的时间 birthtime 文件创建的时间 */ console.log(2);&#125;);console.log(3);----------------------------------------------------// 同步操作console.log(1);let ret = fs.statSync('./data.txt');console.log(ret);console.log(2); 9.2.2 读文件操作1234567891011121314151617181920const fs = require('fs');const path = require('path');let strpath = path.join(__dirname, 'data.txt');fs.readFile(strpath, (err, data) =&gt; &#123; if (err) return; console.log(data.toString());&#125;);// 如果有第二个参数并且是编码，那么回调函数获取到的数据就是字符串// 如果没有第二个参数，那么得到的就是Buffer实例对象fs.readFile(strpath, 'utf8', (err, data) =&gt; &#123; if (err) return; // console.log(data.toString()); console.log(data);&#125;);// 同步操作let ret = fs.readFileSync(strpath, 'utf8');console.log(ret); 9.2.3 写文件操作123456789101112131415161718192021222324252627282930313233343536373839404142434445const fs = require('fs');const path = require('path');let strpath = path.join(__dirname,'data.txt');fs.writeFile(strpath,'hello nihao','utf8',(err)=&gt;&#123; if(!err)&#123; console.log('文件写入成功'); &#125;&#125;);let buf = Buffer.from('hi');fs.writeFile(strpath, buf, 'utf8', (err) =&gt; &#123; if (!err) &#123; console.log('文件写入成功'); &#125;&#125;);// 同步操作fs.writeFileSync(strpath,'tom and jerry');----------------------------------------------------------// 大文件操作（流式操作）const path = require('path');const fs = require('fs');let spath = path.join(__dirname,'../03-source','file.zip');let dpath = path.join('C:\\\\Users\\\\www\\\\Desktop','file.zip');let readStream = fs.createReadStream(spath);let writeStream = fs.createWriteStream(dpath);// 基于事件的处理方式let num = 1;readStream.on('data',(chunk)=&gt;&#123; num++; writeStream.write(chunk);&#125;);readStream.on('end',()=&gt;&#123; console.log('文件处理完成'+num);&#125;);// pipe的作用直接把输入流和输出流// readStream.pipe(writeStream);fs.createReadStream(spath).pipe(fs.createWriteStream(dpath)); 9.2.4 目录操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/* 目录操作 1、创建目录 fs.mkdir(path[, mode], callback) fs.mkdirSync(path[, mode]) 2、读取目录 fs.readdir(path[, options], callback) fs.readdirSync(path[, options]) 3、删除目录 fs.rmdir(path, callback) fs.rmdirSync(path)*/const path = require('path');const fs = require('fs');// 1. 创建目录fs.mkdir(path.join(__dirname,'abc'),(err)=&gt;&#123; console.log(err);&#125;);fs.mkdirSync(path.join(__dirname,'hello'));--------------------------------// 2. 读取目录fs.readdir(__dirname,(err,files)=&gt;&#123; files.forEach((item,index)=&gt;&#123; fs.stat(path.join(__dirname,item),(err,stat)=&gt;&#123; if(stat.isFile())&#123; console.log(item,'文件'); &#125;else if(stat.isDirectory())&#123; console.log(item,'目录'); &#125; &#125;); &#125;);&#125;);let files = fs.readdirSync(__dirname);files.forEach((item,index)=&gt;&#123; fs.stat(path.join(__dirname,item),(err,stat)=&gt;&#123; if(stat.isFile())&#123; console.log(item,'文件'); &#125;else if(stat.isDirectory())&#123; console.log(item,'目录'); &#125; &#125;);&#125;);------------------------------// 3. 删除目录fs.rmdir(path.join(__dirname, 'abc'), (err) =&gt; &#123; console.log(err);&#125;);fs.rmdirSync(path.join(__dirname,'qqq')); 10. 包多个模块可以形成包，不过要满足特定的规则才能形成规范的包 10.1 npmNPM （node.js package management）全球最大的模块生态系统，里面所有的模块都是开源免费的；也是Node.js的包管理工具。官方网站 10.2 npm包安装方式 本地安装 本地安装的包在当前目录下的node_modules里面，本地安装的包一般用于实际的开发工作 全局安装-g 全局安装的包位于Node.js环境的node_modules目录下，全局安装的包一般用于命令行工具 10.3 解决npm安装包被墙的问题10.3.1 –registry1npm config set registry=https//registry.npm.taobao.org 10.3.2 cnpm淘宝NPM镜像,与官方NPM的同步频率目前为10分钟一次官网: http://npm.taobao.org/ 12npm install -g cnpm –registry=https//registry.npm.taobao.org 使用cnpm安装包: cnpm install 包名 10.3.3 nrm作用：修改镜像源 项目地址：https://www.npmjs.com/package/nrm 12345安装：npm install -g nrm使用nrm ls 查看当前所有可用的镜像源地址以及当前所使用的镜像源地址。使用nrm use npm 或 nrm use taobao 切换不同的镜像源地址； 10.4 npm常用命令1234567891011开发环境（平时开发使用的环境）生产环境（项目部署上线之后的服务器环境）--save 向生产环境添加依赖 dependencies--save-dev 向开发环境添加依赖 DevDependencies npm install --production 只会安装生产的依赖包（去掉--production安装所有的依赖包）npm cache clean -f 清除缓存npm config set proxy http:&#x2F;&#x2F;127.0.0.1:61235&#x2F; 设置代理 10.4.1 安装包123456// 1. 如果没有指定版本号，那么安装最新版本npm install -g 包名称 (全局安装)npm install 包名称 (本地安装)// 2. 安装包的时候可以指定版本npm install -g 包名称@版本号 10.4.2 更新包123&#x2F;&#x2F; 更新到最新版本npm update -g 包名&#x2F;&#x2F;注：更新包update不一定生效，可用install重新安装最新版 10.4.3 卸载包1npm uninstall -g 包名 10.5 yarn基本使用类比npm基本使用 yrm：yarn地址管理工具，类似nrm 1234567891011121314151617181920212223242526272829安装yarn工具：npm install -g yarn1、初始化包 npm init yarn init2、安装包 npm install xxx --save yarn add xxx3、移除包 npm uninstall xxx yarn remove xxx4、更新包 npm update xxx yarn upgrade xxx5、安装开发依赖的包 npm install xxx --save-dev yarn add xxx --dev6、全局安装 npm install -g xxx yarn global add xxx7、设置下载镜像的地址 npm config set registry url yarn config set registry url8、安装所有依赖 npm install yarn install9、执行包 npm run yarn run 11. 自定义包11.1 包的规范 package.json必须在包的顶层目录下 二进制文件应该在bin目录下 JavaScript代码应该在lib目录下 文档应该在doc目录下 单元测试应该在test目录下 11.2 package.json字段分析 name：包的名称，必须是唯一的，由小写英文字母、数字和下划线组成，不能包含空格 description：包的简要说明 version：符合语义化版本识别规范的版本字符串 keywords：关键字数组，通常用于搜索 maintainers：维护者数组，每个元素要包含name、email（可选）、web（可选）字段 contributors：贡献者数组，格式与maintainers相同。包的作者应该是贡献者数组的第一- 个元素 bugs：提交bug的地址，可以是网站或者电子邮件地址 licenses：许可证数组，每个元素要包含type（许可证名称）和url（链接到许可证文本的- 地址）字段 repositories：仓库托管地址数组，每个元素要包含type（仓库类型，如git）、url（仓- 库的地址）和path（相对于仓库的路径，可选）字段 dependencies：生产环境包的依赖，一个关联数组，由包的名称和版本号组成 devDependencies：开发环境包的依赖，一个关联数组，由包的名称和版本号组成 11.3 自定义包案例1234567891011121314&#123; \"name\": \"hello\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": &#123; \"test\": \"node index.js\" &#125;, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\", \"dependencies\": &#123;&#125;, \"devDependencies\": &#123;&#125;&#125;","tags":[{"name":"前端技术","slug":"前端技术","permalink":"https://wgy1993.gitee.io/tags/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"},{"name":"Node.js","slug":"Node-js","permalink":"https://wgy1993.gitee.io/tags/Node-js/"},{"name":"NPM","slug":"NPM","permalink":"https://wgy1993.gitee.io/tags/NPM/"}]},{"title":"Vue(二)","date":"2020-04-08T06:38:06.000Z","path":"archives/a5e3c20a.html","text":"1. 生命周期什么是生命周期：从Vue实例创建、运行、到销毁期间，总是伴随着各种各样的事件，这些事件，统称为生命周期！ 生命周期钩子：就是生命周期事件的别名而已；生命周期钩子 = 生命周期函数 = 生命周期事件 主要的生命周期函数分类： 创建期间的生命周期函数： beforeCreate：实例刚在内存中被创建出来，此时，还没有初始化好 data 和 methods 属性 created：实例已经在内存中创建OK，此时 data 和 methods 已经创建OK，此时还没有开始编译模板 beforeMount：此时已经完成了模板的编译，但是还没有挂载到页面中 mounted：此时，已经将编译好的模板，挂载到了页面指定的容器中显示 运行期间的生命周期函数： beforeUpdate：状态更新之前执行此函数， 此时 data 中的状态值是最新的，但是界面上显示的数据还是旧的，因为此时还没有开始重新渲染DOM节点 updated：实例更新完毕之后调用此函数，此时 data 中的状态值 和 界面上显示的数据，都已经完成了更新，界面已经被重新渲染好了！ 销毁期间的生命周期函数： beforeDestroy：实例销毁之前调用。在这一步，实例仍然完全可用。 destroyed：Vue 实例销毁后调用。调用后，Vue 实例指示的所有东西都会解绑定，所有的事件监听器会被移除，所有的子实例也会被销毁。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; msg: 'ok' &#125;, methods: &#123; show() &#123; console.log('执行了show方法') &#125; &#125;, beforeCreate() &#123; // 这是我们遇到的第一个生命周期函数，表示实例完全被创建出来之前，会执行它 // console.log(this.msg) // this.show() // 注意： 在 beforeCreate 生命周期函数执行的时候，data 和 methods 中的 数据都还没有没初始化 &#125;, created() &#123; // 这是遇到的第二个生命周期函数 // console.log(this.msg) // this.show() // 在 created 中，data 和 methods 都已经被初始化好了！ // 如果要调用 methods 中的方法，或者操作 data 中的数据，最早，只能在 created 中操作 &#125;, beforeMount() &#123; // 这是遇到的第3个生命周期函数，表示 模板已经在内存中编辑完成了，但是尚未把模板渲染到页面中 // console.log(document.getElementById('h3').innerText) // 在 beforeMount 执行的时候，页面中的元素，还没有被真正替换过来，只是之前写的一些模板字符串 &#125;, mounted() &#123; // 这是遇到的第4个生命周期函数，表示，内存中的模板，已经真实的挂载到了页面中，用户已经可以看到渲染好的页面了 // console.log(document.getElementById('h3').innerText) // 注意： mounted 是 实例创建期间的最后一个生命周期函数，当执行完 mounted 就表示，实例已经被完全创建好了，此时， // 如果没有其它操作的话，这个实例，就静静的 躺在我们的内存中，一动不动 &#125;, // 接下来的是运行中的两个事件 beforeUpdate() &#123; // 这时候，表示 我们的界面还没有被更新【数据被更新了吗？ 数据肯定被更新了】 /* console.log('界面上元素的内容：' + document.getElementById('h3').innerText) console.log('data 中的 msg 数据是：' + this.msg) */ //得出结论： 当执行 beforeUpdate 的时候，页面中的显示的数据，还是旧的，此时 data 数据是最新的， // 页面尚未和最新的数据保持同步 &#125;, updated() &#123; console.log('界面上元素的内容：' + document.getElementById('h3').innerText) console.log('data 中的 msg 数据是：' + this.msg) // updated 事件执行的时候，页面和 data 数据已经保持同步了，都是最新的 &#125; &#125;);&lt;/script&gt; 2. vue-resource 实现 get, post, jsonp请求2.1 引用直接在页面中，通过script标签，引入 vue-resource 的脚本文件； 注意：引用的先后顺序是：先引用 Vue 的脚本文件，再引用 vue-resource 的脚本文件； 12345678910&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;Document&lt;/title&gt; &lt;script src=\"./lib/vue-2.4.0.js\"&gt;&lt;/script&gt; &lt;!-- 注意：vue-resource 依赖于 Vue，所以先后顺序要注意 --&gt; &lt;!-- this.$http.jsonp --&gt; &lt;script src=\"./lib/vue-resource-1.3.4.js\"&gt;&lt;/script&gt;&lt;/head&gt; 2.2 发起get请求1234567getInfo() &#123; // 发起get请求 // 当发起get请求之后， 通过 .then 来设置成功的回调函数 this.$http.get('http://vue.studyit.io/api/getlunbo').then(function (result) &#123; // 通过 result.body 拿到服务器返回的成功的数据 console.log(result.body) &#125;)&#125; 2.3 发送post请求12345678910postInfo() &#123; var url = 'http://127.0.0.1:8899/api/post'; // post 方法接收三个参数： // 参数1： 要请求的URL地址 // 参数2： 要发送的数据对象 // 参数3： 指定post提交的编码类型为 application/x-www-form-urlencoded this.$http.post(url, &#123; name: 'zs' &#125;, &#123; emulateJSON: true &#125;).then(res =&gt; &#123; console.log(res.body); &#125;);&#125; 2.4 发送JSONP请求12345jsonpInfo() &#123; // 发起JSONP 请求 this.$http.jsonp('http://vue.studyit.io/api/jsonp').then(result =&gt; &#123; console.log(result.body) &#125;)&#125; 2.5 全局配置请求路径如果我们通过全局配置了请求的数据接口根域名，则在每次单独发起 http 请求的时候，请求的 url 路径，应该以相对路径开头，前面不能带 / ，否则不会启用根路径做拼接； 1234567891011121314151617&lt;script&gt; Vue.http.options.root = 'http://vue.studyit.io/'; // 全局启用 emulateJSON 选项 Vue.http.options.emulateJSON = true; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', methods: &#123; getAllList() &#123; this.$http.get('api/getprodlist').then(result =&gt; &#123; &#125;) &#125; &#125; &#125;);&lt;/script&gt; 3. 动画动画能够提高用户的体验，帮助用户更好的理解页面中的功能； 3.1 不使用动画12345678910111213141516&lt;div id=\"app\"&gt; &lt;input type=\"button\" value=\"toggle\" @click=\"flag=!flag\"&gt; &lt;!-- 需求： 点击按钮，让 h3 显示，再点击，让 h3 隐藏 --&gt; &lt;h3 v-if=\"flag\"&gt;这是一个H3&lt;/h3&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; flag: false &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 3.2 使用过渡类名实现动画123456789101112131415161718192021222324252627282930313233343536373839&lt;div id=\"app\"&gt; &lt;input type=\"button\" value=\"toggle\" @click=\"flag=!flag\"&gt; &lt;!-- 需求： 点击按钮，让 h3 显示，再点击，让 h3 隐藏 --&gt; &lt;!-- 1. 使用 transition 元素，把 需要被动画控制的元素，包裹起来 --&gt; &lt;!-- transition 元素，是 Vue 官方提供的 --&gt; &lt;transition&gt; &lt;h3 v-if=\"flag\"&gt;这是一个H3&lt;/h3&gt; &lt;/transition&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; flag: false &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt;&lt;!-- 2. 自定义两组样式，来控制 transition 内部的元素实现动画 --&gt;&lt;style&gt; /* v-enter 【这是一个时间点】 是进入之前，元素的起始状态，此时还没有开始进入 */ /* v-leave-to 【这是一个时间点】 是动画离开之后，离开的终止状态，此时，元素 动画已经结束了 */ .v-enter, .v-leave-to &#123; opacity: 0; transform: translateX(150px); &#125; /* v-enter-active 【入场动画的时间段】 */ /* v-leave-active 【离场动画的时间段】 */ .v-enter-active, .v-leave-active &#123; transition: all 0.8s ease; &#125;&lt;/style&gt; 3.3 修改v-前缀123456789101112131415161718192021222324252627282930313233&lt;div id=\"app\"&gt; &lt;input type=\"button\" value=\"toggle2\" @click=\"flag2=!flag2\"&gt; &lt;transition name=\"my\"&gt; &lt;h6 v-if=\"flag2\"&gt;这是一个H6&lt;/h6&gt; &lt;/transition&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; flag2: false &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt;&lt;!-- 2. 自定义两组样式，来控制 transition 内部的元素实现动画 --&gt;&lt;style&gt; .my-enter, .my-leave-to &#123; opacity: 0; transform: translateY(70px); &#125; .my-enter-active, .my-leave-active &#123; transition: all 0.8s ease; &#125;&lt;/style&gt; 3.4 使用第三方 CSS 动画库1、导入动画类库 1&lt;link rel&#x3D;&quot;stylesheet&quot; type&#x3D;&quot;text&#x2F;css&quot; href&#x3D;&quot;.&#x2F;lib&#x2F;animate.css&quot;&gt; 2、定义 transition 及属性 1234567&lt;!-- 使用 :duration=\"&#123; enter: 200, leave: 400 &#125;\" 来分别设置 入场的时长 和 离场的时长 --&gt;&lt;transition enter-active-class=\"bounceIn\" leave-active-class=\"bounceOut\" :duration=\"&#123; enter: 200, leave: 400 &#125;\"&gt; &lt;h3 v-if=\"flag\" class=\"animated\"&gt;这是一个H3&lt;/h3&gt;&lt;/transition&gt; 3.5 使用动画钩子函数1、定义 transition 组件以及三个钩子函数： 12345678910&lt;div id=\"app\"&gt; &lt;input type=\"button\" value=\"快到碗里来\" @click=\"flag=!flag\"&gt; &lt;!-- 1. 使用 transition 元素把 小球包裹起来 --&gt; &lt;transition @before-enter=\"beforeEnter\" @enter=\"enter\" @after-enter=\"afterEnter\"&gt; &lt;div class=\"ball\" v-show=\"flag\"&gt;&lt;/div&gt; &lt;/transition&gt;&lt;/div&gt; 定义三个 methods 钩子方法： 12345678910111213141516171819202122232425262728293031323334&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; isshow: false &#125;, methods: &#123; // 注意： 动画钩子函数的第一个参数：el，表示要执行动画的那个DOM元素，是个原生的JS DOM对象 // 大家可以认为 ， el 是通过 document.getElementById('') 方式获取到的原生JS DOM对象 beforeEnter(el) &#123; // beforeEnter 表示动画入场之前，此时，动画尚未开始，可以在 beforeEnter 中，设置元素开始动画之前的起始样式 // 设置小球开始动画之前的，起始位置 el.style.transform = \"translate(0, 0)\" &#125;, enter(el, done) &#123; // 这句话，没有实际的作用，但是，如果不写，出不来动画效果； // 可以认为 el.offsetWidth 会强制动画刷新 el.offsetWidth; // enter 表示动画 开始之后的样式，这里，可以设置小球完成动画之后的，结束状态 el.style.transform = \"translate(150px, 450px)\"; el.style.transition = 'all 1s ease'; // 这里的 done， 起始就是 afterEnter 这个函数，也就是说：done 是 afterEnter 函数的引用 done() &#125;, afterEnter(el) &#123; // 动画完成之后，会调用 afterEnter // console.log('ok') this.flag = !this.flag &#125; &#125; &#125;);&lt;/script&gt; 定义样式： 123456.ball &#123; width: 15px; height: 15px; border-radius: 50%; background-color: red;&#125; 3.6 列表动画1234567891011&lt;!-- &lt;ul&gt; --&gt;&lt;!-- 在实现列表过渡的时候，如果需要过渡的元素，是通过 v-for 循环渲染出来的，不能使用 transition 包裹，需要使用 transitionGroup --&gt;&lt;!-- 如果要为 v-for 循环创建的元素设置动画，必须为每一个 元素 设置 :key 属性 --&gt;&lt;!-- 给 transition-group 添加 appear 属性，实现页面刚展示出来时候，入场时候的效果 --&gt;&lt;!-- 通过 为 transition-group 元素，设置 tag 属性，指定 transition-group 渲染为指定的元素，如果不指定 tag 属性，默认，渲染为 span 标签 --&gt;&lt;transition-group appear tag=\"ul\"&gt; &lt;li v-for=\"(item, i) in list\" :key=\"item.id\" @click=\"del(i)\"&gt; &#123; &#123;item.id&#125; &#125; --- &#123; &#123;item.name&#125; &#125; &lt;/li&gt;&lt;/transition-group&gt;&lt;!-- &lt;/ul&gt; --&gt; &lt;transition-group&gt; 组件还有一个特殊之处。不仅可以进入和离开动画，还可以改变定位。要使用这个新功能只需了解新增的 v-move 特性，它会在元素的改变定位的过程中应用。 123456789101112131415161718192021222324252627282930313233343536&lt;style&gt; li &#123; border: 1px dashed #999; margin: 5px; line-height: 35px; padding-left: 5px; font-size: 12px; width: 100%; &#125; li:hover &#123; background-color: hotpink; transition: all 0.8s ease; &#125; .v-enter, .v-leave-to &#123; opacity: 0; transform: translateY(80px); &#125; .v-enter-active, .v-leave-active &#123; transition: all 0.6s ease; &#125; /* 下面的 .v-move 和 .v-leave-active 配合使用，能够实现列表后续的元素，渐渐地漂上来的效果 */ .v-move &#123; transition: all 0.6s ease; &#125; .v-leave-active &#123; position: absolute; &#125;&lt;/style&gt; 4. 组件什么是组件： 组件的出现，就是为了拆分Vue实例的代码量的，能够让我们以不同的组件，来划分不同的功能模块，将来我们需要什么样的功能，就可以去调用对应的组件即可；组件化和模块化的不同： 模块化： 是从代码逻辑的角度进行划分的；方便代码分层开发，保证每个功能模块的职能单一； 组件化： 是从UI界面的角度进行划分的；前端的组件化，方便UI组件的重用； 4.1 组件定义的三种方式：1、使用 Vue.extend 配合 Vue.component 方法 12345678910111213141516171819202122232425262728293031&lt;div id=\"app\"&gt; &lt;!-- 如果要使用组件，直接，把组件的名称，以 HTML 标签的形式，引入到页面中，即可 --&gt; &lt;mycom1&gt;&lt;/mycom1&gt;&lt;/div&gt;&lt;script&gt; // 1.1 使用 Vue.extend 来创建全局的Vue组件 // var com1 = Vue.extend(&#123; // template: '&lt;h3&gt;这是使用 Vue.extend 创建的组件&lt;/h3&gt;' // 通过 template 属性，指定了组件要展示的HTML结构 // &#125;) // 1.2 使用 Vue.component('组件的名称', 创建出来的组件模板对象) // Vue.component('myCom1', com1) // 如果使用 Vue.component 定义全局组件的时候，组件名称使用了 驼峰命名，则在引用组件的时候，需要把 大写的驼峰改为小写的字母，同时，两个单词之前，使用 - 链接； // 如果不使用驼峰,则直接拿名称来使用即可; // Vue.component('mycom1', com1) // Vue.component 第一个参数:组件的名称,将来在引用组件的时候,就是一个 标签形式 来引入 它的 // 第二个参数: Vue.extend 创建的组件 ,其中 template 就是组件将来要展示的HTML内容 Vue.component('mycom1', Vue.extend(&#123; template: '&lt;h3&gt;这是使用 Vue.extend 创建的组件&lt;/h3&gt;' &#125;)); // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 2、直接使用 Vue.component 方法 123456789101112131415161718&lt;div id=\"app\"&gt; &lt;!-- 还是使用 标签形式,引入自己的组件 --&gt; &lt;mycom2&gt;&lt;/mycom2&gt;&lt;/div&gt;&lt;script&gt; // 注意:不论是哪种方式创建出来的组件,组件的 template 属性指向的模板内容,必须有且只能有唯一的一个根元素 Vue.component('mycom2', &#123; template: '&lt;div&gt;&lt;h3&gt;这是直接使用 Vue.component 创建出来的组件&lt;/h3&gt;&lt;span&gt;123&lt;/span&gt;&lt;/div&gt;' &#125;) // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 3、引用template标签 123456789101112131415161718192021222324&lt;div id=\"app\"&gt; &lt;mycom3&gt;&lt;/mycom3&gt;&lt;/div&gt;&lt;!-- 在被控制的 #app 外面,使用 template 元素,定义组件的HTML模板结构 --&gt;&lt;template id=\"tmpl\"&gt; &lt;div&gt; &lt;h1&gt;这是通过 template 元素,在外部定义的组件结构,这个方式,有代码的只能提示和高亮&lt;/h1&gt; &lt;h4&gt;好用,不错!&lt;/h4&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; Vue.component('mycom3', &#123; template: '#tmpl' &#125;); // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 注意： 组件中的DOM结构，有且只能有唯一的根元素（Root Element）来进行包裹！ 4.2 私有组件12345678910111213141516171819202122&lt;div id=\"app2\"&gt; &lt;login&gt;&lt;/login&gt;&lt;/div&gt;&lt;template id=\"tmpl2\"&gt; &lt;h1&gt;这是私有的 login 组件&lt;/h1&gt;&lt;/template&gt;&lt;script&gt; var vm2 = new Vue(&#123; el: '#app2', data: &#123;&#125;, methods: &#123;&#125;, filters: &#123;&#125;, directives: &#123;&#125;, components: &#123; // 定义实例内部私有组件的 login: &#123; template: '#tmpl2' &#125; &#125; &#125;)&lt;/script&gt; 4.3 组件中展示数据和响应事件12345678910111213141516171819202122232425&lt;div id=\"app\"&gt; &lt;mycom1&gt;&lt;/mycom1&gt;&lt;/div&gt;&lt;script&gt; // 1. 组件可以有自己的 data 数据 // 2. 组件的 data 和 实例的 data 有点不一样,实例中的 data 可以为一个对象,但是 组件中的 data 必须是一个方法 // 3. 组件中的 data 除了必须为一个方法之外,这个方法内部,还必须返回一个对象才行; // 4. 组件中 的data 数据,使用方式,和实例中的 data 使用方式完全一样!!! Vue.component('mycom1', &#123; template: '&lt;h1&gt;这是全局组件 --- &#123; &#123;msg&#125; &#125;&lt;/h1&gt;', data: function () &#123; return &#123; msg: '这是组件的中data定义的数据' &#125; &#125; &#125;); // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 5. 组件切换5.1 flag标识符结合v-if和v-else12345678910111213141516171819202122232425&lt;div id=\"app\"&gt; &lt;a href=\"\" @click.prevent=\"flag=true\"&gt;登录&lt;/a&gt; &lt;a href=\"\" @click.prevent=\"flag=false\"&gt;注册&lt;/a&gt; &lt;login v-if=\"flag\"&gt;&lt;/login&gt; &lt;register v-else=\"flag\"&gt;&lt;/register&gt;&lt;/div&gt;&lt;script&gt; Vue.component('login', &#123; template: '&lt;h3&gt;登录组件&lt;/h3&gt;' &#125;); Vue.component('register', &#123; template: '&lt;h3&gt;注册组件&lt;/h3&gt;' &#125;); // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; flag: false &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 5.2 使用Vue提供的compoent12345678910111213141516171819202122232425262728&lt;div id=\"app\"&gt; &lt;a href=\"\" @click.prevent=\"comName='login'\"&gt;登录&lt;/a&gt; &lt;a href=\"\" @click.prevent=\"comName='register'\"&gt;注册&lt;/a&gt; &lt;!-- Vue提供了 component ,来展示对应名称的组件 --&gt; &lt;!-- component 是一个占位符, :is 属性,可以用来指定要展示的组件的名称 --&gt; &lt;component :is=\"comName\"&gt;&lt;/component&gt;&lt;/div&gt;&lt;script&gt; // 组件名称是 字符串 Vue.component('login', &#123; template: '&lt;h3&gt;登录组件&lt;/h3&gt;' &#125;) Vue.component('register', &#123; template: '&lt;h3&gt;注册组件&lt;/h3&gt;' &#125;) // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; comName: 'login' // 当前 component 中的 :is 绑定的组件的名称 &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 5.3 切换动画123456789101112131415161718&lt;!-- 通过 mode 属性,设置组件切换时候的 模式 --&gt;&lt;transition mode=\"out-in\"&gt; &lt;component :is=\"comName\"&gt;&lt;/component&gt;&lt;/transition&gt;&lt;style&gt; .v-enter, .v-leave-to &#123; opacity: 0; transform: translateX(150px); &#125; .v-enter-active, .v-leave-active &#123; transition: all 0.5s ease; &#125;&lt;/style&gt; 6. 组件传值6.1 父组件向子组件传值1234567891011121314151617181920212223242526272829303132333435363738&lt;div id=\"app\"&gt; &lt;!-- 父组件，可以在引用子组件的时候， 通过 属性绑定（v-bind:） 的形式, 把 需要传递给 子组件的数据，以属性绑定的形式，传递到子组件内部，供子组件使用 --&gt; &lt;com1 v-bind:parentmsg=\"msg\"&gt;&lt;/com1&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; msg: '123 啊-父组件中的数据' &#125;, methods: &#123;&#125;, components: &#123; // 结论：经过演示，发现，子组件中，默认无法访问到 父组件中的 data 上的数据 和 methods 中的方法 com1: &#123; data() &#123; // 注意： 子组件中的 data 数据，并不是通过 父组件传递过来的，而是子组件自身私有的， // 比如： 子组件通过 Ajax ，请求回来的数据，都可以放到 data 身上； // data 上的数据，都是可读可写的； return &#123; title: '123', content: 'qqq' &#125; &#125;, template: '&lt;h1 @click=\"change\"&gt;这是子组件 --- &#123; &#123; parentmsg &#125; &#125;&lt;/h1&gt;', // 注意： 组件中的 所有 props 中的数据，都是通过 父组件传递给子组件的 // props 中的数据，都是只读的，无法重新赋值 props: ['parentmsg'], // 把父组件传递过来的 parentmsg 属性，先在 props 数组中，定义一下，这样，才能使用这个数据 methods: &#123; change() &#123; this.parentmsg = '被修改了' &#125; &#125; &#125; &#125; &#125;);&lt;/script&gt; 6.2 父组件向子组件传递方法子组件向父组件传值 原理：父组件将方法的引用，传递到子组件内部，子组件在内部调用父组件传递过来的方法，同时把要发送给父组件的数据，在调用方法的时候当作参数传递进去； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;div id=\"app\"&gt; &lt;!-- 父组件向子组件传递方法，使用的是事件绑定机制；v-on, 当我们自定义了 一个事件属性之后， 那么，子组件就能够，通过某些方式，来调用传递进去的这个方法了 --&gt; &lt;com2 @func=\"show\"&gt;&lt;/com2&gt;&lt;/div&gt;&lt;template id=\"tmpl\"&gt; &lt;div&gt; &lt;h1&gt;这是子组件&lt;/h1&gt; &lt;input type=\"button\" value=\"这是子组件中的按钮 - 点击它，触发 父组件传递过来的 func 方法\" @click=\"myclick\"&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; // 定义了一个字面量类型的 组件模板对象 var com2 = &#123; template: '#tmpl', // 通过指定了一个 Id, 表示说，要去加载 这个指定Id的 template 元素中的内容，当作组件的HTML结构 data() &#123; return &#123; sonmsg: &#123;name: '小头儿子', age: 6&#125; &#125; &#125;, methods: &#123; myclick() &#123; // 当点击子组件的按钮的时候，如何 拿到父组件传递过来的 func 方法，并调用这个方法？？？ // emit 英文原意： 是触发，调用、发射的意思 // this.$emit('func123', 123, 456) this.$emit('func', this.sonmsg) &#125; &#125; &#125; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; datamsgFormSon: null &#125;, methods: &#123; show(data) &#123; // console.log('调用了父组件身上的 show 方法: --- ' + data) this.datamsgFormSon = data &#125; &#125;, components: &#123; com2 &#125; &#125;);&lt;/script&gt; 7. 使用ref获取DOM元素和组件引用1234567891011121314151617181920212223242526272829303132333435363738&lt;div id=\"app\"&gt; &lt;input type=\"button\" value=\"获取元素\" @click=\"getElement\" ref=\"mybtn\"&gt; &lt;h3 id=\"myh3\" ref=\"myh3\"&gt;哈哈哈， 今天天气太好了！！！&lt;/h3&gt; &lt;hr&gt; &lt;login ref=\"mylogin\"&gt;&lt;/login&gt;&lt;/div&gt;&lt;script&gt; var login = &#123; template: '&lt;h1&gt;登录组件&lt;/h1&gt;', data() &#123; return &#123; msg: 'son msg' &#125; &#125;, methods: &#123; show() &#123; console.log('调用了子组件的方法') &#125; &#125; &#125; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123; getElement() &#123; console.log(document.getElementById('myh3').innerText); // ref 是 英文单词 【reference】 值类型 和 引用类型 referenceError console.log(this.$refs.myh3.innerText); console.log(this.$refs.mylogin.msg); this.$refs.mylogin.show(); &#125; &#125;, components: &#123; login &#125; &#125;);&lt;/script&gt; 8. 路由后端路由：对于普通的网站，所有的超链接都是URL地址，所有的URL地址都对应服务器上对应的资源； 前端路由：对于单页面应用程序来说，主要通过URL中的hash(#号)来实现不同页面之间的切换，同时，hash有一个特点：HTTP请求中不会包含hash相关的内容；所以，单页面程序中的页面跳转主要用hash实现； 在单页面应用程序中，这种通过hash改变来切换页面的方式，称作前端路由（区别于后端路由）； 8.1 在 vue 中使用 vue-router8.1.1 导入 vue-router 组件类库123&lt;script src=\"./lib/vue-2.4.0.js\"&gt;&lt;/script&gt;&lt;!-- 1. 安装 vue-router 路由模块 --&gt;&lt;script src=\"./lib/vue-router-3.0.1.js\"&gt;&lt;/script&gt; 8.1.2 创建组件的模板对象12345678// 组件的模板对象var login = &#123; template: '&lt;h1&gt;登录组件&lt;/h1&gt;'&#125;;var register = &#123; template: '&lt;h1&gt;注册组件&lt;/h1&gt;'&#125;; 8.1.3 创建一个路由对象当 导入 vue-router 包之后，在 window 全局对象中，就有了一个 路由的构造函数，叫做 VueRouter 123456789101112131415// 在 new 路由对象的时候，可以为 构造函数，传递一个配置对象var routerObj = new VueRouter(&#123; // route // 这个配置对象中的 route 表示 【路由匹配规则】 的意思 routes: [ // 路由匹配规则 // 每个路由规则，都是一个对象，这个规则对象，身上，有两个必须的属性： // 属性1 是 path， 表示监听 哪个路由链接地址； // 属性2 是 component， 表示，如果 路由是前面匹配到的 path ，则展示 component 属性对应的那个组件 // 注意： component 的属性值，必须是一个 组件的模板对象， 不能是 组件的引用名称； // &#123; path: '/', component: login &#125;, &#123; path: '/', redirect: '/login' &#125;, // 这里的 redirect 和 Node 中的 redirect 完全是两码事 &#123; path: '/login', component: login &#125;, &#123; path: '/register', component: register &#125; ], linkActiveClass: 'myactive'&#125;); 8.1.4 创建 Vue 实例，得到 ViewModel1234567// 创建 Vue 实例，得到 ViewModelvar vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125;, router: routerObj // 将路由规则对象，注册到 vm 实例上，用来监听 URL 地址的变化，然后展示对应的组件&#125;); 8.1.5 使用 router-link 组件来导航123456&lt;!-- &lt;a href=\"#/login\"&gt;登录&lt;/a&gt; --&gt;&lt;!-- &lt;a href=\"#/register\"&gt;注册&lt;/a&gt; --&gt;&lt;!-- router-link 默认渲染为一个a 标签 --&gt;&lt;router-link to=\"/login\" tag=\"span\"&gt;登录&lt;/router-link&gt;&lt;router-link to=\"/register\"&gt;注册&lt;/router-link&gt; 8.1.6 使用 router-view 组件来显示匹配到的组件123&lt;!-- 这是 vue-router 提供的元素，专门用来 当作占位符的，将来，路由规则，匹配到的组件，就会展示到这个 router-view 中去 --&gt;&lt;!-- 所以： 我们可以把 router-view 认为是一个占位符 --&gt;&lt;router-view&gt;&lt;/router-view&gt; 8.2 使用tag属性指定router-link渲染的标签类型12&lt;!-- router-link 默认渲染为一个a 标签 --&gt;&lt;router-link to=\"/login\" tag=\"span\"&gt;登录&lt;/router-link&gt; 8.3 设置路由重定向redirect123routes: [ // 路由匹配规则 &#123; path: '/', redirect: '/login' &#125; // 这里的 redirect 和 Node 中的 redirect 完全是两码事] 8.4 设置路由高亮router-link-active123456789101112131415161718&lt;style&gt; .router-link-active, .myactive &#123; color: red; font-weight: 800; font-style: italic; font-size: 80px; text-decoration: underline; background-color: green; &#125;&lt;/style&gt;var routerObj = new VueRouter(&#123; routes: [ // 路由匹配规则 &#123; path: '/register', component: register &#125; ], linkActiveClass: 'myactive'&#125;); 8.5 设置路由切换动效12345678910111213141516&lt;style&gt; .v-enter, .v-leave-to &#123; opacity: 0; transform: translateX(140px); &#125; .v-enter-active, .v-leave-active &#123; transition: all 0.5s ease; &#125;&lt;/style&gt;&lt;transition mode=\"out-in\"&gt; &lt;router-view&gt;&lt;/router-view&gt;&lt;/transition&gt; 9. 在路由规则中定义参数9.1 通过 this.$route.query来获取路由中的参数123456789101112131415&lt;!-- 如果在路由中，使用 查询字符串，给路由传递参数，则 不需要修改 路由规则的 path 属性 --&gt;&lt;router-link to=\"/login?id=10&amp;name=zs\"&gt;登录&lt;/router-link&gt;var login = &#123; template: '&lt;h1&gt;登录 --- &#123; &#123; $route.query.id &#125; &#125; --- &#123; &#123; $route.query.name &#125; &#125;&lt;/h1&gt;', data() &#123; return &#123; msg: '123' &#125; &#125;, created() &#123; // 组件的生命周期钩子函数 // console.log(this.$route) console.log(this.$route.query.id) &#125;&#125;; 9.2 通过 this.$route.params来获取路由中的参数12345678910111213141516171819202122232425&lt;!-- 如果在路由中，使用 查询字符串，给路由传递参数，则 不需要修改 路由规则的 path 属性 --&gt;&lt;router-link to=\"/login/12/ls\"&gt;登录&lt;/router-link&gt;var login = &#123; template: '&lt;h1&gt;登录 --- &#123; &#123; $route.params.id &#125; &#125; --- &#123; &#123; $route.params.name &#125; &#125;&lt;/h1&gt;', data() &#123; return &#123; msg: '123' &#125; &#125;, created() &#123; // 组件的生命周期钩子函数 console.log(this.$route.params.id) &#125;&#125;;var register = &#123; template: '&lt;h1&gt;注册&lt;/h1&gt;'&#125;;var router = new VueRouter(&#123; routes: [ &#123;path: '/login/:id/:name', component: login&#125;, &#123;path: '/register', component: register&#125; ]&#125;); 10. 路由嵌套10.1 定义组件12345678910111213141516171819202122232425&lt;template id=\"tmpl\"&gt; &lt;div&gt; &lt;h1&gt;这是 Account 组件&lt;/h1&gt; &lt;router-link to=\"/account/login\"&gt;登录&lt;/router-link&gt; &lt;router-link to=\"/account/register\"&gt;注册&lt;/router-link&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; // 组件的模板对象 var account = &#123; template: '#tmpl' &#125;; var login = &#123; template: '&lt;h3&gt;登录&lt;/h3&gt;' &#125;; var register = &#123; template: '&lt;h3&gt;注册&lt;/h3&gt;' &#125;;&lt;/script&gt; 10.2 定义router123456789101112131415161718192021var router = new VueRouter(&#123; routes: [ &#123; path: '/account', component: account, // 使用 children 属性，实现子路由，同时，子路由的 path 前面，不要带 / ， // 否则永远以根路径开始请求，这样不方便我们用户去理解URL地址 children: [ &#123;path: 'login', component: login&#125;, &#123;path: 'register', component: register&#125; ] &#125; // &#123; path: '/account/login', component: login &#125;, // &#123; path: '/account/register', component: register &#125; ]&#125;);&lt;div id=\"app\"&gt; &lt;router-link to=\"/account\"&gt;Account&lt;/router-link&gt; &lt;router-view&gt;&lt;/router-view&gt;&lt;/div&gt; 11. 命名视图实现经典布局1234567891011121314151617181920212223242526272829303132333435363738&lt;div id=\"app\"&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;div class=\"container\"&gt; &lt;router-view name=\"left\"&gt;&lt;/router-view&gt; &lt;router-view name=\"main\"&gt;&lt;/router-view&gt; &lt;/div&gt;&lt;/div&gt;&lt;script&gt; var header = &#123; template: '&lt;h1 class=\"header\"&gt;Header头部区域&lt;/h1&gt;' &#125;; var leftBox = &#123; template: '&lt;h1 class=\"left\"&gt;Left侧边栏区域&lt;/h1&gt;' &#125;; var mainBox = &#123; template: '&lt;h1 class=\"main\"&gt;mainBox主体区域&lt;/h1&gt;' &#125;; // 创建路由对象 var router = new VueRouter(&#123; routes: [ &#123; path: '/', components: &#123; 'default': header, 'left': leftBox, 'main': mainBox &#125; &#125; ] &#125;); // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125;, router &#125;);&lt;/script&gt; 12. 监听12.1 methods keyup事件监听12345678910111213141516171819202122232425262728&lt;div id=\"app\"&gt; &lt;!-- 分析： --&gt; &lt;!-- 1. 我们要监听到 文本框数据的改变，这样才能知道 什么时候去拼接 出一个 fullname --&gt; &lt;!-- 2. 如何监听到 文本框的数据改变呢？？？ --&gt; &lt;input type=\"text\" v-model=\"firstname\" @keyup=\"getFullname\"&gt; + &lt;input type=\"text\" v-model=\"lastname\" @keyup=\"getFullname\"&gt; = &lt;input type=\"text\" v-model=\"fullname\"&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; firstname: '', lastname: '', fullname: '' &#125;, methods: &#123; getFullname() &#123; this.fullname = this.firstname + '-' + this.lastname &#125; &#125; &#125;);&lt;/script&gt; 12.2 watch监听123456789101112131415161718192021222324252627282930&lt;div id=\"app\"&gt; &lt;input type=\"text\" v-model=\"firstname\"&gt; + &lt;input type=\"text\" v-model=\"lastname\"&gt; = &lt;input type=\"text\" v-model=\"fullname\"&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; firstname: '', lastname: '', fullname: '' &#125;, methods: &#123;&#125;, watch: &#123; // 使用这个 属性，可以监视 data 中指定数据的变化，然后触发这个 watch 中对应的 function 处理函数 'firstname': function (newVal, oldVal) &#123;// 第一个参数是新数据，第二个参数是旧数据 // console.log('监视到了 firstname 的变化') // this.fullname = this.firstname + '-' + this.lastname this.fullname = newVal + '-' + this.lastname &#125;, 'lastname': function (newVal) &#123; this.fullname = this.firstname + '-' + newVal &#125; &#125; &#125;);&lt;/script&gt; 12.3 监视路由地址的改变1234567891011121314151617181920212223242526272829// 3. 创建一个路由对象var router = new VueRouter(&#123; routes: [ // 路由规则数组 &#123;path: '/', redirect: '/login'&#125;, &#123;path: '/login', component: login&#125;, &#123;path: '/register', component: register&#125; ], linkActiveClass: 'myactive' // 和激活相关的类&#125;)// 创建 Vue 实例，得到 ViewModelvar vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125;, // router: router router, watch: &#123; // this.$route.path '$route.path': function (newVal, oldVal) &#123; // console.log(newVal + ' --- ' + oldVal) if (newVal === '/login') &#123; console.log('欢迎进入登录页面') &#125; else if (newVal === '/register') &#123; console.log('欢迎进入注册页面') &#125; &#125; &#125;&#125;); 12.4 computed-计算属性的使用12345678910111213141516171819202122232425262728293031323334353637&lt;div id=\"app\"&gt; &lt;input type=\"text\" v-model=\"firstname\"&gt; + &lt;input type=\"text\" v-model=\"middlename\"&gt; + &lt;input type=\"text\" v-model=\"lastname\"&gt; = &lt;input type=\"text\" v-model=\"fullname\"&gt; &lt;p&gt;&#123;&#123; fullname &#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123; fullname &#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123; fullname &#125;&#125;&lt;/p&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; firstname: '', lastname: '', middlename: '' &#125;, methods: &#123;&#125;, computed: &#123; // 在 computed 中，可以定义一些 属性，这些属性，叫做 【计算属性】， 计算属性的， // 本质，就是 一个方法，只不过，我们在使用 这些计算属性的时候，是把 它们的 名称， // 直接当作 属性来使用的；并不会把 计算属性，当作方法去调用； // 注意1： 计算属性，在引用的时候，一定不要加 () 去调用，直接把它 当作 普通 属性去使用就好了； // 注意2： 只要 计算属性，这个 function 内部，所用到的 任何 data 中的数据发送了变化， // 就会 立即重新计算 这个 计算属性的值 // 注意3： 计算属性的求值结果，会被缓存起来，方便下次直接使用； 如果 计算属性方法中，所以来的任何数据， // 都没有发生过变化，则，不会重新对 计算属性求值； 'fullname': function () &#123; console.log('ok') return this.firstname + '-' + this.middlename + '-' + this.lastname &#125; &#125; &#125;);&lt;/script&gt; 12.5 watch、computed和methods之间的对比 computed属性的结果会被缓存，除非依赖的响应式属性变化才会重新计算。主要当作属性来使用； methods方法表示一个具体的操作，主要书写业务逻辑； watch一个对象，键是需要观察的表达式，值是对应回调函数。主要用来监听某些特定数据的变化，从而进行某些具体的业务逻辑操作；可以看作是computed和methods的结合体； 13. 使用render函数渲染组件在页面中渲染基本的组件 123456789101112131415161718192021&lt;div id=\"app\"&gt; &lt;p&gt;33333&lt;/p&gt; &lt;login&gt;&lt;/login&gt;&lt;/div&gt;&lt;script&gt; var login = &#123; template: '&lt;h1&gt;这是登录组件&lt;/h1&gt;' &#125; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125;, components: &#123; login &#125; &#125;);&lt;/script&gt; 在页面中使用render函数渲染组件 123456789101112131415161718192021&lt;div id=\"app\"&gt; &lt;p&gt;444444&lt;/p&gt;&lt;/div&gt;&lt;script&gt; var login = &#123; template: '&lt;h1&gt;这是登录组件&lt;/h1&gt;' &#125; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125;, render: function (createElements) &#123; // createElements 是一个 方法，调用它，能够把 指定的 组件模板，渲染为 html 结构 return createElements(login) // 注意：这里 return 的结果，会 替换页面中 el 指定的那个 容器 &#125; &#125;);&lt;/script&gt; 14. Vuexvuex 是 Vue 配套的公共数据管理工具，它可以把一些共享的数据，保存到 vuex 中，方便 整个程序中的任何组件直接获取或修改我们的公共数据 14.1 安装在 Vue 之后引入 vuex 会进行自动安装 12&lt;script src=\"/path/to/vue.js\"&gt;&lt;/script&gt;&lt;script src=\"/path/to/vuex.js\"&gt;&lt;/script&gt; NPM 1npm install vuex --save Yarn 1yarn add vuex 在一个模块化的打包系统中，您必须显式地通过 Vue.use() 来安装 Vuex： 1234import Vue from 'vue'import Vuex from 'vuex'Vue.use(Vuex) 当使用全局 script 标签引用 Vuex 时，不需要以上安装过程。 每一个 Vuex 应用的核心就是 store（仓库）。“store”基本上就是一个容器，它包含着你的应用中大部分的状态 (state)。Vuex 和单纯的全局对象有以下两点不同： Vuex 的状态存储是响应式的。当 Vue 组件从 store 中读取状态的时候，若 store 中的状态发生变化，那么相应的组件也会相应地得到高效更新。 你不能直接改变 store 中的状态。改变 store 中的状态的唯一途径就是显式地提交 (commit) mutation。这样使得我们可以方便地跟踪每一个状态的变化，从而让我们能够实现一些工具帮助我们更好地了解我们的应用。 14.2 最简单的 Store安装Vuex 之后，让我们来创建一个 store。创建过程直截了当——仅需要提供一个初始 state 对象和一些 mutation： 123456789101112131415import Vue from 'vue'import Vuex from 'vuex'Vue.use(Vuex)const store = new Vuex.Store(&#123; state: &#123; count: 0 &#125;, mutations: &#123; increment (state) &#123; state.count++ &#125; &#125;&#125;) 现在，你可以通过 store.state 来获取状态对象，以及通过 store.commit 方法触发状态变更： 123store.commit('increment')console.log(store.state.count) // -&gt; 1 为了在 Vue 组件中访问 this.$store property，你需要为 Vue 实例提供创建好的 store。Vuex 提供了一个从根组件向所有子组件，以 store 选项的方式“注入”该 store 的机制： 1234new Vue(&#123; el: '#app', store: store,&#125;) 14.3 案例main.js 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 入口文件import Vue from 'vue'// 配置vuex的步骤// 1. 运行 cnpm i vuex -S// 2. 导入包import Vuex from 'vuex'import App from './App.vue'// 3. 注册vuex到vue中Vue.use(Vuex)// 4. new Vuex.Store() 实例，得到一个 数据仓储对象var store = new Vuex.Store(&#123; state: &#123; // 大家可以把 state 想象成 组件中的 data ,专门用来存储数据的 // 如果在 组件中，想要访问，store 中的数据，只能通过 this.$store.state.*** 来访问 count: 0 &#125;, mutations: &#123; // 注意： 如果要操作 store 中的 state 值，只能通过 调用 mutations 提供的方法，才能操作对应的数据，不推荐直接操作 state 中的数据，因为 万一导致了数据的紊乱，不能快速定位到错误的原因，因为，每个组件都可能有操作数据的方法； increment(state) &#123; state.count++ &#125;, // 注意： 如果组件想要调用 mutations 中的方法，只能使用 this.$store.commit('方法名') // 这种 调用 mutations 方法的格式，和 this.$emit('父组件中方法名') subtract(state, obj) &#123; // 注意： mutations 的 函数参数列表中，最多支持两个参数，其中，参数1： 是 state 状态； 参数2： 通过 commit 提交过来的参数； console.log(obj) state.count -= (obj.c + obj.d) &#125; &#125;, getters: &#123; // 注意：这里的 getters， 只负责 对外提供数据，不负责 修改数据，如果想要修改 state 中的数据，请 去找 mutations optCount: function (state) &#123; return '当前最新的count值是：' + state.count &#125; // 经过咱们回顾对比，发现 getters 中的方法， 和组件中的过滤器比较类似，因为 过滤器和 getters 都没有修改原数据， 都是把原数据做了一层包装，提供给了 调用者； // 其次， getters 也和 computed 比较像， 只要 state 中的数据发生变化了，那么，如果 getters 正好也引用了这个数据，那么 就会立即触发 getters 的重新求值； &#125;&#125;)// 总结：// 1. state中的数据，不能直接修改，如果想要修改，必须通过 mutations// 2. 如果组件想要直接 从 state 上获取数据： 需要 this.$store.state.***// 3. 如果 组件，想要修改数据，必须使用 mutations 提供的方法，需要通过 this.$store.commit('方法的名称'， 唯一的一个参数)// 4. 如果 store 中 state 上的数据， 在对外提供的时候，需要做一层包装，那么 ，推荐使用 getters, 如果需要使用 getters ,则用 this.$store.getters.***const vm = new Vue(&#123; el: '#app', render: c =&gt; c(App), store // 5. 将 vuex 创建的 store 挂载到 VM 实例上， 只要挂载到了 vm 上，任何组件都能使用 store 来存取数据&#125;) counter.vue 12345678910111213141516171819202122232425262728293031323334353637383940&lt;template&gt; &lt;div&gt; &lt;input type&#x3D;&quot;button&quot; value&#x3D;&quot;减少&quot; @click&#x3D;&quot;remove&quot;&gt; &lt;input type&#x3D;&quot;button&quot; value&#x3D;&quot;增加&quot; @click&#x3D;&quot;add&quot;&gt; &lt;br&gt; &lt;input type&#x3D;&quot;text&quot; v-model&#x3D;&quot;$store.state.count&quot;&gt; &lt;&#x2F;div&gt;&lt;&#x2F;template&gt;&lt;script&gt; export default &#123; data() &#123; return &#123; &#x2F;&#x2F; count: 0 &#125;; &#125;, methods: &#123; add() &#123; &#x2F;&#x2F; 千万不要这么用，不符合 vuex 的设计理念 &#x2F;&#x2F; this.$store.state.count++; this.$store.commit(&quot;increment&quot;); &#125;, remove() &#123; this.$store.commit(&quot;subtract&quot;, &#123;c: 3, d: 1&#125;); &#125; &#125;, computed: &#123; fullname: &#123; get() &#123; &#125;, set() &#123; &#125; &#125; &#125; &#125;;&lt;&#x2F;script&gt;&lt;style lang&#x3D;&quot;scss&quot; scoped&gt;&lt;&#x2F;style&gt; amount.vue 1234567891011121314&lt;template&gt; &lt;div&gt; &lt;!-- &lt;h3&gt;&#123; &#123; $store.state.count &#125; &#125;&lt;&#x2F;h3&gt; --&gt; &lt;h3&gt;&#123; &#123; $store.getters.optCount &#125; &#125;&lt;&#x2F;h3&gt; &lt;&#x2F;div&gt;&lt;&#x2F;template&gt;&lt;script&gt;&lt;&#x2F;script&gt;&lt;style lang&#x3D;&quot;scss&quot; scoped&gt;&lt;&#x2F;style&gt;","tags":[{"name":"前端技术","slug":"前端技术","permalink":"https://wgy1993.gitee.io/tags/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"},{"name":"Vue","slug":"Vue","permalink":"https://wgy1993.gitee.io/tags/Vue/"}]},{"title":"Vue(一)","date":"2020-03-28T07:24:20.000Z","path":"archives/ad25968.html","text":"1. 什么是Vue Vue.js 是目前最火的一个前端框架，React是最流行的一个前端框架（React除了开发网站，还可以开发手机App， Vue语法也是可以用于进行手机App开发的，需要借助于Weex） Vue.js 是前端的主流框架之一，和Angular.js、React.js 一起，并成为前端三大主流框架！ Vue.js 是一套构建用户界面的框架，只关注视图层，它不仅易于上手，还便于与第三方库或既有项目整合。（Vue有配套的第三方类库，可以整合起来做大型项目的开发） 前端的主要工作？主要负责MVC中的V这一层；主要工作就是和界面打交道，来制作前端页面效果； 2. 为什么要学习流行框架 企业为了提高开发效率：在企业中，时间就是效率，效率就是金钱； 企业中，使用框架，能够提高开发的效率； 提高开发效率的发展历程：原生JS -&gt; Jquery之类的类库 -&gt; 前端模板引擎 -&gt; Angular.js / Vue.js（能够帮助我们减少不必要的DOM操作；提高渲染效率；双向数据绑定的概念【通过框架提供的指令，我们前端程序员只需要关心数据的业务逻辑，不再关心DOM是如何渲染的了】） 在Vue中，一个核心的概念，就是让用户不再操作DOM元素，解放了用户的双手，让程序员可以更多的时间去关注业务逻辑； 增强自己就业时候的竞争力 人无我有，人有我优 你平时不忙的时候，都在干嘛？ 3. 框架和库的区别 框架：是一套完整的解决方案；对项目的侵入性较大，项目如果需要更换框架，则需要重新架构整个项目。 node 中的 express； 库（插件）：提供某一个小功能，对项目的侵入性较小，如果某个库无法完成某些需求，可以很容易切换到其它库实现需求。 从Jquery 切换到 Zepto； 从 EJS 切换到 art-template 4. MVC和MVVM的关系图解 MVC 是后端的分层开发概念； MVVM是前端视图层的概念，主要关注于 视图层分离，也就是说：MVVM把前端的视图层，分为了 三部分 Model, View , VM ViewModel 5. Vue基本代码 和 MVVM 之间的对应关系123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;Document&lt;/title&gt; &lt;!-- 1. 导入Vue的包 --&gt; &lt;script src=\"./lib/vue-2.4.0.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- 将来 new 的Vue实例，会控制这个元素中的所有内容 --&gt;&lt;!-- Vue 实例所控制的这个元素区域，就是我们的 V --&gt;&lt;div id=\"app\"&gt; &lt;p&gt;&#123; &#123; msg &#125; &#125;&lt;/p&gt;&lt;/div&gt;&lt;script&gt; // 2. 创建一个Vue的实例 // 当我们导入包之后，在浏览器的内存中，就多了一个 Vue 构造函数 // 注意：我们 new 出来的这个 vm 对象，就是我们 MVVM中的 VM调度者 var vm = new Vue(&#123; el: '#app', // 表示，当前我们 new 的这个 Vue 实例，要控制页面上的哪个区域 // 这里的 data 就是 MVVM中的 M，专门用来保存每个页面的数据的 data: &#123; // data 属性中，存放的是 el 中要用到的数据 msg: '欢迎学习Vue' // 通过 Vue 提供的指令，很方便的就能把数据渲染到页面上，程序员不再手动操作DOM元素了【前端的Vue之类的框架，不提倡我们去手动操作DOM元素了】 &#125; &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; el 指定要控制的区域 data 是个对象，指定了控制的区域内要用到的数据 methods 虽然带个s后缀，但是是个对象，这里可以自定义了方法 在 VM 实例中，如果要访问 data 上的数据，或者要访问 methods 中的方法， 必须带 this 1234567891011121314151617181920212223242526&lt;script&gt; // 注意：在 VM实例中，如果想要获取 data 上的数据，或者 想要调用 methods 中的 方法，必须通过 this.数据属性名 或 this.方法名来进行访问，这里的this，就表示我们 new 出来的 VM 实例对象 var vm = new Vue(&#123; el: '#app', data: &#123; msg: '猥琐发育，别浪~~！', intervalId: null // 在data上定义 定时器Id &#125;, methods: &#123; lang() &#123; if (this.intervalId != null) return; this.intervalId = setInterval(() =&gt; &#123; var start = this.msg.substring(0, 1) // 获取到 后面的所有字符 var end = this.msg.substring(1) // 重新拼接得到新的字符串，并赋值给 this.msg this.msg = end + start &#125;, 400) // 注意： VM实例，会监听自己身上 data 中所有数据的改变，只要数据一发生变化，就会自动把 最新的数据，从data 上同步到页面中去；【好处：程序员只需要关心数据，不需要考虑如何重新渲染DOM页面】 &#125; &#125; &#125;)&lt;/script&gt; 6. Vue指令6.1 插值表达式{ {} }1&lt;div&gt;&#123; &#123;msg2&#125; &#125;&lt;/div&gt; 6.2 v-cloak使用 v-cloak 能够解决插值表达式闪烁的问题 1234567&lt;style&gt; [v-cloak] &#123; display: none; &#125;&lt;/style&gt;&lt;p v-cloak&gt;++++++++ &#123; &#123; msg &#125; &#125; ----------&lt;/p&gt; 6.3 v-text默认 v-text 是没有闪烁问题的 v-text会覆盖元素中原本的内容，但是插值表达式只会替换自己的这个占位符，不会把整个元素的内容清空 1&lt;div v-text=\"msg\"&gt;&lt;/div&gt; 6.4 v-html执行js表达式 1234567891011121314151617&lt;div v-html=\"msg2\"&gt;1212112&lt;/div&gt;&lt;script&gt; var vm = new Vue(&#123; el: '#app', data: &#123; msg: '123', msg2: '&lt;h1&gt;哈哈，我是一个大大的H1， 我大，我骄傲&lt;/h1&gt;', mytitle: '这是一个自己定义的title' &#125;, methods: &#123; // 这个 methods属性中定义了当前Vue实例所有可用的方法 show: function () &#123; alert('Hello') &#125; &#125; &#125;)&lt;/script&gt; 6.5 v-bindv-bind是 Vue中，提供的用于绑定属性的指令，v-bind 只能实现数据的单向绑定，从 M 自动绑定到 V， 无法实现数据的双向绑定 1&lt;input type=\"button\" value=\"按钮\" v-bind:title=\"mytitle\"&gt; 注意： v-bind: 指令可以被简写为 :要绑定的属性，v-bind 中，可以写合法的JS表达式 1&lt;input type=\"button\" value=\"按钮\" :title=\"mytitle + '123'\"&gt; 6.6 v-onVue 中提供了 v-on: 事件绑定机制 1&lt;input type=\"button\" value=\"按钮\" :title=\"mytitle + '123'\" v-on:click=\"alert('hello')\"&gt; v-on: 指令可以被简写为 @ 1&lt;input type=\"button\" value=\"按钮\" :title=\"mytitle + '123'\" @click=\"alert('hello')\"&gt; 6.7 事件修饰符123456789101112131415161718192021&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123; div1Handler() &#123; console.log('这是触发了 inner div 的点击事件') &#125;, btnHandler() &#123; console.log('这是触发了 btn 按钮 的点击事件') &#125;, linkClick() &#123; console.log('触发了连接的点击事件') &#125;, div2Handler() &#123; console.log('这是触发了 outer div 的点击事件') &#125; &#125; &#125;);&lt;/script&gt; 6.7.1 .stop 阻止冒泡 123&lt;div class=\"inner\" @click=\"div1Handler\"&gt; &lt;input type=\"button\" value=\"戳他\" @click.stop=\"btnHandler\"&gt;&lt;/div&gt; 6.7.2 .prevent 阻止默认事件 1&lt;a href=\"http://www.baidu.com\" @click.prevent=\"linkClick\"&gt;有问题，先去百度&lt;/a&gt; 6.7.3 .capture添加事件侦听器时使用事件捕获模式 123&lt;div class=\"inner\" @click.capture=\"div1Handler\"&gt; &lt;input type=\"button\" value=\"戳他\" @click=\"btnHandler\"&gt;&lt;/div&gt; 6.7.4 .self只当事件在该元素本身（比如不是子元素）触发时触发回调 123&lt;div class=\"inner\" @click.self=\"div1Handler\"&gt; &lt;input type=\"button\" value=\"戳他\" @click=\"btnHandler\"&gt;&lt;/div&gt; 6.7.5 .once事件只触发一次 1&lt;a href=\"http://www.baidu.com\" @click.prevent.once=\"linkClick\"&gt;有问题，先去百度&lt;/a&gt; 6.8 v-mode使用 v-model 指令，可以实现表单元素和 Model 中数据的双向数据绑定 注意： v-model 只能运用在表单元素中 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;Document&lt;/title&gt; &lt;script src=\"./lib/vue-2.4.0.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"app\"&gt; &lt;h4&gt;&#123; &#123; msg &#125; &#125;&lt;/h4&gt; &lt;!-- 注意： v-model 只能运用在 表单元素中 --&gt; &lt;!-- input(radio, text, address, email....) select checkbox textarea --&gt; &lt;input type=\"text\" style=\"width:100%;\" v-model=\"msg\"&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; msg: '大家都是好学生，爱敲代码，爱学习，爱思考，简直是完美，没瑕疵！' &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 6.9 vue中样式v-bind:class1、数组 1&lt;h1 :class=\"['red', 'thin']\"&gt;这是一个邪恶的H1&lt;/h1&gt; 2、数组中使用三元表达式 1&lt;h1 :class=\"['red', 'thin', isactive?'active':'']\"&gt;这是一个邪恶的H1&lt;/h1&gt; 3、数组中嵌套对象 1&lt;h1 :class=\"['red', 'thin', &#123;'active': isactive&#125;]\"&gt;这是一个邪恶的H1&lt;/h1&gt; 4、直接使用对象 1&lt;h1 :class=\"&#123;red:true, italic:true, active:true, thin:true&#125;\"&gt;这是一个邪恶的H1&lt;/h1&gt; 6.10 vue中样式v-bind:style1、直接在元素上通过 :style 的形式，书写样式对象 1&lt;h1 :style=\"&#123;color: 'red', 'font-size': '40px'&#125;\"&gt;这是一个善良的H1&lt;/h1&gt; 2、将样式对象，定义到 data 中，并直接引用到 :style 中 在data上定义样式： 123data: &#123; h1StyleObj: &#123; color: 'red', 'font-size': '40px', 'font-weight': '200' &#125;&#125; 在元素中，通过属性绑定的形式，将样式对象应用到元素中： 1&lt;h1 :style=\"h1StyleObj\"&gt;这是一个善良的H1&lt;/h1&gt; 3、在 :style 中通过数组，引用多个 data 上的样式对象 在data上定义样式： 1234data: &#123; h1StyleObj: &#123; color: 'red', 'font-size': '40px', 'font-weight': '200' &#125;, h1StyleObj2: &#123; fontStyle: 'italic' &#125;&#125; 在元素中，通过属性绑定的形式，将样式对象应用到元素中： 1&lt;h1 :style=\"[h1StyleObj, h1StyleObj2]\"&gt;这是一个善良的H1&lt;/h1&gt; 6.11 v-for6.11.1 v-for循环普通数组123456789101112131415161718192021&lt;div id=\"app\"&gt; &lt;!-- &lt;p&gt;&#123; &#123;list[0]&#125; &#125;&lt;/p&gt; &lt;p&gt;&#123; &#123;list[1]&#125; &#125;&lt;/p&gt; &lt;p&gt;&#123; &#123;list[2]&#125; &#125;&lt;/p&gt; &lt;p&gt;&#123; &#123;list[3]&#125; &#125;&lt;/p&gt; &lt;p&gt;&#123; &#123;list[4]&#125; &#125;&lt;/p&gt; --&gt; &lt;p v-for=\"(item, i) in list\"&gt;索引值：&#123; &#123;i&#125; &#125; --- 每一项：&#123; &#123;item&#125; &#125;&lt;/p&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; list: [1, 2, 3, 4, 5, 6] &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 6.11.2 v-for循环对象数组12345678910111213141516171819&lt;div id=\"app\"&gt; &lt;p v-for=\"(user, i) in list\"&gt;Id：&#123; &#123; user.id &#125; &#125; --- 名字：&#123; &#123; user.name &#125; &#125; --- 索引：&#123; &#123;i&#125; &#125;&lt;/p&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; list: [ &#123;id: 1, name: 'zs1'&#125;, &#123;id: 2, name: 'zs2'&#125;, &#123;id: 3, name: 'zs3'&#125;, &#123;id: 4, name: 'zs4'&#125; ] &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 6.11.3 v-for循环对象12345678910111213141516171819&lt;div id=\"app\"&gt; &lt;!-- 注意：在遍历对象身上的键值对的时候， 除了有 val key ,在第三个位置还有一个索引 --&gt; &lt;p v-for=\"(val, key, i) in user\"&gt;值是： &#123; &#123; val &#125; &#125; --- 键是： &#123; &#123;key&#125; &#125; -- 索引： &#123; &#123;i&#125; &#125;&lt;/p&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; user: &#123; id: 1, name: '托尼·屎大颗', gender: '男' &#125; &#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 6.11.4 v-for迭代数字1234567891011121314&lt;div id=\"app\"&gt; &lt;!-- in 后面我们放过 普通数组，对象数组，对象， 还可以放数字 --&gt; &lt;!-- 注意：如果使用 v-for 迭代数字的话，前面的 count 值从 1 开始 --&gt; &lt;p v-for=\"count in 10\"&gt;这是第 &#123; &#123; count &#125; &#125; 次循环&lt;/p&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123;&#125;, methods: &#123;&#125; &#125;);&lt;/script&gt; 6.11.5 v-for循环中key属性的使用1234&lt;!-- 在组件中，使用v-for循环的时候，或者在一些特殊情况中，如果 v-for 有问题，必须在使用 v-for 的同时，指定唯一的字符串/数字类型 :key 值 --&gt;&lt;p v-for=\"item in list\" :key=\"item.id\"&gt; &lt;input type=\"checkbox\"&gt;&#123; &#123;item.id&#125; &#125; --- &#123; &#123;item.name&#125; &#125;&lt;/p&gt; 2.2.0+ 的版本里，当在组件中使用 v-for 时，key 现在是必须的。 6.12 v-if/v-show12345678910111213141516171819202122232425262728&lt;div id=\"app\"&gt; &lt;input type=\"button\" value=\"toggle\" @click=\"flag=!flag\"&gt; &lt;!-- v-if 的特点：每次都会重新删除或创建元素 --&gt; &lt;!-- v-show 的特点： 每次不会重新进行DOM的删除和创建操作，只是切换了元素的 display:none 样式 --&gt; &lt;!-- v-if 有较高的切换性能消耗 --&gt; &lt;!-- v-show 有较高的初始渲染消耗 --&gt; &lt;!-- 如果元素涉及到频繁的切换，最好不要使用 v-if, 而是推荐使用 v-show --&gt; &lt;!-- 如果元素可能永远也不会被显示出来被用户看到，则推荐使用 v-if --&gt; &lt;h3 v-if=\"flag\"&gt;这是用v-if控制的元素&lt;/h3&gt; &lt;h3 v-show=\"flag\"&gt;这是用v-show控制的元素&lt;/h3&gt;&lt;/div&gt;&lt;script&gt; // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; flag: false &#125;, methods: &#123; &#125; &#125;);&lt;/script&gt; 7. 品牌管理案例-根据条件筛选品牌在Vue中，使用事件绑定机制，为元素指定处理函数的时候，如果加了小括号，就可以给函数传参了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;div id=\"app\"&gt; &lt;div class=\"panel panel-primary\"&gt; &lt;div class=\"panel-body form-inline\"&gt; &lt;label&gt; 搜索名称关键字： &lt;input type=\"text\" class=\"form-control\" v-model=\"keywords\"&gt; &lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;table class=\"table table-bordered table-hover table-striped\"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Id&lt;/th&gt; &lt;th&gt;Name&lt;/th&gt; &lt;th&gt;Ctime&lt;/th&gt; &lt;th&gt;Operation&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;!-- 之前，v-for 中的数据，都是直接从 data 上的list中直接渲染过来的 --&gt; &lt;!-- 现在，我们自定义了一个 search 方法，同时，把所有的关键字，通过传参的形式，传递给了 search 方法 --&gt; &lt;!-- 在 search 方法内部，通过执行 for 循环， 把所有符合搜索关键字的数据，保存到 一个新数组中，返回 --&gt; &lt;tr v-for=\"item in search(keywords)\" :key=\"item.id\"&gt; &lt;td&gt;&#123; &#123; item.id &#125; &#125;&lt;/td&gt; &lt;td v-text=\"item.name\"&gt;&lt;/td&gt; &lt;td&gt;&#123; &#123; item.ctime &#125; &#125;&lt;/td&gt; &lt;td&gt; &lt;a href=\"\" @click.prevent=\"del(item.id)\"&gt;删除&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt;&lt;/div&gt;&lt;script type=\"text/javascript\"&gt; var vm = new Vue(&#123; el: '#app', data: &#123; id: '', name: '', keywords: '', list: [ &#123;id: 1, name: '奔驰', ctime: new Date()&#125;, &#123;id: 2, name: '宝马', ctime: new Date()&#125; ] &#125;, methods: &#123; search(keywords) &#123; /*var newList=[]; this.list.forEach(item=&gt;&#123; if (item.name.indexOf(keywords) !=-1)&#123; newList.push(item); &#125; &#125;) return newList;*/ return this.list.filter(item =&gt; &#123; if (item.name.includes(keywords)) &#123; return item; &#125; &#125;) &#125; &#125; &#125;)&lt;/script&gt; 8. Vue调试工具vue-devtools安装地址 9. 过滤器概念：Vue.js 允许你自定义过滤器，可被用作一些常见的文本格式化。过滤器可以用在两个地方：mustache 插值和 v-bind 表达式。过滤器应该被添加在 JavaScript 表达式的尾部，由“管道”符指示； 过滤器的定义语法 123456Vue.filter('过滤器名称', function () &#123;&#125;)// 过滤器中的function第一个参数，已经被规定死了，永远都是过滤器管道符前面传递过来的数据Vue.filter('过滤器名称', function (data) &#123; return data + '123'&#125;) 9.1 全局的过滤器所谓的全局过滤器，就是所有的VM实例都共享的 123456789101112131415161718192021Vue.filter('dateFormat', function (dateStr, pattern = \"\") &#123; // 根据给定的时间字符串，得到特定的时间 var dt = new Date(dateStr) // yyyy-mm-dd var y = dt.getFullYear() var m = dt.getMonth() + 1 var d = dt.getDate() // return y + '-' + m + '-' + d if (pattern.toLowerCase() === 'yyyy-mm-dd') &#123; return `$&#123;y&#125;-$&#123;m&#125;-$&#123;d&#125;` &#125; else &#123; var hh = dt.getHours() var mm = dt.getMinutes() var ss = dt.getSeconds() return `$&#123;y&#125;-$&#123;m&#125;-$&#123;d&#125; $&#123;hh&#125;:$&#123;mm&#125;:$&#123;ss&#125;` &#125;&#125;) 9.2 私有的过滤器只能在当前 VM 对象所控制的 View 区域进行使用 123456789101112131415161718192021222324252627282930// 如何自定义一个私有的过滤器（局部）var vm2 = new Vue(&#123; el: '#app2', data: &#123; dt: new Date() &#125;, methods: &#123;&#125;, filters: &#123; // 定义私有过滤器 过滤器有两个条件 【过滤器名称 和 处理函数】 // 过滤器调用的时候，采用的是就近原则，如果私有过滤器和全局过滤器名称一致了，这时候优先调用私有过滤器 dateFormat: function (dateStr, pattern = '') &#123; // 根据给定的时间字符串，得到特定的时间 var dt = new Date(dateStr) // yyyy-mm-dd var y = dt.getFullYear() var m = (dt.getMonth() + 1).toString().padStart(2, '0') var d = dt.getDate().toString().padStart(2, '0') if (pattern.toLowerCase() === 'yyyy-mm-dd') &#123; return `$&#123;y&#125;-$&#123;m&#125;-$&#123;d&#125;` &#125; else &#123; var hh = dt.getHours().toString().padStart(2, '0') var mm = dt.getMinutes().toString().padStart(2, '0') var ss = dt.getSeconds().toString().padStart(2, '0') return `$&#123;y&#125;-$&#123;m&#125;-$&#123;d&#125; $&#123;hh&#125;:$&#123;mm&#125;:$&#123;ss&#125; ~~~~~~~` &#125; &#125; &#125;&#125;) 9.3 调用过滤器过滤器调用时候的格式 { { name | 过滤器的名称 } } 1&lt;td&gt;&#123; &#123;item.ctime | dataFormat('yyyy-mm-dd')&#125; &#125;&lt;/td&gt; 多个过滤器共同调用 1234567891011121314151617181920212223&lt;div id=\"app\"&gt; &lt;p&gt;&#123; &#123; msg | msgFormat('疯狂+1', '123') | test &#125; &#125;&lt;/p&gt;&lt;/div&gt;&lt;script&gt; // 定义一个 Vue 全局的过滤器，名字叫做 msgFormat Vue.filter('msgFormat', function (msg, arg, arg2) &#123; // 字符串的 replace 方法，第一个参数，除了可写一个 字符串之外，还可以定义一个正则 return msg.replace(/单纯/g, arg + arg2) &#125;) Vue.filter('test', function (msg) &#123; return msg + '========' &#125;) // 创建 Vue 实例，得到 ViewModel var vm = new Vue(&#123; el: '#app', data: &#123; msg: '曾经，我也是一个单纯的少年，单纯的我，傻傻的问，谁是世界上最单纯的男人' &#125; &#125;);&lt;/script&gt; 9.4 内置过滤器9.4.1 capitalize首字母大写 9.4.2 uppercase全部大写 9.4.3 lowercase全部小写 9.4.4 currency输出金钱以及小数点 参数: 第一个参数 {String} [货币符号] - 默认值: ‘$’ 第二个参数 {Number} [小数位] - 默认值: 2 10. 键盘修饰符10.1 默认键盘修饰符 为文本框回车键绑定事件 1234&lt;label&gt; Name: &lt;input type=\"text\" class=\"form-control\" v-model=\"name\" @keyup.enter=\"add\"&gt;&lt;/label&gt; 10.2 使用自定义的按键修饰符通过Vue.config.keyCodes.名称 = 按键值来自定义案件修饰符的别名 12// 自定义全局按键修饰符Vue.config.keyCodes.f2 = 113; 11. 自定义指令11.1 定义全局的指令使用 Vue.directive() 定义全局的指令 v-focus 其中： 参数1 ： 指令的名称，注意，在定义的时候，指令的名称前面，不需要加 v- 前缀, 但是： 在调用的时候，必须在指令名称前 加上 v- 前缀来进行调用 参数2： 是一个对象，这个对象身上，有一些指令相关的函数，这些函数可以在特定的阶段，执行相关的操作 12345678910111213141516171819202122232425262728293031Vue.directive('focus', &#123; bind: function (el) &#123; // 每当指令绑定到元素上的时候，会立即执行这个 bind 函数，只执行一次 // 注意： 在每个函数中，第一个参数，永远是el ，表示被绑定了指令的那个元素，这个 el 参数，是一个原生的JS对象 // 在元素刚绑定了指令的时候，还没有插入到DOM中去，这时候，调用 focus 方法没有作用 // 因为，一个元素，只有插入DOM之后，才能获取焦点 // el.focus() &#125;, inserted: function (el) &#123; // inserted 表示元素插入到DOM中的时候，会执行 inserted 函数【触发1次】 el.focus() // 和JS行为有关的操作，最好在 inserted 中去执行，放置 JS行为不生效 &#125;, updated: function (el) &#123; // 当VNode更新的时候，会执行 updated， 可能会触发多次 &#125;&#125;)// 自定义一个 设置字体颜色的 指令Vue.directive('color', &#123; // 样式，只要通过指令绑定给了元素，不管这个元素有没有被插入到页面中去，这个元素肯定有了一个内联的样式 // 将来元素肯定会显示到页面中，这时候，浏览器的渲染引擎必然会解析样式，应用给这个元素 bind: function (el, binding) &#123; // el.style.color = 'red' // console.log(binding.name) // 和样式相关的操作，一般都可以在 bind 执行 // console.log(binding.value) // console.log(binding.expression) el.style.color = binding.value &#125;&#125;) 12345&lt;label&gt; 搜索名称关键字： &lt;!-- 注意： Vue中所有的指令，在调用的时候，都以 v- 开头 --&gt; &lt;input type=\"text\" class=\"form-control\" v-model=\"keywords\" id=\"search\" v-focus v-color=\"'green'\"&gt;&lt;/label&gt; 11.2 自定义私有指令123456789101112131415161718var vm2 = new Vue(&#123; el: '#app2', data: &#123; dt: new Date() &#125;, methods: &#123;&#125;, filters: &#123;&#125;, directives: &#123; // 自定义私有指令 'fontweight': &#123; // 设置字体粗细的 bind: function (el, binding) &#123; el.style.fontWeight = binding.value &#125; &#125;, 'fontsize': function (el, binding) &#123; // 注意：这个 function 等同于把代码写到了 bind 和 update 中去 el.style.fontSize = parseInt(binding.value) + 'px' &#125; &#125;&#125;) 123&lt;div id=\"app2\"&gt; &lt;h3 v-color=\"'pink'\" v-fontweight=\"900\" v-fontsize=\"50\"&gt;&#123; &#123; dt | dateFormat &#125; &#125;&lt;/h3&gt;&lt;/div&gt;","tags":[{"name":"前端技术","slug":"前端技术","permalink":"https://wgy1993.gitee.io/tags/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"},{"name":"Vue","slug":"Vue","permalink":"https://wgy1993.gitee.io/tags/Vue/"}]},{"title":"Redis","date":"2020-03-26T11:21:14.000Z","path":"archives/bae4ff13.html","text":"1. Redis简述1.1 关系型数据库（SQL）Mysql，oracle 特点：数据和数据之间，表和字段之间，表和表之间是存在关系的 例如： 部门表 001部分， 员工表 001 用户表，用户名、密码 分类表 和 商品表， 一对多关系。 优点： 数据之间有关系，进行数据的增删改查时非常方便的。 关系型数据库有事务操作。 保证数据的完整性 缺点： 因为数据和数据之间有关系的，关系是由底层大量算法保证 大量算法会拉低系统运行速度 大量算法会消耗系统资源 海量数据的增删改查时会显得无能为力。 很可能宕机 海量数据环境下对数据表进行维护/扩展，也会变得无能为力 update product set cname = ‘手机数码’;//修改所有数据 把商品表的cname字段，由varchar(64), char(100) 适合处理一般量级数据，安全。 1.2 非关系型数据库（NOSQL）为了处理海量数据，需要将关系型数据库的关系去掉。 非关系型数据库设计之初 是为了替代关系型数据库的 Redis 优点： 海量数据的增删改查，非常轻松应对 海量数据的维护非常轻松。 缺点： 数据和数据之间没有关系，所以不能一目了然 非关系型数据库，没有关系，没有强大的事务保证数据的 完整和安全 适合处理海量数据，效率。不一定安全 关系型数据库+非关系型数据库 ====》项目 重要数据 海量操作数据，不重要 1.3 Redis使用环境1、关系型数据库的缓存存在 2、可以做任务队列 3、大量数据运算 4、排行榜 Redis非常擅长做大量数据的排行榜 2. Redis安装Redis是C语言开发，安装Redis需要先将官网下载的源码进行编译，编译依赖gcc环境。如果没有gcc环境，需要安装gcc：（环境已经导入完成） 1yum install gcc-c++ 步骤1： 将Windows下下载的压缩文件上传到Linux下。通过secureCRT进行上传，步骤如下： 12alt + pput F:/redis-3.0.0.tar.gz 步骤2： 解压文件 1tar –zxvf redis-3.0.0.tar.gz 步骤3： 编译Redis (编译，将.c文件编译为.o文件)。进入解压文件夹，cd redis-3.0.0。执行make 如果没有安装gcc，编译将出现错误提示。（如果安装失败，必须删除文件夹，重写解压） 安装成功 步骤4： 安装 1make PREFIX=/usr/local/redis install 安装完后，在/usr/local/redis/bin下有几个可执行文件 12345redis-benchmark ----性能测试工具redis-check-aof ----AOF文件修复工具redis-check-dump ----RDB文件检查工具（快照持久化文件）redis-cli ----命令行客户端redis-server ----redis服务器启动命令 步骤5： copy文件 redis启动需要一个配置文件，可以修改端口号等信息。 1cp redis.conf /usr/local/redis 注：如果没有配置文件redis也可以启动，不过将启用默认配置，这样不方便我们修改端口号等信息 2.1 前端启动 启动redis，客户端连接： 连接6379端口 123redis-cli -h ip地址 -p 端口#运行客户端（默认连接本机6379端口）：./bin/redis-cli 2.2 Redis启动-后端模式 修改redis.conf配置文件， daemonize yes 以后端模式启动。 123vim /usr/local/redis/redis.confdaemonize yes 启动时，指定配置文件 12cd /usr/local/redis/./bin/redis-server ./redis.conf 2.3 Redis的关闭 查询到PID,kill -9 pid 【断电，非正常关闭，一般不用，否则造成数据丢失】 正常关闭 【正常关闭，数据保存】 1./bin/redis-cli shutdown 3. Redis数据类型Redis 使用的是键值对 保存数据。（map） key:全部都是字符串 value:有五种数据类型 4. Redis命令4.1 String字符串类型是Redis中最为基础、常用的数据存储类型，字符串在Redis中是二进制安全的，这便意味着该类型存入和获取的数据相同。在Redis中字符串类型的Value最多可以容纳的数据长度是512M。 二进制安全和数据安全是没有关系的。 MySQL-关系型数据库，二进制不安全。【乱码丢失数据】 Redis二进制数据安全 4.1.1 赋值set key value： 设定key持有指定的字符串value，如果该key存在则进行覆盖操作。总是返回”OK”，如果赋予相同的key，新的value会覆盖老的value 4.1.2 取值get key： 获取key的value。如果与该key关联的value不是String类型，redis将返回错误信息，因为get命令只能用于获取String value；如果该key不存在，返回(nil)。 4.1.3 删除del key ： 删除指定key，返回值是数字类型，表示删了几条数据 4.1.4 扩展 getset key value： 先获取该key的值，然后在设置该key的值。 incr key： 将指定的key的value原子性的递增1.如果该key不存在，其初始值为0，在incr之后其值为1。如果value的值不能转成整型，如hello，该操作将执行失败并返回相应的错误信息。相当于 ++i decr key： 将指定的key的value原子性的递减1.如果该key不存在，其初始值为0，在incr之后其值为-1。如果value的值不能转成整型，如hello，该操作将执行失败并返回相应的错误信息。相当于 –i Incr和decr 只能对字符串是数字的 进行操作。 append key value： 拼凑字符串。如果该key存在，则在原有的value后追加该值；如果该key不存在，则重新创建一个key/value String使用环境：主要用于保存json格式的字符串 4.2 hashRedis中的Hash类型可以看成具有String Key和String Value的map容器。所以该类型非常适合于存储值对象的信息。如Username、Password和Age等。如果Hash中包含很少的字段，那么该类型的数据也将仅占用很少的磁盘空间。每一个Hash可以存储4294967295个键值对。 Hash —-&gt; {username:”张三”,age:”18”,sex:”man”}——javaBean Hash特点：占用的磁盘空间极少 4.2.1 赋值hset key field value：为指定的key设定field/value对（键值对）。 hmset key field value [field2 value2 …]：设置key中的多个filed/value 4.2.2 取值hmget key filed1 filed2 ….：获取key中的多个filed的值 hgetall key：获取key中的所有filed-vaule 4.2.3 删除del key ：删除整个hash 4.2.4 扩展hkeys key ：获得所有的字段 hvals key：获得所有的value 4.3 LinkedListJava List ： 数组ArrayList 、链表LinkedList 为什么redis选取了链表？Redis操作中，最多的操作是进行元素的增删 使用环境： 做大数据集合的增删 任务队列 4.3.1 赋值 lpush key values[value1 value2…]： 在指定的key所关联的list的头部插入所有的values，如果该key不存在，该命令在插入的之前创建一个与该key关联的空链表，之后再向该链表的头部插入数据。插入成功，返回元素的个数。 rpush key values[value1、value2…]： 在该list的尾部添加元素。 4.3.2 取值lrange key start end： 获取链表中从start到end的元素的值，start、end从0开始计数；也可为负数，若为-1则表示链表尾部的元素，-2则表示倒数第二个，依次类推… 4.3.3 删值 lpop key： 返回并弹出指定的key关联的链表中的第一个元素，即头部元素。如果该key不存在，返回nil；若key存在，则返回链表的头部元素。 rpop key： 从尾部弹出元素。 4.3.4 扩展 llen key：返回指定的key关联的链表中的元素的数量。 rpoplpush resource destination：将链表中的尾部元素弹出并添加到头部。[循环操作] 两队排列： 循环队列： 4.4 setJava HashSet 无序，不重复。 Redis操作中，涉及到两个大数据集合的并集，交集，差集运算。 4.4.1 赋值sadd key values[value1、value2…]： 向set中添加数据，如果该key的值已有则不会重复添加 4.4.2 取值smembers key：获取set中所有的成员 sismember key member：判断参数中指定的成员是否在该set中，1表示存在，0表示不存在或者该key本身就不存在。（无论集合中有多少元素都可以极速的返回结果） 4.4.3 删值srem key members[member1、member2…]：删除set中指定的成员 4.4.4 集合运算4.4.4.1 差集运算sdiff key1 key2…：返回key1与key2中相差的成员，而且与key的顺序有关。即返回差集。 属于A并且不属于B的元素构成的集合 4.4.4.2 交集运算sinter key1 key2 key3…：返回交集。 属于A且属于B的元素构成的集合 4.4.4.3 并集运算sunion key1 key2 key3…：返回并集。 属于A或者属于B的元素构成的集合 4.4.5 扩展命令 scard key：获取set中成员的数量 srandmember key：随机返回set中的一个成员 4.5 有序set有序set集合： 有序，不重复 小明，小红，小张 每个元素都需要手动赋予一个分数 5000 小明， 1000 小红 ， 500 小张 集合中：500 小张 1000 小红 5000 小明 集合倒序：5000 小明 1000 小红 500 小张 有序set集合，专门用来做排行榜 4.5.1 赋值zadd key score member score2 member2 … ： 将所有成员以及该成员的分数存放到sorted-set中。如果该元素已经存在则会用新的分数替换原有的分数。返回值是新加入到集合中的元素个数，不包含之前已经存在的元素。 4.5.2 查看 zscore key member：返回指定成员的分数 zrange key start end [withscores]：获取集合中脚标为start-end的成员，[withscores]参数表明返回的成员包含其分数。（分数由小到大排列） zrevrange key start end [withscores]：获取集合中脚标为start-end的成员，[withscores]参数表明返回的成员包含其分数。（分数由大到小排列） 4.5.3 删值zrem key member[member…]：移除集合中指定的成员，可以指定多个成员。 5. 通用redis命令 keys pattern：获取所有与pattern匹配的key，返回所有与该key匹配的keys。 通配符：*表示任意0个或多个任意字符，?表示任意一个字符 keys * 查询所有的key del key1 key2…：删除指定的key exists key：判断该key是否存在，1代表存在，0代表不存在 type key：获取指定key的值类型。该命令将以字符串的格式返回。 返回的字符串为string、list、set、hash和zset，如果key不存在返回none。 设置key有效/过期 expire key ：设置key的生存时间，单位：秒。如果某个key过期，redis会将其删除 ttl key：获取该key所剩的超时时间，如果没有设置超时，返回-1。如果返回-2表示超时不存在。 6. Jedisjava操作redis数据库技术。Redis有什么命令，Jedis就有什么方法。 防火墙设置： 12345#开放6379端口/sbin/iptables -I INPUT -p tcp --dport 6379 -j ACCEPT#保存配置/etc/rc.d/init.d/iptables save 123456789/** * 单实例链接redis数据库 */@Testpublic void run1()&#123; Jedis jedis = new Jedis(\"192.168.236.137\", 6379);// jedis.set(\"name\", \"张三\"); System.out.println(jedis.get(\"name\"));&#125; Jedis连接池： 12345678910111213141516171819/** * Jedis连接池 */@Testpublic void run2()&#123; //1、设置连接池的配置对象 JedisPoolConfig config = new JedisPoolConfig(); //设置池中最大连接数【可选】 config.setMaxTotal(50); //设置空闲时池中保有的最大连接数【可选】 config.setMaxIdle(10); //2、设置连接池对象 JedisPool pool = new JedisPool(config,\"192.168.236.137\", 6379); //3、从池中获取链接对象 Jedis jedis = pool.getResource(); System.out.println(jedis.get(\"name\")); //4、连接归还池中 jedis.close();&#125; 7. 扩展知识7.1 消息订阅与发布订阅新闻，新闻发布 subscribe channel：订阅频道，例：subscribe mychat，订阅mychat这个频道 psubscribe channel：批量订阅频道，例：psubscribe s，订阅以”s”开头的频道 publish channel content：在指定的频道中发布消息，如 publish mychat ‘today is a newday’ 7.2 多数据库MySQL-数据库可以自己用语句自定义创建：create database xxxx; Redis-也是有数据库的。 Redis已经提前创建好了。Redis默认有16个数据库。0，1，2…….15 在redis上所做的所有数据操作，都是默认在0号数据库上操作。数据库和数据库之间，不能共享键值对。 切换数据库：select 数据库名; 把某个键值对进行数据库移植：move newkey 1：将当前库的key移植到1号库中 慎用： 清空当前数据库：flushdb 和redis服务器数据的清空：flushall 7.3 Redis批量操作-事务MySQL-事务： 目的为了保证数据完整性，安全。 Redis-事务： 目的为了进行Redis语句的批量化执行 multi：开启事务用于标记事务的开始，其后执行的命令都将被存入命令队列，直到执行EXEC时，这些命令才会被原子的执行，类似与关系型数据库中的：begin transaction exec：提交事务，类似与关系型数据库中的：commit 执行批量化 discard：事务回滚，类似与关系型数据库中的：rollback 不执行批量化操作 7.4 Redis了解命令 quit：退出客户端ctrl+c dbsize：返回当前数据库中key 的数目 info：查看redis数据 7.5 Redis持久化内存（兔子）：高效、断电数据就会消失 硬盘（乌龟）：读写速度慢于内存的，断电数据依旧存在 持久化：把数据保存在硬盘上 关系型数据库MySQL-持久化： 任何增删改语句，都是在硬盘上做的操作 断电以后，硬盘上的数据还是存在。 非关系型数据库Redis： 默认情况下，所有的增删改，数据都是在内存中进行操作。 断电以后，内存中的数据不存在的。 Redis有两种持久化策略 7.5.1 RDB是Redis的默认持久化机制。RDB相当于照快照。保存的是一种状态。20G数据—-à几kb快照 优点： 快照保存数据速度极快，还原数据速度极快 适用于灾难备份 缺点： 小内存机器不适合使用。 RDB机制符合要求就会照快照。（随时随地启动），会占用一部分系统资源（突然的）,很可能内存不足直接宕机。（宕机后，服务器会关闭，非正常关闭） 适用于：内存比较充裕的计算机。 RDB何时进行照快照： 服务器正常关闭时，会照一次快照 ./bin/redis-cli shutdown key满足一定条件，会照一次快照 save 900 1 #每900秒(15分钟)至少有1个key发生变化，则dump内存快照。 save 300 10 #每300秒(5分钟)至少有10个key发生变化，则dump内存快照 save 60 10000 #每60秒(1分钟)至少有10000个key发生变化，则dump内存快照 key发生变化（key数据添加、修改、删除） 例如1： 12:00 – 12:15 ，有1个key发生变化，在12:15就照一次快照保存 12:00 – 12:15 ，没有任何key发生变化，在12:15不会照快照 例如2： 12:00 – 12:05 ，有11个key发生变化，12:05就照一次快照保存 12:00 – 12:05 ，有9个key发生变化，12:05不会照快站，12:15 会照快照 例如3: 12:00 – 12:01 ，有15000个key发生变化，12:01会照一次快照保存 12:00 – 12:01 ，有1000个key发生变化，12:01不会照快站，12:05会照快照 7.5.2 AOF使用日志功能保存数据操作。默认AOF机制关闭的。 每秒同步（默认）：每秒进行一次AOF保存数据。 安全性低，比较节省系统资源 每修改同步：只要有key变化语句，就进行AOF保存数据。比较安全，但是极为浪费效率 不同步：不进行任何持久化操作 不安全 AOF操作：只会保存导致key变化的语句 AOF配置： always #每次有数据修改发生时都会写入AOF文件 everysec #每秒钟同步一次，该策略为AOF的缺省策略 no #从不同步。高效但是数据不会被持久化 开启AOF机制： 策略的选择： 优点：持续性占用极少量的内存资源 缺点： 日志文件会特别大，不适用于灾难恢复 恢复效率远远低于RDB 适用于：内存比较小的计算机","tags":[{"name":"服务器中间件","slug":"服务器中间件","permalink":"https://wgy1993.gitee.io/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"NoSQL存储","slug":"NoSQL存储","permalink":"https://wgy1993.gitee.io/tags/NoSQL%E5%AD%98%E5%82%A8/"},{"name":"Redis","slug":"Redis","permalink":"https://wgy1993.gitee.io/tags/Redis/"}]},{"title":"Nginx","date":"2020-03-25T07:01:45.000Z","path":"archives/65b69107.html","text":"1. Nginx1.1 什么是NginxNginx 是一款高性能的 http 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器。由俄罗斯的程序设计师伊戈尔·西索夫（Igor Sysoev）所开发，官方测试 nginx 能够支支撑 5 万并发链接，并且 cpu、内存等资源消耗却非常低，运行非常稳定。 1.2 Nginx应用场景 http 服务器。Nginx 是一个 http 服务可以独立提供 http 服务。可以做网页静态服务器。 虚拟主机。可以实现在一台服务器虚拟出多个网站。例如个人网站使用的虚拟主机。 反向代理，负载均衡。当网站的访问量达到一定程度后，单台服务器不能满足用户的请求时，需要用多台服务器集群可以使用 nginx 做反向代理。并且多台服务器可以平均分担负载，不会因为某台服务器负载高宕机而某台服务器闲置的情况。 2. Nginx的安装与启动2.1 在Linux下的安装Nginx2.1.1 环境准备2.1.1.1 需要安装 gcc 的环境1yum install gcc-c++ 2.1.1.2 第三方的开发包PCRE: PCRE(Perl Compatible Regular Expressions)是一个 Perl 库，包括 perl 兼容的正则表达式库。nginx 的 http 模块使用 pcre 来解析正则表达式。 1yum install -y pcre pcre-devel 注：pcre-devel 是使用 pcre 开发的一个二次开发库。nginx 也需要此库。 zlib: zlib 库提供了很多种压缩和解压缩的方式，nginx 使用 zlib 对 http 包的内容进行 gzip。 1yum install -y zlib zlib-devel OpenSSL: OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。nginx 不仅支持 http 协议，还支持 https（即在 ssl 协议上传输 http）。 1yum install -y openssl openssl-devel 2.1.2 Nginx下载官方网站下载 nginx：http://nginx.org/ 2.1.3 Nginx安装1、把 nginx 的源码包nginx-1.8.0.tar.gz上传到 linux 系统 2、解压缩 1tar -zxvf nginx-1.8.0.tar.gz 3、进入nginx-1.8.0目录，使用 configure 命令创建makeFile 文件 123456789101112.&#x2F;configure \\--prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;nginx \\--pid-path&#x3D;&#x2F;var&#x2F;run&#x2F;nginx&#x2F;nginx.pid \\--lock-path&#x3D;&#x2F;var&#x2F;lock&#x2F;nginx.lock \\--error-log-path&#x3D;&#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log \\--http-log-path&#x3D;&#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log \\--with-http_gzip_static_module \\--http-client-body-temp-path&#x3D;&#x2F;var&#x2F;temp&#x2F;nginx&#x2F;client \\--http-proxy-temp-path&#x3D;&#x2F;var&#x2F;temp&#x2F;nginx&#x2F;proxy \\--http-fastcgi-temp-path&#x3D;&#x2F;var&#x2F;temp&#x2F;nginx&#x2F;fastcgi \\--http-uwsgi-temp-path&#x3D;&#x2F;var&#x2F;temp&#x2F;nginx&#x2F;uwsgi \\--http-scgi-temp-path&#x3D;&#x2F;var&#x2F;temp&#x2F;nginx&#x2F;scgi 执行后可以看到Makefile文件 1234567891011121314151617181920212223242526Makefile是一种配置文件， Makefile 一个工程中的源文件不计数，其按类型、功能、模块分别放在若干个目录中，makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为 makefile就像一个Shell脚本一样，其中也可以执行操作系统的命令。configure参数.&#x2F;configure \\--prefix&#x3D;&#x2F;usr \\ 指向安装目录--sbin-path&#x3D;&#x2F;usr&#x2F;sbin&#x2F;nginx \\ 指向（执行）程序文件（nginx）--conf-path&#x3D;&#x2F;etc&#x2F;nginx&#x2F;nginx.conf \\ 指向配置文件--error-log-path&#x3D;&#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log \\ 指向log--http-log-path&#x3D;&#x2F;var&#x2F;log&#x2F;nginx&#x2F;access.log \\ 指向http-log--pid-path&#x3D;&#x2F;var&#x2F;run&#x2F;nginx&#x2F;nginx.pid \\ 指向pid--lock-path&#x3D;&#x2F;var&#x2F;lock&#x2F;nginx.lock \\ 安装文件锁定，防止安装文件被别人利用，或自己误操作。--user&#x3D;nginx \\--group&#x3D;nginx \\--with-http_ssl_module \\ 启用ngx_http_ssl_module支持（使支持https请求，需已安装openssl）--with-http_flv_module \\ 启用ngx_http_flv_module支持（提供寻求内存使用基于时间的偏移量文件）--with-http_stub_status_module \\ 启用ngx_http_stub_status_module支持（获取nginx自上次启动以来的工作状态）--with-http_gzip_static_module \\ 启用ngx_http_gzip_static_module支持（在线实时压缩输出数据流）--http-client-body-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;client&#x2F; \\ 设定http客户端请求临时文件路径--http-proxy-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;proxy&#x2F; \\ 设定http代理临时文件路径--http-fastcgi-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;fcgi&#x2F; \\ 设定http fastcgi临时文件路径--http-uwsgi-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;uwsgi \\ 设定http uwsgi临时文件路径--http-scgi-temp-path&#x3D;&#x2F;var&#x2F;tmp&#x2F;nginx&#x2F;scgi \\ 设定http scgi临时文件路径--with-pcre 启用pcre库 4、编译安装 123makemake install 2.2 Nginx启动与访问注意：启动nginx 之前，上边将临时文件目录指定为/var/temp/nginx/client， 需要在/var 下创建此目录 1mkdir &#x2F;var&#x2F;temp&#x2F;nginx&#x2F;client -p 启动Nginx： 12345cd &#x2F;usr&#x2F;local&#x2F;ngiux&#x2F;sbin.&#x2F;nginxps aux|grep nginx 启动后查看进程 地址栏输入虚拟机的IP即可访问（默认为80端口） 关闭 nginx： 123.&#x2F;nginx -s stop或者.&#x2F;nginx -s quit 重启 nginx： 1.&#x2F;nginx -s reload 先关闭后启动，刷新配置文件 3. Nginx静态网站部署3.1 静态网站的部署将静态页面上传到服务器的/usr/local/nginx/html目录下即可访问 3.2 配置虚拟主机虚拟主机，也叫“网站空间”，就是把一台运行在互联网上的物理服务器划分成多个“虚拟”服务器。虚拟主机技术极大的促进了网络技术的应用和普及。同时虚拟主机的租用服务也成了网络时代的一种新型经济形式。 3.2.1 端口绑定3.2.1.1 上传静态网站将/资料/静态页面/index目录上传至 /usr/local/nginx/index下 将/资料/静态页面/regist目录上传至 /usr/local/nginx/regist下 3.2.1.2 修改Nginx 的配置文件/usr/local/nginx/conf/nginx.conf 12345678910111213141516171819202122232425server &#123; listen 81; # 监听的端口 server_name localhost; # 域名或ip location &#x2F; &#123; # 访问路径配置 root index;# 根目录 index index.html index.htm; # 默认首页 &#125; error_page 500 502 503 504 &#x2F;50x.html; # 错误页面 location &#x3D; &#x2F;50x.html &#123; root html; &#125;&#125;server &#123; listen 82; # 监听的端口 server_name localhost; # 域名或ip location &#x2F; &#123; # 访问路径配置 root regist;# 根目录 index regist.html; # 默认首页 &#125; error_page 500 502 503 504 &#x2F;50x.html; # 错误页面 location &#x3D; &#x2F;50x.html &#123; root html; &#125;&#125; 3.2.1.3 访问测试地址栏输入：http://虚拟主机IP/:81 可以看到首页面 地址栏输入：http://虚拟主机IP/:82 可以看到注册页面 3.2.2 域名绑定3.2.2.1 什么是域名域名（Domain Name），是由一串用“点”分隔的字符组成的Internet上某一台计算机或计算机组的名称，用于在数据传输时标识计算机的电子方位（有时也指地理位置，地理上的域名，指代有行政自主权的一个地方区域）。域名是一个IP地址上有“面具” 。域名的目的是便于记忆和沟通的一组服务器的地址（网站)，电子邮件，FTP等）。域名作为力所能及难忘的互联网参与者的名称。域名按域名系统（DNS）的规则流程组成。在DNS中注册的任何名称都是域名。域名用于各种网络环境和应用程序特定的命名和寻址目的。通常，域名表示互联网协议（IP）资源，例如用于访问因特网的个人计算机，托管网站的服务器计算机，或网站本身或通过因特网传送的任何其他服务。世界上第一个注册的域名是在1985年1月注册的。 3.2.2.2 域名与IP绑定一个域名对应一个 ip 地址，一个 ip 地址可以被多个域名绑定。 本地测试可以修改 hosts 文件（C:\\Windows\\System32\\drivers\\etc） 可以配置域名和 ip 的映射关系，如果 hosts 文件中配置了域名和 ip 的对应关系，不需要走dns 服务器。 12虚拟主机IP www.hmtravel.com虚拟主机IP regist.hmtravel.com 修改nginx配置文件： 12345678910111213141516server &#123; listen 80; server_name www.hmtravel.com; location &#x2F; &#123; root index; index index.html; &#125;&#125;server &#123; listen 80; server_name regist.hmtravel.com; location &#x2F; &#123; root regist; index regist.html; &#125;&#125; 测试： 地址栏输入http://www.hmtravel.com/、http://regist.hmtravel.com/ 4. Nginx反向代理与负载均衡4.1 反向代理4.1.1 什么是反向代理反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 正向代理 正向代理是针对你的客户端，而反向代理是针对服务器的 4.1.2 配置反向代理准备工作： 将travel案例部署到tomcat中（ROOT目录），上传到服务器。 启动TOMCAT，输入网址http://虚拟主机IP:8080 可以看到网站首页 在Nginx主机修改 Nginx配置文件: 12345678910111213upstream tomcat-travel&#123; server 192.168.177.129:8080;&#125;server &#123; listen 80; # 监听的端口 server_name www.hmtravel.com; # 域名或ip location &#x2F; &#123; # 访问路径配置 # root index;# 根目录 proxy_pass http:&#x2F;&#x2F;tomcat-travel; index index.html index.htm; # 默认首页 &#125;&#125; 4.2 负载均衡4.2.1 什么是负载均衡负载均衡建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。 负载均衡，英文名称为Load Balance，其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。 准备工作： 将刚才的存放工程的tomcat复制三份，修改端口分别为8080 ，8081，8082 分别启动这三个tomcat服务 为了能够区分是访问哪个服务器的网站，可以在首页标题加上标记以便区分 修改 Nginx配置文件： 1234567891011121314151617181920upstream tomcat-travel &#123; server 192.168.177.129:8080; server 192.168.177.129:8081; server 192.168.177.129:8082;&#125;server &#123; listen 80; # 监听的端口 server_name www.hmtravel.com; # 域名或ip location &#x2F; &#123; # 访问路径配置 # root index;# 根目录 proxy_pass http:&#x2F;&#x2F;tomcat-travel; index index.html index.htm; # 默认首页 &#125; error_page 500 502 503 504 &#x2F;50x.html; # 错误页面 location &#x3D; &#x2F;50x.html &#123; root html; &#125;&#125; 地址栏输入http:// www.hmtravel.com / 刷新观察每个网页的标题，看是否不同。 经过测试，三台服务器出现的概率各为33.3333333%，交替显示。 如果其中一台服务器性能比较好，想让其承担更多的压力，可以设置权重。 比如想让NO.1出现次数是其它服务器的2倍，则修改配置如下： 12345upstream tomcat-travel &#123; server 192.168.177.129:8080; server 192.168.177.129:8081 weight&#x3D;2; server 192.168.177.129:8082;&#125;","tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://wgy1993.gitee.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Nginx","slug":"Nginx","permalink":"https://wgy1993.gitee.io/tags/Nginx/"}]},{"title":"Linux(二)","date":"2020-03-24T09:09:35.000Z","path":"archives/be6dfc49.html","text":"1. rpm1.1 概述RPM（RedHat Package Manager），Rethat软件包管理工具，类似windows里面的setup.exe， 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。 RPM包的名称格式： Apache-1.3.23-11.i386.rpm “apache” 软件名称 “1.3.23-11”软件的版本号，主版本和此版本 “i386”是软件所运行的硬件平台 “rpm”文件扩展名，代表RPM包 1.2 常用命令1.2.1 查询123基本语法： rpm -qa 查询所安装的所有rpm软件包 rpm –qa | grep rpm软件包 过滤，查找指定RPM软件包 1.2.2 卸载123基本语法： rpm -e rpm软件包 rpm -e --nodeps rpm软件包 不考虑依赖包，强制卸载 1.2.3 安装1234567基本语法： rpm -ivh rpm包全名 -i&#x3D;install，安装 -v&#x3D;verbose，显示详细信息 -h&#x3D;hash，进度条 --nodeps，不检测依赖进度 2. yum2.1 概述YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。 2.2 常用命令123456基本语法： yum install -y httpd 安装httpd并确认安装 yum list 列出所有可用的package和package组 yum clean all 清除所有缓冲数据 yum deplist httpd 列出一个包所有依赖的包 yum remove httpd 删除httpd 2.3 关联网络yum源2.3.1 前期文件准备前提条件linux系统必须可以联网 在Linux环境环境中访问该网络地址：http://mirrors.163.com/.help/centos.html，在使用说明中点击CentOS6-&gt;再点击保存 2.3.2 替换本地yum文件123456781. 把下载的文件移动到&#x2F;etc&#x2F;yum.repos.d&#x2F;目录 mv CentOS6-Base-163.repo &#x2F;etc&#x2F;yum.repos.d&#x2F; 2. 备份CentOS-Base.repo cp CentOS-Base.repo CentOS-Base.repo.bak3. 用CentOS6-Base-163.repo替换CentOS-Base.repo mv CentOS6-Base-163.repo CentOS-Base.repo 2.3.3 安装命令123yum clean all 清除所有缓存yum makecache 生成缓存 3. Linux上软件安装3.1 安装方式 二进制发布包 软件已经针对具体平台编译打包发布，只要解压，修改配置即可 RPM包 软件已经按照redhat的包管理工具规范RPM进行打包发布，需要获取到相应的软件RPM发布包，然后用RPM命令进行安装 Yum在线安装 软件已经以RPM规范打包，但发布在了网络上的一些服务器上，可用yum在线安装服务器上的rpm软件，并且会自动解决软件安装过程中的库依赖问题 源码编译安装 软件以源码工程的形式发布，需要获取到源码工程后用相应开发工具进行编译打包部署。 3.2 上传与下载工具3.2.1 FileZilla 3.2.2 lrzsz使用yum安装方式安装 yum install -y lrzsz 在SecureCRT中设置上传与下载目录 上传 rz： 上传文件时，须将文件放入SecureCRT设置的上传目录upload中 下载 sz： 3.2.3 sftpSecureCRT使用alt + p 组合键打开sftp窗口 使用put 命令上传： 使用get命令下载： 默认下载位置在当前计算的的文档位置 3.3 在Linux上安装JDK3.3.1 上传JDK1234567#上传JDK # 查看安装的jdk信息 rpm -qa | grep java# 卸载已有jdk rpm -e --nodeps java-1.6.0-openjdk-1.6.0.35-1.13.7.1.el6_6.i686 rpm -e --nodeps java-1.7.0-openjdk-1.7.0.79-2.5.5.4.el6.i686 3.3.2 安装JDK123通常将软件安装到&#x2F;usr&#x2F;local直接解压就可以 tar –zxvf jdk.tar.gz -C 目标路径 3.3.3 配置JDK的环境变量12345678910111. vi &#x2F;etc&#x2F;profile2. 在末尾行添加 #set java environment JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;jdk&#x2F;jdk1.7.0_71 CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib.tools.jar PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH export JAVA_HOME CLASSPATH PATH 保存退出3. source &#x2F;etc&#x2F;profile 使更改的配置立即生效4. java -version 查看jdk版本 3.4 在Linux上安装mysql3.4.1 上传mysql123456#上传mysql # 查看安装的mysql信息 rpm -qa | grep mysql# 卸载已有mysql rpm -e --nodeps mysql 3.4.2 安装mysql1234567891011使用tar命令解压 tar -xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar清除yum里所有mysql依赖包 yum remove mysql-libs严格按照顺序安装 rpm -ivh mysql-community-common-5.7.29-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-5.7.29-1.el7.x86_64.rpm rpm -ivh mysql-community-client-5.7.29-1.el7.x86_64.rpm rpm -ivh mysql-community-server-5.7.29-1.el7.x86_64.rpm 3.4.3 配置数据库12345vim &#x2F;etc&#x2F;my.cnf添加character_set_server&#x3D;utf8 设置默认字符集UTF-8init_connect&#x3D;&#39;SET NAMES utf8&#39; 设置默认字符集UTF-8 3.4.4 设置密码和开启远程登录123456789#设置密码策略set global validate_password_policy&#x3D;LOW; 设置密码的验证强度等级set global validate_password_length&#x3D;4; 只要设置密码的长度小于 3 ，都将自动设值为 4set password&#x3D;password(&#39;root&#39;);#开启远程登录grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;root&#39;;flush privileges; 立即生效 3.5 在Linux上安装tomcat Tomcat上传到Linux上 将上传的tomcat解压 在tomcat/bin目录下执行 startup.sh（注意防火墙） 查看目标 tomcat/logs/catalina.out 3.5.1 部署项目到Linux 将war包上传到tomcat中的webapps目录 重启tomcat 3.6 在Linux上安装redis3.6.1 安装gcc-c++redis是C语言开发，安装redis需要先将官网下载的源码进行编译，编译依赖gcc环境。 123456789yum install -y gcc-c++编译redis不成功还需安装:yum -y install centos-release-scl 升级到9.1版本yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutilscl enable devtoolset-9 bash以上为临时启用，如果要长期使用gcc 9.1的话：echo &quot;source &#x2F;opt&#x2F;rh&#x2F;devtoolset-9&#x2F;enable&quot; &gt;&gt;&#x2F;etc&#x2F;profile 3.6.2 获取redis资源123cd &#x2F;usr&#x2F;local wget http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-6.0.5.tar.gz 3.6.3 安装redis1234567891011121314使用tar命令解压 tar -zxvf redis-6.0.5.tar.gz 编译安装 make cd src make install PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;redis make install安装完成后，会在&#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin目录下生成下面几个可执行文件，它们的作用分别是： redis-server：Redis服务器端启动程序 redis-cli：Redis客户端操作工具。也可以用telnet根据其纯文本协议来操作 redis-benchmark：Redis性能测试工具 redis-check-aof：数据修复工具 redis-check-dump：检查导出工具 3.6.4 配置redis1234567891. 复制配置文件到&#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin目录： cd redis-6.0.5 cp redis.conf &#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin2. 修改配置文件 vi &#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin&#x2F;redis.conf daemonize no 修改为 yes 设置为后台服务 注释掉bind 127.0.0.1 默认只支持本机连接，注释掉这行代表可以接受其他机器的连接 protected-mode yes 修改为 no 将redis的保护模式关闭掉，这样可以供外部来访问redis访问 3.6.5 启动redis123456cd redis&#x2F;bin 进入redis&#x2F;bin目录.&#x2F;redis-server redis.conf 启动redis服务端.&#x2F;redis-cli 启动redis客户端pkill redis 停止redis服务","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wgy1993.gitee.io/tags/Linux/"},{"name":"操作系统","slug":"操作系统","permalink":"https://wgy1993.gitee.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"Linux(一)","date":"2020-03-23T08:54:46.000Z","path":"archives/115c672b.html","text":"1. Linux的概述1.1 学习Linux之前先了解UnixUnix是一个强大的多用户、多任务操作系统。于1969年在AT&amp;T的贝尔实验室开发。UNIX的商标权由国际开放标准组织（The Open Group）所拥有。UNIX操作系统是商业版，需要收费，价格比Microsoft Windows正版要贵一些。 1.2 Linux的概述Linux是基于Unix的，诞生于1991年10月5日。是由芬兰赫尔辛基大学学生Linus Torvalds和后来加入的众多爱好者共同开发完成 Linux是一种自由和开放源码的操作系统，存在着许多不同的Linux版本，但它们都使用了Linux内核。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、台式计算机 1.3 Linux的历史Linux最初是由芬兰赫尔辛基大学学生Linus Torvalds由于自己不满意教学中使用的MINIX操作系统， 所以在1990年底由于个人爱好设计出了LINUX系统核心。后来发布于芬兰最大的ftp服务器上，用户可以免费下载，所以它的周边的程序越来越多，Linux本身也逐渐发展壮大起来，之后Linux在不到三年的时间里成为了一个功能完善，稳定可靠的操作系统。 1.4 Linux系统的应用服务器系统Web应用服务器、数据库服务器、接口服务器、DNS、FTP等等； 嵌入式系统路由器、防火墙、手机、PDA、IP 分享器、交换器、家电用品的微电脑控制器等等， 高性能运算、计算密集型应用Linux有强大的运算能力。 桌面应用系统 移动手持系统 1.5 Linux的版本Linux的版本分为两种：内核版本和发行版本； 内核版本是指在Linus领导下的内核小组开发维护的系统内核的版本号 ； 1.6 Linux的主流版本 2. Linux的安装2.1 虚拟机安装2.1.1 什么是虚拟机虚拟机：一台虚拟的电脑. 虚拟机软件: VmWare :收费的. VirtualBox :免费的. 2.2 CentOS的安装centos下载地址： 网易镜像：http://mirrors.163.com/centos/6/isos/ 搜狐镜像：http://mirrors.sohu.com/centos/6/isos/ 2.3 Linux的远程访问安装远程访问的软件SecureCRT 3. Linux的目录结构 4. Linux的常用命令4.1 切换目录命令1234567cd app 切换到app目录cd .. 切换到上一层目录cd &#x2F; 切换到系统根目录cd ~ 切换到用户主目录cd - 切换到上一个所在目录使用tab键来补全文件路径 4.2 列出文件列表123456789ls(list)是一个非常有用的命令，用来显示当前目录下的内容。配合参数的使用，能以不同的方式显示目录内容。 格式：ls[参数] [路径或文件名]常用： 在linux中以 . 开头的文件都是隐藏的文件 ls ls -a 显示所有文件或目录（包含隐藏的文件） ls -l 缩写成ll 4.3 创建目录和移除目录123456mkdir(make directory)命令可用来创建子目录。 mkdir app 在当前目录下创建app目录 mkdir –p app2&#x2F;test 级联创建aap2以及test目rmdir(remove directory)命令可用来删除“空”的子目录： rmdir app 删除app目录 4.4 浏览文件4.4.1 cat、more、less12345678910111213cat用于显示文件的内容,适合少量信息文件。格式：cat[参数]&lt;文件名&gt; cat yum.conf more一般用于要显示的内容会超过一个画面长度的情况。 more yum.conf 按空格键显示下一个画面。ctrl+f 按 b 键显示上一个画面。ctrl+b 回车显示下一行内容。 按 q 键退出查看。 less用法和more类似，不同的是less可以通过PgUp、PgDn键来控制。 less yum.conf PgUp 和 PgDn 进行上下翻页. 4.4.2 tail12345tail命令是在实际使用过程中使用非常多的一个命令，它的功能是：用于显示文件后几行的内容。 用法: tail -10 &#x2F;etc&#x2F;passwd 查看后10行数据 tail -f catalina.log 动态查看日志(*****) ctrl+c 结束查看 4.5 文件操作4.5.1 rm12345678rm 删除文件 用法：rm [选项]... 文件... rm a.txt 删除a.txt文件删除需要用户确认，y&#x2F;n rm -f a.txt 不询问，直接删除rm 删除目录 rm -r a 递归删除询问递归删除 rm -rf a 不询问递归删除 rm -rf * 删除所有文件 rm -rf &#x2F;* 自杀 4.5.2 cp、mv1234567cp(copy)命令可以将文件从一处复制到另一处。一般在使用cp命令时将一个文件复制成另一个文件或复制到某目录时，需要指定源文件名与目标文件名或目录。 cp a.txt b.txt 将a.txt复制为b.txt文件 cp a.txt ..&#x2F; 将a.txt文件复制到上一层目录中mv 移动或者重命名 mv a.txt ..&#x2F; 将a.txt文件移动到上一层目录中 mv a.txt b.txt 将a.txt文件重命名为b.txt 4.5.3 tar1234567891011121314151617tar命令位于&#x2F;bin目录下，它能够将用户所指定的文件或目录打包成一个文件，但不做压缩。一般Linux上常用的压缩方式是选用tar将许多文件打包成一个文件，再以gzip压缩命令压缩成xxx.tar.gz(或称为xxx.tgz)的文件。常用参数： -c：创建一个新tar文件 -v：显示运行过程的信息 -f：指定文件名 -z：调用gzip压缩命令进行压缩 -t：查看压缩文件的内容 -x：解开tar文件打包： tar -cvf xxx.tar .&#x2F;*打包并且压缩： tar -zcvf xxx.tar.gz .&#x2F;* 解压 tar -xvf xxx.tar tar -zxvf xxx.tar.gz -C &#x2F;usr&#x2F;aaa 4.5.4 find1234567find指令用于查找符合条件的文件示例： find &#x2F; -name “ins*” 查找文件名称是以ins开头的文件 find &#x2F; -name “ins*” -ls find &#x2F; -user wgy -ls 查找用户wgy的文件 find &#x2F; -user wgy -type d -ls 查找用户wgy的目录 find &#x2F; -perm 777 -type f -ls 查找权限是777的文件 4.5.5 grep123456查找文件里符合条件的字符串。用法: grep [选项]... PATTERN [FILE]...示例： grep lang anaconda-ks.cfg 在文件中查找lang grep lang anaconda-ks.cfg --color 高亮显示 grep lang anaconda-ks.cfg --color -A5 高亮显示后五行 grep lang anaconda-ks.cfg --color -A5 -B5 高亮显示前后五行 4.6 系统管理命令12345ps 正在运行的某个进程的状态 ps -ef 查看所有进程 ps -ef | grep ssh 查找某一进程 kill 2868 杀掉2868编号的进程 kill -9 2868 强制杀死进程 4.7 管道1234567管道是Linux命令中重要的一个概念，其作用是将一个命令的输出用作另一个命令的输入。示例: ls --help | more 分页查询帮助信息 ps -ef | grep java 查询名称中包含java的进程 ifconfig | more cat index.html | more ps -ef | grep aio 4.8 其他常用命令1234567891011pwd显示当前所在目录touch创建一个空文件 touch a.txt clear&#x2F;crtl+L清屏history 查看所敲命令历史echo echo要显示的内容 &gt;&gt; 存储内容的的文件 5. Vi和Vim编辑器5.1 Vim编辑器1234567891011121314151617181920212223在Linux下一般使用vi编辑器来编辑文件。vi既可以查看文件也可以编辑文件。三种模式：命令行、插入、底行模式。 切换到命令行模式：按Esc键； 切换到插入模式：按 i 、o、a键； i 在当前位置前插入 I 在当前行首插入 a 在当前位置后插入 A 在当前行尾插入 o 在当前行之后插入一行 O 在当前行之前插入一行 切换到底行模式：按 :（冒号） 打开文件：vim file 退出：esc -&gt; :q 修改文件：输入i进入插入模式 保存并退出：esc-&gt;:wq 不保存退出：esc-&gt;:q! 快捷键： dd - 快速删除一行 yy - 复制当前行 nyy - 从当前行向后复制几行 p - 粘贴 R - 替换 5.2 重定向输出&gt;和&gt;&gt;123456&gt; 重定向输出，覆盖原有内容；&gt;&gt; 重定向输出，又追加功能；示例： cat &#x2F;etc&#x2F;passwd &gt; a.txt 将输出定向到a.txt中 cat &#x2F;etc&#x2F;passwd &gt;&gt; a.txt 输出并且追加 ifconfig &gt; ifconfig.txt 6. Linux的权限命令6.1 文件权限 r：对文件是指可读取内容，对目录是可以ls w：对文件是指可修改文件内容，对目录是指可以在其中创建或删除子节点(目录或文件) x：对文件是指是否可以运行这个文件，对目录是指是否可以cd进入这个目录 6.2 Linux三种文件类型普通文件： 包括文本文件、数据文件、可执行的二进制程序文件等。 目录文件： Linux系统把目录看成是一种特殊的文件，利用它构成文件系统的树型结构。 设备文件： Linux系统把每一个设备都看成是一个文件 6.3 文件类型标识普通文件（-） 目录（d） 符号链接（l） 6.4 文件权限管理12345chmod 变更文件或目录的权限。 chmod 755 a.txt chmod u&#x3D;rwx,g&#x3D;rx,o&#x3D;rx a.txt chmod -R 777 &#x2F;home&#x2F;cookiex&#x2F;abc -R表示的是目录以及目录下面的所bai有内容都设置为相同的指定权限位 7. Linux上常用网络操作7.1 主机名配置123hostname 查看主机名hostname xxx 修改主机名 重启后无效如果想要永久生效，可以修改&#x2F;etc&#x2F;sysconfig&#x2F;network文件 7.2 IP地址配置123456789101112ifconfig 查看(修改)ip地址(重启后无效)ifconfig eth0 192.168.12.22 修改ip地址如果想要永久生效修改 &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0文件 DEVICE&#x3D;eth0 #网卡名称 BOOTPROTO&#x3D;static #获取ip的方式(static&#x2F;dhcp&#x2F;bootp&#x2F;none) HWADDR&#x3D;00:0C:29:B5:B2:69 #MAC地址 IPADDR&#x3D;12.168.177.129 #IP地址 NETMASK&#x3D;255.255.255.0 #子网掩码 NETWORK&#x3D;192.168.177.0 #网络地址 BROADCAST&#x3D;192.168.0.255 #广播地址 NBOOT&#x3D;yes # 系统启动时是否设置此网络接口，设置为yes时，系统启动时激活此设备。 7.3 域名映射/etc/hosts文件用于在通过主机名进行访问时做ip地址解析之用,相当于windows系统的C:\\Windows\\System32\\drivers\\etc\\hosts文件的功能 7.4 网络服务管理1234567891011121314service network status 查看指定服务的状态service network stop 停止指定服务service network start 启动指定服务service network restart 重启指定服务service --status -all 查看系统中所有后台服务netstat -nltp 查看系统中网络进程的端口监听情况防火墙设置防火墙根据配置文件&#x2F;etc&#x2F;sysconfig&#x2F;iptables来控制本机的”出”、”入”网络访问行为。service iptables status 查看防火墙状态service iptables stop 关闭防火墙service iptables start 启动防火墙chkconfig iptables off 禁止防火墙自启","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wgy1993.gitee.io/tags/Linux/"},{"name":"操作系统","slug":"操作系统","permalink":"https://wgy1993.gitee.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]}]